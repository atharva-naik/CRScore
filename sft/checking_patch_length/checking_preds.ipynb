{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>patch</th>\n",
       "      <th>patch_context</th>\n",
       "      <th>original_msg</th>\n",
       "      <th>predicted_msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@@ -231,4 +231,8 @@ def setup_app(app):\\n     ...</td>\n",
       "      <td>@app.template_test('list')\\n    def _is_li...</td>\n",
       "      <td>Should we call it `is_list`?</td>\n",
       "      <td>The suggested code change defines a custom tem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@@ -44,7 +44,7 @@ namespace OpenTelemetry.Trac...</td>\n",
       "      <td>builder.AddSource(new Source(SqlCl...</td>\n",
       "      <td>in the instrumentation example, should we use ...</td>\n",
       "      <td>The suggested code change adds a source to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@@ -25,7 +25,7 @@ from scapy.modules.six.moves...</td>\n",
       "      <td>class Field(object):</td>\n",
       "      <td>Why this change ? Is it useful ?</td>\n",
       "      <td>Sure, please provide the suggested code change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@@ -0,0 +1,4 @@\\n+const titleNode = virtualNod...</td>\n",
       "      <td>const titleNode = virtualNode.children.find(({...</td>\n",
       "      <td>I know this is a nitpick, but don't we always ...</td>\n",
       "      <td>The suggested code change is intended to find ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@@ -37,6 +37,11 @@ public class EMailValidator...</td>\n",
       "      <td>/*\\n        Add tests for 4601\\n      ...</td>\n",
       "      <td>We should reformat this emails in the test to ...</td>\n",
       "      <td>The suggested code change adds test cases for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                              patch  \\\n",
       "0      0  @@ -231,4 +231,8 @@ def setup_app(app):\\n     ...   \n",
       "1      1  @@ -44,7 +44,7 @@ namespace OpenTelemetry.Trac...   \n",
       "2      2  @@ -25,7 +25,7 @@ from scapy.modules.six.moves...   \n",
       "3      3  @@ -0,0 +1,4 @@\\n+const titleNode = virtualNod...   \n",
       "4      4  @@ -37,6 +37,11 @@ public class EMailValidator...   \n",
       "\n",
       "                                       patch_context  \\\n",
       "0      @app.template_test('list')\\n    def _is_li...   \n",
       "1              builder.AddSource(new Source(SqlCl...   \n",
       "2                               class Field(object):   \n",
       "3  const titleNode = virtualNode.children.find(({...   \n",
       "4          /*\\n        Add tests for 4601\\n      ...   \n",
       "\n",
       "                                        original_msg  \\\n",
       "0                       Should we call it `is_list`?   \n",
       "1  in the instrumentation example, should we use ...   \n",
       "2                   Why this change ? Is it useful ?   \n",
       "3  I know this is a nitpick, but don't we always ...   \n",
       "4  We should reformat this emails in the test to ...   \n",
       "\n",
       "                                       predicted_msg  \n",
       "0  The suggested code change defines a custom tem...  \n",
       "1  The suggested code change adds a source to the...  \n",
       "2  Sure, please provide the suggested code change...  \n",
       "3  The suggested code change is intended to find ...  \n",
       "4  The suggested code change adds test cases for ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df15 = pd.read_csv(\"/home/mkapadni/work/crscore_plus_plus/sft/checking_patch_length/n=0_pred_validdf.csv\")\n",
    "df15.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The suggested code change defines a custom template filter named `_is_list` that checks if a given value is a list. This can be useful in templates where you need to conditionally render content based on whether a variable is a list or not.\n",
      "\n",
      "**Review:**\n",
      "- **Functionality:** The function correctly uses `isinstance()` to determine if the input `value` is a list.\n",
      "- **Naming Convention:** The function name `_is_list` is a good choice as it clearly indicates its purpose.\n",
      "- **Docstring:** A docstring is included, which is helpful for understanding the function's behavior.\n",
      "- **Consistency:** The use of underscores in the function name and variable names is consistent with Python conventions.\n",
      "\n",
      "Overall, this change enhances the functionality of the Flask application by providing a reusable test for checking list types in templates.\n"
     ]
    }
   ],
   "source": [
    "print(df15['predicted_msg'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:42<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 100 reviews\n",
      "Review 1 score: 0.80\n",
      "Review 2 score: 0.80\n",
      "Review 3 score: 0.20\n",
      "Review 4 score: 0.80\n",
      "Review 5 score: 0.80\n",
      "Review 6 score: 0.40\n",
      "Review 7 score: 0.80\n",
      "Review 8 score: 0.80\n",
      "Review 9 score: 0.80\n",
      "Review 10 score: 0.80\n",
      "Review 11 score: 0.40\n",
      "Review 12 score: 0.80\n",
      "Review 13 score: 0.60\n",
      "Review 14 score: 0.80\n",
      "Review 15 score: 0.40\n",
      "Review 16 score: 0.80\n",
      "Review 17 score: 0.20\n",
      "Review 18 score: 0.80\n",
      "Review 19 score: 0.80\n",
      "Review 20 score: 0.80\n",
      "Review 21 score: 0.60\n",
      "Review 22 score: 0.20\n",
      "Review 23 score: 0.80\n",
      "Review 24 score: 0.60\n",
      "Review 25 score: 0.80\n",
      "Review 26 score: 0.80\n",
      "Review 27 score: 0.80\n",
      "Review 28 score: 1.00\n",
      "Review 29 score: 0.40\n",
      "Review 30 score: 0.80\n",
      "Review 31 score: 0.80\n",
      "Review 32 score: 0.80\n",
      "Review 33 score: 0.60\n",
      "Review 34 score: 0.80\n",
      "Review 35 score: 0.80\n",
      "Review 36 score: 0.60\n",
      "Review 37 score: 0.80\n",
      "Review 38 score: 0.80\n",
      "Review 39 score: 0.80\n",
      "Review 40 score: 0.80\n",
      "Review 41 score: 0.80\n",
      "Review 42 score: 0.60\n",
      "Review 43 score: 0.80\n",
      "Review 44 score: 0.60\n",
      "Review 45 score: 0.60\n",
      "Review 46 score: 0.80\n",
      "Review 47 score: 0.80\n",
      "Review 48 score: 0.80\n",
      "Review 49 score: 0.80\n",
      "Review 50 score: 0.80\n",
      "Review 51 score: 0.80\n",
      "Review 52 score: 0.60\n",
      "Review 53 score: 0.80\n",
      "Review 54 score: 0.20\n",
      "Review 55 score: 1.00\n",
      "Review 56 score: 0.20\n",
      "Review 57 score: 0.40\n",
      "Review 58 score: 0.40\n",
      "Review 59 score: 0.80\n",
      "Review 60 score: 0.60\n",
      "Review 61 score: 0.80\n",
      "Review 62 score: 0.60\n",
      "Review 63 score: 0.80\n",
      "Review 64 score: 0.80\n",
      "Review 65 score: 0.60\n",
      "Review 66 score: 0.80\n",
      "Review 67 score: 0.80\n",
      "Review 68 score: 0.80\n",
      "Review 69 score: 0.80\n",
      "Review 70 score: 0.80\n",
      "Review 71 score: 0.80\n",
      "Review 72 score: 0.80\n",
      "Review 73 score: 0.60\n",
      "Review 74 score: 0.80\n",
      "Review 75 score: 0.80\n",
      "Review 76 score: 0.80\n",
      "Review 77 score: 0.80\n",
      "Review 78 score: 0.60\n",
      "Review 79 score: 0.60\n",
      "Review 80 score: 0.80\n",
      "Review 81 score: 0.80\n",
      "Review 82 score: 0.80\n",
      "Review 83 score: 0.80\n",
      "Review 84 score: 0.80\n",
      "Review 85 score: 0.80\n",
      "Review 86 score: 1.00\n",
      "Review 87 score: 0.80\n",
      "Review 88 score: 0.80\n",
      "Review 89 score: 0.60\n",
      "Review 90 score: 0.80\n",
      "Review 91 score: 0.60\n",
      "Review 92 score: 0.60\n",
      "Review 93 score: 0.80\n",
      "Review 94 score: 0.80\n",
      "Review 95 score: 0.60\n",
      "Review 96 score: 0.80\n",
      "Review 97 score: 0.60\n",
      "Review 98 score: 0.80\n",
      "Review 99 score: 0.60\n",
      "Review 100 score: 0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from typing import List, Dict, Union, Any, Optional\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Choose one of these based on your needs\n",
    "from openai import OpenAI  # For GPT models\n",
    "# For Magicoder or other transformer models\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Constants from original code\n",
    "CODE_CHANGE_AND_REVIEW_SYSTEM_PROMPT = \"\"\"You are a highly skilled software engineer who has a lot of experience reviewing code changes. Your task is to rate the relevance of any given code change\"\"\"\n",
    "\n",
    "CODE_CHANGE_AND_REVIEW_JUDGE_PROMPT = \"\"\"You will be asked to rate the relevance of reviews for given Python, Java or Javascript code changes. A relevant review is one which is both concise and comprehensive. A concise review contains very little text not related to the code change. A comprehensive review contains all the information about a code change that should be covered by a review. A relevant review is comprehensive while being concise.\n",
    "\n",
    "Now look at the code change and review below and score the relevance of the review on a scale of 1 to 5\n",
    "\n",
    "Code Change:\n",
    "{code_change}\n",
    "\n",
    "Review:\n",
    "{review}\n",
    "\n",
    "Your score: \"\"\"\n",
    "\n",
    "LANG_MAP = {\n",
    "    \"py\": \"Python\",\n",
    "    \"js\": \"Javascript\",\n",
    "    \"java\": \"Java\",\n",
    "}\n",
    "\n",
    "class LLM_as_a_Judge:\n",
    "    def __init__(self, model: str, api_key: Optional[str]=None):\n",
    "        self.model = model\n",
    "        if model.startswith(\"gpt\"):\n",
    "            self.client = OpenAI(api_key=api_key)\n",
    "        else:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "            self.client = pipeline(\n",
    "                model=model,\n",
    "                task=\"text-generation\",\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "\n",
    "    def __call__(self, code_change: str, review: str):\n",
    "        if self.model.startswith(\"gpt\"):\n",
    "            inst_to_be_judged = CODE_CHANGE_AND_REVIEW_JUDGE_PROMPT.format(\n",
    "                # lang=LANG_MAP[lang], \n",
    "                code_change=code_change, \n",
    "                review=review\n",
    "            )\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": CODE_CHANGE_AND_REVIEW_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": inst_to_be_judged}\n",
    "            ]\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages\n",
    "            )\n",
    "            response = str(completion.choices[0].message.content).strip()\n",
    "        else:\n",
    "            inst_to_be_judged = CODE_CHANGE_AND_REVIEW_JUDGE_PROMPT.format(\n",
    "                # lang=LANG_MAP[lang], \n",
    "                code_change=code_change[:5000], \n",
    "                review=review\n",
    "            )\n",
    "            prompt = CODE_CHANGE_AND_REVIEW_SYSTEM_PROMPT + \"\\n\" + inst_to_be_judged\n",
    "            result = self.client(\n",
    "                prompt, max_new_tokens=5, \n",
    "                num_return_sequences=1, temperature=0.0\n",
    "            )\n",
    "            response = result[0][\"generated_text\"].replace(prompt,'').split(\"\\n\")[0].strip()\n",
    "            \n",
    "        # Parse the score from the response\n",
    "        if response.startswith(\"1\"): score = 1\n",
    "        elif response.startswith(\"2\"): score = 2\n",
    "        elif response.startswith(\"3\"): score = 3\n",
    "        elif response.startswith(\"4\"): score = 4\n",
    "        elif response.startswith(\"5\"): score = 5\n",
    "        else: score = 1\n",
    "\n",
    "        return score/5, inst_to_be_judged\n",
    "\n",
    "\n",
    "def score_code_reviews(\n",
    "    code_diffs: List[str], \n",
    "    code_reviews: List[str], \n",
    "    # language_tags: List[str], \n",
    "    indices: Optional[List[int]] = None,\n",
    "    system_names: Optional[List[str]] = None,\n",
    "    model: str = \"gpt-4o\",\n",
    "    api_key: Optional[str] = None,\n",
    "    output_file: Optional[str] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Evaluate the relevance of code reviews for given code differences.\n",
    "    \n",
    "    Args:\n",
    "        code_diffs (List[str]): List of code changes/diffs\n",
    "        code_reviews (List[str]): List of reviews to evaluate\n",
    "        language_tags (List[str]): List of language tags ('py', 'js', 'java')\n",
    "        indices (Optional[List[int]]): Optional indices for each sample\n",
    "        system_names (Optional[List[str]]): Optional name of the system that generated each review\n",
    "        model (str): Model to use as judge ('gpt-4o' or a HuggingFace model name)\n",
    "        api_key (Optional[str]): API key for OpenAI (required for GPT models)\n",
    "        output_file (Optional[str]): Path to save results, if specified\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of dictionaries containing evaluation results\n",
    "    \"\"\"\n",
    "    if len(code_diffs) != len(code_reviews):\n",
    "        raise ValueError(\"code_diffs, code_reviews must have the same length\")\n",
    "    \n",
    "    # Initialize the judge\n",
    "    judge = LLM_as_a_Judge(model=model, api_key=api_key)\n",
    "    \n",
    "    # Create indices and system names if not provided\n",
    "    if indices is None:\n",
    "        indices = list(range(len(code_diffs)))\n",
    "    if system_names is None:\n",
    "        system_names = [\"model\"] * len(code_diffs)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = []\n",
    "    \n",
    "    # Evaluate each review\n",
    "    for i, (code_diff, review, idx, system) in enumerate(\n",
    "        tqdm(zip(code_diffs, code_reviews, indices, system_names), \n",
    "             total=len(code_diffs))\n",
    "    ):\n",
    "        # Get score from judge\n",
    "        score, prompt = judge(code_change=code_diff, review=review)\n",
    "        \n",
    "        # Create result record\n",
    "        result = {\n",
    "            \"index\": idx,\n",
    "            \"system\": system,\n",
    "            # \"lang\": lang,\n",
    "            \"diff\": code_diff,\n",
    "            \"review\": review,\n",
    "            \"score\": score,\n",
    "            \"prompt\": prompt\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Write to file if specified\n",
    "        if output_file:\n",
    "            with open(output_file, \"a\") as f:\n",
    "                f.write(json.dumps(result) + \"\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    diffs = df15['patch']\n",
    "    # Example data\n",
    "    # diffs = [\n",
    "    #     \"def add(a, b):\\n    return a + b\\n\\n# Changed to\\ndef add(a, b):\\n    return a + b + 0  # Added unnecessary zero\",\n",
    "    #     \"function greet(name) {\\n    console.log('Hello ' + name);\\n}\\n\\n// Changed to\\nfunction greet(name) {\\n    console.log(`Hello ${name}`);\\n}\"\n",
    "    # ]\n",
    "    \n",
    "    # reviews = [\n",
    "    #     \"The function was modified to add zero to the result, which is unnecessary and might confuse readers.\",\n",
    "    #     \"Changed string concatenation to template literals, which is a more modern approach.\"\n",
    "    # ]\n",
    "\n",
    "    reviews = df15['predicted_msg']\n",
    "    \n",
    "    # languages = [\"py\", \"js\"]\n",
    "    \n",
    "    # Make sure to set your API key if using GPT models\n",
    "    # api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key = \"sk-proj-X3iU98JiVCRWgJgoW8MXT3BlbkFJv8cp8R3GpmTZm4M6e0lP\"\n",
    "    \n",
    "    # Score the reviews\n",
    "    results = score_code_reviews(\n",
    "        code_diffs=diffs,\n",
    "        code_reviews=reviews,\n",
    "        # language_tags=languages,\n",
    "        model=\"gpt-4o\",  # or \"ise-uiuc/Magicoder-S-DS-6.7B\"\n",
    "        api_key=api_key,\n",
    "        output_file=\"review_scores_N_0.jsonl\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Evaluated {len(results)} reviews\")\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"Review {i+1} score: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>system</th>\n",
       "      <th>diff</th>\n",
       "      <th>review</th>\n",
       "      <th>score</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>model</td>\n",
       "      <td>@@ -231,4 +231,8 @@ def setup_app(app):\\n     ...</td>\n",
       "      <td>The suggested code changes appear to be relate...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>You will be asked to rate the relevance of rev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>model</td>\n",
       "      <td>@@ -44,7 +44,7 @@ namespace OpenTelemetry.Trac...</td>\n",
       "      <td>The suggested code change adds SQL client inst...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>You will be asked to rate the relevance of rev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>model</td>\n",
       "      <td>@@ -25,7 +25,7 @@ from scapy.modules.six.moves...</td>\n",
       "      <td>The suggested code change introduces a new cla...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>You will be asked to rate the relevance of rev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>model</td>\n",
       "      <td>@@ -0,0 +1,4 @@\\n+const titleNode = virtualNod...</td>\n",
       "      <td>The suggested code change is intended to find ...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>You will be asked to rate the relevance of rev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>model</td>\n",
       "      <td>@@ -37,6 +37,11 @@ public class EMailValidator...</td>\n",
       "      <td>The provided test cases for `EMailValidator` a...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>You will be asked to rate the relevance of rev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index system                                               diff  \\\n",
       "0      0  model  @@ -231,4 +231,8 @@ def setup_app(app):\\n     ...   \n",
       "1      1  model  @@ -44,7 +44,7 @@ namespace OpenTelemetry.Trac...   \n",
       "2      2  model  @@ -25,7 +25,7 @@ from scapy.modules.six.moves...   \n",
       "3      3  model  @@ -0,0 +1,4 @@\\n+const titleNode = virtualNod...   \n",
       "4      4  model  @@ -37,6 +37,11 @@ public class EMailValidator...   \n",
       "\n",
       "                                              review  score  \\\n",
       "0  The suggested code changes appear to be relate...    0.4   \n",
       "1  The suggested code change adds SQL client inst...    0.4   \n",
       "2  The suggested code change introduces a new cla...    0.2   \n",
       "3  The suggested code change is intended to find ...    0.8   \n",
       "4  The provided test cases for `EMailValidator` a...    0.4   \n",
       "\n",
       "                                              prompt  \n",
       "0  You will be asked to rate the relevance of rev...  \n",
       "1  You will be asked to rate the relevance of rev...  \n",
       "2  You will be asked to rate the relevance of rev...  \n",
       "3  You will be asked to rate the relevance of rev...  \n",
       "4  You will be asked to rate the relevance of rev...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score15 = pd.read_json(\"/home/mkapadni/work/crscore_plus_plus/sft/checking_patch_length/review_scores_N_15.jsonl\", lines=True)\n",
    "score15.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "score0= pd.read_json(\"/home/mkapadni/work/crscore_plus_plus/sft/checking_patch_length/review_scores_N_0.jsonl\", lines=True)\n",
    "score5 = pd.read_json(\"/home/mkapadni/work/crscore_plus_plus/sft/checking_patch_length/review_scores_N_5.jsonl\", lines=True)\n",
    "score10 = pd.read_json(\"/home/mkapadni/work/crscore_plus_plus/sft/checking_patch_length/review_scores_N_10.jsonl\", lines=True)\n",
    "score50 = pd.read_json(\"/home/mkapadni/work/crscore_plus_plus/sft/checking_patch_length/review_scores_N_50.jsonl\", lines=True)\n",
    "score100 = pd.read_json(\"/home/mkapadni/work/crscore_plus_plus/sft/checking_patch_length/review_scores_N_100.jsonl\", lines=True)\n",
    "score200 = pd.read_json(\"/home/mkapadni/work/crscore_plus_plus/sft/checking_patch_length/review_scores_N_200.jsonl\", lines=True)\n",
    "score500 = pd.read_json(\"/home/mkapadni/work/crscore_plus_plus/sft/checking_patch_length/review_scores_N_500.jsonl\", lines=True)\n",
    "score1000 = pd.read_json(\"/home/mkapadni/work/crscore_plus_plus/sft/checking_patch_length/review_scores_N_1000.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for 0\n",
      "0.7079999999999997\n",
      "Average score for 5\n",
      "0.54\n",
      "Average score for 10\n",
      "0.51\n",
      "Average score for 15\n",
      "0.4720000000000001\n",
      "Average score for 50\n",
      "0.442\n",
      "Average score for 100\n",
      "0.426\n",
      "Average score for 200\n",
      "0.41400000000000015\n",
      "Average score for 500\n",
      "0.4020000000000001\n",
      "Average score for 1000\n",
      "0.40776699029126223\n"
     ]
    }
   ],
   "source": [
    "# print avg of scores for all teh datasets\n",
    "print(\"Average score for 0\")\n",
    "print(score0['score'].mean())\n",
    "print(\"Average score for 5\")\n",
    "print(score5['score'].mean())\n",
    "print(\"Average score for 10\")\n",
    "print(score10['score'].mean())\n",
    "print(\"Average score for 15\")\n",
    "print(score15['score'].mean())\n",
    "print(\"Average score for 50\")\n",
    "print(score50['score'].mean())\n",
    "print(\"Average score for 100\")\n",
    "print(score100['score'].mean())\n",
    "print(\"Average score for 200\")\n",
    "print(score200['score'].mean())\n",
    "print(\"Average score for 500\")\n",
    "print(score500['score'].mean())\n",
    "print(\"Average score for 1000\")\n",
    "print(score1000['score'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
