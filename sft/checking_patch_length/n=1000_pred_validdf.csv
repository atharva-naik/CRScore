index,patch,patch_context,original_msg,predicted_msg
0,"@@ -231,4 +231,8 @@ def setup_app(app):
         )
         return rv
 
+    @app.template_test('list')
+    def _is_list(value):
+        return isinstance(value, list)
+
     return app","# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2012, 2013, 2014, 2015 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""Additional extensions and filters for jinja2 module.""""""

import re

from flask import _request_ctx_stack, g, request, url_for

from flask_login import current_user

from jinja2 import ChoiceLoader

from six import iteritems, string_types

from werkzeug.routing import BuildError

from .bccache import BytecodeCacheWithConfig
from .context_processor import setup_app as context_processor_setup_app
from .loader import OrderAwareDispatchingJinjaLoader

ENV_PREFIX = '_collected_'


def render_template_to_string(input, _from_string=False, **context):
    """"""Render a template from the template folder with the given context.

    Code based on
    `<https://github.com/mitsuhiko/flask/blob/master/flask/templating.py>`_

    :param input: the string template, or name of the template to be
                  rendered, or an iterable with template names
                  the first one existing will be rendered
    :param context: the variables that should be available in the
                    context of the template.
    :return: a string

    """"""
    ctx = _request_ctx_stack.top
    ctx.app.update_template_context(context)
    if _from_string:
        template = ctx.app.jinja_env.from_string(input)
    else:
        template = ctx.app.jinja_env.get_or_select_template(input)
    return template.render(context)


def inject_utils():
    """"""Inject variables and functions to jinja execution context.

    In particular it will add:

    - ``url_for``: an Invenio specific wrapper of Flask url_for, that will let
      you obtain URLs for non Flask-native handlers (i.e. not yet ported
      Invenio URLs)
    - ``_``: this can be used to automatically translate a given string.
    - ``is_language_rtl``: True if the chosen language should be read right to
      left.
    """"""
    from invenio.base.i18n import is_language_rtl
    from invenio.modules.records.api import get_record
    from invenio.utils.url import create_url, get_canonical_and_alternates_urls

    def invenio_url_for(endpoint, **values):
        try:
            return url_for(endpoint, **values)
        except BuildError:
            if re.match(""https?://"", endpoint, re.IGNORECASE):
                return endpoint
            if endpoint.startswith('.'):
                endpoint = request.blueprint + endpoint
            url = create_url('/' + endpoint.replace('.', '/'), values, False)
            return url.decode('utf-8')

    user = current_user._get_current_object()
    canonical_url, alternate_urls = get_canonical_and_alternates_urls(
        request.path)
    alternate_urls = dict((ln.replace('_', '-'), alternate_url)
                          for ln, alternate_url in iteritems(alternate_urls))
    return dict(
        current_user=user,
        is_language_rtl=is_language_rtl,
        canonical_url=canonical_url,
        alternate_urls=alternate_urls,
        get_record=get_record,
        url_for=invenio_url_for,
    )


def setup_app(app):
    """"""
    Extend application template filters with custom filters and fixes.

    List of applied filters:

    - filesizeformat
    - path_join
    - quoted_txt2html
    - invenio_format_date
    - invenio_pretty_date
    - invenio_url_args
    """"""
    import os
    from datetime import datetime
    from invenio.utils.date import convert_datetext_to_dategui, \
        convert_datestruct_to_dategui, pretty_date

    from . import config
    app.config.from_object(config)

    context_processor_setup_app(app)
    app.context_processor(inject_utils)

    if app.config.get('JINJA2_BCCACHE', False):
        app.jinja_options = dict(
            app.jinja_options,
            auto_reload=app.config.get('JINJA2_BCCACHE_AUTO_RELOAD', False),
            cache_size=app.config.get('JINJA2_BCCACHE_SIZE', -1),
            bytecode_cache=BytecodeCacheWithConfig(app))

    # Let's customize the template loader to look into packages
    # and application templates folders.
    jinja_loader = ChoiceLoader([
        OrderAwareDispatchingJinjaLoader(app),
        app.jinja_loader,
    ])
    app.jinja_loader = jinja_loader

    for ext in app.config.get('JINJA2_EXTENSIONS', []):
        try:
            app.jinja_env.add_extension(ext)
        except Exception:
            app.logger.exception(
                'Problem with loading extension: ""{0}""'.format(ext))

    def test_not_empty(v):
        return v is not None and v != ''

    @app.template_filter('u')
    def tounicode(value):
        if isinstance(value, str):
            return value.decode('utf8')
        return value

    @app.template_filter('s')
    def tostr(value):
        if not isinstance(value, str):
            if isinstance(value, unicode):
                value = value.encode('utf8')
            value = str(value)
        return value

    @app.template_filter('prefix')
    def _prefix(value, prefix=''):
        return prefix + value if test_not_empty(value) else ''

    @app.template_filter('suffix')
    def _suffix(value, suffix=''):
        return value + suffix if test_not_empty(value) else ''

    @app.template_filter('wrap')
    def _wrap(value, prefix='', suffix=''):
        return prefix + value + suffix if test_not_empty(value) else ''

    @app.template_filter('sentences')
    def _sentences(value, limit, separator='. '):
        """"""Return first `limit` number of sentences ending by `separator`.""""""
        return separator.join(value.split(separator)[:limit])

    @app.template_filter('path_join')
    def _os_path_join(d):
        """"""Shortcut for `os.path.join`.""""""
        return os.path.join(*d)

    @app.template_filter('quoted_txt2html')
    def _quoted_txt2html(*args, **kwargs):
        from invenio.utils.mail import email_quoted_txt2html
        return email_quoted_txt2html(*args, **kwargs)

    @app.template_filter('invenio_format_date')
    def _format_date(date):
        """"""
        Format a date into a human friendly format.

        It uses :py:func:`invenio.utils.date.convert_datetext_to_dategui`
        """"""
        if isinstance(date, datetime):
            return convert_datestruct_to_dategui(
                date.timetuple(),
                getattr(g, 'ln', app.config['CFG_SITE_LANG'])).decode('utf-8')
        return convert_datetext_to_dategui(
            date, getattr(g, 'ln', app.config['CFG_SITE_LANG'])
        ).decode('utf-8')

    @app.template_filter('invenio_pretty_date')
    def _pretty_date(date):
        """"""
        Format a timestamp into a human friendly format.

        It uses :py:func:`invenio.utils.date.pretty_date`
        """"""
        if isinstance(date, datetime) or isinstance(date, string_types):
            return pretty_date(
                date, ln=getattr(g, 'ln', app.config['CFG_SITE_LANG']))
        return date

    @app.template_filter('invenio_url_args')
    def _url_args(d, append=u'?', filter=[]):
        from jinja2.utils import escape
        rv = append + u'&'.join(
            u'%s=%s' % (escape(key), escape(value))
            for key, value in d.iteritems(True)
            if value is not None and key not in filter
            # and not isinstance(value, Undefined)
    @app.template_test('list')
    def _is_list(value):
        return isinstance(value, list)

        )
        return rv

    return app
",Should we call it `is_list`?,"The suggested code change adds several new functions and filters to the Jinja2 environment for an Invenio application. Here's a concise review:

### New Functions and Filters

1. **`render_template_to_string`**: 
   - Renders a template from the template folder with the given context.
   - Based on Flask's `render_template`, but allows for rendering from strings or template names.

2. **`inject_utils`**:
   - Injects utility functions and variables into the Jinja2 context.
   - Includes `url_for`, `_`, `is_language_rtl`, `canonical_url`, `alternate_urls`, `get_record`, and `invenio_url_for`.

3. **`setup_app`**:
   - Sets up additional Jinja2 filters and extensions.
   - Adds filters like `filesizeformat`, `path_join`, `quoted_txt2html`, `invenio_format_date`, `invenio_pretty_date`, and `invenio_url_args`.
   - Customizes the template loader to include package and application templates.

### Changes and Improvements

1. **Template Loader Customization**:
   - The template loader is extended to include both package and application templates, allowing for a more comprehensive search for templates.

2. **Filter Enhancements**:
   - Added filters like `list` to check if a value is a list.
   - Improved the `invenio_url_args` filter to handle filtering out undefined values.

3. **Utility Functionality**:
   - Introduced `invenio_url_for` for handling non-Flask-native handlers.
   - Enhanced `quoted_txt2html` for email quoting.

4. **Date Formatting**:
   - Added `invenio_format_date` and `invenio_pretty_date` for formatting dates and timestamps.

### Overall Impact

This change significantly enhances the functionality of the Jinja2 environment in an Invenio application, providing additional tools for template rendering and utility functions. The customization of the template loader and the addition of new filters make the application more flexible and powerful."
1,"@@ -44,7 +44,7 @@ namespace OpenTelemetry.Trace
             configureSqlClientInstrumentationOptions?.Invoke(sqlOptions);
 
             builder.AddInstrumentation(() => new SqlClientInstrumentation(sqlOptions));
-            builder.AddSource(SqlClientDiagnosticListener.ActivitySourceName);
+            builder.AddSource(new Source(SqlClientDiagnosticListener.ActivitySourceName));
 
             return builder;
         }","// <copyright file=""TracerProviderBuilderExtensions.cs"" company=""OpenTelemetry Authors"">
// Copyright The OpenTelemetry Authors
//
// Licensed under the Apache License, Version 2.0 (the ""License"");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an ""AS IS"" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
// </copyright>

using System;
using OpenTelemetry.Instrumentation.SqlClient;
using OpenTelemetry.Instrumentation.SqlClient.Implementation;

namespace OpenTelemetry.Trace
{
    /// <summary>
    /// Extension methods to simplify registering of dependency instrumentation.
    /// </summary>
    public static class TracerProviderBuilderExtensions
    {
        /// <summary>
        /// Enables SqlClient instrumentation.
        /// </summary>
        /// <param name=""builder""><see cref=""TracerProviderBuilder""/> being configured.</param>
        /// <param name=""configureSqlClientInstrumentationOptions"">SqlClient configuration options.</param>
        /// <returns>The instance of <see cref=""TracerProviderBuilder""/> to chain the calls.</returns>
        public static TracerProviderBuilder AddSqlClientInstrumentation(
            this TracerProviderBuilder builder,
            Action<SqlClientInstrumentationOptions> configureSqlClientInstrumentationOptions = null)
        {
            if (builder == null)
            {
                throw new ArgumentNullException(nameof(builder));
            }

            var sqlOptions = new SqlClientInstrumentationOptions();
            builder.AddSource(new Source(SqlClientDiagnosticListener.ActivitySourceName));
            configureSqlClientInstrumentationOptions?.Invoke(sqlOptions);

            builder.AddInstrumentation(() => new SqlClientInstrumentation(sqlOptions));
            builder.AddSource(SqlClientDiagnosticListener.ActivitySourceName);

            return builder;
        }
    }
}
","in the instrumentation example, should we use minversion and maxversion = the assembly version or should ignore our assembly version?","The suggested code change adds extension methods to simplify the registration of SQL Client instrumentation using OpenTelemetry. Here's a concise review:

1. **Namespace and Class Definition**: The code defines a namespace `OpenTelemetry.Trace` and a class `TracerProviderBuilderExtensions`.

2. **Method Signature**: The method `AddSqlClientInstrumentation` takes a `TracerProviderBuilder` and an optional `Action<SqlClientInstrumentationOptions>` delegate.

3. **Null Check**: It checks if the `builder` is null and throws an `ArgumentNullException` if it is.

4. **Default Options**: It initializes a `SqlClientInstrumentationOptions` object and sets up the activity source for SQL Client instrumentation.

5. **Configuration**: The `configureSqlClientInstrumentationOptions` delegate is used to configure the options, allowing for customization.

6. **Instrumentation Registration**: It adds the SQL Client instrumentation to the builder and ensures the activity source is registered again.

7. **Return Value**: The method returns the `TracerProviderBuilder` instance to allow chaining of method calls.

Overall, the code provides a clean and efficient way to integrate SQL Client instrumentation into an OpenTelemetry tracing setup."
2,"@@ -25,7 +25,7 @@ from scapy.modules.six.moves import range
 ## Fields ##
 ############
 
-class Field(six.with_metaclass(Field_metaclass, object)):
+class Field(object):
     """"""For more informations on how this work, please refer to
        http://www.secdev.org/projects/scapy/files/scapydoc.pdf
        chapter ``Adding a New Field''""""""","## This file is part of Scapy
## See http://www.secdev.org/projects/scapy for more informations
## Copyright (C) Philippe Biondi <phil@secdev.org>
## This program is published under a GPLv2 license

""""""
Fields: basic data structures that make up parts of packets.
""""""

from __future__ import absolute_import
import struct,copy,socket,collections
from scapy.config import conf
from scapy.dadict import DADict
from scapy.volatile import *
from scapy.data import *
from scapy.compat import *
from scapy.utils import *
from scapy.base_classes import BasePacket, Gen, Net, Field_metaclass
from scapy.error import warning
import scapy.modules.six as six
from scapy.modules.six.moves import range


############
class Field(object):
## Fields ##
############

class Field(six.with_metaclass(Field_metaclass, object)):
    """"""For more informations on how this work, please refer to
       http://www.secdev.org/projects/scapy/files/scapydoc.pdf
       chapter ``Adding a New Field''""""""
    __slots__ = [""name"", ""fmt"", ""default"", ""sz"", ""owners""]
    islist = 0
    ismutable = False
    holds_packets = 0
    def __init__(self, name, default, fmt=""H""):
        self.name = name
        if fmt[0] in ""@=<>!"":
            self.fmt = fmt
        else:
            self.fmt = ""!""+fmt
        self.default = self.any2i(None,default)
        self.sz = struct.calcsize(self.fmt)
        self.owners = []

    def register_owner(self, cls):
        self.owners.append(cls)

    def i2len(self, pkt, x):
        """"""Convert internal value to a length usable by a FieldLenField""""""
        return self.sz
    def i2count(self, pkt, x):
        """"""Convert internal value to a number of elements usable by a FieldLenField.
        Always 1 except for list fields""""""
        return 1
    def h2i(self, pkt, x):
        """"""Convert human value to internal value""""""
        return x
    def i2h(self, pkt, x):
        """"""Convert internal value to human value""""""
        return x
    def m2i(self, pkt, x):
        """"""Convert machine value to internal value""""""
        return x
    def i2m(self, pkt, x):
        """"""Convert internal value to machine value""""""
        if x is None:
            x = 0
        elif isinstance(x, str):
            return raw(x)
        return x
    def any2i(self, pkt, x):
        """"""Try to understand the most input values possible and make an internal value from them""""""
        return self.h2i(pkt, x)
    def i2repr(self, pkt, x):
        """"""Convert internal value to a nice representation""""""
        return repr(self.i2h(pkt,x))
    def addfield(self, pkt, s, val):
        """"""Add an internal value  to a string""""""
        return s+struct.pack(self.fmt, self.i2m(pkt,val))
    def getfield(self, pkt, s):
        """"""Extract an internal value from a string""""""
        return  s[self.sz:], self.m2i(pkt, struct.unpack(self.fmt, s[:self.sz])[0])
    def do_copy(self, x):
        if hasattr(x, ""copy""):
            return x.copy()
        if isinstance(x, list):
            x = x[:]
            for i in range(len(x)):
                if isinstance(x[i], BasePacket):
                    x[i] = x[i].copy()
        return x
    def __repr__(self):
        return ""<Field (%s).%s>"" % ("","".join(x.__name__ for x in self.owners),self.name)
    def copy(self):
        return copy.deepcopy(self)
    def randval(self):
        """"""Return a volatile object whose value is both random and suitable for this field""""""
        fmtt = self.fmt[-1]
        if fmtt in ""BHIQ"":
            return {""B"":RandByte,""H"":RandShort,""I"":RandInt, ""Q"":RandLong}[fmtt]()
        elif fmtt == ""s"":
            if self.fmt[0] in ""0123456789"":
                l = int(self.fmt[:-1])
            else:
                l = int(self.fmt[1:-1])
            return RandBin(l)
        else:
            warning(""no random class for [%s] (fmt=%s)."", self.name, self.fmt)




class Emph(object):
    __slots__ = [""fld""]
    def __init__(self, fld):
        self.fld = fld
    def __getattr__(self, attr):
        return getattr(self.fld,attr)
    def __hash__(self):
        return hash(self.fld)
    def __eq__(self, other):
        return self.fld == other


class ActionField(object):
    __slots__ = [""_fld"", ""_action_method"", ""_privdata""]
    def __init__(self, fld, action_method, **kargs):
        self._fld = fld
        self._action_method = action_method
        self._privdata = kargs
    def any2i(self, pkt, val):
        getattr(pkt, self._action_method)(val, self._fld, **self._privdata)
        return getattr(self._fld, ""any2i"")(pkt, val)
    def __getattr__(self, attr):
        return getattr(self._fld,attr)


class ConditionalField(object):
    __slots__ = [""fld"", ""cond""]
    def __init__(self, fld, cond):
        self.fld = fld
        self.cond = cond
    def _evalcond(self,pkt):
        return self.cond(pkt)

    def getfield(self, pkt, s):
        if self._evalcond(pkt):
            return self.fld.getfield(pkt,s)
        else:
            return s,None

    def addfield(self, pkt, s, val):
        if self._evalcond(pkt):
            return self.fld.addfield(pkt,s,val)
        else:
            return s
    def __getattr__(self, attr):
        return getattr(self.fld,attr)


class PadField(object):
    """"""Add bytes after the proxified field so that it ends at the specified
       alignment from its beginning""""""
    __slots__ = [""_fld"", ""_align"", ""_padwith""]
    def __init__(self, fld, align, padwith=None):
        self._fld = fld
        self._align = align
        self._padwith = padwith or b""""

    def padlen(self, flen):
        return -flen%self._align

    def getfield(self, pkt, s):
        remain,val = self._fld.getfield(pkt,s)
        padlen = self.padlen(len(s)-len(remain))
        return remain[padlen:], val

    def addfield(self, pkt, s, val):
        sval = self._fld.addfield(pkt, b"""", val)
        return s+sval+struct.pack(""%is"" % (self.padlen(len(sval))), self._padwith)

    def __getattr__(self, attr):
        return getattr(self._fld,attr)


class DestField(Field):
    __slots__ = [""defaultdst""]
    # Each subclass must have its own bindings attribute
    # bindings = {}
    def __init__(self, name, default):
        self.defaultdst = default
    def dst_from_pkt(self, pkt):
        for addr, condition in self.bindings.get(pkt.payload.__class__, []):
            try:
                if all(pkt.payload.getfieldval(field) == value
                       for field, value in six.iteritems(condition)):
                    return addr
            except AttributeError:
                pass
        return self.defaultdst
    @classmethod
    def bind_addr(cls, layer, addr, **condition):
        cls.bindings.setdefault(layer, []).append((addr, condition))


class MACField(Field):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""6s"")
    def i2m(self, pkt, x):
        if x is None:
            return b""\0\0\0\0\0\0""
        return mac2str(x)
    def m2i(self, pkt, x):
        return str2mac(x)
    def any2i(self, pkt, x):
        if isinstance(x, bytes) and len(x) == 6:
            x = self.m2i(pkt, x)
        return x
    def i2repr(self, pkt, x):
        x = self.i2h(pkt, x)
        if self in conf.resolve:
            x = conf.manufdb._resolve_MAC(x)
        return x
    def randval(self):
        return RandMAC()


class IPField(Field):
    slots = []
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""4s"")
    def h2i(self, pkt, x):
        if isinstance(x, bytes):
            x = plain_str(x)
        if isinstance(x, str):
            try:
                inet_aton(x)
            except socket.error:
                x = Net(x)
        elif isinstance(x, list):
            x = [self.h2i(pkt, n) for n in x]
        return x
    def resolve(self, x):
        if self in conf.resolve:
            try:
                ret = socket.gethostbyaddr(x)[0]
            except:
                pass
            else:
                if ret:
                    return ret
        return x
    def i2m(self, pkt, x):
        return inet_aton(x)
    def m2i(self, pkt, x):
        return inet_ntoa(x)
    def any2i(self, pkt, x):
        return self.h2i(pkt,x)
    def i2repr(self, pkt, x):
        r = self.resolve(self.i2h(pkt, x))
        return r if isinstance(r, str) else repr(r)
    def randval(self):
        return RandIP()

class SourceIPField(IPField):
    __slots__ = [""dstname""]
    def __init__(self, name, dstname):
        IPField.__init__(self, name, None)
        self.dstname = dstname
    def __findaddr(self, pkt):
        if conf.route is None:
            # unused import, only to initialize conf.route
            import scapy.route
        dst = (""0.0.0.0"" if self.dstname is None
               else getattr(pkt, self.dstname))
        if isinstance(dst, (Gen, list)):
            r = {conf.route.route(str(daddr)) for daddr in dst}
            if len(r) > 1:
                warning(""More than one possible route for %r"" % (dst,))
            return min(r)[1]
        return conf.route.route(dst)[1]
    def i2m(self, pkt, x):
        if x is None:
            x = self.__findaddr(pkt)
        return IPField.i2m(self, pkt, x)
    def i2h(self, pkt, x):
        if x is None:
            x = self.__findaddr(pkt)
        return IPField.i2h(self, pkt, x)




class ByteField(Field):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""B"")

class XByteField(ByteField):
    def i2repr(self, pkt, x):
        return lhex(self.i2h(pkt, x))

class OByteField(ByteField):
    def i2repr(self, pkt, x):
        return ""%03o""%self.i2h(pkt, x)

class X3BytesField(XByteField):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""!I"")
    def addfield(self, pkt, s, val):
        return s+struct.pack(self.fmt, self.i2m(pkt,val))[1:4]
    def getfield(self, pkt, s):
        return  s[3:], self.m2i(pkt, struct.unpack(self.fmt, b""\x00""+s[:3])[0])

class ThreeBytesField(X3BytesField, ByteField):
    def i2repr(self, pkt, x):
        return ByteField.i2repr(self, pkt, x)

class SignedByteField(Field):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""b"")


class FieldValueRangeException(Scapy_Exception):
    pass


class FieldAttributeException(Scapy_Exception):
    pass


class YesNoByteField(ByteField):
    """"""
    byte based flag field that shows representation of its number based on a given association

    in its default configuration the following representation is generated:
        x == 0 : 'no'
        x != 0 : 'yes'

    in more sophisticated use-cases (e.g. yes/no/invalid) one can use the config attribute to configure
    key-value, key-range and key-value-set associations that will be used to generate the values representation.

    a range is given by a tuple (<first-val>, <last-value>) including the last value. a single-value tuple
    is treated as scalar.

    a list defines a set of (probably non consecutive) values that should be associated to a given key.

    all values not associated with a key will be shown as number of type unsigned byte.

    config = {
                'no' : 0,
                'foo' : (1,22),
                'yes' : 23,
                'bar' : [24,25, 42, 48, 87, 253]
             }

    generates the following representations:

        x == 0 : 'no'
        x == 15: 'foo'
        x == 23: 'yes'
        x == 42: 'bar'
        x == 43: 43

    using the config attribute one could also revert the stock-yes-no-behavior:

    config = {
                'yes' : 0,
                'no' : (1,255)
             }

    will generate the following value representation:

        x == 0 : 'yes'
        x != 0 : 'no'

    """"""
    __slots__ = ['eval_fn']

    def _build_config_representation(self, config):
        assoc_table = dict()
        for key in config:
            value_spec = config[key]

            value_spec_type = type(value_spec)

            if value_spec_type is int:
                if value_spec < 0 or value_spec > 255:
                    raise FieldValueRangeException('given field value {} invalid - '
                                                   'must be in range [0..255]'.format(value_spec))
                assoc_table[value_spec] = key

            elif value_spec_type is list:
                for value in value_spec:
                    if value < 0 or value > 255:
                        raise FieldValueRangeException('given field value {} invalid - '
                                                       'must be in range [0..255]'.format(value))
                    assoc_table[value] = key

            elif value_spec_type is tuple:
                value_spec_len = len(value_spec)
                if value_spec_len != 2:
                    raise FieldAttributeException('invalid length {} of given config item tuple {} - must be '
                                                  '(<start-range>, <end-range>).'.format(value_spec_len, value_spec))

                value_range_start = value_spec[0]
                if value_range_start < 0 or value_range_start > 255:
                    raise FieldValueRangeException('given field value {} invalid - '
                                                   'must be in range [0..255]'.format(value_range_start))

                value_range_end = value_spec[1]
                if value_range_end < 0 or value_range_end > 255:
                    raise FieldValueRangeException('given field value {} invalid - '
                                                   'must be in range [0..255]'.format(value_range_end))

                for value in range(value_range_start, value_range_end + 1):

                    assoc_table[value] = key

        self.eval_fn = lambda x: assoc_table[x] if x in assoc_table else x

    def __init__(self, name, default, config=None, *args, **kargs):

        if not config:
            # this represents the common use case and therefore it is kept small
            self.eval_fn = lambda x: 'no' if x == 0 else 'yes'
        else:
            self._build_config_representation(config)
        ByteField.__init__(self, name, default, *args, **kargs)

    def i2repr(self, pkt, x):
        return self.eval_fn(x)


class ShortField(Field):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""H"")

class SignedShortField(Field):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""h"")

class LEShortField(Field):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""<H"")

class XShortField(ShortField):
    def i2repr(self, pkt, x):
        return lhex(self.i2h(pkt, x))


class IntField(Field):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""I"")

class SignedIntField(Field):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""i"")
    def randval(self):
        return RandSInt()

class LEIntField(Field):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""<I"")

class LESignedIntField(Field):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""<i"")
    def randval(self):
        return RandSInt()

class XIntField(IntField):
    def i2repr(self, pkt, x):
        return lhex(self.i2h(pkt, x))


class LongField(Field):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""Q"")

class LELongField(LongField):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""<Q"")

class XLongField(LongField):
    def i2repr(self, pkt, x):
        return lhex(self.i2h(pkt, x))

class IEEEFloatField(Field):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""f"")

class IEEEDoubleField(Field):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""d"")


class StrField(Field):
    __slots__ = [""remain""]
    def __init__(self, name, default, fmt=""H"", remain=0):
        Field.__init__(self,name,default,fmt)
        self.remain = remain
    def i2len(self, pkt, i):
        return len(i)
    def any2i(self, pkt, x):
        if isinstance(x, str if six.PY3 else unicode):
            x = raw(x)
        return super(StrField, self).any2i(pkt, x)
    def i2repr(self, pkt, x):
        val = super(StrField, self).i2repr(pkt, x)
        if val[:2] in ['b""', ""b'""]:
            return val[1:]
        return val
    def i2m(self, pkt, x):
        if x is None:
            return b""""
        if not isinstance(x, bytes):
            return raw(x)
        return x
    def addfield(self, pkt, s, val):
        return s + self.i2m(pkt, val)
    def getfield(self, pkt, s):
        if self.remain == 0:
            return b"""", self.m2i(pkt, s)
        else:
            return s[-self.remain:],self.m2i(pkt, s[:-self.remain])
    def randval(self):
        return RandBin(RandNum(0,1200))

class PacketField(StrField):
    __slots__ = [""cls""]
    holds_packets = 1
    def __init__(self, name, default, cls, remain=0):
        StrField.__init__(self, name, default, remain=remain)
        self.cls = cls
    def i2m(self, pkt, i):
        if i is None:
            return b""""
        return raw(i)
    def m2i(self, pkt, m):
        return self.cls(m)
    def getfield(self, pkt, s):
        i = self.m2i(pkt, s)
        remain = b""""
        if conf.padding_layer in i:
            r = i[conf.padding_layer]
            del(r.underlayer.payload)
            remain = r.load
        return remain,i

class PacketLenField(PacketField):
    __slots__ = [""length_from""]
    def __init__(self, name, default, cls, length_from=None):
        PacketField.__init__(self, name, default, cls)
        self.length_from = length_from
    def getfield(self, pkt, s):
        l = self.length_from(pkt)
        try:
            i = self.m2i(pkt, s[:l])
        except Exception:
            if conf.debug_dissector:
                raise
            i = conf.raw_layer(load=s[:l])
        return s[l:],i


class PacketListField(PacketField):
    """""" PacketListField represents a series of Packet instances that might occur right in the middle of another Packet
    field list.
    This field type may also be used to indicate that a series of Packet instances have a sibling semantic instead of
    a parent/child relationship (i.e. a stack of layers).
    """"""
    __slots__ = [""count_from"", ""length_from"", ""next_cls_cb""]
    islist = 1
    def __init__(self, name, default, cls=None, count_from=None, length_from=None, next_cls_cb=None):
        """""" The number of Packet instances that are dissected by this field can be parametrized using one of three
        different mechanisms/parameters:
            * count_from: a callback that returns the number of Packet instances to dissect. The callback prototype is:
            count_from(pkt:Packet) -> int
            * length_from: a callback that returns the number of bytes that must be dissected by this field. The
            callback prototype is:
            length_from(pkt:Packet) -> int
            * next_cls_cb: a callback that enables a Scapy developer to dynamically discover if another Packet instance
            should be dissected or not. See below for this callback prototype.

        The bytes that are not consumed during the dissection of this field are passed to the next field of the current
        packet.

        For the serialization of such a field, the list of Packets that are contained in a PacketListField can be
        heterogeneous and is unrestricted.

        The type of the Packet instances that are dissected with this field is specified or discovered using one of the
        following mechanism:
            * the cls parameter may contain a callable that returns an instance of the dissected Packet. This
                may either be a reference of a Packet subclass (e.g. DNSRROPT in layers/dns.py) to generate an
                homogeneous PacketListField or a function deciding the type of the Packet instance
                (e.g. _CDPGuessAddrRecord in contrib/cdp.py)
            * the cls parameter may contain a class object with a defined ""dispatch_hook"" classmethod. That
                method must return a Packet instance. The dispatch_hook callmethod must implement the following prototype:
                dispatch_hook(cls, _pkt:Optional[Packet], *args, **kargs) -> Packet_metaclass
                The _pkt parameter may contain a reference to the packet instance containing the PacketListField that is
                being dissected.
            * the next_cls_cb parameter may contain a callable whose prototype is:
                cbk(pkt:Packet, lst:List[Packet], cur:Optional[Packet], remain:str) -> Optional[Packet_metaclass]
                The pkt argument contains a reference to the Packet instance containing the PacketListField that is
                being dissected. The lst argument is the list of all Packet instances that were previously parsed during
                the current PacketListField dissection, save for the very last Packet instance. The cur argument
                contains a reference to that very last parsed Packet instance. The remain argument contains the bytes
                that may still be consumed by the current PacketListField dissection operation. This callback returns
                either the type of the next Packet to dissect or None to indicate that no more Packet are to be
                dissected.
                These four arguments allows a variety of dynamic discovery of the number of Packet to dissect and of the
                type of each one of these Packets, including: type determination based on current Packet instances or
                its underlayers, continuation based on the previously parsed Packet instances within that
                PacketListField, continuation based on a look-ahead on the bytes to be dissected...

        The cls and next_cls_cb parameters are semantically exclusive, although one could specify both. If both are
        specified, cls is silently ignored. The same is true for count_from and next_cls_cb.
        length_from and next_cls_cb are compatible and the dissection will end, whichever of the two stop conditions
        comes first.

        @param name: the name of the field
        @param default: the default value of this field; generally an empty Python list
        @param cls: either a callable returning a Packet instance or a class object defining a dispatch_hook class
            method
        @param count_from: a callback returning the number of Packet instances to dissect
        @param length_from: a callback returning the number of bytes to dissect
        @param next_cls_cb: a callback returning either None or the type of the next Packet to dissect.
        """"""
        if default is None:
            default = []  # Create a new list for each instance
        PacketField.__init__(self, name, default, cls)
        self.count_from = count_from
        self.length_from = length_from
        self.next_cls_cb = next_cls_cb

    def any2i(self, pkt, x):
        if not isinstance(x, list):
            return [x]
        else:
            return x
    def i2count(self, pkt, val):
        if isinstance(val, list):
            return len(val)
        return 1
    def i2len(self, pkt, val):
        return sum( len(p) for p in val )
    def do_copy(self, x):
        if x is None:
            return None
        else:
            return [p if isinstance(p, six.string_types) else p.copy() for p in x]
    def getfield(self, pkt, s):
        c = l = cls = None
        if self.length_from is not None:
            l = self.length_from(pkt)
        elif self.count_from is not None:
            c = self.count_from(pkt)
        if self.next_cls_cb is not None:
            cls = self.next_cls_cb(pkt, [], None, s)
            c = 1

        lst = []
        ret = b""""
        remain = s
        if l is not None:
            remain,ret = s[:l],s[l:]
        while remain:
            if c is not None:
                if c <= 0:
                    break
                c -= 1
            try:
                if cls is not None:
                    p = cls(remain)
                else:
                    p = self.m2i(pkt, remain)
            except Exception:
                if conf.debug_dissector:
                    raise
                p = conf.raw_layer(load=remain)
                remain = b""""
            else:
                if conf.padding_layer in p:
                    pad = p[conf.padding_layer]
                    remain = pad.load
                    del(pad.underlayer.payload)
                    if self.next_cls_cb is not None:
                        cls = self.next_cls_cb(pkt, lst, p, remain)
                        if cls is not None:
                            c += 1
                else:
                    remain = b""""
            lst.append(p)
        return remain+ret,lst
    def addfield(self, pkt, s, val):
        return s + b"""".join(raw(v) for v in val)


class StrFixedLenField(StrField):
    __slots__ = [""length_from""]
    def __init__(self, name, default, length=None, length_from=None):
        StrField.__init__(self, name, default)
        self.length_from  = length_from
        if length is not None:
            self.length_from = lambda pkt,length=length: length
    def i2repr(self, pkt, v):
        if isinstance(v, bytes):
            v = v.rstrip(b""\0"")
        return super(StrFixedLenField, self).i2repr(pkt, v)
    def getfield(self, pkt, s):
        l = self.length_from(pkt)
        return s[l:], self.m2i(pkt,s[:l])
    def addfield(self, pkt, s, val):
        l = self.length_from(pkt)
        return s+struct.pack(""%is""%l,self.i2m(pkt, val))
    def randval(self):
        try:
            l = self.length_from(None)
        except:
            l = RandNum(0,200)
        return RandBin(l)

class StrFixedLenEnumField(StrFixedLenField):
    __slots__ = [""enum""]
    def __init__(self, name, default, length=None, enum=None, length_from=None):
        StrFixedLenField.__init__(self, name, default, length=length, length_from=length_from)
        self.enum = enum
    def i2repr(self, pkt, v):
        r = v.rstrip(""\0"" if isinstance(v, str) else b""\0"")
        rr = repr(r)
        if v in self.enum:
            rr = ""%s (%s)"" % (rr, self.enum[v])
        elif r in self.enum:
            rr = ""%s (%s)"" % (rr, self.enum[r])
        return rr

class NetBIOSNameField(StrFixedLenField):
    def __init__(self, name, default, length=31):
        StrFixedLenField.__init__(self, name, default, length)
    def i2m(self, pkt, x):
        l = self.length_from(pkt)//2
        if x is None:
            x = b""""
        x += b"" ""*(l)
        x = x[:l]
        x = b"""".join(chb(0x41 + orb(b)>>4) + chb(0x41 + orb(b)&0xf) for b in x)
        x = b"" ""+x
        return x
    def m2i(self, pkt, x):
        x = x.strip(b""\x00"").strip(b"" "")
        return b"""".join(map(lambda x,y: chb((((orb(x)-1)&0xf)<<4)+((orb(y)-1)&0xf)), x[::2],x[1::2]))

class StrLenField(StrField):
    __slots__ = [""length_from""]
    def __init__(self, name, default, fld=None, length_from=None):
        StrField.__init__(self, name, default)
        self.length_from = length_from
    def getfield(self, pkt, s):
        l = self.length_from(pkt)
        return s[l:], self.m2i(pkt,s[:l])

class XStrField(StrField):
    """"""
    StrField which value is printed as hexadecimal.
    """"""

    def i2repr(self, pkt, x):
        if x is None:
            return repr(x)
        return bytes_hex(x).decode()

class XStrLenField(StrLenField):
    """"""
    StrLenField which value is printed as hexadecimal.
    """"""

    def i2repr(self, pkt, x):
        if not x:
            return repr(x)
        return bytes_hex(x[:self.length_from(pkt)]).decode()

class XStrFixedLenField(StrFixedLenField):
    """"""
    StrFixedLenField which value is printed as hexadecimal.
    """"""

    def i2repr(self, pkt, x):
        if not x:
            return repr(x)
        return bytes_hex(x[:self.length_from(pkt)]).decode()

class StrLenFieldUtf16(StrLenField):
    def h2i(self, pkt, x):
        return plain_str(x).encode('utf-16')[2:]
    def i2h(self, pkt, x):
        return x.decode('utf-16')

class BoundStrLenField(StrLenField):
    __slots__ = [""minlen"", ""maxlen""]
    def __init__(self,name, default, minlen= 0, maxlen= 255, fld=None, length_from=None):
        StrLenField.__init__(self, name, default, fld, length_from)
        self.minlen = minlen
        self.maxlen = maxlen

    def randval(self):
        return RandBin(RandNum(self.minlen, self.maxlen))

class FieldListField(Field):
    __slots__ = [""field"", ""count_from"", ""length_from""]
    islist = 1
    def __init__(self, name, default, field, length_from=None, count_from=None):
        if default is None:
            default = []  # Create a new list for each instance
        self.field = field
        Field.__init__(self, name, default)
        self.count_from = count_from
        self.length_from = length_from

    def i2count(self, pkt, val):
        if isinstance(val, list):
            return len(val)
        return 1
    def i2len(self, pkt, val):
        return int(sum(self.field.i2len(pkt,v) for v in val))

    def i2m(self, pkt, val):
        if val is None:
            val = []
        return val
    def any2i(self, pkt, x):
        if not isinstance(x, list):
            return [self.field.any2i(pkt, x)]
        else:
            return [self.field.any2i(pkt, e) for e in x]
    def i2repr(self, pkt, x):
        res = []
        for v in x:
            r = self.field.i2repr(pkt, v)
            res.append(r)
        return ""[%s]"" % "", "".join(res)
    def addfield(self, pkt, s, val):
        val = self.i2m(pkt, val)
        for v in val:
            s = self.field.addfield(pkt, s, v)
        return s
    def getfield(self, pkt, s):
        c = l = None
        if self.length_from is not None:
            l = self.length_from(pkt)
        elif self.count_from is not None:
            c = self.count_from(pkt)

        val = []
        ret = b""""
        if l is not None:
            s,ret = s[:l],s[l:]

        while s:
            if c is not None:
                if c <= 0:
                    break
                c -= 1
            s,v = self.field.getfield(pkt, s)
            val.append(v)
        return s+ret, val

class FieldLenField(Field):
    __slots__ = [""length_of"", ""count_of"", ""adjust""]
    def __init__(self, name, default,  length_of=None, fmt = ""H"", count_of=None, adjust=lambda pkt,x:x, fld=None):
        Field.__init__(self, name, default, fmt)
        self.length_of = length_of
        self.count_of = count_of
        self.adjust = adjust
        if fld is not None:
            #FIELD_LENGTH_MANAGEMENT_DEPRECATION(self.__class__.__name__)
            self.length_of = fld
    def i2m(self, pkt, x):
        if x is None:
            if self.length_of is not None:
                fld,fval = pkt.getfield_and_val(self.length_of)
                f = fld.i2len(pkt, fval)
            else:
                fld,fval = pkt.getfield_and_val(self.count_of)
                f = fld.i2count(pkt, fval)
            x = self.adjust(pkt,f)
        return x

class StrNullField(StrField):
    def addfield(self, pkt, s, val):
        return s+self.i2m(pkt, val)+b""\x00""
    def getfield(self, pkt, s):
        l = s.find(b""\x00"")
        if l < 0:
            #XXX \x00 not found
            return b"""",s
        return s[l+1:],self.m2i(pkt, s[:l])
    def randval(self):
        return RandTermString(RandNum(0,1200),b""\x00"")

class StrStopField(StrField):
    __slots__ = [""stop"", ""additionnal""]
    def __init__(self, name, default, stop, additionnal=0):
        Field.__init__(self, name, default)
        self.stop = stop
        self.additionnal = additionnal
    def getfield(self, pkt, s):
        l = s.find(self.stop)
        if l < 0:
            return b"""",s
#            raise Scapy_Exception,""StrStopField: stop value [%s] not found"" %stop
        l += len(self.stop)+self.additionnal
        return s[l:],s[:l]
    def randval(self):
        return RandTermString(RandNum(0,1200),self.stop)

class LenField(Field):
    __slots__ = [""adjust""]
    def __init__(self, name, default, fmt=""H"", adjust=lambda x: x):
        Field.__init__(self, name, default, fmt)
        self.adjust = adjust
    def i2m(self, pkt, x):
        if x is None:
            x = self.adjust(len(pkt.payload))
        return x

class BCDFloatField(Field):
    def i2m(self, pkt, x):
        return int(256*x)
    def m2i(self, pkt, x):
        return x/256.0

class BitField(Field):
    __slots__ = [""rev"", ""size""]
    def __init__(self, name, default, size):
        Field.__init__(self, name, default)
        self.rev = size < 0
        self.size = abs(size)
    def reverse(self, val):
        if self.size == 16:
            # Replaces socket.ntohs (but work on both little/big endian)
            val = struct.unpack('>H',struct.pack('<H', int(val)))[0]
        elif self.size == 32:
            # Same here but for socket.ntohl
            val = struct.unpack('>I',struct.pack('<I', int(val)))[0]
        return val

    def addfield(self, pkt, s, val):
        val = self.i2m(pkt, val)
        if isinstance(s, tuple):
            s,bitsdone,v = s
        else:
            bitsdone = 0
            v = 0
        if self.rev:
            val = self.reverse(val)
        v <<= self.size
        v |= val & ((1<<self.size) - 1)
        bitsdone += self.size
        while bitsdone >= 8:
            bitsdone -= 8
            s = s+struct.pack(""!B"", v >> bitsdone)
            v &= (1<<bitsdone)-1
        if bitsdone:
            return s,bitsdone,v
        else:
            return s
    def getfield(self, pkt, s):
        if isinstance(s, tuple):
            s,bn = s
        else:
            bn = 0
        # we don't want to process all the string
        nb_bytes = (self.size+bn-1)//8 + 1
        w = s[:nb_bytes]

        # split the substring byte by byte
        _bytes = struct.unpack('!%dB' % nb_bytes , w)

        b = 0
        for c in range(nb_bytes):
            b |= int(_bytes[c]) << (nb_bytes-c-1)*8

        # get rid of high order bits
        b &= (1 << (nb_bytes*8-bn)) - 1

        # remove low order bits
        b = b >> (nb_bytes*8 - self.size - bn)

        if self.rev:
            b = self.reverse(b)

        bn += self.size
        s = s[bn//8:]
        bn = bn%8
        b = self.m2i(pkt, b)
        if bn:
            return (s,bn),b
        else:
            return s,b
    def randval(self):
        return RandNum(0,2**self.size-1)
    def i2len(self, pkt, x):
        return float(self.size)/8


class BitFieldLenField(BitField):
    __slots__ = [""length_of"", ""count_of"", ""adjust""]
    def __init__(self, name, default, size, length_of=None, count_of=None, adjust=lambda pkt,x:x):
        BitField.__init__(self, name, default, size)
        self.length_of = length_of
        self.count_of = count_of
        self.adjust = adjust
    def i2m(self, pkt, x):
        return (FieldLenField.i2m.__func__ if six.PY2 else FieldLenField.i2m)(self, pkt, x)


class XBitField(BitField):
    def i2repr(self, pkt, x):
        return lhex(self.i2h(pkt,x))


class _EnumField(Field):
    def __init__(self, name, default, enum, fmt = ""H""):",Why this change ? Is it useful ?,"The suggested code change introduces several new classes and modifications to existing ones within the `scapy.fields` module. Here's a concise review of the changes:

### New Classes and Fields

1. **Emph**: A simple wrapper around another field that provides a `__getattr__` method to delegate attribute access to the wrapped field.

2. **ActionField**: A field that applies an action method to the value before converting it to an internal format.

3. **ConditionalField**: A field that conditionally applies another field based on a condition.

4. **PadField**: A field that adds padding bytes to ensure the total length of the field is aligned to a specified alignment.

5. **DestField**: A base class for destination fields that can be bound to specific conditions to determine the destination address.

6. **MACField**: A field for MAC addresses, handling conversion between binary and human-readable formats.

7. **IPField**: A field for IPv4 addresses, supporting various conversion methods and resolving hostnames.

8. **SourceIPField**: An extension of `IPField` that uses a default destination name to resolve the source IP address.

9. **ByteField**, **XByteField**, **OByteField**, **X3BytesField**, **ThreeBytesField**, **SignedByteField**: Various integer fields with different representations.

10. **YesNoByteField**: A field that represents a boolean value using a custom mapping.

11. **ShortField**, **SignedShortField**, **LEShortField**, **XShortField**: Integer fields with different byte orders and representations.

12. **IntField**, **SignedIntField**, **LEIntField**, **LESignedIntField**, **XIntField**: Integer fields with different byte orders and representations.

13. **LongField**, **LELongField**, **XLongField**: Integer fields with different byte orders and representations.

14. **IEEEFloatField**, **IEEEDoubleField**: Floating-point fields with IEEE 754 encoding.

15. **StrField**: A generic string field with optional padding.

16. **PacketField**: A field for encapsulating other packets.

17. **PacketLenField**: A field for encapsulating packets with a known length.

18. **PacketListField**: A field for encapsulating a list of packets with dynamic discovery of packet types.

19. **StrFixedLenField**, **StrFixedLenEnumField**: Fixed-length string fields with optional enumeration support.

20"
3,"@@ -0,0 +1,4 @@
+const titleNode = virtualNode.children.find(({ props }) => {
+	return props.nodeName === 'title';
+});
+return !!titleNode && titleNode.actualNode.textContent.trim() !== '';","const titleNode = virtualNode.children.find(({ props }) => {
	return props.nodeName === 'title';
});
return !!titleNode && titleNode.actualNode.textContent.trim() !== '';

","I know this is a nitpick, but don't we always go `nodeName.toUpperCase`?","The suggested code change is intended to find a `<title>` element within a virtual DOM node and check if its text content is not empty after trimming whitespace. Here's a concise review:

- **Purpose**: The code aims to ensure that the `<title>` element exists and contains non-empty text content.
- **Logic**:
  - `virtualNode.children.find(...)`: This line searches for a child node with a `props.nodeName` of `'title'`.
  - `!!titleNode`: This checks if the found node is truthy (i.e., not `null` or `undefined`).
  - `titleNode.actualNode.textContent.trim() !== ''`: This checks if the trimmed text content of the `<title>` node is not empty.
- **Simplicity**: The code is straightforward and efficient for this purpose.
- **Readability**: The use of `!!` for checking truthiness makes the code more readable by clearly indicating that the result should be a boolean.

Overall, the code is well-structured and performs the intended task effectively."
4,"@@ -37,6 +37,11 @@ public class EMailValidatorTest {
         assertEquals(false, EMailValidator.isEmailValid(""dora@.com"", null));
         assertEquals(false, EMailValidator.isEmailValid("""", null));
         assertEquals(false, EMailValidator.isEmailValid(null, null));
+        /*
+        Add tests for 4601
+        */
+        assertEquals(true, EMailValidator.isEmailValid(""ziller@wiso.uni-koeln.de"", null));
+        assertEquals(true, EMailValidator.isEmailValid(""jayned@essex.ac.uk"", null));
     }
 
 }","package edu.harvard.iq.dataverse;

import static org.junit.Assert.assertEquals;
import org.junit.Test;

public class EMailValidatorTest {

    @Test
    public void testIsEmailValid() {
        assertEquals(true, EMailValidator.isEmailValid(""pete@mailinator.com"", null));
        /**
         * @todo How can "" leadingWhitespace@mailinator.com"" be a valid email
         * address?
         */
        assertEquals(true, EMailValidator.isEmailValid("" leadingWhitespace@mailinator.com"", null));
        /**
         * @todo How can ""trailingWhitespace@mailinator.com "" be a valid email
         * address?
         */
        assertEquals(true, EMailValidator.isEmailValid(""trailingWhitespace@mailinator.com "", null));
        assertEquals(false, EMailValidator.isEmailValid(""elisah.da mota@example.com"", null));
        assertEquals(false, EMailValidator.isEmailValid(""pete1@mailinator.com;pete2@mailinator.com"", null));
        /**
         * These examples are all from https://randomuser.me and seem to be
         * valid according to
         * http://sphinx.mythic-beasts.com/~pdw/cgi-bin/emailvalidate (except
         * رونیکا.محمدخان@example.com).
         *
         */
        assertEquals(true, EMailValidator.isEmailValid(""michélle.pereboom@example.com"", null));
        assertEquals(true, EMailValidator.isEmailValid(""begüm.vriezen@example.com"", null));
        assertEquals(true, EMailValidator.isEmailValid(""lótus.gonçalves@example.com"", null));
        assertEquals(true, EMailValidator.isEmailValid(""lótus.gonçalves@éxample.com"", null));
        assertEquals(true, EMailValidator.isEmailValid(""begüm.vriezen@example.cologne"", null));
        assertEquals(true, EMailValidator.isEmailValid(""رونیکا.محمدخان@example.com"", null));
        assertEquals(false, EMailValidator.isEmailValid(""lótus.gonçalves@example.cóm"", null));
        /*
        Add tests for 4601
        */
        assertEquals(true, EMailValidator.isEmailValid(""ziller@wiso.uni-koeln.de"", null));
        assertEquals(true, EMailValidator.isEmailValid(""jayned@essex.ac.uk"", null));
        assertEquals(false, EMailValidator.isEmailValid(""dora@.com"", null));
        assertEquals(false, EMailValidator.isEmailValid("""", null));
        assertEquals(false, EMailValidator.isEmailValid(null, null));
    }

}
","We should reformat this emails in the test to be a little less ""real"".","The suggested code change is a unit test for the `EMailValidator` class in the `edu.harvard.iq.dataverse` package. The test cases cover various scenarios, including valid and invalid email addresses, as well as edge cases such as leading and trailing whitespace, semicolons, and internationalized domain names.

Here's a concise review of the suggested code change:

- **Test Cases**: The test method `testIsEmailValid` includes several assertions to verify the correctness of the `isEmailValid` method in the `EMailValidator` class.
- **Edge Cases**: The test cases handle leading and trailing whitespace, semicolons, and internationalized domain names, which are important considerations for email validation.
- **Validation**: The test cases use `assertEquals` to ensure that the method returns the expected boolean values based on the input email addresses.
- **Documentation**: The comments in the test method indicate areas where further investigation is needed, such as why certain email addresses are considered valid despite being malformed.

Overall, this test suite provides a comprehensive set of checks to ensure the robustness of the email validation logic in the `EMailValidator` class."
5,"@@ -449,14 +449,18 @@ class LNGossip(LNWorker):
         self.features |= LnFeatures.GOSSIP_QUERIES_OPT
         self.features |= LnFeatures.GOSSIP_QUERIES_REQ
         self.unknown_ids = set()
+        self.has_started = False
 
     def start_network(self, network: 'Network'):
         assert network
+        if self.has_started:
+            return
         super().start_network(network)
         asyncio.run_coroutine_threadsafe(self.taskgroup.spawn(self.maintain_db()), self.network.asyncio_loop)
+        self.has_started = True
 
     async def maintain_db(self):
-        await self.channel_db.load_data()
+        await self.channel_db.data_loaded.wait()
         while True:
             if len(self.unknown_ids) == 0:
                 self.channel_db.prune_old_policies(self.max_age)","# Copyright (C) 2018 The Electrum developers
# Distributed under the MIT software license, see the accompanying
# file LICENCE or http://www.opensource.org/licenses/mit-license.php

import asyncio
import os
from decimal import Decimal
import random
import time
from typing import Optional, Sequence, Tuple, List, Dict, TYPE_CHECKING, NamedTuple, Union, Mapping, Any
import threading
import socket
import aiohttp
import json
from datetime import datetime, timezone
from functools import partial
from collections import defaultdict
import concurrent
from concurrent import futures
import urllib.parse

import dns.resolver
import dns.exception
from aiorpcx import run_in_thread, TaskGroup, NetAddress

from . import constants, util
from . import keystore
from .util import profiler
from .invoices import PR_TYPE_LN, PR_UNPAID, PR_EXPIRED, PR_PAID, PR_INFLIGHT, PR_FAILED, PR_ROUTING, LNInvoice, LN_EXPIRY_NEVER
from .util import NetworkRetryManager, JsonRPCClient
from .lnutil import LN_MAX_FUNDING_SAT
from .keystore import BIP32_KeyStore
from .bitcoin import COIN
from .transaction import Transaction
from .crypto import sha256
from .bip32 import BIP32Node
from .util import bh2u, bfh, InvoiceError, resolve_dns_srv, is_ip_address, log_exceptions
from .util import ignore_exceptions, make_aiohttp_session, SilentTaskGroup
from .util import timestamp_to_datetime, random_shuffled_copy
from .util import MyEncoder, is_private_netaddress
from .logging import Logger
from .lntransport import LNTransport, LNResponderTransport
from .lnpeer import Peer, LN_P2P_NETWORK_TIMEOUT
from .lnaddr import lnencode, LnAddr, lndecode
from .ecc import der_sig_from_sig_string
from .lnchannel import Channel
from .lnchannel import ChannelState, PeerState
from . import lnutil
from .lnutil import funding_output_script
from .bitcoin import redeem_script_to_address
from .lnutil import (Outpoint, LNPeerAddr,
                     get_compressed_pubkey_from_bech32, extract_nodeid,
                     PaymentFailure, split_host_port, ConnStringFormatError,
                     generate_keypair, LnKeyFamily, LOCAL, REMOTE,
                     MIN_FINAL_CLTV_EXPIRY_FOR_INVOICE,
                     NUM_MAX_EDGES_IN_PAYMENT_PATH, SENT, RECEIVED, HTLCOwner,
                     UpdateAddHtlc, Direction, LnFeatures,
                     ShortChannelID, PaymentAttemptLog, PaymentAttemptFailureDetails,
                     BarePaymentAttemptLog, derive_payment_secret_from_payment_preimage)
from .lnutil import ln_dummy_address, ln_compare_features, IncompatibleLightningFeatures
from .transaction import PartialTxOutput, PartialTransaction, PartialTxInput
from .lnonion import OnionFailureCode, process_onion_packet, OnionPacket
from .lnmsg import decode_msg
from .i18n import _
from .lnrouter import (RouteEdge, LNPaymentRoute, LNPaymentPath, is_route_sane_to_use,
                       NoChannelPolicy, LNPathInconsistent)
from .address_synchronizer import TX_HEIGHT_LOCAL
from . import lnsweep
from .lnwatcher import LNWalletWatcher
from .crypto import pw_encode_with_version_and_mac, pw_decode_with_version_and_mac
from .lnutil import ChannelBackupStorage
from .lnchannel import ChannelBackup
from .channel_db import UpdateStatus
from .submarine_swaps import SwapManager

if TYPE_CHECKING:
    from .network import Network
    from .wallet import Abstract_Wallet


SAVED_PR_STATUS = [PR_PAID, PR_UNPAID, PR_INFLIGHT] # status that are persisted


NUM_PEERS_TARGET = 4


FALLBACK_NODE_LIST_TESTNET = (
    LNPeerAddr(host='203.132.95.10', port=9735, pubkey=bfh('038863cf8ab91046230f561cd5b386cbff8309fa02e3f0c3ed161a3aeb64a643b9')),
    LNPeerAddr(host='2401:d002:4402:0:bf1d:986a:7598:6d49', port=9735, pubkey=bfh('038863cf8ab91046230f561cd5b386cbff8309fa02e3f0c3ed161a3aeb64a643b9')),
    LNPeerAddr(host='50.116.3.223', port=9734, pubkey=bfh('03236a685d30096b26692dce0cf0fa7c8528bdf61dbf5363a3ef6d5c92733a3016')),
    LNPeerAddr(host='3.16.119.191', port=9735, pubkey=bfh('03d5e17a3c213fe490e1b0c389f8cfcfcea08a29717d50a9f453735e0ab2a7c003')),
    LNPeerAddr(host='34.250.234.192', port=9735, pubkey=bfh('03933884aaf1d6b108397e5efe5c86bcf2d8ca8d2f700eda99db9214fc2712b134')),
    LNPeerAddr(host='88.99.209.230', port=9735, pubkey=bfh('0260d9119979caedc570ada883ff614c6efb93f7f7382e25d73ecbeba0b62df2d7')),
    LNPeerAddr(host='160.16.233.215', port=9735, pubkey=bfh('023ea0a53af875580899da0ab0a21455d9c19160c4ea1b7774c9d4be6810b02d2c')),
    LNPeerAddr(host='197.155.6.173', port=9735, pubkey=bfh('0269a94e8b32c005e4336bfb743c08a6e9beb13d940d57c479d95c8e687ccbdb9f')),
    LNPeerAddr(host='2c0f:fb18:406::4', port=9735, pubkey=bfh('0269a94e8b32c005e4336bfb743c08a6e9beb13d940d57c479d95c8e687ccbdb9f')),
    LNPeerAddr(host='163.172.94.64', port=9735, pubkey=bfh('030f0bf260acdbd3edcad84d7588ec7c5df4711e87e6a23016f989b8d3a4147230')),
    LNPeerAddr(host='23.237.77.12', port=9735, pubkey=bfh('02312627fdf07fbdd7e5ddb136611bdde9b00d26821d14d94891395452f67af248')),
    LNPeerAddr(host='197.155.6.172', port=9735, pubkey=bfh('02ae2f22b02375e3e9b4b4a2db4f12e1b50752b4062dbefd6e01332acdaf680379')),
    LNPeerAddr(host='2c0f:fb18:406::3', port=9735, pubkey=bfh('02ae2f22b02375e3e9b4b4a2db4f12e1b50752b4062dbefd6e01332acdaf680379')),
    LNPeerAddr(host='23.239.23.44', port=9740, pubkey=bfh('034fe52e98a0e9d3c21b767e1b371881265d8c7578c21f5afd6d6438da10348b36')),
    LNPeerAddr(host='2600:3c01::f03c:91ff:fe05:349c', port=9740, pubkey=bfh('034fe52e98a0e9d3c21b767e1b371881265d8c7578c21f5afd6d6438da10348b36')),
)

FALLBACK_NODE_LIST_MAINNET = [
    LNPeerAddr(host='172.81.181.3', port=9735, pubkey=bfh('0214382bdce7750dfcb8126df8e2b12de38536902dc36abcebdaeefdeca1df8284')),
    LNPeerAddr(host='35.230.100.60', port=9735, pubkey=bfh('023f5e3582716bed96f6f26cfcd8037e07474d7b4743afdc8b07e692df63464d7e')),
    LNPeerAddr(host='40.69.71.114', port=9735, pubkey=bfh('028303182c9885da93b3b25c9621d22cf34475e63c123942e402ab530c0556e675')),
    LNPeerAddr(host='94.177.171.73', port=9735, pubkey=bfh('0276e09a267592e7451a939c932cf685f0754de382a3ca85d2fb3a864d4c365ad5')),
    LNPeerAddr(host='34.236.113.58', port=9735, pubkey=bfh('02fa50c72ee1e2eb5f1b6d9c3032080c4c864373c4201dfa2966aa34eee1051f97')),
    LNPeerAddr(host='52.50.244.44', port=9735, pubkey=bfh('030c3f19d742ca294a55c00376b3b355c3c90d61c6b6b39554dbc7ac19b141c14f')),
    LNPeerAddr(host='157.245.68.47', port=9735, pubkey=bfh('03c2abfa93eacec04721c019644584424aab2ba4dff3ac9bdab4e9c97007491dda')),
    LNPeerAddr(host='18.221.23.28', port=9735, pubkey=bfh('03abf6f44c355dec0d5aa155bdbdd6e0c8fefe318eff402de65c6eb2e1be55dc3e')),
    LNPeerAddr(host='52.224.178.244', port=9735, pubkey=bfh('026b105ac13212c48714c6be9b11577a9ce10f10e1c88a45ce217e6331209faf8b')),
    LNPeerAddr(host='34.239.230.56', port=9735, pubkey=bfh('03864ef025fde8fb587d989186ce6a4a186895ee44a926bfc370e2c366597a3f8f')),
    LNPeerAddr(host='46.229.165.136', port=9735, pubkey=bfh('0390b5d4492dc2f5318e5233ab2cebf6d48914881a33ef6a9c6bcdbb433ad986d0')),
    LNPeerAddr(host='157.230.28.160', port=9735, pubkey=bfh('0279c22ed7a068d10dc1a38ae66d2d6461e269226c60258c021b1ddcdfe4b00bc4')),
    LNPeerAddr(host='74.108.13.152', port=9735, pubkey=bfh('0331f80652fb840239df8dc99205792bba2e559a05469915804c08420230e23c7c')),
    LNPeerAddr(host='167.172.44.148', port=9735, pubkey=bfh('0395033b252c6f40e3756984162d68174e2bd8060a129c0d3462a9370471c6d28f')),
    LNPeerAddr(host='138.68.14.104', port=9735, pubkey=bfh('03bb88ccc444534da7b5b64b4f7b15e1eccb18e102db0e400d4b9cfe93763aa26d')),
    LNPeerAddr(host='3.124.63.44', port=9735, pubkey=bfh('0242a4ae0c5bef18048fbecf995094b74bfb0f7391418d71ed394784373f41e4f3')),
    LNPeerAddr(host='2001:470:8:2e1::43', port=9735, pubkey=bfh('03baa70886d9200af0ffbd3f9e18d96008331c858456b16e3a9b41e735c6208fef')),
    LNPeerAddr(host='2601:186:c100:6bcd:219:d1ff:fe75:dc2f', port=9735, pubkey=bfh('0298f6074a454a1f5345cb2a7c6f9fce206cd0bf675d177cdbf0ca7508dd28852f')),
    LNPeerAddr(host='2001:41d0:e:734::1', port=9735, pubkey=bfh('03a503d8e30f2ff407096d235b5db63b4fcf3f89a653acb6f43d3fc492a7674019')),
    LNPeerAddr(host='2a01:4f9:2b:2254::2', port=9735, pubkey=bfh('02f3069a342ae2883a6f29e275f06f28a56a6ea2e2d96f5888a3266444dcf542b6')),
    LNPeerAddr(host='2a02:8070:24c1:100:528c:2997:6dbc:a054', port=9735, pubkey=bfh('02a45def9ae014fdd2603dd7033d157faa3a55a72b06a63ae22ef46d9fafdc6e8d')),
    LNPeerAddr(host='2600:3c01::f03c:91ff:fe05:349c', port=9736, pubkey=bfh('02731b798b39a09f9f14e90ee601afb6ebb796d6e5797de14582a978770b33700f')),
    LNPeerAddr(host='2a00:8a60:e012:a00::21', port=9735, pubkey=bfh('027ce055380348d7812d2ae7745701c9f93e70c1adeb2657f053f91df4f2843c71')),
    LNPeerAddr(host='2604:a880:400:d1::8bd:1001', port=9735, pubkey=bfh('03649c72a4816f0cd546f84aafbd657e92a30ab474de7ab795e8b5650a427611f7')),
    LNPeerAddr(host='2a01:4f8:c0c:7b31::1', port=9735, pubkey=bfh('02c16cca44562b590dd279c942200bdccfd4f990c3a69fad620c10ef2f8228eaff')),
    LNPeerAddr(host='2001:41d0:1:b40d::1', port=9735, pubkey=bfh('026726a4b043d413b45b334876d17b8a98848129604429ec65532ba286a42efeac')),
]


class PaymentInfo(NamedTuple):
    payment_hash: bytes
    amount: Optional[int]  # in satoshis  # TODO make it msat and rename to amount_msat
    direction: int
    status: int


class NoPathFound(PaymentFailure):
    def __str__(self):
        return _('No path found')


class ErrorAddingPeer(Exception): pass


class LNWorker(Logger, NetworkRetryManager[LNPeerAddr]):

    def __init__(self, xprv):
        Logger.__init__(self)
        NetworkRetryManager.__init__(
            self,
            max_retry_delay_normal=3600,
            init_retry_delay_normal=600,
            max_retry_delay_urgent=300,
            init_retry_delay_urgent=4,
        )
        self.lock = threading.RLock()
        self.node_keypair = generate_keypair(BIP32Node.from_xkey(xprv), LnKeyFamily.NODE_KEY)
        self._peers = {}  # type: Dict[bytes, Peer]  # pubkey -> Peer  # needs self.lock
        self.taskgroup = SilentTaskGroup()
        self.listen_server = None  # type: Optional[asyncio.AbstractServer]
        # set some feature flags as baseline for both LNWallet and LNGossip
        # note that e.g. DATA_LOSS_PROTECT is needed for LNGossip as many peers require it
        self.features = LnFeatures(0)
        self.features |= LnFeatures.OPTION_DATA_LOSS_PROTECT_OPT
        self.features |= LnFeatures.OPTION_STATIC_REMOTEKEY_OPT
        self.features |= LnFeatures.VAR_ONION_OPT
        self.features |= LnFeatures.PAYMENT_SECRET_OPT

        util.register_callback(self.on_proxy_changed, ['proxy_set'])

    @property
    def peers(self) -> Mapping[bytes, Peer]:
        """"""Returns a read-only copy of peers.""""""
        with self.lock:
            return self._peers.copy()

    def channels_for_peer(self, node_id):
        return {}

    async def maybe_listen(self):
        # FIXME: only one LNWorker can listen at a time (single port)
        listen_addr = self.config.get('lightning_listen')
        if listen_addr:
            self.logger.info(f'lightning_listen enabled. will try to bind: {listen_addr!r}')
            try:
                netaddr = NetAddress.from_string(listen_addr)
            except Exception as e:
                self.logger.error(f""failed to parse config key 'lightning_listen'. got: {e!r}"")
                return
            addr = str(netaddr.host)
            async def cb(reader, writer):
                transport = LNResponderTransport(self.node_keypair.privkey, reader, writer)
                try:
                    node_id = await transport.handshake()
                except Exception as e:
                    self.logger.info(f'handshake failure from incoming connection: {e!r}')
                    return
                peer = Peer(self, node_id, transport)
                with self.lock:
                    self._peers[node_id] = peer
                await self.taskgroup.spawn(peer.main_loop())
            try:
                self.listen_server = await asyncio.start_server(cb, addr, netaddr.port)
            except OSError as e:
                self.logger.error(f""cannot listen for lightning p2p. error: {e!r}"")

    @ignore_exceptions  # don't kill outer taskgroup
    async def main_loop(self):
        self.logger.info(""starting taskgroup."")
        try:
            async with self.taskgroup as group:
                await group.spawn(self._maintain_connectivity())
        except asyncio.CancelledError:
            raise
        except Exception as e:
            self.logger.exception(""taskgroup died."")
        finally:
            self.logger.info(""taskgroup stopped."")

    async def _maintain_connectivity(self):
        while True:
            await asyncio.sleep(1)
            now = time.time()
            if len(self._peers) >= NUM_PEERS_TARGET:
                continue
            peers = await self._get_next_peers_to_try()
            for peer in peers:
                if self._can_retry_addr(peer, now=now):
                    try:
                        await self._add_peer(peer.host, peer.port, peer.pubkey)
                    except ErrorAddingPeer as e:
                        self.logger.info(f""failed to add peer: {peer}. exc: {e!r}"")

    async def _add_peer(self, host: str, port: int, node_id: bytes) -> Peer:
        if node_id in self._peers:
            return self._peers[node_id]
        port = int(port)
        peer_addr = LNPeerAddr(host, port, node_id)
        self._trying_addr_now(peer_addr)
        self.logger.info(f""adding peer {peer_addr}"")
        if node_id == self.node_keypair.pubkey:
            raise ErrorAddingPeer(""cannot connect to self"")
        transport = LNTransport(self.node_keypair.privkey, peer_addr,
                                proxy=self.network.proxy)
        peer = Peer(self, node_id, transport)
        await self.taskgroup.spawn(peer.main_loop())
        with self.lock:
            self._peers[node_id] = peer
        return peer

    def peer_closed(self, peer: Peer) -> None:
        with self.lock:
            self._peers.pop(peer.pubkey, None)

    def num_peers(self) -> int:
        return sum([p.is_initialized() for p in self.peers.values()])

    def start_network(self, network: 'Network'):
        assert network
        self.network = network
        self.config = network.config
        self.channel_db = self.network.channel_db
        self._add_peers_from_config()
        asyncio.run_coroutine_threadsafe(self.main_loop(), self.network.asyncio_loop)

    def stop(self):
        if self.listen_server:
            self.network.asyncio_loop.call_soon_threadsafe(self.listen_server.close)
        asyncio.run_coroutine_threadsafe(self.taskgroup.cancel_remaining(), self.network.asyncio_loop)
        util.unregister_callback(self.on_proxy_changed)

    def _add_peers_from_config(self):
        peer_list = self.config.get('lightning_peers', [])
        for host, port, pubkey in peer_list:
            asyncio.run_coroutine_threadsafe(
                self._add_peer(host, int(port), bfh(pubkey)),
                self.network.asyncio_loop)

    def is_good_peer(self, peer):
        # the purpose of this method is to filter peers that advertise the desired feature bits
        # it is disabled for now, because feature bits published in node announcements seem to be unreliable
        return True
        node_id = peer.pubkey
        node = self.channel_db._nodes.get(node_id)
        if not node:
            return False
        try:
            ln_compare_features(self.features, node.features)
        except IncompatibleLightningFeatures:
            return False
        #self.logger.info(f'is_good {peer.host}')
        return True

    def on_peer_successfully_established(self, peer: Peer) -> None:
        if isinstance(peer.transport, LNTransport):
            peer_addr = peer.transport.peer_addr
            # reset connection attempt count
            self._on_connection_successfully_established(peer_addr)
            # add into channel db
            if self.channel_db:
                self.channel_db.add_recent_peer(peer_addr)
            # save network address into channels we might have with peer
            for chan in peer.channels.values():
                chan.add_or_update_peer_addr(peer_addr)

    async def _get_next_peers_to_try(self) -> Sequence[LNPeerAddr]:
        now = time.time()
        await self.channel_db.data_loaded.wait()
        # first try from recent peers
        recent_peers = self.channel_db.get_recent_peers()
        for peer in recent_peers:
            if not peer:
                continue
            if peer.pubkey in self._peers:
                continue
            if not self._can_retry_addr(peer, now=now):
                continue
            if not self.is_good_peer(peer):
                continue
            return [peer]
        # try random peer from graph
        unconnected_nodes = self.channel_db.get_200_randomly_sorted_nodes_not_in(self.peers.keys())
        if unconnected_nodes:
            for node_id in unconnected_nodes:
                addrs = self.channel_db.get_node_addresses(node_id)
                if not addrs:
                    continue
                host, port, timestamp = self.choose_preferred_address(list(addrs))
                try:
                    peer = LNPeerAddr(host, port, node_id)
                except ValueError:
                    continue
                if not self._can_retry_addr(peer, now=now):
                    continue
                if not self.is_good_peer(peer):
                    continue
                #self.logger.info('taking random ln peer from our channel db')
                return [peer]

        # getting desperate... let's try hardcoded fallback list of peers
        if constants.net in (constants.BitcoinTestnet, ):
            fallback_list = FALLBACK_NODE_LIST_TESTNET
        elif constants.net in (constants.BitcoinMainnet, ):
            fallback_list = FALLBACK_NODE_LIST_MAINNET
        else:
            return []  # regtest??

        fallback_list = [peer for peer in fallback_list if self._can_retry_addr(peer, now=now)]
        if fallback_list:
            return [random.choice(fallback_list)]

        # last resort: try dns seeds (BOLT-10)
        return await run_in_thread(self._get_peers_from_dns_seeds)

    def _get_peers_from_dns_seeds(self) -> Sequence[LNPeerAddr]:
        # NOTE: potentially long blocking call, do not run directly on asyncio event loop.
        # Return several peers to reduce the number of dns queries.
        if not constants.net.LN_DNS_SEEDS:
            return []
        dns_seed = random.choice(constants.net.LN_DNS_SEEDS)
        self.logger.info('asking dns seed ""{}"" for ln peers'.format(dns_seed))
        try:
            # note: this might block for several seconds
            # this will include bech32-encoded-pubkeys and ports
            srv_answers = resolve_dns_srv('r{}.{}'.format(
                constants.net.LN_REALM_BYTE, dns_seed))
        except dns.exception.DNSException as e:
            self.logger.info(f'failed querying (1) dns seed ""{dns_seed}"" for ln peers: {repr(e)}')
            return []
        random.shuffle(srv_answers)
        num_peers = 2 * NUM_PEERS_TARGET
        srv_answers = srv_answers[:num_peers]
        # we now have pubkeys and ports but host is still needed
        peers = []
        for srv_ans in srv_answers:
            try:
                # note: this might block for several seconds
                answers = dns.resolver.query(srv_ans['host'])
            except dns.exception.DNSException as e:
                self.logger.info(f'failed querying (2) dns seed ""{dns_seed}"" for ln peers: {repr(e)}')
                continue
            try:
                ln_host = str(answers[0])
                port = int(srv_ans['port'])
                bech32_pubkey = srv_ans['host'].split('.')[0]
                pubkey = get_compressed_pubkey_from_bech32(bech32_pubkey)
                peers.append(LNPeerAddr(ln_host, port, pubkey))
            except Exception as e:
                self.logger.info(f'error with parsing peer from dns seed: {repr(e)}')
                continue
        self.logger.info(f'got {len(peers)} ln peers from dns seed')
        return peers

    @staticmethod
    def choose_preferred_address(addr_list: Sequence[Tuple[str, int, int]]) -> Tuple[str, int, int]:
        assert len(addr_list) >= 1
        # choose first one that is an IP
        for host, port, timestamp in addr_list:
            if is_ip_address(host):
                return host, port, timestamp
        # otherwise choose one at random
        # TODO maybe filter out onion if not on tor?
        choice = random.choice(addr_list)
        return choice

    def on_proxy_changed(self, event, *args):
        for peer in self.peers.values():
            peer.close_and_cleanup()
        self._clear_addr_retry_times()

    @log_exceptions
    async def add_peer(self, connect_str: str) -> Peer:
        node_id, rest = extract_nodeid(connect_str)
        peer = self._peers.get(node_id)
        if not peer:
            if rest is not None:
                host, port = split_host_port(rest)
            else:
                addrs = self.channel_db.get_node_addresses(node_id)
                if not addrs:
                    raise ConnStringFormatError(_('Don\'t know any addresses for node:') + ' ' + bh2u(node_id))
                host, port, timestamp = self.choose_preferred_address(list(addrs))
            port = int(port)
            # Try DNS-resolving the host (if needed). This is simply so that
            # the caller gets a nice exception if it cannot be resolved.
            try:
                await asyncio.get_event_loop().getaddrinfo(host, port)
            except socket.gaierror:
                raise ConnStringFormatError(_('Hostname does not resolve (getaddrinfo failed)'))
            # add peer
            peer = await self._add_peer(host, port, node_id)
        return peer


class LNGossip(LNWorker):
    max_age = 14*24*3600
    LOGGING_SHORTCUT = 'g'

    def __init__(self):
        seed = os.urandom(32)
        node = BIP32Node.from_rootseed(seed, xtype='standard')
        xprv = node.to_xprv()
        super().__init__(xprv)
        self.has_started = False
        if self.has_started:
            return
        self.has_started = True
        await self.channel_db.data_loaded.wait()
        self.features |= LnFeatures.GOSSIP_QUERIES_OPT
        self.features |= LnFeatures.GOSSIP_QUERIES_REQ
        self.unknown_ids = set()

    def start_network(self, network: 'Network'):
        assert network
        super().start_network(network)
        asyncio.run_coroutine_threadsafe(self.taskgroup.spawn(self.maintain_db()), self.network.asyncio_loop)

    async def maintain_db(self):
        await self.channel_db.load_data()
        while True:
            if len(self.unknown_ids) == 0:
                self.channel_db.prune_old_policies(self.max_age)
                self.channel_db.prune_orphaned_channels()
            await asyncio.sleep(120)

    async def add_new_ids(self, ids):
        known = self.channel_db.get_channel_ids()
        new = set(ids) - set(known)
        self.unknown_ids.update(new)
        util.trigger_callback('unknown_channels', len(self.unknown_ids))
        util.trigger_callback('gossip_peers', self.num_peers())
        util.trigger_callback('ln_gossip_sync_progress')

    def get_ids_to_query(self):
        N = 500
        l = list(self.unknown_ids)
        self.unknown_ids = set(l[N:])
        util.trigger_callback('unknown_channels', len(self.unknown_ids))
        util.trigger_callback('ln_gossip_sync_progress')
        return l[0:N]

    def get_sync_progress_estimate(self) -> Tuple[Optional[int], Optional[int]]:
        if self.num_peers() == 0:
            return None, None
        nchans_with_0p, nchans_with_1p, nchans_with_2p = self.channel_db.get_num_channels_partitioned_by_policy_count()
        num_db_channels = nchans_with_0p + nchans_with_1p + nchans_with_2p
        # some channels will never have two policies (only one is in gossip?...)
        # so if we have at least 1 policy for a channel, we consider that channel ""complete"" here
        current_est = num_db_channels - nchans_with_0p
        total_est = len(self.unknown_ids) + num_db_channels
        return current_est, total_est


class LNWallet(LNWorker):

    lnwatcher: Optional['LNWalletWatcher']

    def __init__(self, wallet: 'Abstract_Wallet', xprv):
        Logger.__init__(self)
        self.wallet = wallet
        self.db = wallet.db
        self.config = wallet.config
        LNWorker.__init__(self, xprv)
        self.lnwatcher = None
        self.features |= LnFeatures.OPTION_DATA_LOSS_PROTECT_REQ
        self.features |= LnFeatures.OPTION_STATIC_REMOTEKEY_REQ
        self.payments = self.db.get_dict('lightning_payments')     # RHASH -> amount, direction, is_paid  # FIXME amt should be msat
        self.preimages = self.db.get_dict('lightning_preimages')   # RHASH -> preimage
        # note: this sweep_address is only used as fallback; as it might result in address-reuse
        self.sweep_address = wallet.get_new_sweep_address_for_channel()
        self.logs = defaultdict(list)  # type: Dict[str, List[PaymentAttemptLog]]  # key is RHASH  # (not persisted)
        self.is_routing = set()        # (not persisted) keys of invoices that are in PR_ROUTING state
        # used in tests
        self.enable_htlc_settle = asyncio.Event()
        self.enable_htlc_settle.set()

        # note: accessing channels (besides simple lookup) needs self.lock!
        self._channels = {}  # type: Dict[bytes, Channel]
        channels = self.db.get_dict(""channels"")
        for channel_id, c in random_shuffled_copy(channels.items()):
            self._channels[bfh(channel_id)] = Channel(c, sweep_address=self.sweep_address, lnworker=self)

        self.pending_payments = defaultdict(asyncio.Future)  # type: Dict[bytes, asyncio.Future[BarePaymentAttemptLog]]

        self.swap_manager = SwapManager(wallet=self.wallet, lnworker=self)

    @property
    def channels(self) -> Mapping[bytes, Channel]:
        """"""Returns a read-only copy of channels.""""""
        with self.lock:
            return self._channels.copy()

    def get_channel_by_id(self, channel_id: bytes) -> Optional[Channel]:
        return self._channels.get(channel_id, None)

    @ignore_exceptions
    @log_exceptions
    async def sync_with_local_watchtower(self):
        watchtower = self.network.local_watchtower
        if watchtower:
            while True:
                for chan in self.channels.values():
                    await self.sync_channel_with_watchtower(chan, watchtower.sweepstore)
                await asyncio.sleep(5)

    @ignore_exceptions
    @log_exceptions
    async def sync_with_remote_watchtower(self):
        while True:
            # periodically poll if the user updated 'watchtower_url'
            await asyncio.sleep(5)
            watchtower_url = self.config.get('watchtower_url')
            if not watchtower_url:
                continue
            parsed_url = urllib.parse.urlparse(watchtower_url)
            if not (parsed_url.scheme == 'https' or is_private_netaddress(parsed_url.hostname)):
                self.logger.warning(f""got watchtower URL for remote tower but we won't use it! ""
                                    f""can only use HTTPS (except if private IP): not using {watchtower_url!r}"")
                continue
            # try to sync with the remote watchtower
            try:
                async with make_aiohttp_session(proxy=self.network.proxy) as session:
                    watchtower = JsonRPCClient(session, watchtower_url)
                    watchtower.add_method('get_ctn')
                    watchtower.add_method('add_sweep_tx')
                    for chan in self.channels.values():
                        await self.sync_channel_with_watchtower(chan, watchtower)
            except aiohttp.client_exceptions.ClientConnectorError:
                self.logger.info(f'could not contact remote watchtower {watchtower_url}')

    async def sync_channel_with_watchtower(self, chan: Channel, watchtower):
        outpoint = chan.funding_outpoint.to_str()
        addr = chan.get_funding_address()
        current_ctn = chan.get_oldest_unrevoked_ctn(REMOTE)
        watchtower_ctn = await watchtower.get_ctn(outpoint, addr)
        for ctn in range(watchtower_ctn + 1, current_ctn):
            sweeptxs = chan.create_sweeptxs(ctn)
            for tx in sweeptxs:
                await watchtower.add_sweep_tx(outpoint, ctn, tx.inputs()[0].prevout.to_str(), tx.serialize())

    def start_network(self, network: 'Network'):
        assert network
        self.lnwatcher = LNWalletWatcher(self, network)
        self.lnwatcher.start_network(network)
        self.network = network
        self.swap_manager.start_network(network=network, lnwatcher=self.lnwatcher)

        for chan in self.channels.values():
            self.lnwatcher.add_channel(chan.funding_outpoint.to_str(), chan.get_funding_address())

        super().start_network(network)
        for coro in [
                self.maybe_listen(),
                self.lnwatcher.on_network_update('network_updated'), # shortcut (don't block) if funding tx locked and verified
                self.reestablish_peers_and_channels(),
                self.sync_with_local_watchtower(),
                self.sync_with_remote_watchtower(),
        ]:
            tg_coro = self.taskgroup.spawn(coro)
            asyncio.run_coroutine_threadsafe(tg_coro, self.network.asyncio_loop)

    def stop(self):
        super().stop()
        self.lnwatcher.stop()
        self.lnwatcher = None

    def peer_closed(self, peer):
        for chan in self.channels_for_peer(peer.pubkey).values():
            chan.peer_state = PeerState.DISCONNECTED
            util.trigger_callback('channel', self.wallet, chan)
        super().peer_closed(peer)

    def get_settled_payments(self):
        # return one item per payment_hash
        # note: with AMP we will have several channels per payment
        out = defaultdict(list)
        for chan in self.channels.values():
            d = chan.get_settled_payments()
            for k, v in d.items():
                out[k] += v
        return out

    def get_payment_value(self, info: Optional['PaymentInfo'], plist):
        amount_msat = 0
        fee_msat = None
        for chan_id, htlc, _direction in plist:
            amount_msat += int(_direction) * htlc.amount_msat
            if _direction == SENT and info and info.amount:
                fee_msat = (fee_msat or 0) - info.amount*1000 - amount_msat
        timestamp = min([htlc.timestamp for chan_id, htlc, _direction in plist])
        return amount_msat, fee_msat, timestamp

    def get_lightning_history(self):
        out = {}
        for key, plist in self.get_settled_payments().items():
            if len(plist) == 0:
                continue
            payment_hash = bytes.fromhex(key)
            info = self.get_payment_info(payment_hash)
            amount_msat, fee_msat, timestamp = self.get_payment_value(info, plist)
            if info is not None:
                label = self.wallet.get_label(key)
                direction = ('sent' if info.direction == SENT else 'received') if len(plist)==1 else 'self-payment'
            else:
                direction = 'forwarding'
                label = _('Forwarding')
            preimage = self.get_preimage(payment_hash).hex()
            item = {
                'type': 'payment',
                'label': label,
                'timestamp': timestamp or 0,
                'date': timestamp_to_datetime(timestamp),
                'direction': direction,
                'amount_msat': amount_msat,
                'fee_msat': fee_msat,
                'payment_hash': key,
                'preimage': preimage,
            }
            # add group_id to swap transactions
            swap = self.swap_manager.get_swap(payment_hash)
            if swap:
                if swap.is_reverse:
                    item['group_id'] = swap.spending_txid
                    item['group_label'] = 'Reverse swap' + ' ' + self.config.format_amount_and_units(swap.lightning_amount)
                else:
                    item['group_id'] = swap.funding_txid
                    item['group_label'] = 'Forward swap' + ' ' + self.config.format_amount_and_units(swap.onchain_amount)
            # done
            out[payment_hash] = item
        return out

    def get_onchain_history(self):
        out = {}
        # add funding events
        for chan in self.channels.values():
            item = chan.get_funding_height()
            if item is None:
                continue
            funding_txid, funding_height, funding_timestamp = item
            item = {
                'channel_id': bh2u(chan.channel_id),
                'type': 'channel_opening',
                'label': self.wallet.get_label_for_txid(funding_txid) or (_('Open channel') + ' ' + chan.get_id_for_log()),
                'txid': funding_txid,
                'amount_msat': chan.balance(LOCAL, ctn=0),
                'direction': 'received',
                'timestamp': funding_timestamp,
                'fee_msat': None,
            }
            out[funding_txid] = item
            item = chan.get_closing_height()
            if item is None:
                continue
            closing_txid, closing_height, closing_timestamp = item
            item = {
                'channel_id': bh2u(chan.channel_id),
                'txid': closing_txid,
                'label': self.wallet.get_label_for_txid(closing_txid) or (_('Close channel') + ' ' + chan.get_id_for_log()),
                'type': 'channel_closure',
                'amount_msat': -chan.balance_minus_outgoing_htlcs(LOCAL),
                'direction': 'sent',
                'timestamp': closing_timestamp,
                'fee_msat': None,
            }
            out[closing_txid] = item
        # add info about submarine swaps
        settled_payments = self.get_settled_payments()
        current_height = self.wallet.get_local_height()
        for payment_hash_hex, swap in self.swap_manager.swaps.items():
            txid = swap.spending_txid if swap.is_reverse else swap.funding_txid
            if txid is None:
                continue
            if payment_hash_hex in settled_payments:
                plist = settled_payments[payment_hash_hex]
                info = self.get_payment_info(bytes.fromhex(payment_hash_hex))
                amount_msat, fee_msat, timestamp = self.get_payment_value(info, plist)
            else:
                amount_msat = 0
            label = 'Reverse swap' if swap.is_reverse else 'Forward swap'
            delta = current_height - swap.locktime
            if not swap.is_redeemed and swap.spending_txid is None and delta < 0:
                label += f' (refundable in {-delta} blocks)' # fixme: only if unspent
            out[txid] = {
                'txid': txid,
                'group_id': txid,
                'amount_msat': 0,
                #'amount_msat': amount_msat, # must not be added
                'type': 'swap',
                'label': self.wallet.get_label_for_txid(txid) or label,
            }
        return out

    def get_history(self):
        out = list(self.get_lightning_history().values()) + list(self.get_onchain_history().values())
        # sort by timestamp
        out.sort(key=lambda x: (x.get('timestamp') or float(""inf"")))
        balance_msat = 0
        for item in out:
            balance_msat += item['amount_msat']
            item['balance_msat'] = balance_msat
        return out

    def suggest_peer(self):
        r = []
        for node_id, peer in self.peers.items():
            if not peer.is_initialized():
                continue
            if not all([chan.is_closed() for chan in peer.channels.values()]):
                continue
            r.append(node_id)
        return random.choice(r) if r else None

    def channels_for_peer(self, node_id):
        assert type(node_id) is bytes
        return {chan_id: chan for (chan_id, chan) in self.channels.items()
                if chan.node_id == node_id}

    def channel_state_changed(self, chan: Channel):
        self.save_channel(chan)
        util.trigger_callback('channel', self.wallet, chan)

    def save_channel(self, chan: Channel):
        assert type(chan) is Channel
        if chan.config[REMOTE].next_per_commitment_point == chan.config[REMOTE].current_per_commitment_point:
            raise Exception(""Tried to save channel with next_point == current_point, this should not happen"")
        self.wallet.save_db()
        util.trigger_callback('channel', self.wallet, chan)

    def channel_by_txo(self, txo: str) -> Optional[Channel]:
        for chan in self.channels.values():
            if chan.funding_outpoint.to_str() == txo:
                return chan

    async def on_channel_update(self, chan):

        if chan.get_state() == ChannelState.OPEN and chan.should_be_closed_due_to_expiring_htlcs(self.network.get_local_height()):
            self.logger.info(f""force-closing due to expiring htlcs"")
            await self.try_force_closing(chan.channel_id)

        elif chan.get_state() == ChannelState.FUNDED:
            peer = self._peers.get(chan.node_id)
            if peer and peer.is_initialized():
                peer.send_funding_locked(chan)

        elif chan.get_state() == ChannelState.OPEN:
            peer = self._peers.get(chan.node_id)
            if peer:
                await peer.maybe_update_fee(chan)
                conf = self.lnwatcher.get_tx_height(chan.funding_outpoint.txid).conf
                peer.on_network_update(chan, conf)

        elif chan.get_state() == ChannelState.FORCE_CLOSING:
            force_close_tx = chan.force_close_tx()
            txid = force_close_tx.txid()
            height = self.lnwatcher.get_tx_height(txid).height
            if height == TX_HEIGHT_LOCAL:
                self.logger.info('REBROADCASTING CLOSING TX')
                await self.network.try_broadcasting(force_close_tx, 'force-close')

    @log_exceptions
    async def _open_channel_coroutine(self, *, connect_str: str, funding_tx: PartialTransaction,
                                      funding_sat: int, push_sat: int,
                                      password: Optional[str]) -> Tuple[Channel, PartialTransaction]:
        peer = await self.add_peer(connect_str)
        # will raise if init fails
        await asyncio.wait_for(peer.initialized, LN_P2P_NETWORK_TIMEOUT)
        chan, funding_tx = await peer.channel_establishment_flow(
            password=password,
            funding_tx=funding_tx,
            funding_sat=funding_sat,
            push_msat=push_sat * 1000,
            temp_channel_id=os.urandom(32))
        util.trigger_callback('channels_updated', self.wallet)
        self.wallet.add_transaction(funding_tx)  # save tx as local into the wallet
        self.wallet.set_label(funding_tx.txid(), _('Open channel'))
        if funding_tx.is_complete():
            await self.network.try_broadcasting(funding_tx, 'open_channel')
        return chan, funding_tx

    def add_channel(self, chan: Channel):
        with self.lock:
            self._channels[chan.channel_id] = chan
        self.lnwatcher.add_channel(chan.funding_outpoint.to_str(), chan.get_funding_address())

    def add_new_channel(self, chan: Channel):
        self.add_channel(chan)
        channels_db = self.db.get_dict('channels')
        channels_db[chan.channel_id.hex()] = chan.storage
        for addr in chan.get_wallet_addresses_channel_might_want_reserved():
            self.wallet.set_reserved_state_of_address(addr, reserved=True)
        self.save_channel(chan)
        self.wallet.save_backup()

    def mktx_for_open_channel(self, *, coins: Sequence[PartialTxInput], funding_sat: int,
                              fee_est=None) -> PartialTransaction:
        dummy_address = ln_dummy_address()
        outputs = [PartialTxOutput.from_address_and_value(dummy_address, funding_sat)]
        tx = self.wallet.make_unsigned_transaction(
            coins=coins,
            outputs=outputs,
            fee=fee_est)
        tx.set_rbf(False)
        return tx

    def open_channel(self, *, connect_str: str, funding_tx: PartialTransaction,
                     funding_sat: int, push_amt_sat: int, password: str = None,
                     timeout: Optional[int] = 20) -> Tuple[Channel, PartialTransaction]:
        if funding_sat > LN_MAX_FUNDING_SAT:
            raise Exception(_(""Requested channel capacity is over protocol allowed maximum.""))
        coro = self._open_channel_coroutine(connect_str=connect_str, funding_tx=funding_tx, funding_sat=funding_sat,
                                            push_sat=push_amt_sat, password=password)
        fut = asyncio.run_coroutine_threadsafe(coro, self.network.asyncio_loop)
        try:
            chan, funding_tx = fut.result(timeout=timeout)
        except concurrent.futures.TimeoutError:
            raise Exception(_(""open_channel timed out""))
        return chan, funding_tx

    def pay(self, invoice: str, *, amount_msat: int = None, attempts: int = 1) -> Tuple[bool, List[PaymentAttemptLog]]:
        """"""
        Can be called from other threads
        """"""
        coro = self._pay(invoice, amount_msat=amount_msat, attempts=attempts)
        fut = asyncio.run_coroutine_threadsafe(coro, self.network.asyncio_loop)
        return fut.result()

    def get_channel_by_short_id(self, short_channel_id: bytes) -> Optional[Channel]:
        for chan in self.channels.values():
            if chan.short_channel_id == short_channel_id:
                return chan

    @log_exceptions
    async def _pay(
            self,
            invoice: str,
            *,
            amount_msat: int = None,
            attempts: int = 1,
            full_path: LNPaymentPath = None,
    ) -> Tuple[bool, List[PaymentAttemptLog]]:
        lnaddr = self._check_invoice(invoice, amount_msat=amount_msat)
        payment_hash = lnaddr.paymenthash
        key = payment_hash.hex()
        amount = int(lnaddr.amount * COIN)
        status = self.get_payment_status(payment_hash)
        if status == PR_PAID:
            raise PaymentFailure(_(""This invoice has been paid already""))
        if status == PR_INFLIGHT:
            raise PaymentFailure(_(""A payment was already initiated for this invoice""))
        info = PaymentInfo(lnaddr.paymenthash, amount, SENT, PR_UNPAID)
        self.save_payment_info(info)
        self.wallet.set_label(key, lnaddr.get_description())
        self.logs[key] = log = []
        success = False
        reason = ''
        for i in range(attempts):
            try:
                # note: path-finding runs in a separate thread so that we don't block the asyncio loop
                # graph updates might occur during the computation
                self.set_invoice_status(key, PR_ROUTING)
                util.trigger_callback('invoice_status', self.wallet, key)
                route = await run_in_thread(partial(self._create_route_from_invoice, lnaddr, full_path=full_path))
                self.set_invoice_status(key, PR_INFLIGHT)
                util.trigger_callback('invoice_status', self.wallet, key)
                payment_attempt_log = await self._pay_to_route(route, lnaddr)
            except Exception as e:
                log.append(PaymentAttemptLog(success=False, exception=e))
                self.set_invoice_status(key, PR_UNPAID)
                reason = str(e)
                break
            log.append(payment_attempt_log)
            success = payment_attempt_log.success
            if success:
                break
        else:
            reason = _('Failed after {} attempts').format(attempts)
        util.trigger_callback('invoice_status', self.wallet, key)
        if success:
            util.trigger_callback('payment_succeeded', self.wallet, key)
        else:
            util.trigger_callback('payment_failed', self.wallet, key, reason)
        return success, log

    async def _pay_to_route(self, route: LNPaymentRoute, lnaddr: LnAddr) -> PaymentAttemptLog:
        short_channel_id = route[0].short_channel_id
        chan = self.get_channel_by_short_id(short_channel_id)
        peer = self._peers.get(route[0].node_id)
        if not peer:
            raise Exception('Dropped peer')
        await peer.initialized
        htlc = peer.pay(route=route,
                        chan=chan,
                        amount_msat=lnaddr.get_amount_msat(),
                        payment_hash=lnaddr.paymenthash,
                        min_final_cltv_expiry=lnaddr.get_min_final_cltv_expiry(),
                        payment_secret=lnaddr.payment_secret)
        util.trigger_callback('htlc_added', chan, htlc, SENT)
        payment_attempt = await self.await_payment(lnaddr.paymenthash)
        if payment_attempt.success:
            failure_log = None
        else:
            if payment_attempt.error_bytes:
                # TODO ""decode_onion_error"" might raise, catch and maybe blacklist/penalise someone?
                failure_msg, sender_idx = chan.decode_onion_error(payment_attempt.error_bytes, route, htlc.htlc_id)
                is_blacklisted = self.handle_error_code_from_failed_htlc(failure_msg, sender_idx, route, peer)
                if is_blacklisted:
                    # blacklist channel after reporter node
                    # TODO this should depend on the error (even more granularity)
                    # also, we need finer blacklisting (directed edges; nodes)
                    try:
                        short_chan_id = route[sender_idx + 1].short_channel_id
                    except IndexError:
                        self.logger.info(""payment destination reported error"")
                    else:
                        self.network.path_finder.add_to_blacklist(short_chan_id)
            else:
                # probably got ""update_fail_malformed_htlc"". well... who to penalise now?
                assert payment_attempt.failure_message is not None
                sender_idx = None
                failure_msg = payment_attempt.failure_message
                is_blacklisted = False
            failure_log = PaymentAttemptFailureDetails(sender_idx=sender_idx,
                                                       failure_msg=failure_msg,
                                                       is_blacklisted=is_blacklisted)
        return PaymentAttemptLog(route=route,
                                 success=payment_attempt.success,
                                 preimage=payment_attempt.preimage,
                                 failure_details=failure_log)

    def handle_error_code_from_failed_htlc(self, failure_msg, sender_idx, route, peer):
        code, data = failure_msg.code, failure_msg.data
        self.logger.info(f""UPDATE_FAIL_HTLC {repr(code)} {data}"")
        self.logger.info(f""error reported by {bh2u(route[sender_idx].node_id)}"")
        # handle some specific error codes
        failure_codes = {
            OnionFailureCode.TEMPORARY_CHANNEL_FAILURE: 0,
            OnionFailureCode.AMOUNT_BELOW_MINIMUM: 8,
            OnionFailureCode.FEE_INSUFFICIENT: 8,
            OnionFailureCode.INCORRECT_CLTV_EXPIRY: 4,
            OnionFailureCode.EXPIRY_TOO_SOON: 0,
            OnionFailureCode.CHANNEL_DISABLED: 2,
        }
        if code in failure_codes:
            offset = failure_codes[code]
            channel_update_len = int.from_bytes(data[offset:offset+2], byteorder=""big"")
            channel_update_as_received = data[offset+2: offset+2+channel_update_len]
            payload = self._decode_channel_update_msg(channel_update_as_received)
            if payload is None:
                self.logger.info(f'could not decode channel_update for failed htlc: {channel_update_as_received.hex()}')
                return True
            r = self.channel_db.add_channel_update(payload)
            blacklist = False
            short_channel_id = ShortChannelID(payload['short_channel_id'])
            if r == UpdateStatus.GOOD:
                self.logger.info(f""applied channel update to {short_channel_id}"")
                peer.maybe_save_remote_update(payload)
            elif r == UpdateStatus.ORPHANED:
                # maybe it is a private channel (and data in invoice was outdated)
                self.logger.info(f""Could not find {short_channel_id}. maybe update is for private channel?"")
                start_node_id = route[sender_idx].node_id
                self.channel_db.add_channel_update_for_private_channel(payload, start_node_id)
            elif r == UpdateStatus.EXPIRED:
                blacklist = True
            elif r == UpdateStatus.DEPRECATED:
                self.logger.info(f'channel update is not more recent.')
                blacklist = True
            elif r == UpdateStatus.UNCHANGED:
                blacklist = True
        else:
            blacklist = True
        return blacklist

    @classmethod
    def _decode_channel_update_msg(cls, chan_upd_msg: bytes) -> Optional[Dict[str, Any]]:
        channel_update_as_received = chan_upd_msg
        channel_update_typed = (258).to_bytes(length=2, byteorder=""big"") + channel_update_as_received
        # note: some nodes put channel updates in error msgs with the leading msg_type already there.
        #       we try decoding both ways here.
        try:
            message_type, payload = decode_msg(channel_update_typed)
            if payload['chain_hash'] != constants.net.rev_genesis_bytes(): raise Exception()
            payload['raw'] = channel_update_typed
            return payload
        except:  # FIXME: too broad
            try:
                message_type, payload = decode_msg(channel_update_as_received)
                if payload['chain_hash'] != constants.net.rev_genesis_bytes(): raise Exception()
                payload['raw'] = channel_update_as_received
                return payload
            except:
                return None

    @staticmethod
    def _check_invoice(invoice: str, *, amount_msat: int = None) -> LnAddr:
        addr = lndecode(invoice, expected_hrp=constants.net.SEGWIT_HRP)
        if addr.is_expired():
            raise InvoiceError(_(""This invoice has expired""))
        if amount_msat:  # replace amt in invoice. main usecase is paying zero amt invoices
            existing_amt_msat = addr.get_amount_msat()
            if existing_amt_msat and amount_msat < existing_amt_msat:
                raise Exception(""cannot pay lower amt than what is originally in LN invoice"")
            addr.amount = Decimal(amount_msat) / COIN / 1000
        if addr.amount is None:
            raise InvoiceError(_(""Missing amount""))
        if addr.get_min_final_cltv_expiry() > lnutil.NBLOCK_CLTV_EXPIRY_TOO_FAR_INTO_FUTURE:
            raise InvoiceError(""{}\n{}"".format(
                _(""Invoice wants us to risk locking funds for unreasonably long.""),
                f""min_final_cltv_expiry: {addr.get_min_final_cltv_expiry()}""))
        return addr

    @profiler
    def _create_route_from_invoice(self, decoded_invoice: 'LnAddr',
                                   *, full_path: LNPaymentPath = None) -> LNPaymentRoute:
        amount_msat = decoded_invoice.get_amount_msat()
        invoice_pubkey = decoded_invoice.pubkey.serialize()
        # use 'r' field from invoice
        route = None  # type: Optional[LNPaymentRoute]
        # only want 'r' tags
        r_tags = list(filter(lambda x: x[0] == 'r', decoded_invoice.tags))
        # strip the tag type, it's implicitly 'r' now
        r_tags = list(map(lambda x: x[1], r_tags))
        # if there are multiple hints, we will use the first one that works,
        # from a random permutation
        random.shuffle(r_tags)
        channels = list(self.channels.values())
        scid_to_my_channels = {chan.short_channel_id: chan for chan in channels
                               if chan.short_channel_id is not None}
        for private_route in r_tags:
            if len(private_route) == 0:
                continue
            if len(private_route) > NUM_MAX_EDGES_IN_PAYMENT_PATH:
                continue
            border_node_pubkey = private_route[0][0]
            if full_path:
                # user pre-selected path. check that end of given path coincides with private_route:
                if [edge.short_channel_id for edge in full_path[-len(private_route):]] != [edge[1] for edge in private_route]:
                    continue
                path = full_path[:-len(private_route)]
            else:
                # find path now on public graph, to border node
                path = self.network.path_finder.find_path_for_payment(self.node_keypair.pubkey, border_node_pubkey, amount_msat,
                                                                      my_channels=scid_to_my_channels)
            if not path:
                continue
            try:
                route = self.network.path_finder.create_route_from_path(path, self.node_keypair.pubkey,
                                                                        my_channels=scid_to_my_channels)
            except NoChannelPolicy:
                continue
            # we need to shift the node pubkey by one towards the destination:
            private_route_nodes = [edge[0] for edge in private_route][1:] + [invoice_pubkey]
            private_route_rest = [edge[1:] for edge in private_route]
            prev_node_id = border_node_pubkey
            for node_pubkey, edge_rest in zip(private_route_nodes, private_route_rest):
                short_channel_id, fee_base_msat, fee_proportional_millionths, cltv_expiry_delta = edge_rest
                short_channel_id = ShortChannelID(short_channel_id)
                # if we have a routing policy for this edge in the db, that takes precedence,
                # as it is likely from a previous failure
                channel_policy = self.channel_db.get_policy_for_node(short_channel_id=short_channel_id,
                                                                     node_id=prev_node_id,
                                                                     my_channels=scid_to_my_channels)
                if channel_policy:
                    fee_base_msat = channel_policy.fee_base_msat
                    fee_proportional_millionths = channel_policy.fee_proportional_millionths
                    cltv_expiry_delta = channel_policy.cltv_expiry_delta
                node_info = self.channel_db.get_node_info_for_node_id(node_id=node_pubkey)
                route.append(RouteEdge(node_id=node_pubkey,
                                       short_channel_id=short_channel_id,
                                       fee_base_msat=fee_base_msat,
                                       fee_proportional_millionths=fee_proportional_millionths,
                                       cltv_expiry_delta=cltv_expiry_delta,
                                       node_features=node_info.features if node_info else 0))
                prev_node_id = node_pubkey
            # test sanity
            if not is_route_sane_to_use(route, amount_msat, decoded_invoice.get_min_final_cltv_expiry()):
                self.logger.info(f""rejecting insane route {route}"")
                route = None
                continue
            break
        # if could not find route using any hint; try without hint now
        if route is None:
            if full_path:  # user pre-selected path
                path = full_path
            else:  # find path now
                path = self.network.path_finder.find_path_for_payment(self.node_keypair.pubkey, invoice_pubkey, amount_msat,
                                                                      my_channels=scid_to_my_channels)
            if not path:
                raise NoPathFound()
            route = self.network.path_finder.create_route_from_path(path, self.node_keypair.pubkey,
                                                                    my_channels=scid_to_my_channels)
            if not is_route_sane_to_use(route, amount_msat, decoded_invoice.get_min_final_cltv_expiry()):
                self.logger.info(f""rejecting insane route {route}"")
                raise NoPathFound()
        assert len(route) > 0
        if route[-1].node_id != invoice_pubkey:
            raise LNPathInconsistent(""last node_id != invoice pubkey"")
        # add features from invoice
        invoice_features = decoded_invoice.get_tag('9') or 0
        route[-1].node_features |= invoice_features
        return route

    def add_request(self, amount_sat, message, expiry) -> str:
        coro = self._add_request_coro(amount_sat, message, expiry)
        fut = asyncio.run_coroutine_threadsafe(coro, self.network.asyncio_loop)
        try:
            return fut.result(timeout=5)
        except concurrent.futures.TimeoutError:
            raise Exception(_(""add invoice timed out""))

    @log_exceptions
    async def create_invoice(self, amount_sat: Optional[int], message, expiry: int):
        timestamp = int(time.time())
        routing_hints = await self._calc_routing_hints_for_invoice(amount_sat)
        if not routing_hints:
            self.logger.info(""Warning. No routing hints added to invoice. ""
                             ""Other clients will likely not be able to send to us."")
        payment_preimage = os.urandom(32)
        payment_hash = sha256(payment_preimage)
        info = PaymentInfo(payment_hash, amount_sat, RECEIVED, PR_UNPAID)
        amount_btc = amount_sat/Decimal(COIN) if amount_sat else None
        if expiry == 0:
            expiry = LN_EXPIRY_NEVER
        lnaddr = LnAddr(paymenthash=payment_hash,
                        amount=amount_btc,
                        tags=[('d', message),
                              ('c', MIN_FINAL_CLTV_EXPIRY_FOR_INVOICE),
                              ('x', expiry),
                              ('9', self.features.for_invoice())]
                        + routing_hints,
                        date=timestamp,
                        payment_secret=derive_payment_secret_from_payment_preimage(payment_preimage))
        invoice = lnencode(lnaddr, self.node_keypair.privkey)
        self.save_preimage(payment_hash, payment_preimage)
        self.save_payment_info(info)
        return lnaddr, invoice

    async def _add_request_coro(self, amount_sat: Optional[int], message, expiry: int) -> str:
        lnaddr, invoice = await self.create_invoice(amount_sat, message, expiry)
        key = bh2u(lnaddr.paymenthash)
        req = LNInvoice.from_bech32(invoice)
        self.wallet.add_payment_request(req)
        self.wallet.set_label(key, message)
        return key

    def save_preimage(self, payment_hash: bytes, preimage: bytes):
        assert sha256(preimage) == payment_hash
        self.preimages[bh2u(payment_hash)] = bh2u(preimage)
        self.wallet.save_db()

    def get_preimage(self, payment_hash: bytes) -> Optional[bytes]:
        r = self.preimages.get(bh2u(payment_hash))
        return bfh(r) if r else None

    def get_payment_info(self, payment_hash: bytes) -> Optional[PaymentInfo]:
        """"""returns None if payment_hash is a payment we are forwarding""""""
        key = payment_hash.hex()
        with self.lock:
            if key in self.payments:
                amount, direction, status = self.payments[key]
                return PaymentInfo(payment_hash, amount, direction, status)

    def save_payment_info(self, info: PaymentInfo) -> None:
        key = info.payment_hash.hex()
        assert info.status in SAVED_PR_STATUS
        with self.lock:
            self.payments[key] = info.amount, info.direction, info.status
        self.wallet.save_db()

    def get_payment_status(self, payment_hash):
        info = self.get_payment_info(payment_hash)
        return info.status if info else PR_UNPAID

    def get_invoice_status(self, invoice):
        key = invoice.rhash
        log = self.logs[key]
        if key in self.is_routing:
            return PR_ROUTING
        # status may be PR_FAILED
        status = self.get_payment_status(bfh(key))
        if status == PR_UNPAID and log:
            status = PR_FAILED
        return status

    def set_invoice_status(self, key, status):
        if status == PR_ROUTING:
            self.is_routing.add(key)
        elif key in self.is_routing:
            self.is_routing.remove(key)
        if status in SAVED_PR_STATUS:
            self.set_payment_status(bfh(key), status)

    async def await_payment(self, payment_hash: bytes) -> BarePaymentAttemptLog:
        payment_attempt = await self.pending_payments[payment_hash]
        self.pending_payments.pop(payment_hash)
        return payment_attempt

    def set_payment_status(self, payment_hash: bytes, status):
        info = self.get_payment_info(payment_hash)
        if info is None:
            # if we are forwarding
            return
        info = info._replace(status=status)
        self.save_payment_info(info)

    def payment_failed(self, chan, payment_hash: bytes, error_bytes: bytes, failure_message):
        self.set_payment_status(payment_hash, PR_UNPAID)
        f = self.pending_payments.get(payment_hash)
        if f and not f.cancelled():
            payment_attempt = BarePaymentAttemptLog(
                success=False,
                error_bytes=error_bytes,
                failure_message=failure_message)
            f.set_result(payment_attempt)
        else:
            chan.logger.info('received unexpected payment_failed, probably from previous session')
            key = payment_hash.hex()
            util.trigger_callback('invoice_status', self.wallet, key)
            util.trigger_callback('payment_failed', self.wallet, key, '')
        util.trigger_callback('ln_payment_failed', payment_hash, chan.channel_id)

    def payment_sent(self, chan, payment_hash: bytes):
        self.set_payment_status(payment_hash, PR_PAID)
        preimage = self.get_preimage(payment_hash)
        f = self.pending_payments.get(payment_hash)
        if f and not f.cancelled():
            payment_attempt = BarePaymentAttemptLog(
                success=True,
                preimage=preimage)
            f.set_result(payment_attempt)
        else:
            chan.logger.info('received unexpected payment_sent, probably from previous session')
            key = payment_hash.hex()
            util.trigger_callback('invoice_status', self.wallet, key)
            util.trigger_callback('payment_succeeded', self.wallet, key)
        util.trigger_callback('ln_payment_completed', payment_hash, chan.channel_id)

    def payment_received(self, chan, payment_hash: bytes):
        self.set_payment_status(payment_hash, PR_PAID)
        util.trigger_callback('request_status', self.wallet, payment_hash.hex(), PR_PAID)
        util.trigger_callback('ln_payment_completed', payment_hash, chan.channel_id)

    async def _calc_routing_hints_for_invoice(self, amount_sat: Optional[int]):
        """"""calculate routing hints (BOLT-11 'r' field)""""""
        routing_hints = []
        channels = list(self.channels.values())
        random.shuffle(channels)  # not sure this has any benefit but let's not leak channel order
        scid_to_my_channels = {chan.short_channel_id: chan for chan in channels
                               if chan.short_channel_id is not None}
        if amount_sat:
            amount_msat = 1000 * amount_sat
        else:
            # for no amt invoices, check if channel can receive at least 1 msat
            amount_msat = 1
        # note: currently we add *all* our channels; but this might be a privacy leak?
        for chan in channels:
            # do minimal filtering of channels.
            # we include channels that cannot *right now* receive (e.g. peer disconnected or balance insufficient)
            if not (chan.is_open() and not chan.is_frozen_for_receiving()):
                continue
            if amount_msat > 1000 * chan.constraints.capacity:
                continue
            chan_id = chan.short_channel_id
            assert isinstance(chan_id, bytes), chan_id
            channel_info = self.channel_db.get_channel_info(chan_id, my_channels=scid_to_my_channels)
            # note: as a fallback, if we don't have a channel update for the
            # incoming direction of our private channel, we fill the invoice with garbage.
            # the sender should still be able to pay us, but will incur an extra round trip
            # (they will get the channel update from the onion error)
            # at least, that's the theory. https://github.com/lightningnetwork/lnd/issues/2066
            fee_base_msat = fee_proportional_millionths = 0
            cltv_expiry_delta = 1  # lnd won't even try with zero
            missing_info = True
            if channel_info:
                policy = self.channel_db.get_policy_for_node(channel_info.short_channel_id, chan.node_id,
                                                             my_channels=scid_to_my_channels)
                if policy:
                    fee_base_msat = policy.fee_base_msat
                    fee_proportional_millionths = policy.fee_proportional_millionths
                    cltv_expiry_delta = policy.cltv_expiry_delta
                    missing_info = False
            if missing_info:
                self.logger.info(f""Warning. Missing channel update for our channel {chan_id}; ""
                                 f""filling invoice with incorrect data."")
            routing_hints.append(('r', [(chan.node_id,
                                         chan_id,
                                         fee_base_msat,
                                         fee_proportional_millionths,
                                         cltv_expiry_delta)]))
        return routing_hints

    def delete_payment(self, payment_hash_hex: str):
        try:
            with self.lock:
                del self.payments[payment_hash_hex]
        except KeyError:
            return
        self.wallet.save_db()

    def get_balance(self):
        with self.lock:
            return Decimal(sum(chan.balance(LOCAL) if not chan.is_closed() else 0
                               for chan in self.channels.values())) / 1000

    def num_sats_can_send(self) -> Union[Decimal, int]:
        with self.lock:
            return Decimal(max(chan.available_to_spend(LOCAL) if chan.is_open() else 0
                               for chan in self.channels.values()))/1000 if self.channels else 0

    def num_sats_can_receive(self) -> Union[Decimal, int]:
        with self.lock:
            return Decimal(max(chan.available_to_spend(REMOTE) if chan.is_open() else 0
                               for chan in self.channels.values()))/1000 if self.channels else 0

    def can_pay_invoice(self, invoice: LNInvoice) -> bool:
        return invoice.get_amount_sat() <= self.num_sats_can_send()

    def can_receive_invoice(self, invoice: LNInvoice) -> bool:
        return invoice.get_amount_sat() <= self.num_sats_can_receive()

    async def close_channel(self, chan_id):
        chan = self._channels[chan_id]
        peer = self._peers[chan.node_id]
        return await peer.close_channel(chan_id)

    async def force_close_channel(self, chan_id):
        # returns txid or raises
        chan = self._channels[chan_id]
        tx = chan.force_close_tx()
        await self.network.broadcast_transaction(tx)
        chan.set_state(ChannelState.FORCE_CLOSING)
        return tx.txid()

    async def try_force_closing(self, chan_id):
        # fails silently but sets the state, so that we will retry later
        chan = self._channels[chan_id]
        tx = chan.force_close_tx()
        chan.set_state(ChannelState.FORCE_CLOSING)
        await self.network.try_broadcasting(tx, 'force-close')

    def remove_channel(self, chan_id):
        chan = self._channels[chan_id]
        assert chan.get_state() == ChannelState.REDEEMED
        with self.lock:
            self._channels.pop(chan_id)
            self.db.get('channels').pop(chan_id.hex())
        for addr in chan.get_wallet_addresses_channel_might_want_reserved():
            self.wallet.set_reserved_state_of_address(addr, reserved=False)

        util.trigger_callback('channels_updated', self.wallet)
        util.trigger_callback('wallet_updated', self.wallet)

    @ignore_exceptions
    @log_exceptions
    async def reestablish_peer_for_given_channel(self, chan: Channel) -> None:
        now = time.time()
        peer_addresses = []
        # will try last good address first, from gossip
        last_good_addr = self.channel_db.get_last_good_address(chan.node_id)
        if last_good_addr:
            peer_addresses.append(last_good_addr)
        # will try addresses for node_id from gossip
        addrs_from_gossip = self.channel_db.get_node_addresses(chan.node_id) or []
        for host, port, ts in addrs_from_gossip:
            peer_addresses.append(LNPeerAddr(host, port, chan.node_id))
        # will try addresses stored in channel storage
        peer_addresses += list(chan.get_peer_addresses())
        # Done gathering addresses.
        # Now select first one that has not failed recently.
        for peer in peer_addresses:
            if self._can_retry_addr(peer, urgent=True, now=now):
                await self._add_peer(peer.host, peer.port, peer.pubkey)
                return

    async def reestablish_peers_and_channels(self):
        while True:
            await asyncio.sleep(1)
            for chan in self.channels.values():
                if chan.is_closed():
                    continue
                # reestablish
                if not chan.should_try_to_reestablish_peer():
                    continue
                peer = self._peers.get(chan.node_id, None)
                if peer:
                    await peer.taskgroup.spawn(peer.reestablish_channel(chan))
                else:
                    await self.taskgroup.spawn(self.reestablish_peer_for_given_channel(chan))

    def current_feerate_per_kw(self):
        from .simple_config import FEE_LN_ETA_TARGET, FEERATE_FALLBACK_STATIC_FEE, FEERATE_REGTEST_HARDCODED
        if constants.net is constants.BitcoinRegtest:
            return FEERATE_REGTEST_HARDCODED // 4
        feerate_per_kvbyte = self.network.config.eta_target_to_fee(FEE_LN_ETA_TARGET)
        if feerate_per_kvbyte is None:
            feerate_per_kvbyte = FEERATE_FALLBACK_STATIC_FEE
        return max(253, feerate_per_kvbyte // 4)

    def create_channel_backup(self, channel_id):
        chan = self._channels[channel_id]
        # do not backup old-style channels
        assert chan.is_static_remotekey_enabled()
        peer_addresses = list(chan.get_peer_addresses())
        peer_addr = peer_addresses[0]
        return ChannelBackupStorage(
            node_id = chan.node_id,
            privkey = self.node_keypair.privkey,
            funding_txid = chan.funding_outpoint.txid,
            funding_index = chan.funding_outpoint.output_index,","this looks wrong -- `data_loaded` is an asyncio.Event, not a bool","### Review of Code Changes

#### Key Points:
1. **Class Structure**: The `LNWorker` class is extended to `LNWallet` and `LNGossip`, each with their own methods and properties.
2. **Feature Flags**: Both classes have feature flags set, with `LNWallet` requiring `DATA_LOSS_PROTECT_REQ` and `STATIC_REMOTEKEY_REQ`.
3. **Payment Handling**: The `LNWallet` class includes methods for creating and paying invoices, handling payments, and managing channel states.
4. **Channel Management**: Methods for opening, closing, and managing channels are provided.
5. **Routing and Path Finding**: The `LNWallet` class uses a path finder to determine payment routes.
6. **Logging and Callbacks**: Logging and callback mechanisms are implemented to track various events.

#### Specific Changes:
- **LNWallet Class**:
  - Added methods for creating and paying invoices.
  - Implemented logic for managing payment history and balances.
  - Added support for channel backups and reestablishing connections.
  
- **LNGossip Class**:
  - Maintains a database of known nodes and periodically queries them for updates.
  - Implements logging and callback mechanisms for gossip-related events.

#### Potential Improvements:
1. **Error Handling**: Enhance error handling in critical sections to prevent crashes.
2. **Performance Optimization**: Optimize performance, especially in path finding and channel management.
3. **Security**: Ensure secure handling of sensitive information such as private keys and payment secrets.
4. **Testing**: Add comprehensive unit and integration tests to cover all functionalities.

Overall, the code provides a robust framework for managing Lightning Network operations, including payment processing, channel management, and gossip synchronization."
6,"@@ -0,0 +1,11 @@
+class MoveClientAdminRole < ActiveRecord::Migration
+  def change
+    role = Role.find_or_create_by(name: 'client_admin')
+
+    User.client_admin_emails.each do |email_address|
+      user = User.find_by_email_address(email_address) or next
+      user.add_role(role)
+      user.save!
+    end
+  end
+end","class MoveClientAdminRole < ActiveRecord::Migration
  def change
    role = Role.find_or_create_by(name: 'client_admin')

    User.client_admin_emails.each do |email_address|
      user = User.find_by_email_address(email_address) or next
      user.add_role(role)
      user.save!
    end
  end
end

","We should avoid using the `.client_admin_emails` method here, since it will presumably be removed and thus break this migration in the future.","The suggested code change is to add the `client_admin` role to all users with client admin emails. The code first finds or creates the `client_admin` role using `Role.find_or_create_by(name: 'client_admin')`. Then, it iterates over each email address in `User.client_admin_emails`, finds the corresponding user using `User.find_by_email_address(email_address)`, adds the `client_admin` role to the user using `user.add_role(role)`, and saves the user using `user.save!`. This ensures that all users with client admin emails have the `client_admin` role assigned to them."
7,"@@ -29,6 +29,7 @@ class ServiceProvider extends ModuleServiceProvider
         $this->registerComponents();
         $this->registerThemeLogging();
         $this->registerCombinerEvents();
+        $this->registerDatabaseLayerModels();
 
         /*
          * Backend specific","<?php namespace Cms;

use App;
use Event;
use Backend;
use BackendMenu;
use BackendAuth;
use Backend\Models\UserRole;
use Backend\Classes\WidgetManager;
use October\Rain\Support\ModuleServiceProvider;
use System\Classes\SettingsManager;
use Cms\Classes\ComponentManager;
use Cms\Classes\Page as CmsPage;
use Cms\Classes\CmsObject;
use Cms\Models\ThemeData;
use Cms\Models\ThemeLog;

class ServiceProvider extends ModuleServiceProvider
{
    /**
     * Register the service provider.
     *
     * @return void
     */
    public function register()
    {
        parent::register('cms');

        $this->registerDatabaseLayerModels();
        $this->registerComponents();
        $this->registerThemeLogging();
        $this->registerCombinerEvents();

        /*
         * Backend specific
         */
        if (App::runningInBackend()) {
            $this->registerBackendNavigation();
            $this->registerBackendReportWidgets();
            $this->registerBackendPermissions();
            $this->registerBackendWidgets();
            $this->registerBackendSettings();
        }
    }

    /**
     * Bootstrap the module events.
     *
     * @return void
     */
    public function boot()
    {
        parent::boot('cms');

        $this->bootMenuItemEvents();
        $this->bootRichEditorEvents();
    }

    /**
     * Register components.
     */
    protected function registerComponents()
    {
        ComponentManager::instance()->registerComponents(function ($manager) {
            $manager->registerComponent(\Cms\Components\ViewBag::class, 'viewBag');
            $manager->registerComponent(\Cms\Components\Resources::class, 'resources');
        });
    }

    /**
     * Registers theme logging on templates.
     */
    protected function registerThemeLogging()
    {
        CmsObject::extend(function ($model) {
            ThemeLog::bindEventsToModel($model);
        });
    }

    /**
     * Registers events for the asset combiner.
     */
    protected function registerCombinerEvents()
    {
        if (App::runningInBackend() || App::runningInConsole()) {
            return;
        }

        Event::listen('cms.combiner.beforePrepare', function ($combiner, $assets) {
            $filters = array_flatten($combiner->getFilters());
            ThemeData::applyAssetVariablesToCombinerFilters($filters);
        });

        Event::listen('cms.combiner.getCacheKey', function ($combiner, $holder) {
            $holder->key = $holder->key . ThemeData::getCombinerCacheKey();
        });
    }

    /*
     * Register navigation
     */
    protected function registerBackendNavigation()
    {
        BackendMenu::registerCallback(function ($manager) {
            $manager->registerMenuItems('October.Cms', [
                'cms' => [
                    'label'       => 'cms::lang.cms.menu_label',
                    'icon'        => 'icon-magic',
                    'iconSvg'     => 'modules/cms/assets/images/cms-icon.svg',
                    'url'         => Backend::url('cms'),
                    'order'       => 100,
                    'permissions' => [
                        'cms.manage_content',
                        'cms.manage_assets',
                        'cms.manage_pages',
                        'cms.manage_layouts',
                        'cms.manage_partials'
                    ],
                    'sideMenu' => [
                        'pages' => [
                            'label'        => 'cms::lang.page.menu_label',
                            'icon'         => 'icon-copy',
                            'url'          => 'javascript:;',
                            'attributes'   => ['data-menu-item' => 'pages'],
                            'permissions'  => ['cms.manage_pages'],
                            'counterLabel' => 'cms::lang.page.unsaved_label'
                        ],
                        'partials' => [
                            'label'        => 'cms::lang.partial.menu_label',
                            'icon'         => 'icon-tags',
                            'url'          => 'javascript:;',
                            'attributes'   => ['data-menu-item' => 'partials'],
                            'permissions'  => ['cms.manage_partials'],
                            'counterLabel' => 'cms::lang.partial.unsaved_label'
                        ],
                        'layouts' => [
                            'label'        => 'cms::lang.layout.menu_label',
                            'icon'         => 'icon-th-large',
                            'url'          => 'javascript:;',
                            'attributes'   => ['data-menu-item' => 'layouts'],
                            'permissions'  => ['cms.manage_layouts'],
                            'counterLabel' => 'cms::lang.layout.unsaved_label'
                        ],
                        'content' => [
                            'label'        => 'cms::lang.content.menu_label',
                            'icon'         => 'icon-file-text-o',
                            'url'          => 'javascript:;',
                            'attributes'   => ['data-menu-item' => 'content'],
                            'permissions'  => ['cms.manage_content'],
                            'counterLabel' => 'cms::lang.content.unsaved_label'
                        ],
                        'assets' => [
                            'label'        => 'cms::lang.asset.menu_label',
                            'icon'         => 'icon-picture-o',
                            'url'          => 'javascript:;',
                            'attributes'   => ['data-menu-item' => 'assets'],
                            'permissions'  => ['cms.manage_assets'],
                            'counterLabel' => 'cms::lang.asset.unsaved_label'
                        ],
                        'components' => [
                            'label'       => 'cms::lang.component.menu_label',
                            'icon'        => 'icon-puzzle-piece',
                            'url'         => 'javascript:;',
                            'attributes'  => ['data-menu-item' => 'components'],
                            'permissions' => ['cms.manage_pages', 'cms.manage_layouts', 'cms.manage_partials']
                        ]
                    ]
                ]
            ]);
        });
    }

    /*
     * Register report widgets
     */
    protected function registerBackendReportWidgets()
    {
        WidgetManager::instance()->registerReportWidgets(function ($manager) {
            $manager->registerReportWidget(\Cms\ReportWidgets\ActiveTheme::class, [
                'label'   => 'cms::lang.dashboard.active_theme.widget_title_default',
                'context' => 'dashboard'
            ]);
        });
    }

    /*
     * Register permissions
     */
    protected function registerBackendPermissions()
    {
        BackendAuth::registerCallback(function ($manager) {
            $manager->registerPermissions('October.Cms', [
                'cms.manage_content' => [
                    'label' => 'cms::lang.permissions.manage_content',
                    'tab' => 'cms::lang.permissions.name',
                    'roles' => UserRole::CODE_DEVELOPER,
                    'order' => 100
                ],
                'cms.manage_assets' => [
                    'label' => 'cms::lang.permissions.manage_assets',
                    'tab' => 'cms::lang.permissions.name',
                    'roles' => UserRole::CODE_DEVELOPER,
                    'order' => 100
                ],
                'cms.manage_pages' => [
                    'label' => 'cms::lang.permissions.manage_pages',
                    'tab' => 'cms::lang.permissions.name',
                    'roles' => UserRole::CODE_DEVELOPER,
                    'order' => 100
                ],
                'cms.manage_layouts' => [
                    'label' => 'cms::lang.permissions.manage_layouts',
                    'tab' => 'cms::lang.permissions.name',
                    'roles' => UserRole::CODE_DEVELOPER,
                    'order' => 100
                ],
                'cms.manage_partials' => [
                    'label' => 'cms::lang.permissions.manage_partials',
                    'tab' => 'cms::lang.permissions.name',
                    'roles' => UserRole::CODE_DEVELOPER,
                    'order' => 100
                ],
                'cms.manage_themes' => [
                    'label' => 'cms::lang.permissions.manage_themes',
                    'tab' => 'cms::lang.permissions.name',
                    'roles' => UserRole::CODE_DEVELOPER,
                    'order' => 100
                ],
                'cms.manage_theme_options' => [
                    'label' => 'cms::lang.permissions.manage_theme_options',
                    'tab' => 'cms::lang.permissions.name',
                    'order' => 100
                ],
            ]);
        });
    }

    /*
     * Register widgets
     */
    protected function registerBackendWidgets()
    {
        WidgetManager::instance()->registerFormWidgets(function ($manager) {
            $manager->registerFormWidget(FormWidgets\Components::class);
        });
    }

    /*
     * Register settings
     */
    protected function registerBackendSettings()
    {
        SettingsManager::instance()->registerCallback(function ($manager) {
            $manager->registerSettingItems('October.Cms', [
                'theme' => [
                    'label'       => 'cms::lang.theme.settings_menu',
                    'description' => 'cms::lang.theme.settings_menu_description',
                    'category'    => SettingsManager::CATEGORY_CMS,
                    'icon'        => 'icon-picture-o',
                    'url'         => Backend::url('cms/themes'),
                    'permissions' => ['cms.manage_themes', 'cms.manage_theme_options'],
                    'order'       => 200
                ],
                'maintenance_settings' => [
                    'label'       => 'cms::lang.maintenance.settings_menu',
                    'description' => 'cms::lang.maintenance.settings_menu_description',
                    'category'    => SettingsManager::CATEGORY_CMS,
                    'icon'        => 'icon-plug',
                    'class'       => Models\MaintenanceSetting::class,
                    'permissions' => ['cms.manage_themes'],
                    'order'       => 300
                ],
                'theme_logs' => [
                    'label'       => 'cms::lang.theme_log.menu_label',
                    'description' => 'cms::lang.theme_log.menu_description',
                    'category'    => SettingsManager::CATEGORY_LOGS,
                    'icon'        => 'icon-magic',
                    'url'         => Backend::url('cms/themelogs'),
                    'permissions' => ['system.access_logs'],
                    'order'       => 910,
                    'keywords'    => 'theme change log'
                ]
            ]);
        });
    }

    /**
     * Registers events for menu items.
     */
    protected function bootMenuItemEvents()
    {
        Event::listen('pages.menuitem.listTypes', function () {
            return [
                'cms-page' => 'cms::lang.page.cms_page'
            ];
        });

        Event::listen('pages.menuitem.getTypeInfo', function ($type) {
            if ($type === 'cms-page') {
                return CmsPage::getMenuTypeInfo($type);
            }
        });

        Event::listen('pages.menuitem.resolveItem', function ($type, $item, $url, $theme) {
            if ($type === 'cms-page') {
                return CmsPage::resolveMenuItem($item, $url, $theme);
            }
        });
    }

    /**
     * Registers events for rich editor page links.
     */
    protected function bootRichEditorEvents()
    {
        Event::listen('backend.richeditor.listTypes', function () {
            return [
                'cms-page' => 'cms::lang.page.cms_page'
            ];
        });

        Event::listen('backend.richeditor.getTypeInfo', function ($type) {
            if ($type === 'cms-page') {
                return CmsPage::getRichEditorTypeInfo($type);
            }
        });
    }
}
",Can you rename this to `registerHalcyonModels` @bennothommo ?,"The provided PHP code is a service provider class for the October CMS framework, which is responsible for registering various components, services, and configurations related to the CMS. Here's a concise review of the suggested code changes:

### Key Points:
1. **Module Registration**: The `register` method initializes the module by calling `parent::register('cms')`, which sets up the basic module registration.

2. **Database Layer Models**: The `registerDatabaseLayerModels` method registers database models used by the CMS.

3. **Components**: The `registerComponents` method registers custom components using `ComponentManager`.

4. **Theme Logging**: The `registerThemeLogging` method binds theme logging events to CMS objects.

5. **Asset Combiner Events**: The `registerCombinerEvents` method listens for events related to the asset combiner, applying theme variables to filters and updating cache keys.

6. **Backend Navigation**: The `registerBackendNavigation` method registers backend navigation items and their associated permissions.

7. **Report Widgets**: The `registerBackendReportWidgets` method registers report widgets for the dashboard.

8. **Permissions**: The `registerBackendPermissions` method registers backend permissions for various CMS functionalities.

9. **Widgets**: The `registerBackendWidgets` method registers form widgets.

10. **Settings**: The `registerBackendSettings` method registers backend settings items.

11. **Menu Item Events**: The `bootMenuItemEvents` method listens for events related to menu item types and resolves menu items.

12. **Rich Editor Events**: The `bootRichEditorEvents` method listens for events related to rich editor page links.

### Review:
- **Consistency**: The code is well-organized with clear separation of concerns, making it easy to understand and maintain.
- **Comments**: There are comments explaining each method and its purpose, which helps in understanding the flow of the code.
- **Event Listeners**: The event listeners are correctly registered and handle specific events related to the CMS functionality.
- **Permissions**: The permissions are registered with appropriate roles, ensuring that only authorized users can access certain features.

Overall, the code is clean, efficient, and follows best practices for a CMS service provider in October CMS."
8,"@@ -37,6 +37,15 @@ public final class Const {
 
     public static final String V4_PREFIX = String.format(""/v4/%s/registry"", DOMAIN_NAME);
 
+    public static final String REGISTRY_VERSION;
+    static {
+      if (VERSION_V3.equals(CURRENT_VERSION)) {
+        REGISTRY_VERSION = ""3.0.0""; // old is strange
+      } else {
+        REGISTRY_VERSION = ""0.4.0"";
+      }
+    }
+
     public static final String MICROSERVICE_OPERATION_ALL;
     static {
       if (VERSION_V3.equals(CURRENT_VERSION)) {","/*
 * Copyright 2017 Huawei Technologies Co., Ltd
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package io.servicecomb.serviceregistry.api;

import io.servicecomb.serviceregistry.config.ServiceRegistryConfig;

/**
 * Created by   on 2017/1/9.
 */
public final class Const {
  private Const() {
  }

  public static final class REGISTRY_API {
    public static final String DOMAIN_NAME = ServiceRegistryConfig.INSTANCE.getDomainName();

    public static final String CURRENT_VERSION = ServiceRegistryConfig.INSTANCE.getRegistryApiVersion();

    // 2017-10-21 add new implementations for v4. We can remove v3 support after a period. 
    public static final String VERSION_V3 = ""v3"";

    public static final String LASTEST_API_VERSION = ""v4"";
    public static final String REGISTRY_VERSION;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        REGISTRY_VERSION = ""3.0.0""; // old is strange
      } else {
        REGISTRY_VERSION = ""0.4.0"";
      }
    }


    public static final String V4_PREFIX = String.format(""/v4/%s/registry"", DOMAIN_NAME);

    public static final String MICROSERVICE_OPERATION_ALL;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_OPERATION_ALL = ""/registry/v3/microservices"";
      } else {
        MICROSERVICE_OPERATION_ALL = V4_PREFIX + ""/microservices"";
      }
    }

    public static final String MICROSERVICE_OPERATION_ONE;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_OPERATION_ONE = ""/registry/v3/microservices/%s"";
      } else {
        MICROSERVICE_OPERATION_ONE = V4_PREFIX + ""/microservices/%s"";
      }
    }

    public static final String MICROSERVICE_INSTANCE_OPERATION_ALL;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_INSTANCE_OPERATION_ALL = ""/registry/v3/microservices/%s/instances"";
      } else {
        MICROSERVICE_INSTANCE_OPERATION_ALL = V4_PREFIX + ""/microservices/%s/instances"";
      }
    }

    public static final String MICROSERVICE_INSTANCE_OPERATION_ONE;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_INSTANCE_OPERATION_ONE = ""/registry/v3/microservices/%s/instances/%s"";
      } else {
        MICROSERVICE_INSTANCE_OPERATION_ONE = V4_PREFIX + ""/microservices/%s/instances/%s"";
      }
    }

    public static final String MICROSERVICE_INSTANCES;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_INSTANCES = ""/registry/v3/instances"";
      } else {
        MICROSERVICE_INSTANCES = V4_PREFIX + ""/instances"";
      }
    }

    public static final String MICROSERVICE_PROPERTIES;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_PROPERTIES = ""/registry/v3/microservices/%s/properties"";
      } else {
        MICROSERVICE_PROPERTIES = V4_PREFIX + ""/microservices/%s/properties"";
      }
    }

    public static final String MICROSERVICE_INSTANCE_PROPERTIES;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_INSTANCE_PROPERTIES = ""/registry/v3/microservices/%s/instances/%s/properties"";
      } else {
        MICROSERVICE_INSTANCE_PROPERTIES = V4_PREFIX + ""/microservices/%s/instances/%s/properties"";
      }
    }

    public static final String MICROSERVICE_HEARTBEAT;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_HEARTBEAT = ""/registry/v3/microservices/%s/instances/%s/heartbeat"";
      } else {
        MICROSERVICE_HEARTBEAT = V4_PREFIX + ""/microservices/%s/instances/%s/heartbeat"";
      }
    }

    public static final String MICROSERVICE_EXISTENCE;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_EXISTENCE = ""/registry/v3/existence"";
      } else {
        MICROSERVICE_EXISTENCE = V4_PREFIX + ""/existence"";
      }
    }

    public static final String MICROSERVICE_SCHEMA;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_SCHEMA = ""/registry/v3/microservices/%s/schemas/%s"";
      } else {
        MICROSERVICE_SCHEMA = V4_PREFIX + ""/microservices/%s/schemas/%s"";
      }
    }

    public static final String MICROSERVICE_WATCH;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_WATCH = ""/registry/v3/microservices/%s/watcher"";
      } else {
        MICROSERVICE_WATCH = V4_PREFIX + ""/microservices/%s/watcher"";
      }
    }

  }

  public static final String REGISTRY_APP_ID = ""default"";

  public static final String REGISTRY_SERVICE_NAME = ""SERVICECENTER"";

  public static final String REGISTRY_VERSION = ""3.0.0"";

  public static final String APP_SERVICE_SEPARATOR = "":"";

  public static final String PATH_CHECKSESSION = ""checksession"";

  public static final String URL_PREFIX = ""urlPrefix"";
}
","I think we just need keep tracking the API version instead of the service center version. For Java chassis, it only knows about the service center version.","The suggested code change appears to be updating the `Const` class to include constants for the new API version ""v4"" and corresponding endpoints. The changes involve modifying the `REGISTRY_API` class to include new fields for the v4 API versions and updating the existing fields to use the new prefix. The `REGISTRY_VERSION` field is also updated to reflect the new version. Overall, the changes appear to be consistent with the overall structure and naming conventions of the codebase."
9,"@@ -72,6 +72,13 @@ public enum Platform {
     }
   },
 
+  WIN10(""windows 10"", ""win10"") {
+    @Override
+    public Platform family() {
+      return WINDOWS;
+    }
+  },
+
   MAC(""mac"", ""darwin"", ""os x"") {},
 
   SNOW_LEOPARD(""snow leopard"", ""os x 10.6"") {","// Licensed to the Software Freedom Conservancy (SFC) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The SFC licenses this file
// to you under the Apache License, Version 2.0 (the
// ""License""); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

package org.openqa.selenium;

import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * Represents the known and supported Platforms that WebDriver runs on. This is pretty close to the
 * Operating System, but differs slightly, because this class is used to extract information such as
 * program locations and line endings.
 */
// Useful URLs:
// http://hg.openjdk.java.net/jdk7/modules/jdk/file/a37326fa7f95/src/windows/native/java/lang/java_props_md.c
public enum Platform {

  /**
   * Never returned, but can be used to request a browser running on any version of Windows.
   */
  WINDOWS("""") {},

  /**
   * For versions of Windows that ""feel like"" Windows XP. These are ones that store files in
   * ""\Program Files\"" and documents under ""\\documents and settings\\username""
   */
  XP(""Windows Server 2003"", ""xp"", ""windows"", ""winnt"") {
    @Override
    public Platform family() {
      return WINDOWS;
    }
  },

  /**
   * For versions of Windows that ""feel like"" Windows Vista.
   */
  VISTA(""windows vista"", ""Windows Server 2008"", ""windows 7"", ""win7"") {
    @Override
    public Platform family() {
      return WINDOWS;
    }
  },

  /**
   * For versions of Windows that ""feel like"" Windows 8.
   */
  WIN8(""Windows Server 2012"", ""windows 8"", ""win8"") {
    @Override
    public Platform family() {
      return WINDOWS;
    }
  },

  WIN8_1(""windows 8.1"", ""win8.1"") {
    @Override
    public Platform family() {
      return WINDOWS;
  WIN10(""windows 10"", ""win10"") {
    @Override
    public Platform family() {
      return WINDOWS;
    }
  },

    }
  },

  MAC(""mac"", ""darwin"", ""os x"") {},

  SNOW_LEOPARD(""snow leopard"", ""os x 10.6"") {
    @Override
    public Platform family() {
      return MAC;
    }
    @Override
    public String toString() {
      return ""OS X 10.6"";
    }
  },

  MOUNTAIN_LION(""mountain lion"", ""os x 10.8"") {
    @Override
    public Platform family() {
      return MAC;
    }
    @Override
    public String toString() {
      return ""OS X 10.8"";
    }
  },

  MAVERICKS(""mavericks"", ""os x 10.9"") {
    @Override
    public Platform family() {
      return MAC;
    }
    @Override
    public String toString() {
      return ""OS X 10.9"";
    }
  },

  YOSEMITE(""yosemite"", ""os x 10.10"") {
    @Override
    public Platform family() {
      return MAC;
    }
    @Override
    public String toString() {
      return ""OS X 10.10"";
    }
  },

  /**
   * Many platforms have UNIX traits, amongst them LINUX, Solaris and BSD.
   */
  UNIX(""solaris"", ""bsd"") {},

  LINUX(""linux"") {
    @Override
    public Platform family() {
      return UNIX;
    }
  },

  ANDROID(""android"", ""dalvik"") {
    public String getLineEnding() {
      return ""\n"";
    }

    @Override
    public Platform family() {
      return LINUX;
    }
  },

  /**
   * Never returned, but can be used to request a browser running on any operating system.
   */
  ANY("""") {
    @Override
    public boolean is(Platform compareWith) {
      return this == compareWith;
    }
  };

  private final String[] partOfOsName;
  private final int minorVersion;
  private final int majorVersion;

  private Platform(String... partOfOsName) {
    this.partOfOsName = partOfOsName;

    String version = System.getProperty(""os.version"", ""0.0.0"");
    int major = 0;
    int min = 0;

    Pattern pattern = Pattern.compile(""^(\\d+)\\.(\\d+).*"");
    Matcher matcher = pattern.matcher(version);
    if (matcher.matches()) {
      try {
        major = Integer.parseInt(matcher.group(1));
        min = Integer.parseInt(matcher.group(2));
      } catch (NumberFormatException e) {
        // These things happen
      }
    }

    majorVersion = major;
    minorVersion = min;
  }

  public String[] getPartOfOsName() {
    return partOfOsName;
  }

  /**
   * Get current platform (not necessarily the same as operating system).
   *
   * @return current platform
   */
  public static Platform getCurrent() {
    return extractFromSysProperty(System.getProperty(""os.name""));
  }

  /**
   * Extracts platforms based on system properties in Java and uses a heuristic to determine the
   * most likely operating system.  If unable to determine the operating system, it will default to
   * UNIX.
   *
   * @param osName the operating system name to determine the platform of
   * @return the most likely platform based on given operating system name
   */
  public static Platform extractFromSysProperty(String osName) {
    return extractFromSysProperty(osName, System.getProperty(""os.version""));
  }

  /**
   * Extracts platforms based on system properties in Java and uses a heuristic to determine the
   * most likely operating system.  If unable to determine the operating system, it will default to
   * UNIX.
   *
   * @param osName the operating system name to determine the platform of
   * @param osVersion the operating system version to determine the platform of
   * @return the most likely platform based on given operating system name and version
   */
  public static Platform extractFromSysProperty(String osName, String osVersion) {
    osName = osName.toLowerCase();
    // os.name for android is linux
    if (""dalvik"".equalsIgnoreCase(System.getProperty(""java.vm.name""))) {
      return Platform.ANDROID;
    }
    // Windows 8 can't be detected by osName alone
    if (osVersion.equals(""6.2"") && osName.startsWith(""windows nt"")) {
        return WIN8;
    }
    // Windows 8 can't be detected by osName alone
    if (osVersion.equals(""6.3"") && osName.startsWith(""windows nt"")) {
        return WIN8_1;
    }
    Platform mostLikely = UNIX;
    String previousMatch = null;
    for (Platform os : Platform.values()) {
      for (String matcher : os.partOfOsName) {
        if ("""".equals(matcher)) {
          continue;
        }
        matcher = matcher.toLowerCase();
        if (os.isExactMatch(osName, matcher)) {
          return os;
        }
        if (os.isCurrentPlatform(osName, matcher) && isBetterMatch(previousMatch, matcher)) {
          previousMatch = matcher;
          mostLikely = os;
        }
      }
    }

    // Default to assuming we're on a UNIX variant (including LINUX)
    return mostLikely;
  }

  /**
   * Gets a platform with the name matching the parameter.
   *
   * @param name the platform name
   * @return the Platform enum value matching the parameter
   */
  public static Platform fromString(String name) {
    try {
      return Platform.valueOf(name);
    } catch (IllegalArgumentException ex) {
      for (Platform os : Platform.values()) {
        for (String matcher : os.partOfOsName) {
          if (name.toLowerCase().equals(matcher.toLowerCase())) {
            return os;
          }
        }
      }
      throw new WebDriverException(""Unrecognized platform: "" + name);
    }
  }

  /**
   * Decides whether the previous match is better or not than the current match.  If previous match
   * is null, the newer match is always better.
   *
   * @param previous the previous match
   * @param matcher the newer match
   * @return true if newer match is better, false otherwise
   */
  private static boolean isBetterMatch(String previous, String matcher) {
    return previous == null || matcher.length() >= previous.length();
  }

  /**
   * Heuristic for comparing two platforms.  If platforms (which is not the same thing as operating
   * systems) are found to be approximately similar in nature, this will return true.  For instance
   * the LINUX platform is similar to UNIX, and will give a positive result if compared.
   *
   * @param compareWith the platform to compare with
   * @return true if platforms are approximately similar, false otherwise
   */
  public boolean is(Platform compareWith) {
    return this == compareWith || this.family().is(compareWith);
  }

  /**
   * Returns a platform that represents a family for the current platform.  For instance
   * the LINUX if a part of the UNIX family, the XP is a part of the WINDOWS family.
   *
   * @return the family platform for the current one
   */
  public Platform family() {
    return ANY;
  }

  private boolean isCurrentPlatform(String osName, String matchAgainst) {
    return osName.contains(matchAgainst);
  }

  private boolean isExactMatch(String osName, String matchAgainst) {
    return matchAgainst.equals(osName);
  }

  /**
   * Returns the major version of this platform.
   *
   * @return the major version of specified platform
   */
  public int getMajorVersion() {
    return majorVersion;
  }

  /**
   * Returns the minor version of this platform.
   *
   * @return the minor version of specified platform
   */
  public int getMinorVersion() {
    return minorVersion;
  }

}
","we'll also have to think about adding ""Windows Server 2014"" or whatever they come up with later.","The provided code defines an `enum` called `Platform` that represents different operating systems and their versions. Here's a concise review of the suggested code changes:

1. **Enum Definition**: The enum is well-defined with clear comments explaining each platform and its characteristics.

2. **Constructor**: The constructor initializes the platform based on the operating system name and version. It uses regular expressions to parse the version string.

3. **Static Methods**:
   - `getCurrent()`: Returns the current platform.
   - `extractFromSysProperty(String osName)`: Extracts the platform based on the operating system name.
   - `extractFromSysProperty(String osName, String osVersion)`: Extracts the platform based on both the operating system name and version.
   - `fromString(String name)`: Converts a string to a platform enum value.
   - `isBetterMatch(String previous, String matcher)`: Determines if a newer match is better than a previous one.
   - `is(Platform compareWith)`: Checks if the current platform is the same as or a member of another platform.
   - `family()`: Returns the family platform for the current one.
   - `isCurrentPlatform(String osName, String matchAgainst)`: Checks if the operating system name contains the match against string.
   - `isExactMatch(String osName, String matchAgainst)`: Checks if the operating system name is exactly the same as the match against string.

4. **Methods**:
   - `getMajorVersion()`: Returns the major version of the platform.
   - `getMinorVersion()`: Returns the minor version of the platform.

Overall, the code is well-structured, easy to understand, and provides a comprehensive way to handle different operating systems and their versions."
10,"@@ -29,7 +29,7 @@ import (
 type Protocol interface {
 	// RunStateTransition returns the state root CID resulting from applying the input ts to the
 	// prior `stateID`.  It returns an error if the transition is invalid.
-	RunStateTransition(ctx context.Context, ts block.TipSet, blsMsgs [][]*types.UnsignedMessage, secpMsgs [][]*types.SignedMessage, ancestors []block.TipSet, parentWeight fbig.Int, parentStateRoot cid.Cid, parentReceiptRoot cid.Cid) (cid.Cid, []vm.MessageReceipt, error)
+	RunStateTransition(ctx context.Context, ts block.TipSet, blsMsgs [][]*types.UnsignedMessage, secpMsgs [][]*types.SignedMessage, parentWeight fbig.Int, parentStateRoot cid.Cid, parentReceiptRoot cid.Cid) (cid.Cid, []vm.MessageReceipt, error)
 
 	// BlockTime returns the block time used by the consensus protocol.
 	BlockTime() time.Duration","package consensus

// This interface is (mostly) stateless.  All of its methods are
// pure functions that only depend on their inputs.

// Note: State does creep in through the cbor and block stores used to keep state tree and
// actor storage data in the Expected implementation.  However those stores
// are global to the filecoin node so accessing the correct state is simple.
// Furthermore these stores are providing content addressed values.
// The output of these interface functions does not change based on the store state
// except for errors in the case the stores do not have a mapping.
import (
	""context""
	""time""

	""github.com/filecoin-project/go-filecoin/internal/pkg/block""
	""github.com/filecoin-project/go-filecoin/internal/pkg/vm""
	fbig ""github.com/filecoin-project/specs-actors/actors/abi/big""
	""github.com/ipfs/go-cid""

	""github.com/filecoin-project/go-filecoin/internal/pkg/types""
)

// Protocol is an interface defining a blockchain consensus protocol.  The
// methods here were arrived at after significant work fitting consensus into
// the system and the implementation level. The method set is not necessarily
// the most theoretically obvious or pleasing and should not be considered
// finalized.
	RunStateTransition(ctx context.Context, ts block.TipSet, blsMsgs [][]*types.UnsignedMessage, secpMsgs [][]*types.SignedMessage, parentWeight fbig.Int, parentStateRoot cid.Cid, parentReceiptRoot cid.Cid) (cid.Cid, []vm.MessageReceipt, error)
type Protocol interface {
	// RunStateTransition returns the state root CID resulting from applying the input ts to the
	// prior `stateID`.  It returns an error if the transition is invalid.
	RunStateTransition(ctx context.Context, ts block.TipSet, blsMsgs [][]*types.UnsignedMessage, secpMsgs [][]*types.SignedMessage, ancestors []block.TipSet, parentWeight fbig.Int, parentStateRoot cid.Cid, parentReceiptRoot cid.Cid) (cid.Cid, []vm.MessageReceipt, error)

	// BlockTime returns the block time used by the consensus protocol.
	BlockTime() time.Duration
}
",FYI I've been meaning to delete this forever. In the future you can get rid of this instead of fixing.,"The suggested code change introduces a new method `BlockTime` to the `Protocol` interface, which returns the block time used by the consensus protocol. This addition enhances the interface's functionality by providing a way to query the block time directly, which can be useful for various purposes such as scheduling tasks or adjusting the behavior of the consensus algorithm based on the block time.

### Review:
- **New Method**: The addition of `BlockTime` is a logical enhancement to the `Protocol` interface, providing a clear and concise way to access the block time.
- **Consistency**: The method signature remains consistent with the existing methods, maintaining the interface's integrity.
- **Use Case**: The method can be used by other parts of the system to determine the block time, which is crucial for scheduling tasks or implementing logic that depends on the block time.
- **Documentation**: The method is well-documented, explaining its purpose and usage, which is important for developers using the interface.

Overall, this change improves the usability and flexibility of the `Protocol` interface by adding a new method that provides essential information about the consensus protocol's block time."
11,"@@ -78,6 +78,15 @@ public class CsrfWebFilter implements WebFilter {
 		this.csrfTokenRepository = csrfTokenRepository;
 	}
 
+	/**
+	 * Gets the {@link ServerCsrfTokenRepository}
+	 * @return The {@link ServerCsrfTokenRepository}
+	 * @since 5.1
+	 */
+	public ServerCsrfTokenRepository getCsrfTokenRepository() {
+		return this.csrfTokenRepository;
+	}
+
 	public void setRequireCsrfProtectionMatcher(
 		ServerWebExchangeMatcher requireCsrfProtectionMatcher) {
 		Assert.notNull(requireCsrfProtectionMatcher, ""requireCsrfProtectionMatcher cannot be null"");","/*
 * Copyright 2002-2017 the original author or authors.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.springframework.security.web.server.csrf;

import java.util.Arrays;
import java.util.HashSet;
import java.util.Set;

import reactor.core.publisher.Mono;

import org.springframework.http.HttpMethod;
import org.springframework.http.HttpStatus;
import org.springframework.security.web.server.authorization.HttpStatusServerAccessDeniedHandler;
import org.springframework.security.web.server.authorization.ServerAccessDeniedHandler;
import org.springframework.security.web.server.util.matcher.ServerWebExchangeMatcher;
import org.springframework.util.Assert;
import org.springframework.web.server.ServerWebExchange;
import org.springframework.web.server.WebFilter;
import org.springframework.web.server.WebFilterChain;

/**
 * <p>
 * Applies
 * <a href=""https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF)"" >CSRF</a>
 * protection using a synchronizer token pattern. Developers are required to ensure that
 * {@link CsrfWebFilter} is invoked for any request that allows state to change. Typically
 * this just means that they should ensure their web application follows proper REST
 * semantics (i.e. do not change state with the HTTP methods GET, HEAD, TRACE, OPTIONS).
 * </p>
 *
 * <p>
 * Typically the {@link ServerCsrfTokenRepository} implementation chooses to store the
 * {@link CsrfToken} in {@link org.springframework.web.server.WebSession} with
 * {@link WebSessionServerCsrfTokenRepository}. This is preferred to storing the token in
 * a cookie which can be modified by a client application.
 * </p>
 * <p>
 * The {@code Mono&lt;CsrfToken&gt;} is exposes as a request attribute with the name of
 * {@code CsrfToken.class.getName()}. If the token is new it will automatically be saved
 * at the time it is subscribed.
 * </p>
 *
 * @author Rob Winch
 * @since 5.0
 */
public class CsrfWebFilter implements WebFilter {
	public static final ServerWebExchangeMatcher DEFAULT_CSRF_MATCHER = new DefaultRequireCsrfProtectionMatcher();

	private ServerWebExchangeMatcher requireCsrfProtectionMatcher = DEFAULT_CSRF_MATCHER;

	private ServerCsrfTokenRepository csrfTokenRepository = new WebSessionServerCsrfTokenRepository();

	private ServerAccessDeniedHandler accessDeniedHandler = new HttpStatusServerAccessDeniedHandler(HttpStatus.FORBIDDEN);

	public void setAccessDeniedHandler(
		ServerAccessDeniedHandler accessDeniedHandler) {
		Assert.notNull(accessDeniedHandler, ""accessDeniedHandler"");
		this.accessDeniedHandler = accessDeniedHandler;
	}

	public void setCsrfTokenRepository(
		ServerCsrfTokenRepository csrfTokenRepository) {
		Assert.notNull(csrfTokenRepository, ""csrfTokenRepository cannot be null"");
	/**
	 * Gets the {@link ServerCsrfTokenRepository}
	 * @return The {@link ServerCsrfTokenRepository}
	 * @since 5.1
	 */
	public ServerCsrfTokenRepository getCsrfTokenRepository() {
		return this.csrfTokenRepository;
	}

		this.csrfTokenRepository = csrfTokenRepository;
	}

	public void setRequireCsrfProtectionMatcher(
		ServerWebExchangeMatcher requireCsrfProtectionMatcher) {
		Assert.notNull(requireCsrfProtectionMatcher, ""requireCsrfProtectionMatcher cannot be null"");
		this.requireCsrfProtectionMatcher = requireCsrfProtectionMatcher;
	}

	@Override
	public Mono<Void> filter(ServerWebExchange exchange, WebFilterChain chain) {
		return this.requireCsrfProtectionMatcher.matches(exchange)
			.filter( matchResult -> matchResult.isMatch())
			.filter( matchResult -> !exchange.getAttributes().containsKey(CsrfToken.class.getName()))
			.flatMap(m -> validateToken(exchange))
			.flatMap(m -> continueFilterChain(exchange, chain))
			.switchIfEmpty(continueFilterChain(exchange, chain).then(Mono.empty()))
			.onErrorResume(CsrfException.class, e -> this.accessDeniedHandler
				.handle(exchange, e));
	}

	private Mono<Void> validateToken(ServerWebExchange exchange) {
		return this.csrfTokenRepository.loadToken(exchange)
			.switchIfEmpty(Mono.defer(() -> Mono.error(new CsrfException(""CSRF Token has been associated to this client""))))
			.filterWhen(expected -> containsValidCsrfToken(exchange, expected))
			.switchIfEmpty(Mono.defer(() -> Mono.error(new CsrfException(""Invalid CSRF Token""))))
			.then();
	}

	private Mono<Boolean> containsValidCsrfToken(ServerWebExchange exchange, CsrfToken expected) {
		return exchange.getFormData()
			.flatMap(data -> Mono.justOrEmpty(data.getFirst(expected.getParameterName())))
			.switchIfEmpty(Mono.justOrEmpty(exchange.getRequest().getHeaders().getFirst(expected.getHeaderName())))
			.map(actual -> actual.equals(expected.getToken()));
	}

	private Mono<Void> continueFilterChain(ServerWebExchange exchange, WebFilterChain chain) {
		return Mono.defer(() ->{
			Mono<CsrfToken> csrfToken = csrfToken(exchange);
			exchange.getAttributes().put(CsrfToken.class.getName(), csrfToken);
			return chain.filter(exchange);
		});
	}

	private Mono<CsrfToken> csrfToken(ServerWebExchange exchange) {
		return this.csrfTokenRepository.loadToken(exchange)
			.switchIfEmpty(generateToken(exchange));
	}

	private Mono<CsrfToken> generateToken(ServerWebExchange exchange) {
		return this.csrfTokenRepository.generateToken(exchange)
			.delayUntil(token -> this.csrfTokenRepository.saveToken(exchange, token));
	}

	private static class DefaultRequireCsrfProtectionMatcher implements ServerWebExchangeMatcher {
		private static final Set<HttpMethod> ALLOWED_METHODS = new HashSet<>(
			Arrays.asList(HttpMethod.GET, HttpMethod.HEAD, HttpMethod.TRACE, HttpMethod.OPTIONS));

		@Override
		public Mono<MatchResult> matches(ServerWebExchange exchange) {
			return Mono.just(exchange.getRequest())
				.map(r -> r.getMethod())
				.filter(m -> ALLOWED_METHODS.contains(m))
				.flatMap(m -> MatchResult.notMatch())
				.switchIfEmpty(MatchResult.match());
		}
	}
}
",We should not add a getter as it breaks encapsulation,"The suggested code change appears to be a complete implementation of a CSRF (Cross-Site Request Forgery) protection filter for Spring Security WebFlux applications. Here's a concise review:

### Key Points:
1. **Class and Package**: The class `CsrfWebFilter` is part of the `org.springframework.security.web.server.csrf` package, indicating its role in handling CSRF protection.

2. **Dependencies**: The class imports necessary classes from Spring Security WebFlux, including `Mono`, `ServerWebExchange`, `WebFilter`, `WebFilterChain`, `HttpStatusServerAccessDeniedHandler`, `DefaultRequireCsrfProtectionMatcher`, and `WebSessionServerCsrfTokenRepository`.

3. **Fields**:
   - `requireCsrfProtectionMatcher`: A matcher that determines if CSRF protection is needed for a given request.
   - `csrfTokenRepository`: The repository used to store and retrieve CSRF tokens.
   - `accessDeniedHandler`: Handles access denied scenarios.

4. **Methods**:
   - `filter`: The main method that processes each request. It checks if CSRF protection is required, validates the token, and continues the filter chain or handles access denied errors.
   - `validateToken`: Loads the CSRF token and checks if it is valid.
   - `containsValidCsrfToken`: Checks if the provided token matches the one stored in the request or session.
   - `continueFilterChain`: Saves the CSRF token in the request attributes and continues the filter chain.
   - `csrfToken`: Retrieves the CSRF token from the repository or generates a new one if none exists.
   - `generateToken`: Generates a new CSRF token and saves it to the repository.

5. **Static Class**: `DefaultRequireCsrfProtectionMatcher` is a simple matcher that allows certain HTTP methods (GET, HEAD, TRACE, OPTIONS) without CSRF protection.

### Summary:
This implementation provides a robust CSRF protection mechanism for Spring Security WebFlux applications. It ensures that only requests that allow state changes are protected, uses a session-based token storage, and provides a custom access denied handler. The code is well-structured and includes comments for clarity."
12,"@@ -166,14 +166,14 @@ class ProductRepositoryTest extends DatabaseTestCase
         $product1 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . 1);
         $product2 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . 45);
 
-        $this->setProductOrderingPriority($product1, 0);
-        $this->setProductOrderingPriority($product2, 1);
+        $this->setProductOrderingPriority($product1, 2);
+        $this->setProductOrderingPriority($product2, 3);
 
         $results = $this->getProductsForSearchOrderedByPriority('sencor');
         $this->assertSame($product2, $results[0]);
         $this->assertSame($product1, $results[1]);
 
-        $this->setProductOrderingPriority($product2, -1);
+        $this->setProductOrderingPriority($product2, 1);
 
         $results = $this->getProductsForSearchOrderedByPriority('sencor');
         $this->assertSame($product1, $results[0]);","<?php

namespace Tests\ShopBundle\Database\Model\Product;

use Shopsys\FrameworkBundle\Component\Domain\Domain;
use Shopsys\FrameworkBundle\DataFixtures\Demo\CategoryDataFixture;
use Shopsys\FrameworkBundle\DataFixtures\Demo\PricingGroupDataFixture;
use Shopsys\FrameworkBundle\DataFixtures\Demo\ProductDataFixture;
use Shopsys\FrameworkBundle\Model\Category\Category;
use Shopsys\FrameworkBundle\Model\Product\Filter\ProductFilterData;
use Shopsys\FrameworkBundle\Model\Product\Listing\ProductListOrderingModeService;
use Shopsys\FrameworkBundle\Model\Product\Product;
use Shopsys\FrameworkBundle\Model\Product\ProductDataFactory;
use Shopsys\FrameworkBundle\Model\Product\ProductFacade;
use Shopsys\FrameworkBundle\Model\Product\ProductRepository;
use Tests\ShopBundle\Test\DatabaseTestCase;

class ProductRepositoryTest extends DatabaseTestCase
{
    public function testVisibleAndNotSellingDeniedProductIsListed()
    {
        $this->getAllListableQueryBuilderTest(1, true);
    }

    public function testVisibleAndSellingDeniedProductIsNotListed()
    {
        $this->getAllListableQueryBuilderTest(6, false);
    }

    public function testProductVariantIsNotListed()
    {
        $this->getAllListableQueryBuilderTest(53, false);
    }

    public function testProductMainVariantIsListed()
    {
        $this->getAllListableQueryBuilderTest(148, true);
    }

    private function getAllListableQueryBuilderTest($productReferenceId, $isExpectedInResult)
    {
        $productRepository = $this->getContainer()->get(ProductRepository::class);
        /* @var $productRepository \Shopsys\FrameworkBundle\Model\Product\ProductRepository */
        $pricingGroup = $this->getReference(PricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1);
        /* @var $pricingGroup \Shopsys\FrameworkBundle\Model\Pricing\Group\PricingGroup */

        $domain = $this->getContainer()->get(Domain::class);
        /* @var $domain \Shopsys\FrameworkBundle\Component\Domain\Domain */

        $product = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . $productReferenceId);
        $productId = $product->getId();

        $queryBuilder = $productRepository->getAllListableQueryBuilder($domain->getId(), $pricingGroup);
        $queryBuilder->andWhere('p.id = :id')
            ->setParameter('id', $productId);
        $result = $queryBuilder->getQuery()->execute();

        $this->assertSame(in_array($product, $result, true), $isExpectedInResult);
    }

    public function testVisibleAndNotSellingDeniedProductIsSellable()
    {
        $this->getAllSellableQueryBuilderTest(1, true);
    }

    public function testVisibleAndSellingDeniedProductIsNotSellable()
    {
        $this->getAllSellableQueryBuilderTest(6, false);
    }

    public function testProductVariantIsSellable()
    {
        $this->getAllSellableQueryBuilderTest(53, true);
    }

    public function testProductMainVariantIsNotSellable()
    {
        $this->getAllSellableQueryBuilderTest(148, false);
    }

    private function getAllSellableQueryBuilderTest($productReferenceId, $isExpectedInResult)
    {
        $productRepository = $this->getContainer()->get(ProductRepository::class);
        /* @var $productRepository \Shopsys\FrameworkBundle\Model\Product\ProductRepository */
        $pricingGroup = $this->getReference(PricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1);
        /* @var $pricingGroup \Shopsys\FrameworkBundle\Model\Pricing\Group\PricingGroup */

        $domain = $this->getContainer()->get(Domain::class);
        /* @var $domain \Shopsys\FrameworkBundle\Component\Domain\Domain */

        $product = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . $productReferenceId);
        $productId = $product->getId();

        $queryBuilder = $productRepository->getAllSellableQueryBuilder($domain->getId(), $pricingGroup);
        $queryBuilder->andWhere('p.id = :id')
            ->setParameter('id', $productId);
        $result = $queryBuilder->getQuery()->execute();

        $this->assertSame(in_array($product, $result, true), $isExpectedInResult);
    }

    public function testVisibleAndNotSellingDeniedProductIsOfferred()
    {
        $this->getAllOfferedQueryBuilderTest(1, true);
    }

    public function testVisibleAndSellingDeniedProductIsNotOfferred()
    {
        $this->getAllOfferedQueryBuilderTest(6, false);
    }

    public function testProductVariantIsOfferred()
    {
        $this->getAllOfferedQueryBuilderTest(53, true);
    }

    public function testProductMainVariantIsOfferred()
    {
        $this->getAllOfferedQueryBuilderTest(69, true);
    }

    private function getAllOfferedQueryBuilderTest($productReferenceId, $isExpectedInResult)
    {
        $productRepository = $this->getContainer()->get(ProductRepository::class);
        /* @var $productRepository \Shopsys\FrameworkBundle\Model\Product\ProductRepository */
        $pricingGroup = $this->getReference(PricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1);
        /* @var $pricingGroup \Shopsys\FrameworkBundle\Model\Pricing\Group\PricingGroup */

        $domain = $this->getContainer()->get(Domain::class);
        /* @var $domain \Shopsys\FrameworkBundle\Component\Domain\Domain */

        $product = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . $productReferenceId);
        $productId = $product->getId();

        $queryBuilder = $productRepository->getAllOfferedQueryBuilder($domain->getId(), $pricingGroup);
        $queryBuilder->andWhere('p.id = :id')
            ->setParameter('id', $productId);
        $result = $queryBuilder->getQuery()->execute();

        $this->assertSame(in_array($product, $result, true), $isExpectedInResult);
    }

    public function testOrderingByProductPriorityInCategory()
    {
        $category = $this->getReference(CategoryDataFixture::CATEGORY_FOOD);
        /* @var $category \Shopsys\FrameworkBundle\DataFixtures\Demo\CategoryDataFixture */
        $product1 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . 70);
        $product2 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . 71);

        $this->setProductOrderingPriority($product1, 0);
        $this->setProductOrderingPriority($product2, -1);

        $results = $this->getProductsInCategoryOrderedByPriority($category);
        $this->assertSame($product1, $results[0]);
        $this->assertSame($product2, $results[1]);

        $this->setProductOrderingPriority($product2, 1);

        $results = $this->getProductsInCategoryOrderedByPriority($category);
        $this->assertSame($product2, $results[0]);
        $this->assertSame($product1, $results[1]);
    }

    public function testOrderingByProductPriorityInSearch()
    {
        $this->setProductOrderingPriority($product1, 2);
        $this->setProductOrderingPriority($product2, 3);
        $this->setProductOrderingPriority($product2, 1);
        $product1 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . 1);
        $product2 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . 45);

        $this->setProductOrderingPriority($product1, 0);
        $this->setProductOrderingPriority($product2, 1);

        $results = $this->getProductsForSearchOrderedByPriority('sencor');
        $this->assertSame($product2, $results[0]);
        $this->assertSame($product1, $results[1]);

        $this->setProductOrderingPriority($product2, -1);

        $results = $this->getProductsForSearchOrderedByPriority('sencor');
        $this->assertSame($product1, $results[0]);
        $this->assertSame($product2, $results[1]);
    }

    /**
     * @param \Shopsys\FrameworkBundle\Model\Product\Product $product
     * @param int $priority
     */
    private function setProductOrderingPriority(Product $product, $priority)
    {
        $productDataFactory = $this->getContainer()->get(ProductDataFactory::class);
        /* @var $productDataFactory \Shopsys\FrameworkBundle\Model\Product\ProductDataFactory */
        $productFacade = $this->getContainer()->get(ProductFacade::class);
        /* @var $productFacade \Shopsys\FrameworkBundle\Model\Product\ProductFacade */

        $productData = $productDataFactory->createFromProduct($product);
        $productData->orderingPriority = $priority;
        $productFacade->edit($product->getId(), $productData);
    }

    /**
     * @param string $searchText
     * @return \Shopsys\FrameworkBundle\Model\Product\Product[]
     */
    private function getProductsForSearchOrderedByPriority($searchText)
    {
        $productRepository = $this->getContainer()->get(ProductRepository::class);
        /* @var $productRepository \Shopsys\FrameworkBundle\Model\Product\ProductRepository */
        $pricingGroup = $this->getReference(PricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1);
        /* @var $pricingGroup \Shopsys\FrameworkBundle\Model\Pricing\Group\PricingGroup */

        $paginationResult = $productRepository->getPaginationResultForSearchListable(
            $searchText,
            1,
            'en',
            new ProductFilterData(),
            ProductListOrderingModeService::ORDER_BY_PRIORITY,
            $pricingGroup,
            1,
            PHP_INT_MAX
        );

        return $paginationResult->getResults();
    }

    /**
     * @param \Shopsys\FrameworkBundle\Model\Category\Category $category
     * @return \Shopsys\FrameworkBundle\Model\Product\Product[]
     */
    private function getProductsInCategoryOrderedByPriority(Category $category)
    {
        $productRepository = $this->getContainer()->get(ProductRepository::class);
        /* @var $productRepository \Shopsys\FrameworkBundle\Model\Product\ProductRepository */
        $pricingGroup = $this->getReference(PricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1);
        /* @var $pricingGroup \Shopsys\FrameworkBundle\Model\Pricing\Group\PricingGroup */

        $paginationResult = $productRepository->getPaginationResultForListableInCategory(
            $category,
            1,
            'en',
            new ProductFilterData(),
            ProductListOrderingModeService::ORDER_BY_PRIORITY,
            $pricingGroup,
            1,
            PHP_INT_MAX
        );

        return $paginationResult->getResults();
    }
}
",Why is this test updated? I thought that the behaviour of product ordering will be the same.,"The provided code is a series of unit tests for the `ProductRepository` class in the `Tests\ShopBundle\Database\Model\Product` namespace. The tests cover various functionalities such as listing products based on their visibility, selling status, and offering status, as well as ordering products by priority in both categories and search results.

### Key Points:

1. **Test Methods**:
   - Each test method starts with `testVisibleAndNotSellingDeniedProductIs...`, `testVisibleAndSellingDeniedProductIsNot...`, etc., which checks if a product is listed or not based on its visibility and selling status.
   - Similarly, methods like `testProductVariantIsNotListed`, `testProductMainVariantIsListed`, etc., verify the listing behavior for different types of products (variants vs. main variants).

2. **Query Builder Usage**:
   - The `getAllListableQueryBuilderTest`, `getAllSellableQueryBuilderTest`, and `getAllOfferedQueryBuilderTest` methods use a query builder to filter products based on their visibility, selling status, and offering status, respectively.
   - They then check if the expected product is included in the result set.

3. **Ordering by Priority**:
   - The `testOrderingByProductPriorityInCategory` and `testOrderingByProductPriorityInSearch` methods test the ordering of products by priority in both category listings and search results.
   - They set priorities for products and verify that they are ordered correctly.

4. **Helper Methods**:
   - The `setProductOrderingPriority` method sets the ordering priority for a product using the `ProductDataFactory` and `ProductFacade`.
   - The `getProductsForSearchOrderedByPriority` and `getProductsInCategoryOrderedByPriority` methods retrieve products from the repository and order them by priority.

### Review:

- **Code Structure**: The code is organized into clear sections, making it easy to understand the purpose of each test method.
- **Test Coverage**: The tests cover a wide range of scenarios, ensuring comprehensive coverage of the `ProductRepository` functionality.
- **Readability**: The comments and docstrings help explain the purpose and functionality of each method.
- **Consistency**: The naming conventions and structure are consistent throughout the codebase, enhancing readability and maintainability.

Overall, the code is well-structured, well-documented, and provides a robust set of tests to ensure the correctness of the `ProductRepository` class."
13,"@@ -170,6 +170,17 @@ func TestYARPCMaxMsgSize(t *testing.T) {
 	})
 }
 
+func TestDataRace(t *testing.T) {
+	t.Parallel()
+	value := strings.Repeat(""a"", 32768)
+	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
+		assert.NoError(t, e.SetValueYARPC(context.Background(), ""foo"", value))
+		getValue, err := e.GetValueYARPC(context.Background(), ""foo"")
+		assert.NoError(t, err)
+		assert.Equal(t, value, getValue)
+	})
+}
+
 func TestApplicationErrorPropagation(t *testing.T) {
 	t.Parallel()
 	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {","// Copyright (c) 2018 Uber Technologies, Inc.
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the ""Software""), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in
// all copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
// THE SOFTWARE.

package grpc

import (
	""bytes""
	""context""
	""fmt""
	""math""
	""net""
	""strings""
	""testing""

	""github.com/gogo/protobuf/proto""
	""github.com/stretchr/testify/assert""
	""github.com/stretchr/testify/require""
	""go.uber.org/multierr""
	""go.uber.org/yarpc/api/transport""
	""go.uber.org/yarpc/encoding/protobuf""
	""go.uber.org/yarpc/internal/clientconfig""
	""go.uber.org/yarpc/internal/examples/protobuf/example""
	""go.uber.org/yarpc/internal/examples/protobuf/examplepb""
	""go.uber.org/yarpc/internal/grpcctx""
	""go.uber.org/yarpc/internal/testtime""
	intyarpcerrors ""go.uber.org/yarpc/internal/yarpcerrors""
	""go.uber.org/yarpc/pkg/procedure""
	""go.uber.org/yarpc/yarpcerrors""
	""google.golang.org/grpc""
	""google.golang.org/grpc/codes""
	""google.golang.org/grpc/status""
)

func TestYARPCBasic(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		_, err := e.GetValueYARPC(context.Background(), ""foo"")
		assert.Equal(t, yarpcerrors.Newf(yarpcerrors.CodeNotFound, ""foo""), err)
		assert.NoError(t, e.SetValueYARPC(context.Background(), ""foo"", ""bar""))
		value, err := e.GetValueYARPC(context.Background(), ""foo"")
		assert.NoError(t, err)
		assert.Equal(t, ""bar"", value)
	})
}

func TestGRPCBasic(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		_, err := e.GetValueGRPC(context.Background(), ""foo"")
		assert.Equal(t, status.Error(codes.NotFound, ""foo""), err)
		assert.NoError(t, e.SetValueGRPC(context.Background(), ""foo"", ""bar""))
		value, err := e.GetValueGRPC(context.Background(), ""foo"")
		assert.NoError(t, err)
		assert.Equal(t, ""bar"", value)
	})
}

func TestYARPCWellKnownError(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		e.KeyValueYARPCServer.SetNextError(status.Error(codes.FailedPrecondition, ""bar 1""))
		err := e.SetValueYARPC(context.Background(), ""foo"", ""bar"")
		assert.Equal(t, yarpcerrors.Newf(yarpcerrors.CodeFailedPrecondition, ""bar 1""), err)
	})
}

func TestYARPCNamedError(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		e.KeyValueYARPCServer.SetNextError(intyarpcerrors.NewWithNamef(yarpcerrors.CodeUnknown, ""bar"", ""baz 1""))
		err := e.SetValueYARPC(context.Background(), ""foo"", ""bar"")
		assert.Equal(t, intyarpcerrors.NewWithNamef(yarpcerrors.CodeUnknown, ""bar"", ""baz 1""), err)
	})
}

func TestYARPCNamedErrorNoMessage(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		e.KeyValueYARPCServer.SetNextError(intyarpcerrors.NewWithNamef(yarpcerrors.CodeUnknown, ""bar"", """"))
		err := e.SetValueYARPC(context.Background(), ""foo"", ""bar"")
		assert.Equal(t, intyarpcerrors.NewWithNamef(yarpcerrors.CodeUnknown, ""bar"", """"), err)
	})
}

func TestGRPCWellKnownError(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		e.KeyValueYARPCServer.SetNextError(status.Error(codes.FailedPrecondition, ""bar 1""))
		err := e.SetValueGRPC(context.Background(), ""foo"", ""bar"")
		assert.Equal(t, status.Error(codes.FailedPrecondition, ""bar 1""), err)
	})
}

func TestGRPCNamedError(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		e.KeyValueYARPCServer.SetNextError(intyarpcerrors.NewWithNamef(yarpcerrors.CodeUnknown, ""bar"", ""baz 1""))
		err := e.SetValueGRPC(context.Background(), ""foo"", ""bar"")
		assert.Equal(t, status.Error(codes.Unknown, ""bar: baz 1""), err)
	})
}

func TestGRPCNamedErrorNoMessage(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		e.KeyValueYARPCServer.SetNextError(intyarpcerrors.NewWithNamef(yarpcerrors.CodeUnknown, ""bar"", """"))
		err := e.SetValueGRPC(context.Background(), ""foo"", ""bar"")
		assert.Equal(t, status.Error(codes.Unknown, ""bar""), err)
	})
}

func TestYARPCResponseAndError(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		err := e.SetValueYARPC(context.Background(), ""foo"", ""bar"")
		assert.NoError(t, err)
		e.KeyValueYARPCServer.SetNextError(status.Error(codes.FailedPrecondition, ""bar 1""))
		value, err := e.GetValueYARPC(context.Background(), ""foo"")
		assert.Equal(t, ""bar"", value)
		assert.Equal(t, yarpcerrors.Newf(yarpcerrors.CodeFailedPrecondition, ""bar 1""), err)
	})
}

func TestGRPCResponseAndError(t *testing.T) {
	t.Skip(""grpc-go clients do not support returning both a response and error as of now"")
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		err := e.SetValueGRPC(context.Background(), ""foo"", ""bar"")
		assert.NoError(t, err)
		e.KeyValueYARPCServer.SetNextError(status.Error(codes.FailedPrecondition, ""bar 1""))
		value, err := e.GetValueGRPC(context.Background(), ""foo"")
		assert.Equal(t, ""bar"", value)
		assert.Equal(t, status.Error(codes.FailedPrecondition, ""bar 1""), err)
	})
}

func TestYARPCMaxMsgSize(t *testing.T) {
	t.Parallel()
	value := strings.Repeat(""a"", defaultServerMaxRecvMsgSize*2)
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		assert.Equal(t, yarpcerrors.CodeResourceExhausted, yarpcerrors.FromError(e.SetValueYARPC(context.Background(), ""foo"", value)).Code())
	})
	doWithTestEnv(t, []TransportOption{
		ClientMaxRecvMsgSize(math.MaxInt32),
		ClientMaxSendMsgSize(math.MaxInt32),
		ServerMaxRecvMsgSize(math.MaxInt32),
		ServerMaxSendMsgSize(math.MaxInt32),
	}, nil, nil, func(t *testing.T, e *testEnv) {
		assert.NoError(t, e.SetValueYARPC(context.Background(), ""foo"", value))
		getValue, err := e.GetValueYARPC(context.Background(), ""foo"")
		assert.NoError(t, err)
		assert.Equal(t, value, getValue)
func TestDataRace(t *testing.T) {
	t.Parallel()
	value := strings.Repeat(""a"", 32768)
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		assert.NoError(t, e.SetValueYARPC(context.Background(), ""foo"", value))
		getValue, err := e.GetValueYARPC(context.Background(), ""foo"")
		assert.NoError(t, err)
		assert.Equal(t, value, getValue)
	})
}

	})
}

func TestApplicationErrorPropagation(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		response, err := e.Call(
			context.Background(),
			""GetValue"",
			&examplepb.GetValueRequest{Key: ""foo""},
			protobuf.Encoding,
			transport.Headers{},
		)
		require.Equal(t, yarpcerrors.NotFoundErrorf(""foo""), err)
		require.True(t, response.ApplicationError)

		response, err = e.Call(
			context.Background(),
			""SetValue"",
			&examplepb.SetValueRequest{Key: ""foo"", Value: ""hello""},
			protobuf.Encoding,
			transport.Headers{},
		)
		require.NoError(t, err)
		require.False(t, response.ApplicationError)

		response, err = e.Call(
			context.Background(),
			""GetValue"",
			&examplepb.GetValueRequest{Key: ""foo""},
			""bad_encoding"",
			transport.Headers{},
		)
		require.True(t, yarpcerrors.IsInvalidArgument(err))
		require.False(t, response.ApplicationError)
	})
}

func doWithTestEnv(t *testing.T, transportOptions []TransportOption, inboundOptions []InboundOption, outboundOptions []OutboundOption, f func(*testing.T, *testEnv)) {
	testEnv, err := newTestEnv(transportOptions, inboundOptions, outboundOptions)
	require.NoError(t, err)
	defer func() {
		assert.NoError(t, testEnv.Close())
	}()
	f(t, testEnv)
}

type testEnv struct {
	Caller              string
	Service             string
	Inbound             *Inbound
	Outbound            *Outbound
	ClientConn          *grpc.ClientConn
	ContextWrapper      *grpcctx.ContextWrapper
	ClientConfig        transport.ClientConfig
	Procedures          []transport.Procedure
	KeyValueGRPCClient  examplepb.KeyValueClient
	KeyValueYARPCClient examplepb.KeyValueYARPCClient
	KeyValueYARPCServer *example.KeyValueYARPCServer
}

func newTestEnv(transportOptions []TransportOption, inboundOptions []InboundOption, outboundOptions []OutboundOption) (_ *testEnv, err error) {
	keyValueYARPCServer := example.NewKeyValueYARPCServer()
	procedures := examplepb.BuildKeyValueYARPCProcedures(keyValueYARPCServer)
	testRouter := newTestRouter(procedures)

	t := NewTransport(transportOptions...)
	if err := t.Start(); err != nil {
		return nil, err
	}
	defer func() {
		if err != nil {
			err = multierr.Append(err, t.Stop())
		}
	}()

	listener, err := net.Listen(""tcp"", ""127.0.0.1:0"")
	if err != nil {
		return nil, err
	}

	inbound := t.NewInbound(listener, inboundOptions...)
	inbound.SetRouter(testRouter)
	if err := inbound.Start(); err != nil {
		return nil, err
	}
	defer func() {
		if err != nil {
			err = multierr.Append(err, inbound.Stop())
		}
	}()

	clientConn, err := grpc.Dial(listener.Addr().String(), grpc.WithInsecure())
	if err != nil {
		return nil, err
	}
	defer func() {
		if err != nil {
			err = multierr.Append(err, clientConn.Close())
		}
	}()
	keyValueClient := examplepb.NewKeyValueClient(clientConn)

	outbound := t.NewSingleOutbound(listener.Addr().String(), outboundOptions...)
	if err := outbound.Start(); err != nil {
		return nil, err
	}
	defer func() {
		if err != nil {
			err = multierr.Append(err, outbound.Stop())
		}
	}()

	caller := ""example-client""
	service := ""example""
	clientConfig := clientconfig.MultiOutbound(
		caller,
		service,
		transport.Outbounds{
			ServiceName: caller,
			Unary:       outbound,
		},
	)
	keyValueYARPCClient := examplepb.NewKeyValueYARPCClient(clientConfig)

	contextWrapper := grpcctx.NewContextWrapper().
		WithCaller(""example-client"").
		WithService(""example"").
		WithEncoding(string(protobuf.Encoding))

	return &testEnv{
		caller,
		service,
		inbound,
		outbound,
		clientConn,
		contextWrapper,
		clientConfig,
		procedures,
		keyValueClient,
		keyValueYARPCClient,
		keyValueYARPCServer,
	}, nil
}

func (e *testEnv) Call(
	ctx context.Context,
	methodName string,
	message proto.Message,
	encoding transport.Encoding,
	headers transport.Headers,
) (*transport.Response, error) {
	data, err := proto.Marshal(message)
	if err != nil {
		return nil, err
	}
	ctx, cancel := context.WithTimeout(ctx, testtime.Second)
	defer cancel()
	return e.Outbound.Call(
		ctx,
		&transport.Request{
			Caller:   e.Caller,
			Service:  e.Service,
			Encoding: encoding,
			Procedure: procedure.ToName(
				""uber.yarpc.internal.examples.protobuf.example.KeyValue"",
				methodName,
			),
			Headers: headers,
			Body:    bytes.NewReader(data),
		},
	)
}

func (e *testEnv) GetValueYARPC(ctx context.Context, key string) (string, error) {
	ctx, cancel := context.WithTimeout(ctx, testtime.Second)
	defer cancel()
	response, err := e.KeyValueYARPCClient.GetValue(ctx, &examplepb.GetValueRequest{key})
	if response != nil {
		return response.Value, err
	}
	return """", err
}

func (e *testEnv) SetValueYARPC(ctx context.Context, key string, value string) error {
	ctx, cancel := context.WithTimeout(ctx, testtime.Second)
	defer cancel()
	_, err := e.KeyValueYARPCClient.SetValue(ctx, &examplepb.SetValueRequest{key, value})
	return err
}

func (e *testEnv) GetValueGRPC(ctx context.Context, key string) (string, error) {
	ctx, cancel := context.WithTimeout(ctx, testtime.Second)
	defer cancel()
	response, err := e.KeyValueGRPCClient.GetValue(e.ContextWrapper.Wrap(ctx), &examplepb.GetValueRequest{key})
	if response != nil {
		return response.Value, err
	}
	return """", err
}

func (e *testEnv) SetValueGRPC(ctx context.Context, key string, value string) error {
	ctx, cancel := context.WithTimeout(ctx, testtime.Second)
	defer cancel()
	_, err := e.KeyValueGRPCClient.SetValue(e.ContextWrapper.Wrap(ctx), &examplepb.SetValueRequest{key, value})
	return err
}

func (e *testEnv) Close() error {
	return multierr.Combine(
		e.ClientConn.Close(),
		e.Outbound.Stop(),
		e.Inbound.Stop(),
	)
}

type testRouter struct {
	procedures []transport.Procedure
}

func newTestRouter(procedures []transport.Procedure) *testRouter {
	return &testRouter{procedures}
}

func (r *testRouter) Procedures() []transport.Procedure {
	return r.procedures
}

func (r *testRouter) Choose(_ context.Context, request *transport.Request) (transport.HandlerSpec, error) {
	for _, procedure := range r.procedures {
		if procedure.Name == request.Procedure {
			return procedure.HandlerSpec, nil
		}
	}
	return transport.HandlerSpec{}, fmt.Errorf(""no procedure for name %s"", request.Procedure)
}
",does this test fail without the fix?,"The suggested code changes appear to be testing various aspects of a YARPC service using gRPC. Here's a concise review:

### Key Changes and Observations:

1. **Test Functions**: 
   - `TestYARPCBasic`, `TestGRPCBasic`, etc., are used to verify basic functionality of the YARPC and gRPC services.
   - Each test function sets up a test environment, calls the respective method, and checks the expected results.

2. **Error Handling**:
   - Tests handle errors appropriately, checking for specific error codes and messages.
   - For example, `TestYARPCWellKnownError` and `TestGRPCWellKnownError` check for `codes.FailedPrecondition`.

3. **Named Errors**:
   - `TestYARPCNamedError` and `TestGRPCNamedError` demonstrate handling named errors from YARPC and gRPC.
   - These tests ensure that the error messages are correctly formatted.

4. **Response and Error Handling**:
   - `TestYARPCResponseAndError` and `TestGRPCResponseAndError` test scenarios where both a response and an error are returned.
   - This ensures that the client can handle both cases correctly.

5. **Max Message Size**:
   - `TestYARPCMaxMsgSize` tests the behavior when the message size exceeds the server's maximum allowed size.
   - It verifies that the correct error code (`codes.ResourceExhausted`) is returned.

6. **Data Race**:
   - `TestDataRace` tests the behavior when a large message is sent, ensuring no data races occur.

7. **Application Error Propagation**:
   - `TestApplicationErrorPropagation` tests how application errors are propagated through the service.
   - It checks for `ApplicationError` flags and ensures that invalid encoding returns an appropriate error.

8. **Environment Setup**:
   - `doWithTestEnv` sets up the test environment with specified transport, inbound, and outbound options.
   - It initializes the necessary components like the router, client, and server.

9. **Helper Functions**:
   - Helper functions like `Call`, `GetValueYARPC`, `SetValueYARPC`, `GetValueGRPC`, and `SetValueGRPC` are used to simplify calling the service methods.
   - These functions also handle context timeouts and error wrapping.

### Overall Quality:
- The code is well-structured and follows best practices for testing and setting up environments.
- The use"
14,"@@ -24,7 +24,12 @@ public class PMDASMVisitor extends ClassVisitor {
 
     private String outerName;
 
-    private Map<String, String> packages = new HashMap<>();
+    private Map<String, String> packages = new HashMap() {
+        @Override
+        public Object put(Object key, Object value) {
+            return super.put(key, value);
+        }
+    };
 
     private AnnotationVisitor annotationVisitor = new PMDAnnotationVisitor(this);
 ","/**
 * BSD-style license; for more info see http://pmd.sourceforge.net/license.html
 */

package net.sourceforge.pmd.lang.java.typeresolution.visitors;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.objectweb.asm.AnnotationVisitor;
import org.objectweb.asm.Attribute;
import org.objectweb.asm.ClassVisitor;
import org.objectweb.asm.FieldVisitor;
import org.objectweb.asm.Label;
import org.objectweb.asm.MethodVisitor;
import org.objectweb.asm.Opcodes;
import org.objectweb.asm.Type;
import org.objectweb.asm.signature.SignatureReader;
import org.objectweb.asm.signature.SignatureVisitor;

public class PMDASMVisitor extends ClassVisitor {
    private Map<String, String> packages = new HashMap() {
        @Override
        public Object put(Object key, Object value) {
            return super.put(key, value);
        }
    };

    private String outerName;

    private Map<String, String> packages = new HashMap<>();

    private AnnotationVisitor annotationVisitor = new PMDAnnotationVisitor(this);

    private FieldVisitor fieldVisitor = new PMDFieldVisitor(this);

    private SignatureVisitor sigVisitor = new PMDSignatureVisitor(this);

    private MethodVisitor methodVisitor = new PMDMethodVisitor(this);

    public List<String> innerClasses;

    public PMDASMVisitor(String outerName) {
        super(Opcodes.ASM5);
        this.outerName = outerName;
    }

    public Map<String, String> getPackages() {
        return packages;
    }

    public List<String> getInnerClasses() {
        return innerClasses;
    }

    private String parseClassName(String name) {
        if (name == null) {
            return null;
        }

        String className = name;
        int n = name.lastIndexOf('/');
        if (n > -1) {
            className = name.substring(n + 1);
        }
        name = name.replace('/', '.');
        packages.put(className, name);
        n = className.indexOf('$');
        if (n > -1) {
            // TODO I don't think the first one, with Class$Inner is needed -
            // come back and check
            packages.put(className.substring(n + 1), name);
            packages.put(className.replace('$', '.'), name);
        }

        return name;
    }

    private void parseClassName(String[] names) {
        if (names != null) {
            for (String s : names) {
                parseClassName(s);
            }
        }
    }

    private void extractSignature(String sig) {
        if (sig != null) {
            new SignatureReader(sig).accept(sigVisitor);
        }
    }

    /* Start ClassVisitor implementations */

    @Override
    public void visit(int version, int access, String name, String sig, String superName, String[] interfaces) {
        parseClassName(name);
        parseClassName(interfaces);
        if (sig != null) {
            extractSignature(sig);
        }
    }

    @Override
    public AnnotationVisitor visitAnnotation(String desc, boolean visible) {
        addType(Type.getType(desc));
        return annotationVisitor;
    }

    @Override
    public FieldVisitor visitField(int access, String name, String desc, String sig, Object value) {
        if (sig != null) {
            extractSignature(sig);
        }

        addType(Type.getType(desc));
        if (value instanceof Type) {
            addType((Type) value);
        }
        return fieldVisitor;
    }

    @Override
    public MethodVisitor visitMethod(int access, String name, String desc, String sig, String[] exceptions) {
        if (sig != null) {
            extractSignature(sig);
        }
        addMethodDesc(desc);
        parseClassName(exceptions);
        return methodVisitor;
    }

    @Override
    public void visitSource(String source, String debug) {
    }

    @Override
    public void visitInnerClass(String name, String outerName, String innerName, int access) {
        if (!this.outerName.replace('.', '/').equals(outerName)) {
            // do not consider the inner class if it is not a member of our
            // outer class
            return;
        }

        if (innerClasses == null) {
            innerClasses = new ArrayList<>();
        }
        if (!innerClasses.contains(name.replace('/', '.'))) {
            innerClasses.add(name.replace('/', '.'));
        }
        packages.put(innerName, name.replace('/', '.'));
    }

    @Override
    public void visitOuterClass(String owner, String name, String desc) {
    }

    @Override
    public void visitEnd() {
    }

    private void addMethodDesc(String desc) {
        addTypes(desc);
        addType(Type.getReturnType(desc));
    }

    private void addTypes(String desc) {
        Type[] types = Type.getArgumentTypes(desc);
        for (Type type : types) {
            addType(type);
        }
    }

    private void addType(Type t) {
        switch (t.getSort()) {
        case Type.ARRAY:
            addType(t.getElementType());
            break;
        case Type.OBJECT:
            parseClassName(t.getClassName().replace('.', '/'));
            break;
        default:
            // Do nothing
            break;
        }
    }

    @Override
    public void visitAttribute(Attribute attr) {
    }

    /*
     * Start visitors
     */

    private static class PMDFieldVisitor extends FieldVisitor {

        private PMDASMVisitor parent;

        PMDFieldVisitor(PMDASMVisitor visitor) {
            super(Opcodes.ASM5);
            parent = visitor;
        }

        @Override
        public AnnotationVisitor visitAnnotation(String desc, boolean visible) {
            parent.addType(Type.getType(desc));
            return parent.annotationVisitor;
        }

        @Override
        public void visitAttribute(Attribute attr) {
        }

        @Override
        public void visitEnd() {
        }
    }

    private static class PMDAnnotationVisitor extends AnnotationVisitor {
        private PMDASMVisitor parent;

        PMDAnnotationVisitor(PMDASMVisitor visitor) {
            super(Opcodes.ASM5);
            parent = visitor;
        }

        @Override
        public AnnotationVisitor visitAnnotation(String name, String desc) {
            parent.addType(Type.getType(desc));
            return this;
        }

        @Override
        public void visitEnum(String name, String desc, String value) {
            parent.addType(Type.getType(desc));
        }

        @Override
        public AnnotationVisitor visitArray(String name) {
            return this;
        }

        @Override
        public void visitEnd() {
        }

        @Override
        public void visit(String name, Object value) {
            if (value instanceof Type) {
                parent.addType((Type) value);
            }
        }
    }

    private static class PMDSignatureVisitor extends SignatureVisitor {
        private PMDASMVisitor parent;

        PMDSignatureVisitor(PMDASMVisitor visitor) {
            super(Opcodes.ASM5);
            this.parent = visitor;
        }

        @Override
        public void visitFormalTypeParameter(String name) {
        }

        @Override
        public SignatureVisitor visitClassBound() {
            return this;
        }

        @Override
        public SignatureVisitor visitInterfaceBound() {
            return this;
        }

        @Override
        public SignatureVisitor visitSuperclass() {
            return this;
        }

        @Override
        public SignatureVisitor visitInterface() {
            return this;
        }

        @Override
        public SignatureVisitor visitParameterType() {
            return this;
        }

        @Override
        public SignatureVisitor visitReturnType() {
            return this;
        }

        @Override
        public SignatureVisitor visitExceptionType() {
            return this;
        }

        @Override
        public void visitBaseType(char descriptor) {
        }

        @Override
        public void visitTypeVariable(String name) {
        }

        @Override
        public SignatureVisitor visitArrayType() {
            return this;
        }

        @Override
        public void visitClassType(String name) {
            parent.parseClassName(name);
        }

        @Override
        public void visitInnerClassType(String name) {
            parent.parseClassName(name);
        }

        @Override
        public void visitTypeArgument() {
        }

        @Override
        public SignatureVisitor visitTypeArgument(char wildcard) {
            return this;
        }

        @Override
        public void visitEnd() {
        }
    }

    private static class PMDMethodVisitor extends MethodVisitor {
        private PMDASMVisitor parent;

        PMDMethodVisitor(PMDASMVisitor visitor) {
            super(Opcodes.ASM5);
            parent = visitor;
        }

        @Override
        public AnnotationVisitor visitParameterAnnotation(int parameter, String desc, boolean visible) {
            parent.addType(Type.getType(desc));
            return parent.annotationVisitor;
        }

        public AnnotationVisitor visitAnnotation(String name, String desc) {
            parent.addType(Type.getType(desc));
            return parent.annotationVisitor;
        }

        @Override
        public void visitTypeInsn(int opcode, String desc) {
            if (desc.charAt(0) == '[') {
                parent.addType(Type.getType(desc));
            } else {
                parent.parseClassName(desc);
            }
        }

        @Override
        public void visitFieldInsn(int opcode, String owner, String name, String desc) {
            parent.parseClassName(owner);
            parent.addType(Type.getType(desc));
        }

        @Override
        public void visitMethodInsn(int opcode, String owner, String name, String desc, boolean itf) {
            parent.parseClassName(owner);
            parent.addMethodDesc(desc);
        }

        /**
         * the constant to be loaded on the stack. This parameter must be a non
         * null Integer, a Float, a Long, a Double a String (or a Type for
         * .class constants, for classes whose version is 49.0 or more).
         *
         * @see org.objectweb.asm.MethodVisitor#visitLdcInsn(java.lang.Object)
         */
        @Override
        public void visitLdcInsn(Object cst) {
            if (cst instanceof Type) {
                parent.addType((Type) cst);
            } else if (cst instanceof String) {
                parent.parseClassName((String) cst);
            }
        }

        @Override
        public void visitMultiANewArrayInsn(String desc, int dims) {
            parent.addType(Type.getType(desc));
        }

        @Override
        public void visitLocalVariable(String name, String desc, String sig, Label start, Label end, int index) {
            parent.extractSignature(sig);
        }

        @Override
        public void visitCode() {
        }

        @Override
        public void visitFrame(int type, int nLocal, Object[] local, int nStack, Object[] stack) {
        }

        @Override
        public void visitInsn(int opcode) {
        }

        @Override
        public void visitIntInsn(int opcode, int operand) {
        }

        @Override
        public void visitVarInsn(int opcode, int var) {
        }

        @Override
        public void visitJumpInsn(int opcode, Label label) {
        }

        @Override
        public void visitLabel(Label label) {
        }

        @Override
        public void visitIincInsn(int var, int increment) {
        }

        @Override
        public void visitTableSwitchInsn(int min, int max, Label dflt, Label... labels) {
        }

        @Override
        public void visitLookupSwitchInsn(Label dflt, int[] keys, Label[] labels) {
        }

        @Override
        public void visitTryCatchBlock(Label start, Label end, Label handler, String type) {
            parent.parseClassName(type);
        }

        @Override
        public void visitLineNumber(int line, Label start) {
        }

        @Override
        public void visitMaxs(int maxStack, int maxLocals) {
        }

        @Override
        public AnnotationVisitor visitAnnotationDefault() {
            return parent.annotationVisitor;
        }

        @Override
        public AnnotationVisitor visitAnnotation(String desc, boolean visible) {
            parent.addType(Type.getType(desc));
            return parent.annotationVisitor;
        }

        @Override
        public void visitEnd() {
        }

        @Override
        public void visitAttribute(Attribute attr) {
        }

    }
}
",what's the point of this override?,"The suggested code change appears to be a Java class that implements the `ClassVisitor` interface from the ASM library. This class is designed to traverse a Java class file and extract information about its structure, such as package names, field types, method signatures, and annotations.

Here's a concise review of the suggested code change:

1. **Class Structure**: The class `PMDASMVisitor` extends `ClassVisitor` and overrides several methods to handle different aspects of a Java class file.

2. **Package and Inner Class Handling**: The class maintains a map of package names to their corresponding fully qualified names. It also collects inner class names in a list.

3. **Annotation and Field/Method Type Handling**: The class uses `AnnotationVisitor`, `FieldVisitor`, and `MethodVisitor` to handle annotations, field types, and method signatures, respectively. It adds these types to a map for later processing.

4. **Signature Parsing**: The class uses a `SignatureVisitor` to parse method and field signatures and extract relevant information.

5. **Visitor Implementations**: Several nested classes (`PMDFieldVisitor`, `PMDAnnotationVisitor`, `PMDSignatureVisitor`, `PMDMethodVisitor`) implement the respective visitor interfaces to handle specific aspects of the class file.

6. **Utility Methods**: The class includes utility methods like `parseClassName` and `addType` to help manage type information.

Overall, the code provides a comprehensive way to analyze Java class files using ASM, which can be useful for various purposes such as static analysis or code transformation."
15,"@@ -62,6 +62,14 @@ type blockJournal struct {
 	j          diskJournal
 	refs       map[BlockID]blockRefMap
 	isShutdown bool
+
+	// Tracks the total size of on-disk blocks that will be put to the
+	// server (i.e., does not count reference adds).  It is only
+	// accurate for users of this journal that properly flush entries;
+	// in particular, direct calls to `removeReferences` can cause
+	// this count to deviate from the actual disk usage of the
+	// journal.
+	blockBytes int64
 }
 
 type bserverOpName string","// Copyright 2016 Keybase Inc. All rights reserved.
// Use of this source code is governed by a BSD
// license that can be found in the LICENSE file.

package libkbfs

import (
	""errors""
	""fmt""
	""io/ioutil""
	""os""
	""path/filepath""
	""reflect""

	""github.com/keybase/client/go/logger""
	""golang.org/x/net/context""
)

// blockJournal stores a single ordered list of block operations for a
// single TLF, along with the associated block data, in flat files in
// a directory on disk.
//
// The directory layout looks like:
//
// dir/block_journal/EARLIEST
// dir/block_journal/LATEST
// dir/block_journal/0...000
// dir/block_journal/0...001
// dir/block_journal/0...fff
// dir/blocks/0100/0...01/data
// dir/blocks/0100/0...01/key_server_half
// ...
// dir/blocks/01ff/f...ff/data
// dir/blocks/01ff/f...ff/key_server_half
//
// Each entry in the journal in dir/block_journal contains the
// mutating operation and arguments for a single operation, except for
// block data. (See diskJournal comments for more details about the
// journal.)
//
// The block data is stored separately in dir/blocks. Each block has
// its own subdirectory with its ID as a name.  The block
// subdirectories are splayed over (# of possible hash types) * 256
// subdirectories -- one byte for the hash type (currently only one)
// plus the first byte of the hash data -- using the first four
// characters of the name to keep the number of directories in dir
// itself to a manageable number, similar to git. Each block directory
// has data, which is the raw block data that should hash to the block
// ID, and key_server_half, which contains the raw data for the
// associated key server half.
//
// blockJournal is not goroutine-safe, so any code that uses it must
// guarantee that only one goroutine at a time calls its functions.
type blockJournal struct {
	codec  Codec
	crypto cryptoPure
	dir    string

	log      logger.Logger
	deferLog logger.Logger


	// Tracks the total size of on-disk blocks that will be put to the
	// server (i.e., does not count reference adds).  It is only
	// accurate for users of this journal that properly flush entries;
	// in particular, direct calls to `removeReferences` can cause
	// this count to deviate from the actual disk usage of the
	// journal.
	blockBytes int64
	j          diskJournal
	refs       map[BlockID]blockRefMap
	isShutdown bool
}

type bserverOpName string

const (
	blockPutOp    bserverOpName = ""blockPut""
	addRefOp      bserverOpName = ""addReference""
	removeRefsOp  bserverOpName = ""removeReferences""
	archiveRefsOp bserverOpName = ""archiveReferences""
)

// A blockJournalEntry is just the name of the operation and the
// associated block ID and contexts. Fields are exported only for
// serialization.
type blockJournalEntry struct {
	// Must be one of the four ops above.
	Op bserverOpName
	// Must have exactly one entry with one context for blockPutOp
	// and addRefOp.
	Contexts map[BlockID][]BlockContext
}

// Get the single context stored in this entry. Only applicable to
// blockPutOp and addRefOp.
func (e blockJournalEntry) getSingleContext() (
	BlockID, BlockContext, error) {
	switch e.Op {
	case blockPutOp, addRefOp:
		if len(e.Contexts) != 1 {
			return BlockID{}, BlockContext{}, fmt.Errorf(
				""Op %s doesn't have exactly one context: %v"",
				e.Op, e.Contexts)
		}
		for id, idContexts := range e.Contexts {
			if len(idContexts) != 1 {
				return BlockID{}, BlockContext{}, fmt.Errorf(
					""Op %s doesn't have exactly one context for id=%s: %v"",
					e.Op, id, idContexts)
			}
			return id, idContexts[0], nil
		}
	}

	return BlockID{}, BlockContext{}, fmt.Errorf(
		""getSingleContext() erroneously called on op %s"", e.Op)
}

// makeBlockJournal returns a new blockJournal for the given
// directory. Any existing journal entries are read.
func makeBlockJournal(
	ctx context.Context, codec Codec, crypto cryptoPure, dir string,
	log logger.Logger) (*blockJournal, error) {
	journalPath := filepath.Join(dir, ""block_journal"")
	deferLog := log.CloneWithAddedDepth(1)
	j := makeDiskJournal(
		codec, journalPath, reflect.TypeOf(blockJournalEntry{}))
	journal := &blockJournal{
		codec:    codec,
		crypto:   crypto,
		dir:      dir,
		log:      log,
		deferLog: deferLog,
		j:        j,
	}

	refs, err := journal.readJournal(ctx)
	if err != nil {
		return nil, err
	}

	journal.refs = refs
	return journal, nil
}

// The functions below are for building various non-journal paths.

func (j *blockJournal) blocksPath() string {
	return filepath.Join(j.dir, ""blocks"")
}

func (j *blockJournal) blockPath(id BlockID) string {
	idStr := id.String()
	return filepath.Join(j.blocksPath(), idStr[:4], idStr[4:])
}

func (j *blockJournal) blockDataPath(id BlockID) string {
	return filepath.Join(j.blockPath(id), ""data"")
}

func (j *blockJournal) keyServerHalfPath(id BlockID) string {
	return filepath.Join(j.blockPath(id), ""key_server_half"")
}

// The functions below are for reading and writing journal entries.

func (j *blockJournal) readJournalEntry(ordinal journalOrdinal) (
	blockJournalEntry, error) {
	entry, err := j.j.readJournalEntry(ordinal)
	if err != nil {
		return blockJournalEntry{}, err
	}

	return entry.(blockJournalEntry), nil
}

// readJournal reads the journal and returns a map of all the block
// references in the journal.
func (j *blockJournal) readJournal(ctx context.Context) (
	map[BlockID]blockRefMap, error) {
	refs := make(map[BlockID]blockRefMap)

	first, err := j.j.readEarliestOrdinal()
	if os.IsNotExist(err) {
		return refs, nil
	} else if err != nil {
		return nil, err
	}
	last, err := j.j.readLatestOrdinal()
	if err != nil {
		return nil, err
	}

	j.log.CDebugf(ctx, ""Reading journal entries %d to %d"", first, last)

	for i := first; i <= last; i++ {
		e, err := j.readJournalEntry(i)
		if err != nil {
			return nil, err
		}

		// Handle single ops separately.
		switch e.Op {
		case blockPutOp, addRefOp:
			id, context, err := e.getSingleContext()
			if err != nil {
				return nil, err
			}

			blockRefs := refs[id]
			if blockRefs == nil {
				blockRefs = make(blockRefMap)
				refs[id] = blockRefs
			}

			err = blockRefs.put(context, liveBlockRef)
			if err != nil {
				return nil, err
			}
			continue
		}

		for id, idContexts := range e.Contexts {
			blockRefs := refs[id]

			switch e.Op {
			case removeRefsOp:
				if blockRefs == nil {
					// All refs are already gone,
					// which is not an error.
					continue
				}

				for _, context := range idContexts {
					err := blockRefs.remove(context)
					if err != nil {
						return nil, err
					}
				}

				if len(blockRefs) == 0 {
					delete(refs, id)
				}

			case archiveRefsOp:
				if blockRefs == nil {
					blockRefs = make(blockRefMap)
					refs[id] = blockRefs
				}

				for _, context := range idContexts {
					err := blockRefs.put(
						context, archivedBlockRef)
					if err != nil {
						return nil, err
					}
				}

			default:
				return nil, fmt.Errorf(""Unknown op %s"", e.Op)
			}
		}
	}
	return refs, nil
}

func (j *blockJournal) writeJournalEntry(
	ordinal journalOrdinal, entry blockJournalEntry) error {
	return j.j.writeJournalEntry(ordinal, entry)
}

func (j *blockJournal) appendJournalEntry(
	op bserverOpName, contexts map[BlockID][]BlockContext) error {
	return j.j.appendJournalEntry(nil, blockJournalEntry{
		Op:       op,
		Contexts: contexts,
	})
}

func (j *blockJournal) length() (uint64, error) {
	return j.j.length()
}

func (j *blockJournal) getRefEntry(
	id BlockID, refNonce BlockRefNonce) (blockRefEntry, error) {
	refs := j.refs[id]
	if refs == nil {
		return blockRefEntry{}, BServerErrorBlockNonExistent{}
	}

	e, ok := refs[refNonce]
	if !ok {
		return blockRefEntry{}, BServerErrorBlockNonExistent{}
	}

	return e, nil
}

func (j *blockJournal) putRefEntry(
	id BlockID, refEntry blockRefEntry) error {
	existingRefEntry, err := j.getRefEntry(
		id, refEntry.Context.GetRefNonce())
	var exists bool
	switch err.(type) {
	case BServerErrorBlockNonExistent:
		exists = false
	case nil:
		exists = true
	default:
		return err
	}

	if exists {
		err = existingRefEntry.checkContext(refEntry.Context)
		if err != nil {
			return err
		}
	}

	if j.refs[id] == nil {
		j.refs[id] = make(blockRefMap)
	}

	return j.refs[id].put(refEntry.Context, refEntry.Status)
}

func (j *blockJournal) getData(id BlockID) (
	[]byte, BlockCryptKeyServerHalf, error) {
	data, err := ioutil.ReadFile(j.blockDataPath(id))
	if os.IsNotExist(err) {
		return nil, BlockCryptKeyServerHalf{},
			BServerErrorBlockNonExistent{}
	} else if err != nil {
		return nil, BlockCryptKeyServerHalf{}, err
	}

	keyServerHalfPath := j.keyServerHalfPath(id)
	buf, err := ioutil.ReadFile(keyServerHalfPath)
	if os.IsNotExist(err) {
		return nil, BlockCryptKeyServerHalf{},
			BServerErrorBlockNonExistent{}
	} else if err != nil {
		return nil, BlockCryptKeyServerHalf{}, err
	}

	// Check integrity.

	dataID, err := j.crypto.MakePermanentBlockID(data)
	if err != nil {
		return nil, BlockCryptKeyServerHalf{}, err
	}

	if id != dataID {
		return nil, BlockCryptKeyServerHalf{}, fmt.Errorf(
			""Block ID mismatch: expected %s, got %s"", id, dataID)
	}

	var serverHalf BlockCryptKeyServerHalf
	err = serverHalf.UnmarshalBinary(buf)
	if err != nil {
		return nil, BlockCryptKeyServerHalf{}, err
	}

	return data, serverHalf, nil
}

// All functions below are public functions.

var errBlockJournalShutdown = errors.New(""blockJournal is shutdown"")

func (j *blockJournal) getDataWithContext(
	id BlockID, context BlockContext) (
	[]byte, BlockCryptKeyServerHalf, error) {
	if j.isShutdown {
		return nil, BlockCryptKeyServerHalf{},
			errBlockJournalShutdown
	}

	refEntry, err := j.getRefEntry(id, context.GetRefNonce())
	if err != nil {
		return nil, BlockCryptKeyServerHalf{}, err
	}

	err = refEntry.checkContext(context)
	if err != nil {
		return nil, BlockCryptKeyServerHalf{}, err
	}

	return j.getData(id)
}

func (j *blockJournal) getAll() (
	map[BlockID]map[BlockRefNonce]blockRefLocalStatus, error) {
	if j.isShutdown {
		return nil, errBlockJournalShutdown
	}

	res := make(map[BlockID]map[BlockRefNonce]blockRefLocalStatus)

	for id, refs := range j.refs {
		if len(refs) == 0 {
			continue
		}

		res[id] = make(map[BlockRefNonce]blockRefLocalStatus)
		for ref, refEntry := range refs {
			res[id][ref] = refEntry.Status
		}
	}

	return res, nil
}

func (j *blockJournal) putData(
	ctx context.Context, id BlockID, context BlockContext, buf []byte,
	serverHalf BlockCryptKeyServerHalf) (err error) {
	j.log.CDebugf(ctx, ""Putting %d bytes of data for block %s with context %v"",
		len(buf), id, context)
	defer func() {
		if err != nil {
			j.deferLog.CDebugf(ctx,
				""Put for block %s with context %v failed with %v"",
				id, context, err)
		}
	}()

	err = validateBlockServerPut(j.crypto, id, context, buf)
	if err != nil {
		return err
	}

	if j.isShutdown {
		return errBlockJournalShutdown
	}

	// Check the data and retrieve the server half, if they exist.
	_, existingServerHalf, err := j.getDataWithContext(id, context)
	var exists bool
	switch err.(type) {
	case BServerErrorBlockNonExistent:
		exists = false
	case nil:
		exists = true
	default:
		return err
	}

	if exists {
		// If the entry already exists, everything should be
		// the same, except for possibly additional
		// references.

		// We checked that both buf and the existing data hash
		// to id, so no need to check that they're both equal.

		if existingServerHalf != serverHalf {
			return fmt.Errorf(
				""key server half mismatch: expected %s, got %s"",
				existingServerHalf, serverHalf)
		}
	}

	err = os.MkdirAll(j.blockPath(id), 0700)
	if err != nil {
		return err
	}

	err = ioutil.WriteFile(j.blockDataPath(id), buf, 0600)
	if err != nil {
		return err
	}

	// TODO: Add integrity-checking for key server half?

	err = ioutil.WriteFile(
		j.keyServerHalfPath(id), serverHalf.data[:], 0600)
	if err != nil {
		return err
	}

	err = j.putRefEntry(id, blockRefEntry{
		Status:  liveBlockRef,
		Context: context,
	})
	if err != nil {
		return err
	}

	return j.appendJournalEntry(
		blockPutOp, map[BlockID][]BlockContext{id: {context}})
}

func (j *blockJournal) addReference(
	ctx context.Context, id BlockID, context BlockContext) (
	err error) {
	j.log.CDebugf(ctx, ""Adding reference for block %s with context %v"",
		id, context)
	defer func() {
		if err != nil {
			j.deferLog.CDebugf(ctx,
				""Adding reference for block %s with context %v failed with %v"",
				id, context, err)
		}
	}()

	if j.isShutdown {
		return errBlockJournalShutdown
	}

	refs := j.refs[id]
	if refs == nil {
		return BServerErrorBlockNonExistent{fmt.Sprintf(""Block ID %s ""+
			""doesn't exist and cannot be referenced."", id)}
	}

	// Only add it if there's a non-archived reference.
	hasNonArchivedRef := false
	for _, refEntry := range refs {
		if refEntry.Status == liveBlockRef {
			hasNonArchivedRef = true
			break
		}
	}

	if !hasNonArchivedRef {
		return BServerErrorBlockArchived{fmt.Sprintf(""Block ID %s has ""+
			""been archived and cannot be referenced."", id)}
	}

	// TODO: Figure out if we should allow adding a reference even
	// if all the existing references are archived, or if we have
	// no references at all. Also figure out what to do with an
	// addReference without a preceding Put.

	err = j.putRefEntry(id, blockRefEntry{
		Status:  liveBlockRef,
		Context: context,
	})
	if err != nil {
		return err
	}

	return j.appendJournalEntry(
		addRefOp, map[BlockID][]BlockContext{id: {context}})
}

func (j *blockJournal) removeReferences(
	ctx context.Context, contexts map[BlockID][]BlockContext,
	removeUnreferencedBlocks bool) (liveCounts map[BlockID]int, err error) {
	j.log.CDebugf(ctx, ""Removing references for %v (remove unreferenced blocks=%t)"",
		contexts, removeUnreferencedBlocks)
	defer func() {
		if err != nil {
			j.deferLog.CDebugf(ctx,
				""Removing references for %v (remove unreferenced blocks=%t)"",
				contexts, removeUnreferencedBlocks, err)
		}
	}()

	if j.isShutdown {
		return nil, errBlockJournalShutdown
	}

	liveCounts = make(map[BlockID]int)

	for id, idContexts := range contexts {
		refs := j.refs[id]
		if refs == nil {
			// This block is already gone; no error.
			continue
		}

		for _, context := range idContexts {
			err := refs.remove(context)
			if err != nil {
				return nil, err
			}
		}

		count := len(refs)
		if count == 0 {
			delete(j.refs, id)
			if removeUnreferencedBlocks {
				err := os.RemoveAll(j.blockPath(id))
				if err != nil {
					return nil, err
				}
			}
		}
		liveCounts[id] = count
	}

	err = j.appendJournalEntry(removeRefsOp, contexts)
	if err != nil {
		return nil, err
	}

	return liveCounts, nil
}

func (j *blockJournal) archiveReferences(
	ctx context.Context, contexts map[BlockID][]BlockContext) (err error) {
	j.log.CDebugf(ctx, ""Archiving references for %v"", contexts)
	defer func() {
		if err != nil {
			j.deferLog.CDebugf(ctx,
				""Archiving references for %v,"", contexts, err)
		}
	}()

	if j.isShutdown {
		return errBlockJournalShutdown
	}

	for id, idContexts := range contexts {
		for _, context := range idContexts {
			refNonce := context.GetRefNonce()
			refEntry, err := j.getRefEntry(id, refNonce)
			switch err.(type) {
			case BServerErrorBlockNonExistent:
				return BServerErrorBlockNonExistent{
					fmt.Sprintf(
						""Block ID %s (ref %s) doesn't ""+
							""exist and cannot be archived."",
						id, refNonce),
				}
			case nil:
				break

			default:
				return err
			}

			err = refEntry.checkContext(context)
			if err != nil {
				return err
			}

			refEntry.Status = archivedBlockRef
			err = j.putRefEntry(id, refEntry)
			if err != nil {
				return err
			}
		}
	}

	return j.appendJournalEntry(archiveRefsOp, contexts)
}

// getNextEntryToFlush returns the info for the next journal entry to
// flush, if any. If there is no next journal entry to flush, the
// returned *blockJournalEntry will be nil.
func (j *blockJournal) getNextEntryToFlush(ctx context.Context) (
	journalOrdinal, *blockJournalEntry, []byte,
	BlockCryptKeyServerHalf, error) {
	if j.isShutdown {
		return 0, nil, nil, BlockCryptKeyServerHalf{},
			errBlockJournalShutdown
	}

	earliestOrdinal, err := j.j.readEarliestOrdinal()
	if os.IsNotExist(err) {
		return 0, nil, nil, BlockCryptKeyServerHalf{}, nil
	} else if err != nil {
		return 0, nil, nil, BlockCryptKeyServerHalf{}, err
	}

	e, err := j.readJournalEntry(earliestOrdinal)
	if err != nil {
		return 0, nil, nil, BlockCryptKeyServerHalf{}, err
	}

	var data []byte
	var serverHalf BlockCryptKeyServerHalf
	if e.Op == blockPutOp {
		id, _, err := e.getSingleContext()
		if err != nil {
			return 0, nil, nil, BlockCryptKeyServerHalf{}, err
		}

		data, serverHalf, err = j.getData(id)
		if err != nil {
			return 0, nil, nil, BlockCryptKeyServerHalf{}, err
		}
	}

	return earliestOrdinal, &e, data, serverHalf, nil
}

func flushBlockJournalEntry(
	ctx context.Context, log logger.Logger,
	bserver BlockServer, tlfID TlfID, entry blockJournalEntry, data []byte,
	serverHalf BlockCryptKeyServerHalf) error {
	log.CDebugf(ctx, ""Flushing block op %v"", entry)

	switch entry.Op {
	case blockPutOp:
		id, context, err := entry.getSingleContext()
		if err != nil {
			return err
		}

		err = bserver.Put(ctx, tlfID, id, context, data, serverHalf)
		if err != nil {
			return err
		}

	case addRefOp:
		id, context, err := entry.getSingleContext()
		if err != nil {
			return err
		}

		// TODO: If the reference add fails, retry with a
		// Put. This is tricky: see KBFS-1148 and KBFS-1255.
		err = bserver.AddBlockReference(ctx, tlfID, id, context)
		if err != nil {
			if isRecoverableBlockError(err) {
				log.CWarningf(ctx,
					""Recoverable block error encountered on AddBlockReference: %v"", err)
			}
			return err
		}

	case removeRefsOp:
		_, err := bserver.RemoveBlockReferences(
			ctx, tlfID, entry.Contexts)
		if err != nil {
			return err
		}

	case archiveRefsOp:
		err := bserver.ArchiveBlockReferences(
			ctx, tlfID, entry.Contexts)
		if err != nil {
			return err
		}

	default:
		return fmt.Errorf(""Unknown op %s"", entry.Op)
	}

	return nil
}

func (j *blockJournal) removeFlushedEntry(ctx context.Context,
	ordinal journalOrdinal, _ blockJournalEntry) error {
	if j.isShutdown {
		// TODO: This creates a race condition if we shut down
		// after we've flushed an op but before we remove
		// it. Make sure we handle re-flushed ops
		// idempotently.
		return errBlockJournalShutdown
	}

	earliestOrdinal, err := j.j.readEarliestOrdinal()
	if err != nil {
		return err
	}

	if ordinal != earliestOrdinal {
		return fmt.Errorf(""Expected ordinal %d, got %d"",
			ordinal, earliestOrdinal)
	}

	_, err = j.j.removeEarliest()
	if err != nil {
		return err
	}

	// TODO: This is a hack to work around KBFS-1439. Figure out a
	// better way to update j.refs to reflect the removed entry.
	refs, err := j.readJournal(ctx)
	if err != nil {
		return err
	}
	j.refs = refs

	return nil
}

func (j *blockJournal) shutdown() {
	j.isShutdown = true

	// Double-check the on-disk journal with the in-memory one.
	ctx := context.Background()
	refs, err := j.readJournal(ctx)
	if err != nil {
		panic(err)
	}

	if !reflect.DeepEqual(refs, j.refs) {
		panic(fmt.Sprintf(""refs = %v != j.refs = %v"", refs, j.refs))
	}
}
","A better name might be `bytesToFlush`, to emphasize that it only matters when flushing.","The suggested code change appears to be a complete implementation of a block journal for a Keybase file system. The block journal is responsible for storing and managing block operations and their associated data on disk. The code includes functions for reading and writing journal entries, managing block references, and flushing entries to a block server. The code also includes error handling and logging mechanisms. Overall, the implementation appears to be well-designed and robust."
16,"@@ -706,7 +706,6 @@ class ArrayAssignmentAnalyzer
                     }
                 } elseif ($type instanceof TNonEmptyList
                     && $key_value instanceof Type\Atomic\TLiteralInt
-                    && ($key_value->value === $key_value->value || $key_value->value < $type->count)
                     && count($key_values) === 1
                 ) {
                     $has_matching_objectlike_property = true;","<?php
namespace Psalm\Internal\Analyzer\Statements\Expression\Assignment;

use PhpParser;
use Psalm\Internal\Analyzer\ClassLikeAnalyzer;
use Psalm\Internal\Analyzer\Statements\Expression\ExpressionIdentifier;
use Psalm\Internal\Analyzer\Statements\Expression\Fetch\ArrayFetchAnalyzer;
use Psalm\Internal\Analyzer\Statements\ExpressionAnalyzer;
use Psalm\Internal\Analyzer\StatementsAnalyzer;
use Psalm\Context;
use Psalm\IssueBuffer;
use Psalm\Issue\InvalidArrayAssignment;
use Psalm\Type;
use Psalm\Type\Atomic\TKeyedArray;
use Psalm\Type\Atomic\TArray;
use Psalm\Type\Atomic\TList;
use Psalm\Type\Atomic\TNonEmptyArray;
use Psalm\Type\Atomic\TNonEmptyList;
use function array_reverse;
use function array_shift;
use function count;
use function array_unshift;
use function preg_match;
use function is_string;
use function implode;
use function array_pop;

/**
 * @internal
 */
class ArrayAssignmentAnalyzer
{
    public static function analyze(
        StatementsAnalyzer $statements_analyzer,
        PhpParser\Node\Expr\ArrayDimFetch $stmt,
        Context $context,
        ?PhpParser\Node\Expr $assign_value,
        Type\Union $assignment_value_type
    ): void {
        $nesting = 0;
        $var_id = ExpressionIdentifier::getVarId(
            $stmt->var,
            $statements_analyzer->getFQCLN(),
            $statements_analyzer,
            $nesting
        );

        self::updateArrayType(
            $statements_analyzer,
            $stmt,
            $assign_value,
            $assignment_value_type,
            $context
        );

        if (!$statements_analyzer->node_data->getType($stmt->var) && $var_id) {
            $context->vars_in_scope[$var_id] = Type::getMixed();
        }
    }

    /**
     * @return false|null
     */
    public static function updateArrayType(
        StatementsAnalyzer $statements_analyzer,
        PhpParser\Node\Expr\ArrayDimFetch $stmt,
        ?PhpParser\Node\Expr $assign_value,
        Type\Union $assignment_type,
        Context $context
    ): ?bool {
        $root_array_expr = $stmt;

        $child_stmts = [];

        while ($root_array_expr->var instanceof PhpParser\Node\Expr\ArrayDimFetch) {
            $child_stmts[] = $root_array_expr;
            $root_array_expr = $root_array_expr->var;
        }

        $child_stmts[] = $root_array_expr;
        $root_array_expr = $root_array_expr->var;

        if (ExpressionAnalyzer::analyze(
            $statements_analyzer,
            $root_array_expr,
            $context,
            true
        ) === false) {
            // fall through
        }

        $codebase = $statements_analyzer->getCodebase();

        $root_type = $statements_analyzer->node_data->getType($root_array_expr) ?: Type::getMixed();

        if ($root_type->hasMixed()) {
            if (ExpressionAnalyzer::analyze(
                $statements_analyzer,
                $stmt->var,
                $context,
                true
            ) === false) {
                // fall through
            }

            if ($stmt->dim) {
                if (ExpressionAnalyzer::analyze(
                    $statements_analyzer,
                    $stmt->dim,
                    $context
                ) === false) {
                    // fall through
                }
            }
        }

        $child_stmts = array_reverse($child_stmts);

        $current_type = $root_type;

        $current_dim = $stmt->dim;

        $reversed_child_stmts = [];

        // gets a variable id that *may* contain array keys
        $root_var_id = ExpressionIdentifier::getArrayVarId(
            $root_array_expr,
            $statements_analyzer->getFQCLN(),
            $statements_analyzer
        );

        $var_id_additions = [];

        $parent_var_id = null;

        $offset_already_existed = false;
        $full_var_id = true;

        $child_stmt = null;

        // First go from the root element up, and go as far as we can to figure out what
        // array types there are
        while ($child_stmts) {
            $child_stmt = array_shift($child_stmts);

            if (count($child_stmts)) {
                array_unshift($reversed_child_stmts, $child_stmt);
            }

            $child_stmt_dim_type = null;

            $dim_value = null;

            if ($child_stmt->dim) {
                if (ExpressionAnalyzer::analyze(
                    $statements_analyzer,
                    $child_stmt->dim,
                    $context
                ) === false) {
                    return false;
                }

                if (!($child_stmt_dim_type = $statements_analyzer->node_data->getType($child_stmt->dim))) {
                    return null;
                }

                if ($child_stmt->dim instanceof PhpParser\Node\Scalar\String_
                    || (($child_stmt->dim instanceof PhpParser\Node\Expr\ConstFetch
                            || $child_stmt->dim instanceof PhpParser\Node\Expr\ClassConstFetch)
                       && $child_stmt_dim_type->isSingleStringLiteral())
                ) {
                    if ($child_stmt->dim instanceof PhpParser\Node\Scalar\String_) {
                        $dim_value = new Type\Atomic\TLiteralString($child_stmt->dim->value);
                    } else {
                        $dim_value = $child_stmt_dim_type->getSingleStringLiteral();
                    }

                    if (preg_match('/^(0|[1-9][0-9]*)$/', $dim_value->value)) {
                        $var_id_additions[] = '[' . $dim_value->value . ']';
                    } else {
                        $var_id_additions[] = '[\'' . $dim_value->value . '\']';
                    }
                } elseif ($child_stmt->dim instanceof PhpParser\Node\Scalar\LNumber
                    || (($child_stmt->dim instanceof PhpParser\Node\Expr\ConstFetch
                            || $child_stmt->dim instanceof PhpParser\Node\Expr\ClassConstFetch)
                        && $child_stmt_dim_type->isSingleIntLiteral())
                ) {
                    if ($child_stmt->dim instanceof PhpParser\Node\Scalar\LNumber) {
                        $dim_value = new Type\Atomic\TLiteralInt($child_stmt->dim->value);
                    } else {
                        $dim_value = $child_stmt_dim_type->getSingleIntLiteral();
                    }

                    $var_id_additions[] = '[' . $dim_value->value . ']';
                } elseif ($child_stmt->dim instanceof PhpParser\Node\Expr\Variable
                    && is_string($child_stmt->dim->name)
                ) {
                    $var_id_additions[] = '[$' . $child_stmt->dim->name . ']';
                } elseif ($child_stmt->dim instanceof PhpParser\Node\Expr\PropertyFetch
                    && $child_stmt->dim->name instanceof PhpParser\Node\Identifier
                ) {
                    $object_id = ExpressionIdentifier::getArrayVarId(
                        $child_stmt->dim->var,
                        $statements_analyzer->getFQCLN(),
                        $statements_analyzer
                    );

                    if ($object_id) {
                        $var_id_additions[] = '[' . $object_id . '->' . $child_stmt->dim->name->name . ']';
                    }
                } elseif ($child_stmt->dim instanceof PhpParser\Node\Expr\ClassConstFetch
                    && $child_stmt->dim->name instanceof PhpParser\Node\Identifier
                    && $child_stmt->dim->class instanceof PhpParser\Node\Name
                ) {
                    $object_name = ClassLikeAnalyzer::getFQCLNFromNameObject(
                        $child_stmt->dim->class,
                        $statements_analyzer->getAliases()
                    );
                    $var_id_additions[] = '[' . $object_name . '::' . $child_stmt->dim->name->name . ']';
                } else {
                    $var_id_additions[] = '[' . $child_stmt_dim_type . ']';
                    $full_var_id = false;
                }
            } else {
                $var_id_additions[] = '';
                $full_var_id = false;
            }

            if (!($child_stmt_var_type = $statements_analyzer->node_data->getType($child_stmt->var))) {
                return null;
            }

            if ($child_stmt_var_type->isEmpty()) {
                $child_stmt_var_type = Type::getEmptyArray();
                $statements_analyzer->node_data->setType($child_stmt->var, $child_stmt_var_type);
            }

            $array_var_id = $root_var_id . implode('', $var_id_additions);

            if ($parent_var_id && isset($context->vars_in_scope[$parent_var_id])) {
                $child_stmt_var_type = clone $context->vars_in_scope[$parent_var_id];
                $statements_analyzer->node_data->setType($child_stmt->var, $child_stmt_var_type);
            }

            $array_type = clone $child_stmt_var_type;

            $child_stmt_type = ArrayFetchAnalyzer::getArrayAccessTypeGivenOffset(
                $statements_analyzer,
                $child_stmt,
                $array_type,
                $child_stmt_dim_type ?: Type::getInt(),
                true,
                $array_var_id,
                $context,
                $assign_value,
                $child_stmts ? null : $assignment_type
            );

            $statements_analyzer->node_data->setType(
                $child_stmt,
                $child_stmt_type
            );

            $statements_analyzer->node_data->setType($child_stmt->var, $array_type);

            if ($root_var_id) {
                if (!$parent_var_id) {
                    $rooted_parent_id = $root_var_id;
                    $root_type = $array_type;
                } else {
                    $rooted_parent_id = $parent_var_id;
                }

                $context->vars_in_scope[$rooted_parent_id] = $array_type;
                $context->possibly_assigned_var_ids[$rooted_parent_id] = true;
            }

            if (!$child_stmts) {
                // we need this slight hack as the type we're putting it has to be
                // different from the type we're getting out
                if ($array_type->isSingle() && $array_type->hasClassStringMap()) {
                    $assignment_type = $child_stmt_type;
                }

                $child_stmt_type = $assignment_type;
                $statements_analyzer->node_data->setType($child_stmt, $assignment_type);

                self::taintArrayAssignment(
                    $statements_analyzer,
                    $child_stmt->var,
                    $array_type,
                    $assignment_type,
                    $array_var_id,
                    $dim_value !== null ? [$dim_value] : []
                );
            }

            $current_type = $child_stmt_type;
            $current_dim = $child_stmt->dim;

            $parent_var_id = $array_var_id;
        }

        if ($root_var_id
            && $full_var_id
            && $child_stmt
            && ($child_stmt_var_type = $statements_analyzer->node_data->getType($child_stmt->var))
            && !$child_stmt_var_type->hasObjectType()
        ) {
            $array_var_id = $root_var_id . implode('', $var_id_additions);
            $parent_var_id = $root_var_id . implode('', \array_slice($var_id_additions, 0, -1));

            if (isset($context->vars_in_scope[$array_var_id])
                && !$context->vars_in_scope[$array_var_id]->possibly_undefined
            ) {
                $offset_already_existed = true;
            }

            $context->vars_in_scope[$array_var_id] = clone $assignment_type;
            $context->possibly_assigned_var_ids[$array_var_id] = true;
        }

        // only update as many child stmts are we were able to process above
        foreach ($reversed_child_stmts as $child_stmt) {
            $child_stmt_type = $statements_analyzer->node_data->getType($child_stmt);

            if (!$child_stmt_type) {
                throw new \InvalidArgumentException('Should never get here');
            }

            $key_values = [];

            if ($current_dim instanceof PhpParser\Node\Scalar\String_) {
                $key_values[] = new Type\Atomic\TLiteralString($current_dim->value);
            } elseif ($current_dim instanceof PhpParser\Node\Scalar\LNumber) {
                $key_values[] = new Type\Atomic\TLiteralInt($current_dim->value);
            } elseif ($current_dim
                && ($current_dim_type = $statements_analyzer->node_data->getType($current_dim))
            ) {
                $string_literals = $current_dim_type->getLiteralStrings();
                $int_literals = $current_dim_type->getLiteralInts();

                $all_atomic_types = $current_dim_type->getAtomicTypes();

                if (count($string_literals) + count($int_literals) === count($all_atomic_types)) {
                    foreach ($string_literals as $string_literal) {
                        $key_values[] = clone $string_literal;
                    }

                    foreach ($int_literals as $int_literal) {
                        $key_values[] = clone $int_literal;
                    }
                }
            }

            if ($key_values) {
                $new_child_type = self::updateTypeWithKeyValues(
                    $codebase,
                    $child_stmt_type,
                    $current_type,
                    $key_values
                );
            } else {
                if (!$current_dim) {
                    $array_assignment_type = new Type\Union([
                        new TList($current_type),
                    ]);
                } else {
                    $current_dim_type = $statements_analyzer->node_data->getType($current_dim);

                    $array_assignment_type = new Type\Union([
                        new TArray([
                            $current_dim_type && !$current_dim_type->hasMixed()
                                ? $current_dim_type
                                : Type::getArrayKey(),
                            $current_type,
                        ]),
                    ]);
                }

                $new_child_type = Type::combineUnionTypes(
                    $child_stmt_type,
                    $array_assignment_type,
                    $codebase,
                    true,
                    true
                );
            }

            $new_child_type->removeType('null');
            $new_child_type->possibly_undefined = false;

            if (!$child_stmt_type->hasObjectType()) {
                $child_stmt_type = $new_child_type;
                $statements_analyzer->node_data->setType($child_stmt, $new_child_type);
            }

            $current_type = $child_stmt_type;
            $current_dim = $child_stmt->dim;

            array_pop($var_id_additions);

            $array_var_id = null;

            if ($root_var_id) {
                $array_var_id = $root_var_id . implode('', $var_id_additions);
                $context->vars_in_scope[$array_var_id] = clone $child_stmt_type;
                $context->possibly_assigned_var_ids[$array_var_id] = true;
            }

            if ($codebase->taint) {
                self::taintArrayAssignment(
                    $statements_analyzer,
                    $child_stmt->var,
                    $statements_analyzer->node_data->getType($child_stmt->var) ?: Type::getMixed(),
                    $new_child_type,
                    $array_var_id,
                    $key_values
                );
            }
        }

        $root_is_string = $root_type->isString();
        $key_values = [];

        if ($current_dim instanceof PhpParser\Node\Scalar\String_) {
            $key_values[] = new Type\Atomic\TLiteralString($current_dim->value);
        } elseif ($current_dim instanceof PhpParser\Node\Scalar\LNumber && !$root_is_string) {
            $key_values[] = new Type\Atomic\TLiteralInt($current_dim->value);
        } elseif ($current_dim
            && ($current_dim_type = $statements_analyzer->node_data->getType($current_dim))
            && !$root_is_string
        ) {
            $string_literals = $current_dim_type->getLiteralStrings();
            $int_literals = $current_dim_type->getLiteralInts();

            $all_atomic_types = $current_dim_type->getAtomicTypes();

            if (count($string_literals) + count($int_literals) === count($all_atomic_types)) {
                foreach ($string_literals as $string_literal) {
                    $key_values[] = clone $string_literal;
                }

                foreach ($int_literals as $int_literal) {
                    $key_values[] = clone $int_literal;
                }
            }
        }

        if ($key_values) {
            $new_child_type = self::updateTypeWithKeyValues(
                $codebase,
                $root_type,
                $current_type,
                $key_values
            );
        } elseif (!$root_is_string) {
            if ($current_dim) {
                if ($current_dim_type = $statements_analyzer->node_data->getType($current_dim)) {
                    if ($current_dim_type->hasMixed()) {
                        $current_dim_type = Type::getArrayKey();
                    }

                    $array_atomic_key_type = ArrayFetchAnalyzer::replaceOffsetTypeWithInts(
                        $current_dim_type
                    );
                } else {
                    $array_atomic_key_type = Type::getArrayKey();
                }

                if ($offset_already_existed
                    && $child_stmt
                    && $parent_var_id
                    && ($parent_type = $context->vars_in_scope[$parent_var_id] ?? null)
                ) {
                    if ($parent_type->hasList()) {
                        $array_atomic_type = new TNonEmptyList(
                            $current_type
                        );
                    } elseif ($parent_type->hasClassStringMap()
                        && $current_dim_type
                        && $current_dim_type->isTemplatedClassString()
                    ) {
                        /**
                         * @var Type\Atomic\TClassStringMap
                         * @psalm-suppress PossiblyUndefinedStringArrayOffset
                         */
                        $class_string_map = $parent_type->getAtomicTypes()['array'];
                        /**
                         * @var Type\Atomic\TTemplateParamClass
                         */
                        $offset_type_part = \array_values($current_dim_type->getAtomicTypes())[0];

                        $template_result = new \Psalm\Internal\Type\TemplateResult(
                            [],
                            [
                                $offset_type_part->param_name => [
                                    $offset_type_part->defining_class => [
                                        new Type\Union([
                                            new Type\Atomic\TTemplateParam(
                                                $class_string_map->param_name,
                                                $offset_type_part->as_type
                                                    ? new Type\Union([$offset_type_part->as_type])
                                                    : Type::getObject(),
                                                'class-string-map'
                                            )
                                        ])
                                    ]
                                ]
                            ]
                        );

                        $current_type->replaceTemplateTypesWithArgTypes(
                            $template_result,
                            $codebase
                        );

                        $array_atomic_type = new Type\Atomic\TClassStringMap(
                            $class_string_map->param_name,
                            $class_string_map->as_type,
                            $current_type
                        );
                    } else {
                        $array_atomic_type = new TNonEmptyArray([
                            $array_atomic_key_type,
                            $current_type,
                        ]);
                    }
                } else {
                    $array_atomic_type = new TNonEmptyArray([
                        $array_atomic_key_type,
                        $current_type,
                    ]);
                }
            } else {
                $array_atomic_type = new TNonEmptyList($current_type);
            }

            $from_countable_object_like = false;

            $new_child_type = null;

            if (!$current_dim && !$context->inside_loop) {
                $atomic_root_types = $root_type->getAtomicTypes();

                if (isset($atomic_root_types['array'])) {
                    if ($array_atomic_type instanceof Type\Atomic\TClassStringMap) {
                        $array_atomic_type = new TNonEmptyArray([
                            $array_atomic_type->getStandinKeyParam(),
                            $array_atomic_type->value_param
                        ]);
                    } elseif ($atomic_root_types['array'] instanceof TNonEmptyArray
                        || $atomic_root_types['array'] instanceof TNonEmptyList
                    ) {
                        $array_atomic_type->count = $atomic_root_types['array']->count;
                    } elseif ($atomic_root_types['array'] instanceof TKeyedArray
                        && $atomic_root_types['array']->sealed
                    ) {
                        $array_atomic_type->count = count($atomic_root_types['array']->properties);
                        $from_countable_object_like = true;

                        if ($atomic_root_types['array']->is_list
                            && $array_atomic_type instanceof TList
                        ) {
                            $array_atomic_type = clone $atomic_root_types['array'];

                            $new_child_type = new Type\Union([$array_atomic_type]);
                        }
                    } elseif ($array_atomic_type instanceof TList) {
                        $array_atomic_type = new TNonEmptyList(
                            $array_atomic_type->type_param
                        );
                    } else {
                        $array_atomic_type = new TNonEmptyArray(
                            $array_atomic_type->type_params
                        );
                    }
                }
            }

            $array_assignment_type = new Type\Union([
                $array_atomic_type,
            ]);

            if (!$new_child_type) {
                $new_child_type = Type::combineUnionTypes(
                    $root_type,
                    $array_assignment_type,
                    $codebase,
                    true,
                    true
                );
            }

            if ($from_countable_object_like) {
                $atomic_root_types = $new_child_type->getAtomicTypes();

                if (isset($atomic_root_types['array'])
                    && ($atomic_root_types['array'] instanceof TNonEmptyArray
                        || $atomic_root_types['array'] instanceof TNonEmptyList)
                    && $atomic_root_types['array']->count !== null
                ) {
                    $atomic_root_types['array']->count++;
                }
            }
        } else {
            $new_child_type = $root_type;
        }

        $new_child_type->removeType('null');

        if (!$root_type->hasObjectType()) {
            $root_type = $new_child_type;
        }

        $statements_analyzer->node_data->setType($root_array_expr, $root_type);

        if ($root_array_expr instanceof PhpParser\Node\Expr\PropertyFetch) {
            if ($root_array_expr->name instanceof PhpParser\Node\Identifier) {
                InstancePropertyAssignmentAnalyzer::analyze(
                    $statements_analyzer,
                    $root_array_expr,
                    $root_array_expr->name->name,
                    null,
                    $root_type,
                    $context,
                    false
                );
            } else {
                if (ExpressionAnalyzer::analyze($statements_analyzer, $root_array_expr->name, $context) === false) {
                    return false;
                }

                if (ExpressionAnalyzer::analyze($statements_analyzer, $root_array_expr->var, $context) === false) {
                    return false;
                }
            }
        } elseif ($root_array_expr instanceof PhpParser\Node\Expr\StaticPropertyFetch
            && $root_array_expr->name instanceof PhpParser\Node\Identifier
        ) {
            StaticPropertyAssignmentAnalyzer::analyze(
                $statements_analyzer,
                $root_array_expr,
                null,
                $root_type,
                $context
            );
        } elseif ($root_var_id) {
            $context->vars_in_scope[$root_var_id] = $root_type;
        }

        if ($root_array_expr instanceof PhpParser\Node\Expr\MethodCall
            || $root_array_expr instanceof PhpParser\Node\Expr\StaticCall
            || $root_array_expr instanceof PhpParser\Node\Expr\FuncCall
        ) {
            if ($root_type->hasArray()) {
                if (IssueBuffer::accepts(
                    new InvalidArrayAssignment(
                        'Assigning to the output of a function has no effect',
                        new \Psalm\CodeLocation($statements_analyzer->getSource(), $root_array_expr)
                    ),
                    $statements_analyzer->getSuppressedIssues()
                )
                ) {
                    // do nothing
                }
            }
        }

        return null;
    }

    /**
     * @param non-empty-list<Type\Atomic\TLiteralInt|Type\Atomic\TLiteralString> $key_values
     */
    private static function updateTypeWithKeyValues(
        \Psalm\Codebase $codebase,
        Type\Union $child_stmt_type,
        Type\Union $current_type,
        array $key_values
    ) : Type\Union {
        $has_matching_objectlike_property = false;
        $has_matching_string = false;

        foreach ($child_stmt_type->getAtomicTypes() as $type) {
            foreach ($key_values as $key_value) {
                if ($type instanceof TKeyedArray) {
                    if (isset($type->properties[$key_value->value])) {
                        $has_matching_objectlike_property = true;

                        $type->properties[$key_value->value] = clone $current_type;
                    }
                } elseif ($type instanceof Type\Atomic\TString
                    && $key_value instanceof Type\Atomic\TLiteralInt
                ) {
                    $has_matching_string = true;

                    if ($type instanceof Type\Atomic\TLiteralString
                        && $current_type->isSingleStringLiteral()
                    ) {
                        $new_char = $current_type->getSingleStringLiteral()->value;

                        if (\strlen($new_char) === 1) {
                            $type->value[0] = $new_char;
                        }
                    }
                } elseif ($type instanceof TNonEmptyList
                    && $key_value instanceof Type\Atomic\TLiteralInt
                    && ($key_value->value === $key_value->value || $key_value->value < $type->count)
                    && count($key_values) === 1
                ) {
                    $has_matching_objectlike_property = true;

                    $type->type_param = Type::combineUnionTypes(
                        clone $current_type,
                        $type->type_param,
                        $codebase,
                        true,
                        false
                    );
                }
            }
        }

        $child_stmt_type->bustCache();

        if (!$has_matching_objectlike_property && !$has_matching_string) {
            if (count($key_values) === 1) {
                $key_value = $key_values[0];

                $object_like = new TKeyedArray(
                    [$key_value->value => clone $current_type],
                    $key_value instanceof Type\Atomic\TLiteralClassString
                        ? [(string) $key_value->value => true]
                        : null
                );

                $object_like->sealed = true;

                $array_assignment_type = new Type\Union([
                    $object_like,
                ]);
            } else {
                $array_assignment_literals = $key_values;

                $array_assignment_type = new Type\Union([
                    new Type\Atomic\TNonEmptyArray([
                        new Type\Union($array_assignment_literals),
                        clone $current_type
                    ])
                ]);
            }

            return Type::combineUnionTypes(
                $child_stmt_type,
                $array_assignment_type,
                $codebase,
                true,
                false
            );
        }

        return $child_stmt_type;
    }

    /**
     * @param list<Type\Atomic\TLiteralInt|Type\Atomic\TLiteralString> $key_values $key_values
     */
    private static function taintArrayAssignment(
        StatementsAnalyzer $statements_analyzer,
        PhpParser\Node\Expr $stmt,
        Type\Union $stmt_type,
        Type\Union $child_stmt_type,
        ?string $array_var_id,
        array $key_values
    ) : void {
        $codebase = $statements_analyzer->getCodebase();

        if ($codebase->taint
            && $child_stmt_type->parent_nodes
            && !\in_array('TaintedInput', $statements_analyzer->getSuppressedIssues())
        ) {
            $var_location = new \Psalm\CodeLocation($statements_analyzer->getSource(), $stmt);

            $new_parent_node = \Psalm\Internal\Taint\TaintNode::getForAssignment(
                $array_var_id ?: 'array-assignment',
                $var_location
            );

            $codebase->taint->addTaintNode($new_parent_node);

            foreach ($child_stmt_type->parent_nodes as $parent_node) {
                if ($key_values) {
                    foreach ($key_values as $key_value) {
                        $codebase->taint->addPath(
                            $parent_node,
                            $new_parent_node,
                            'array-assignment-\'' . $key_value->value . '\''
                        );
                    }
                } else {
                    $codebase->taint->addPath(
                        $parent_node,
                        $new_parent_node,
                        'array-assignment'
                    );
                }
            }

            $stmt_type->parent_nodes[] = $new_parent_node;
        }
    }
}
","while the lhs of that or looks really redundant, the rhs does not immediately strike as useless","This code is responsible for analyzing array assignments in PHP code using Psalm, a static analysis tool for PHP. The main function `analyze` updates the type of the array being assigned to based on the value being assigned and the context in which the assignment occurs.

### Key Points:

1. **Type Analysis**: The function analyzes the array and its elements to determine the final type of the array after the assignment.

2. **Dynamic Array Keys**: It handles dynamic array keys, such as strings or integers, and updates the type accordingly.

3. **Object-like Properties**: If the array is an object-like structure (e.g., associative array), it updates the properties with the new type.

4. **Taint Analysis**: If the codebase is configured to perform taint analysis, it adds taint nodes to track the flow of data through the array.

5. **Error Handling**: It checks for errors during type analysis and reports them if necessary.

6. **Context Management**: It updates the context with the new type of the array and its elements.

7. **Performance Considerations**: The function uses caching and optimization techniques to improve performance, especially when dealing with large arrays or complex expressions.

### Potential Improvements:

- **Code Readability**: The code could benefit from better documentation and comments to explain the logic and purpose of each section.
- **Performance Optimization**: Further optimizations could be made to handle very large arrays more efficiently.
- **Edge Case Handling**: Additional edge cases, such as nested arrays with mixed types, should be handled more robustly.

Overall, this code provides a comprehensive solution for analyzing array assignments in PHP code using Psalm."
17,"@@ -216,6 +216,7 @@ bool GenerateRustModuleRootFile(const Parser &parser,
       for (auto it = sub_modules.begin(); it != sub_modules.end(); it++) {
         code += ""pub mod "" + it->first + "" {"";
         code.IncrementIdentLevel();
+        code += ""#![deny(unsafe_op_in_unsafe_fn)]"";
         code += ""use super::*;"";
         it->second.GenerateImports(code);
         code.DecrementIdentLevel();","/*
 * Copyright 2018 Google Inc. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// independent from idl_parser, since this code is not needed for most clients

#include ""flatbuffers/code_generators.h""
#include ""flatbuffers/flatbuffers.h""
#include ""flatbuffers/idl.h""
#include ""flatbuffers/util.h""

namespace flatbuffers {

// Convert a camelCaseIdentifier or CamelCaseIdentifier to a
// snake_case_identifier.
std::string MakeSnakeCase(const std::string &in) {
  std::string s;
  for (size_t i = 0; i < in.length(); i++) {
    if (i == 0) {
      s += CharToLower(in[0]);
    } else if (in[i] == '_') {
      s += '_';
    } else if (!islower(in[i])) {
      // Prevent duplicate underscores for Upper_Snake_Case strings
      // and UPPERCASE strings.
      if (islower(in[i - 1])) { s += '_'; }
      s += CharToLower(in[i]);
    } else {
      s += in[i];
    }
  }
  return s;
}

// Convert a string to all uppercase.
std::string MakeUpper(const std::string &in) {
  std::string s;
  for (size_t i = 0; i < in.length(); i++) { s += CharToUpper(in[i]); }
  return s;
}

std::string UnionTypeFieldName(const FieldDef &field) {
  return MakeSnakeCase(field.name + ""_type"");
}

// Encapsulate all logical field types in this enum. This allows us to write
// field logic based on type switches, instead of branches on the properties
// set on the Type.
// TODO(rw): for backwards compatibility, we can't use a strict `enum class`
//           declaration here. could we use the `-Wswitch-enum` warning to
//           achieve the same effect?
enum FullType {
  ftInteger = 0,
  ftFloat = 1,
  ftBool = 2,

  ftStruct = 3,
  ftTable = 4,

  ftEnumKey = 5,
  ftUnionKey = 6,

  ftUnionValue = 7,

  // TODO(rw): bytestring?
  ftString = 8,

  ftVectorOfInteger = 9,
  ftVectorOfFloat = 10,
  ftVectorOfBool = 11,
  ftVectorOfEnumKey = 12,
  ftVectorOfStruct = 13,
  ftVectorOfTable = 14,
  ftVectorOfString = 15,
  ftVectorOfUnionValue = 16,

  ftArrayOfBuiltin = 17,
  ftArrayOfEnum = 18,
  ftArrayOfStruct = 19,
};

// Convert a Type to a FullType (exhaustive).
FullType GetFullType(const Type &type) {
  // N.B. The order of these conditionals matters for some types.

  if (IsString(type)) {
    return ftString;
  } else if (type.base_type == BASE_TYPE_STRUCT) {
    if (type.struct_def->fixed) {
      return ftStruct;
    } else {
      return ftTable;
    }
  } else if (IsVector(type)) {
    switch (GetFullType(type.VectorType())) {
      case ftInteger: {
        return ftVectorOfInteger;
      }
      case ftFloat: {
        return ftVectorOfFloat;
      }
      case ftBool: {
        return ftVectorOfBool;
      }
      case ftStruct: {
        return ftVectorOfStruct;
      }
      case ftTable: {
        return ftVectorOfTable;
      }
      case ftString: {
        return ftVectorOfString;
      }
      case ftEnumKey: {
        return ftVectorOfEnumKey;
      }
      case ftUnionKey:
      case ftUnionValue: {
        FLATBUFFERS_ASSERT(false && ""vectors of unions are unsupported"");
        break;
      }
      default: {
        FLATBUFFERS_ASSERT(false && ""vector of vectors are unsupported"");
      }
    }
  } else if (IsArray(type)) {
    switch (GetFullType(type.VectorType())) {
      case ftInteger:
      case ftFloat:
      case ftBool: {
        return ftArrayOfBuiltin;
      }
      case ftStruct: {
        return ftArrayOfStruct;
      }
      case ftEnumKey: {
        return ftArrayOfEnum;
      }
      default: {
        FLATBUFFERS_ASSERT(false && ""Unsupported type for fixed array"");
      }
    }
  } else if (type.enum_def != nullptr) {
    if (type.enum_def->is_union) {
      if (type.base_type == BASE_TYPE_UNION) {
        return ftUnionValue;
      } else if (IsInteger(type.base_type)) {
        return ftUnionKey;
      } else {
        FLATBUFFERS_ASSERT(false && ""unknown union field type"");
      }
    } else {
      return ftEnumKey;
    }
  } else if (IsScalar(type.base_type)) {
    if (IsBool(type.base_type)) {
      return ftBool;
    } else if (IsInteger(type.base_type)) {
      return ftInteger;
    } else if (IsFloat(type.base_type)) {
      return ftFloat;
    } else {
      FLATBUFFERS_ASSERT(false && ""unknown number type"");
    }
  }

  FLATBUFFERS_ASSERT(false && ""completely unknown type"");

  // this is only to satisfy the compiler's return analysis.
  return ftBool;
}

bool IsBitFlagsEnum(const EnumDef &enum_def) {
  return enum_def.attributes.Lookup(""bit_flags"") != nullptr;
}

// TableArgs make required non-scalars ""Option<_>"".
// TODO(cneo): Rework how we do defaults and stuff.
bool IsOptionalToBuilder(const FieldDef &field) {
  return field.IsOptional() || !IsScalar(field.value.type.base_type);
}

bool GenerateRustModuleRootFile(const Parser &parser,
                                const std::string &output_dir) {
  // We gather the symbols into a tree of namespaces (which are rust mods) and
  // generate a file that gathers them all.
  struct Module {
    std::map<std::string, Module> sub_modules;
    std::vector<std::string> generated_files;
    // Add a symbol into the tree.
    void Insert(const Definition *s, const std::string suffix) {
      const Definition &symbol = *s;
      Module *current_module = this;
      for (auto it = symbol.defined_namespace->components.begin();
           it != symbol.defined_namespace->components.end(); it++) {
        std::string ns_component = MakeSnakeCase(*it);
        current_module = &current_module->sub_modules[ns_component];
      }
      current_module->generated_files.push_back(MakeSnakeCase(symbol.name) +
                                                suffix);
    }
    // Recursively create the importer file.
    void GenerateImports(CodeWriter &code) {
        code += ""#![deny(unsafe_op_in_unsafe_fn)]"";
      for (auto it = sub_modules.begin(); it != sub_modules.end(); it++) {
        code += ""pub mod "" + it->first + "" {"";
        code.IncrementIdentLevel();
        code += ""use super::*;"";
        it->second.GenerateImports(code);
        code.DecrementIdentLevel();
        code += ""} // "" + it->first;
      }
      for (auto it = generated_files.begin(); it != generated_files.end();
           it++) {
        code += ""mod "" + *it + "";"";
        code += ""pub use self::"" + *it + ""::*;"";
      }
    }
  };
  Module root_module;
  for (auto it = parser.enums_.vec.begin(); it != parser.enums_.vec.end();
       it++) {
    root_module.Insert(*it, parser.opts.filename_suffix);
  }
  for (auto it = parser.structs_.vec.begin(); it != parser.structs_.vec.end();
       it++) {
    root_module.Insert(*it, parser.opts.filename_suffix);
  }
  CodeWriter code(""  "");
  // TODO(caspern): Move generated warning out of BaseGenerator.
  code +=
      ""// Automatically generated by the Flatbuffers compiler. ""
      ""Do not modify."";
  root_module.GenerateImports(code);
  const bool success =
      SaveFile((output_dir + ""mod.rs"").c_str(), code.ToString(), false);
  code.Clear();
  return success;
}

namespace rust {

class RustGenerator : public BaseGenerator {
 public:
  RustGenerator(const Parser &parser, const std::string &path,
                const std::string &file_name)
      : BaseGenerator(parser, path, file_name, """", ""::"", ""rs""),
        cur_name_space_(nullptr) {
    const char *keywords[] = {
      // clang-format off
      // list taken from:
      // https://doc.rust-lang.org/book/second-edition/appendix-01-keywords.html
      //
      // we write keywords one per line so that we can easily compare them with
      // changes to that webpage in the future.

      // currently-used keywords
      ""as"",
      ""break"",
      ""const"",
      ""continue"",
      ""crate"",
      ""else"",
      ""enum"",
      ""extern"",
      ""false"",
      ""fn"",
      ""for"",
      ""if"",
      ""impl"",
      ""in"",
      ""let"",
      ""loop"",
      ""match"",
      ""mod"",
      ""move"",
      ""mut"",
      ""pub"",
      ""ref"",
      ""return"",
      ""Self"",
      ""self"",
      ""static"",
      ""struct"",
      ""super"",
      ""trait"",
      ""true"",
      ""type"",
      ""unsafe"",
      ""use"",
      ""where"",
      ""while"",

      // future possible keywords
      ""abstract"",
      ""alignof"",
      ""become"",
      ""box"",
      ""do"",
      ""final"",
      ""macro"",
      ""offsetof"",
      ""override"",
      ""priv"",
      ""proc"",
      ""pure"",
      ""sizeof"",
      ""typeof"",
      ""unsized"",
      ""virtual"",
      ""yield"",

      // other rust terms we should not use
      ""std"",
      ""usize"",
      ""isize"",
      ""u8"",
      ""i8"",
      ""u16"",
      ""i16"",
      ""u32"",
      ""i32"",
      ""u64"",
      ""i64"",
      ""u128"",
      ""i128"",
      ""f32"",
      ""f64"",

      // These are terms the code generator can implement on types.
      //
      // In Rust, the trait resolution rules (as described at
      // https://github.com/rust-lang/rust/issues/26007) mean that, as long
      // as we impl table accessors as inherent methods, we'll never create
      // conflicts with these keywords. However, that's a fairly nuanced
      // implementation detail, and how we implement methods could change in
      // the future. as a result, we proactively block these out as reserved
      // words.
      ""follow"",
      ""push"",
      ""size"",
      ""alignment"",
      ""to_little_endian"",
      ""from_little_endian"",
      nullptr,

      // used by Enum constants
      ""ENUM_MAX"",
      ""ENUM_MIN"",
      ""ENUM_VALUES"",
      // clang-format on
    };
    for (auto kw = keywords; *kw; kw++) keywords_.insert(*kw);
    code_.SetPadding(""  "");
  }

  bool generate() {
    if (parser_.opts.one_file) {
      return GenerateOneFile();
    } else {
      return GenerateIndividualFiles();
    }
  }

  template<typename T>
  bool GenerateSymbols(const SymbolTable<T> &symbols,
                       std::function<void(const T &)> gen_symbol) {
    for (auto it = symbols.vec.begin(); it != symbols.vec.end(); it++) {
      const T &symbol = **it;
      if (symbol.generated) continue;
      code_.Clear();
      code_ += ""// "" + std::string(FlatBuffersGeneratedWarning());
      code_ += ""extern crate flatbuffers;"";
      code_ += ""use std::mem;"";
      code_ += ""use std::cmp::Ordering;"";
      code_ += ""use self::flatbuffers::{EndianScalar, Follow};"";
      code_ += ""use super::*;"";
      cur_name_space_ = symbol.defined_namespace;
      gen_symbol(symbol);
      std::stringstream file_path;
      file_path << path_;
      // Create filepath.
      if (symbol.defined_namespace)
        for (auto i = symbol.defined_namespace->components.begin();
             i != symbol.defined_namespace->components.end(); i++) {
          file_path << MakeSnakeCase(*i) << kPathSeparator;
          EnsureDirExists(file_path.str());
        }
      file_path << MakeSnakeCase(symbol.name) << parser_.opts.filename_suffix
                << "".rs"";
      const bool save_success =
          SaveFile(file_path.str().c_str(), code_.ToString(),
                   /*binary=*/false);
      if (!save_success) return false;
    }
    return true;
  }

  bool GenerateIndividualFiles() {
    code_.Clear();
    // Don't bother with imports. Use absolute paths everywhere.
    return GenerateSymbols<EnumDef>(
               parser_.enums_, [&](const EnumDef &e) { this->GenEnum(e); }) &&
           GenerateSymbols<StructDef>(
               parser_.structs_, [&](const StructDef &s) {
                 if (s.fixed) {
                   this->GenStruct(s);
                 } else {
                   this->GenTable(s);
                   if (this->parser_.opts.generate_object_based_api) {
                     this->GenTableObject(s);
                   }
                 }
                 if (this->parser_.root_struct_def_ == &s) {
                   this->GenRootTableFuncs(s);
                 }
               });
  }

  // Generates code organized by .fbs files. This is broken legacy behavior
  // that does not work with multiple fbs files with shared namespaces.
  // Iterate through all definitions we haven't generated code for (enums,
  // structs, and tables) and output them to a single file.
  bool GenerateOneFile() {
    code_.Clear();
    code_ += ""// "" + std::string(FlatBuffersGeneratedWarning()) + ""\n\n"";

    assert(!cur_name_space_);

    // Generate imports for the global scope in case no namespace is used
    // in the schema file.
    GenNamespaceImports(0);
    code_ += """";

    // Generate all code in their namespaces, once, because Rust does not
    // permit re-opening modules.
    //
    // TODO(rw): Use a set data structure to reduce namespace evaluations from
    //           O(n**2) to O(n).
    for (auto ns_it = parser_.namespaces_.begin();
         ns_it != parser_.namespaces_.end(); ++ns_it) {
      const auto &ns = *ns_it;

      // Generate code for all the enum declarations.
      for (auto it = parser_.enums_.vec.begin(); it != parser_.enums_.vec.end();
           ++it) {
        const auto &enum_def = **it;
        if (enum_def.defined_namespace == ns && !enum_def.generated) {
          SetNameSpace(enum_def.defined_namespace);
          GenEnum(enum_def);
        }
      }

      // Generate code for all structs.
      for (auto it = parser_.structs_.vec.begin();
           it != parser_.structs_.vec.end(); ++it) {
        const auto &struct_def = **it;
        if (struct_def.defined_namespace == ns && struct_def.fixed &&
            !struct_def.generated) {
          SetNameSpace(struct_def.defined_namespace);
          GenStruct(struct_def);
        }
      }

      // Generate code for all tables.
      for (auto it = parser_.structs_.vec.begin();
           it != parser_.structs_.vec.end(); ++it) {
        const auto &struct_def = **it;
        if (struct_def.defined_namespace == ns && !struct_def.fixed &&
            !struct_def.generated) {
          SetNameSpace(struct_def.defined_namespace);
          GenTable(struct_def);
          if (parser_.opts.generate_object_based_api) {
            GenTableObject(struct_def);
          }
        }
      }

      // Generate global helper functions.
      if (parser_.root_struct_def_) {
        auto &struct_def = *parser_.root_struct_def_;
        if (struct_def.defined_namespace != ns) { continue; }
        SetNameSpace(struct_def.defined_namespace);
        GenRootTableFuncs(struct_def);
      }
    }
    if (cur_name_space_) SetNameSpace(nullptr);

    const auto file_path = GeneratedFileName(path_, file_name_, parser_.opts);
    const auto final_code = code_.ToString();
    return SaveFile(file_path.c_str(), final_code, false);
  }

 private:
  CodeWriter code_;

  std::set<std::string> keywords_;

  // This tracks the current namespace so we can insert namespace declarations.
  const Namespace *cur_name_space_;

  const Namespace *CurrentNameSpace() const { return cur_name_space_; }

  // Determine if a Type needs a lifetime template parameter when used in the
  // Rust builder args.
  bool TableBuilderTypeNeedsLifetime(const Type &type) const {
    switch (GetFullType(type)) {
      case ftInteger:
      case ftFloat:
      case ftBool:
      case ftEnumKey:
      case ftUnionKey:
      case ftUnionValue: {
        return false;
      }
      default: {
        return true;
      }
    }
  }

  // Determine if a table args rust type needs a lifetime template parameter.
  bool TableBuilderArgsNeedsLifetime(const StructDef &struct_def) const {
    FLATBUFFERS_ASSERT(!struct_def.fixed);

    for (auto it = struct_def.fields.vec.begin();
         it != struct_def.fields.vec.end(); ++it) {
      const auto &field = **it;
      if (field.deprecated) { continue; }

      if (TableBuilderTypeNeedsLifetime(field.value.type)) { return true; }
    }

    return false;
  }

  std::string EscapeKeyword(const std::string &name) const {
    return keywords_.find(name) == keywords_.end() ? name : name + ""_"";
  }
  std::string NamespacedNativeName(const Definition &def) {
    return WrapInNameSpace(def.defined_namespace, NativeName(def));
  }

  std::string NativeName(const Definition &def) {
    return parser_.opts.object_prefix + Name(def) + parser_.opts.object_suffix;
  }

  std::string Name(const Definition &def) const {
    return EscapeKeyword(def.name);
  }

  std::string Name(const EnumVal &ev) const { return EscapeKeyword(ev.name); }

  std::string WrapInNameSpace(const Definition &def) const {
    return WrapInNameSpace(def.defined_namespace, Name(def));
  }
  std::string WrapInNameSpace(const Namespace *ns,
                              const std::string &name) const {
    if (CurrentNameSpace() == ns) return name;
    std::string prefix = GetRelativeNamespaceTraversal(CurrentNameSpace(), ns);
    return prefix + name;
  }

  // Determine the namespace traversal needed from the Rust crate root.
  // This may be useful in the future for referring to included files, but is
  // currently unused.
  std::string GetAbsoluteNamespaceTraversal(const Namespace *dst) const {
    std::stringstream stream;

    stream << ""::"";
    for (auto d = dst->components.begin(); d != dst->components.end(); ++d) {
      stream << MakeSnakeCase(*d) + ""::"";
    }
    return stream.str();
  }

  // Determine the relative namespace traversal needed to reference one
  // namespace from another namespace. This is useful because it does not force
  // the user to have a particular file layout. (If we output absolute
  // namespace paths, that may require users to organize their Rust crates in a
  // particular way.)
  std::string GetRelativeNamespaceTraversal(const Namespace *src,
                                            const Namespace *dst) const {
    // calculate the path needed to reference dst from src.
    // example: f(A::B::C, A::B::C) -> (none)
    // example: f(A::B::C, A::B)    -> super::
    // example: f(A::B::C, A::B::D) -> super::D
    // example: f(A::B::C, A)       -> super::super::
    // example: f(A::B::C, D)       -> super::super::super::D
    // example: f(A::B::C, D::E)    -> super::super::super::D::E
    // example: f(A, D::E)          -> super::D::E
    // does not include leaf object (typically a struct type).

    std::stringstream stream;
    size_t common = 0;
    std::vector<std::string> s, d;
    if (src) s = src->components;
    if (dst) d = dst->components;
    while (common < s.size() && common < d.size() && s[common] == d[common])
      common++;
    // If src namespace is empty, this must be an absolute path.
    for (size_t i = common; i < s.size(); i++) stream << ""super::"";
    for (size_t i = common; i < d.size(); i++)
      stream << MakeSnakeCase(d[i]) + ""::"";
    return stream.str();
  }

  // Generate a comment from the schema.
  void GenComment(const std::vector<std::string> &dc, const char *prefix = """") {
    for (auto it = dc.begin(); it != dc.end(); it++) {
      code_ += std::string(prefix) + ""///"" + *it;
    }
  }

  // Return a Rust type from the table in idl.h.
  std::string GetTypeBasic(const Type &type) const {
    switch (GetFullType(type)) {
      case ftInteger:
      case ftFloat:
      case ftBool:
      case ftEnumKey:
      case ftUnionKey: {
        break;
      }
      default: {
        FLATBUFFERS_ASSERT(false && ""incorrect type given"");
      }
    }

    // clang-format off
    static const char * const ctypename[] = {
    #define FLATBUFFERS_TD(ENUM, IDLTYPE, CTYPE, JTYPE, GTYPE, NTYPE, PTYPE, \
                           RTYPE, ...) \
      #RTYPE,
      FLATBUFFERS_GEN_TYPES(FLATBUFFERS_TD)
    #undef FLATBUFFERS_TD
    };
    // clang-format on

    if (type.enum_def) { return WrapInNameSpace(*type.enum_def); }
    return ctypename[type.base_type];
  }

  // Look up the native type for an enum. This will always be an integer like
  // u8, i32, etc.
  std::string GetEnumTypeForDecl(const Type &type) {
    const auto ft = GetFullType(type);
    if (!(ft == ftEnumKey || ft == ftUnionKey)) {
      FLATBUFFERS_ASSERT(false && ""precondition failed in GetEnumTypeForDecl"");
    }

    // clang-format off
    static const char *ctypename[] = {
    #define FLATBUFFERS_TD(ENUM, IDLTYPE, CTYPE, JTYPE, GTYPE, NTYPE, PTYPE, \
                           RTYPE, ...) \
      #RTYPE,
      FLATBUFFERS_GEN_TYPES(FLATBUFFERS_TD)
    #undef FLATBUFFERS_TD
    };
    // clang-format on

    // Enums can be bools, but their Rust representation must be a u8, as used
    // in the repr attribute (#[repr(bool)] is an invalid attribute).
    if (type.base_type == BASE_TYPE_BOOL) return ""u8"";
    return ctypename[type.base_type];
  }

  // Return a Rust type for any type (scalar, table, struct) specifically for
  // using a FlatBuffer.
  std::string GetTypeGet(const Type &type) const {
    switch (GetFullType(type)) {
      case ftInteger:
      case ftFloat:
      case ftBool:
      case ftEnumKey:
      case ftUnionKey: {
        return GetTypeBasic(type);
      }
      case ftArrayOfBuiltin:
      case ftArrayOfEnum:
      case ftArrayOfStruct: {
        return ""["" + GetTypeGet(type.VectorType()) + ""; "" +
               NumToString(type.fixed_length) + ""]"";
      }
      case ftTable: {
        return WrapInNameSpace(type.struct_def->defined_namespace,
                               type.struct_def->name) +
               ""<'a>"";
      }
      default: {
        return WrapInNameSpace(type.struct_def->defined_namespace,
                               type.struct_def->name);
      }
    }
  }

  std::string GetEnumValue(const EnumDef &enum_def,
                           const EnumVal &enum_val) const {
    return Name(enum_def) + ""::"" + Name(enum_val);
  }

  // 1 suffix since old C++ can't figure out the overload.
  void ForAllEnumValues1(const EnumDef &enum_def,
                         std::function<void(const EnumVal &)> cb) {
    for (auto it = enum_def.Vals().begin(); it != enum_def.Vals().end(); ++it) {
      const auto &ev = **it;
      code_.SetValue(""VARIANT"", Name(ev));
      code_.SetValue(""VALUE"", enum_def.ToString(ev));
      code_.IncrementIdentLevel();
      cb(ev);
      code_.DecrementIdentLevel();
    }
  }
  void ForAllEnumValues(const EnumDef &enum_def, std::function<void()> cb) {
    std::function<void(const EnumVal &)> wrapped = [&](const EnumVal &unused) {
      (void)unused;
      cb();
    };
    ForAllEnumValues1(enum_def, wrapped);
  }
  // Generate an enum declaration,
  // an enum string lookup table,
  // an enum match function,
  // and an enum array of values
  void GenEnum(const EnumDef &enum_def) {
    code_.SetValue(""ENUM_NAME"", Name(enum_def));
    code_.SetValue(""BASE_TYPE"", GetEnumTypeForDecl(enum_def.underlying_type));
    code_.SetValue(""ENUM_NAME_SNAKE"", MakeSnakeCase(Name(enum_def)));
    code_.SetValue(""ENUM_NAME_CAPS"", MakeUpper(MakeSnakeCase(Name(enum_def))));
    const EnumVal *minv = enum_def.MinValue();
    const EnumVal *maxv = enum_def.MaxValue();
    FLATBUFFERS_ASSERT(minv && maxv);
    code_.SetValue(""ENUM_MIN_BASE_VALUE"", enum_def.ToString(*minv));
    code_.SetValue(""ENUM_MAX_BASE_VALUE"", enum_def.ToString(*maxv));

    if (IsBitFlagsEnum(enum_def)) {
      // Defer to the convenient and canonical bitflags crate. We declare it in
      // a module to #allow camel case constants in a smaller scope. This
      // matches Flatbuffers c-modeled enums where variants are associated
      // constants but in camel case.
      code_ += ""#[allow(non_upper_case_globals)]"";
      code_ += ""mod bitflags_{{ENUM_NAME_SNAKE}} {"";
      code_ += ""  flatbuffers::bitflags::bitflags! {"";
      GenComment(enum_def.doc_comment, ""    "");
      code_ += ""    #[derive(Default)]"";
      code_ += ""    pub struct {{ENUM_NAME}}: {{BASE_TYPE}} {"";
      ForAllEnumValues1(enum_def, [&](const EnumVal &ev) {
        this->GenComment(ev.doc_comment, ""    "");
        code_ += ""    const {{VARIANT}} = {{VALUE}};"";
      });
      code_ += ""    }"";
      code_ += ""  }"";
      code_ += ""}"";
      code_ += ""pub use self::bitflags_{{ENUM_NAME_SNAKE}}::{{ENUM_NAME}};"";
      code_ += """";

      code_.SetValue(""FROM_BASE"", ""unsafe { Self::from_bits_unchecked(b) }"");
      code_.SetValue(""INTO_BASE"", ""self.bits()"");
    } else {
      // Normal, c-modelled enums.
      // Deprecated associated constants;
      const std::string deprecation_warning =
          ""#[deprecated(since = \""2.0.0\"", note = \""Use associated constants""
          "" instead. This will no longer be generated in 2021.\"")]"";
      code_ += deprecation_warning;
      code_ +=
          ""pub const ENUM_MIN_{{ENUM_NAME_CAPS}}: {{BASE_TYPE}}""
          "" = {{ENUM_MIN_BASE_VALUE}};"";
      code_ += deprecation_warning;
      code_ +=
          ""pub const ENUM_MAX_{{ENUM_NAME_CAPS}}: {{BASE_TYPE}}""
          "" = {{ENUM_MAX_BASE_VALUE}};"";
      auto num_fields = NumToString(enum_def.size());
      code_ += deprecation_warning;
      code_ += ""#[allow(non_camel_case_types)]"";
      code_ += ""pub const ENUM_VALUES_{{ENUM_NAME_CAPS}}: [{{ENUM_NAME}}; "" +
               num_fields + ""] = ["";
      ForAllEnumValues1(enum_def, [&](const EnumVal &ev) {
        code_ += GetEnumValue(enum_def, ev) + "","";
      });
      code_ += ""];"";
      code_ += """";

      GenComment(enum_def.doc_comment);
      // Derive Default to be 0. flatc enforces this when the enum
      // is put into a struct, though this isn't documented behavior, it is
      // needed to derive defaults in struct objects.
      code_ +=
          ""#[derive(Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash, ""
          ""Default)]"";
      code_ += ""#[repr(transparent)]"";
      code_ += ""pub struct {{ENUM_NAME}}(pub {{BASE_TYPE}});"";
      code_ += ""#[allow(non_upper_case_globals)]"";
      code_ += ""impl {{ENUM_NAME}} {"";
      ForAllEnumValues1(enum_def, [&](const EnumVal &ev) {
        this->GenComment(ev.doc_comment);
        code_ += ""pub const {{VARIANT}}: Self = Self({{VALUE}});"";
      });
      code_ += """";
      // Generate Associated constants
      code_ += ""  pub const ENUM_MIN: {{BASE_TYPE}} = {{ENUM_MIN_BASE_VALUE}};"";
      code_ += ""  pub const ENUM_MAX: {{BASE_TYPE}} = {{ENUM_MAX_BASE_VALUE}};"";
      code_ += ""  pub const ENUM_VALUES: &'static [Self] = &["";
      ForAllEnumValues(enum_def, [&]() { code_ += ""  Self::{{VARIANT}},""; });
      code_ += ""  ];"";
      code_ += ""  /// Returns the variant's name or \""\"" if unknown."";
      code_ += ""  pub fn variant_name(self) -> Option<&'static str> {"";
      code_ += ""    match self {"";
      ForAllEnumValues(enum_def, [&]() {
        code_ += ""    Self::{{VARIANT}} => Some(\""{{VARIANT}}\""),"";
      });
      code_ += ""      _ => None,"";
      code_ += ""    }"";
      code_ += ""  }"";
      code_ += ""}"";

      // Generate Debug. Unknown variants are printed like ""<UNKNOWN 42>"".
      code_ += ""impl std::fmt::Debug for {{ENUM_NAME}} {"";
      code_ +=
          ""  fn fmt(&self, f: &mut std::fmt::Formatter) ->""
          "" std::fmt::Result {"";
      code_ += ""    if let Some(name) = self.variant_name() {"";
      code_ += ""      f.write_str(name)"";
      code_ += ""    } else {"";
      code_ += ""      f.write_fmt(format_args!(\""<UNKNOWN {:?}>\"", self.0))"";
      code_ += ""    }"";
      code_ += ""  }"";
      code_ += ""}"";

      code_.SetValue(""FROM_BASE"", ""Self(b)"");
      code_.SetValue(""INTO_BASE"", ""self.0"");
    }

    // Generate Follow and Push so we can serialize and stuff.
    code_ += ""impl<'a> flatbuffers::Follow<'a> for {{ENUM_NAME}} {"";
    code_ += ""  type Inner = Self;"";
    code_ += ""  #[inline]"";
    code_ += ""  fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {"";
    code_ += ""    let b = unsafe {"";
    code_ += ""      flatbuffers::read_scalar_at::<{{BASE_TYPE}}>(buf, loc)"";
    code_ += ""    };"";
    code_ += ""    {{FROM_BASE}}"";
    code_ += ""  }"";
    code_ += ""}"";
    code_ += """";
    code_ += ""impl flatbuffers::Push for {{ENUM_NAME}} {"";
    code_ += ""    type Output = {{ENUM_NAME}};"";
    code_ += ""    #[inline]"";
    code_ += ""    fn push(&self, dst: &mut [u8], _rest: &[u8]) {"";
    code_ +=
        ""        unsafe { flatbuffers::emplace_scalar::<{{BASE_TYPE}}>""
        ""(dst, {{INTO_BASE}}); }"";
    code_ += ""    }"";
    code_ += ""}"";
    code_ += """";
    code_ += ""impl flatbuffers::EndianScalar for {{ENUM_NAME}} {"";
    code_ += ""  #[inline]"";
    code_ += ""  fn to_little_endian(self) -> Self {"";
    code_ += ""    let b = {{BASE_TYPE}}::to_le({{INTO_BASE}});"";
    code_ += ""    {{FROM_BASE}}"";
    code_ += ""  }"";
    code_ += ""  #[inline]"";
    code_ += ""  #[allow(clippy::wrong_self_convention)]"";
    code_ += ""  fn from_little_endian(self) -> Self {"";
    code_ += ""    let b = {{BASE_TYPE}}::from_le({{INTO_BASE}});"";
    code_ += ""    {{FROM_BASE}}"";
    code_ += ""  }"";
    code_ += ""}"";
    code_ += """";

    // Generate verifier - deferring to the base type.
    code_ += ""impl<'a> flatbuffers::Verifiable for {{ENUM_NAME}} {"";
    code_ += ""  #[inline]"";
    code_ += ""  fn run_verifier("";
    code_ += ""    v: &mut flatbuffers::Verifier, pos: usize"";
    code_ += ""  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {"";
    code_ += ""    use self::flatbuffers::Verifiable;"";
    code_ += ""    {{BASE_TYPE}}::run_verifier(v, pos)"";
    code_ += ""  }"";
    code_ += ""}"";
    code_ += """";
    // Enums are basically integers.
    code_ += ""impl flatbuffers::SimpleToVerifyInSlice for {{ENUM_NAME}} {}"";

    if (enum_def.is_union) {
      // Generate typesafe offset(s) for unions
      code_.SetValue(""NAME"", Name(enum_def));
      code_.SetValue(""UNION_OFFSET_NAME"", Name(enum_def) + ""UnionTableOffset"");
      code_ += ""pub struct {{UNION_OFFSET_NAME}} {}"";
      code_ += """";
      if (parser_.opts.generate_object_based_api) { GenUnionObject(enum_def); }
    }
  }

  // CASPER: dedup Object versions from non object versions.
  void ForAllUnionObjectVariantsBesidesNone(const EnumDef &enum_def,
                                            std::function<void()> cb) {
    for (auto it = enum_def.Vals().begin(); it != enum_def.Vals().end(); ++it) {
      auto &enum_val = **it;
      if (enum_val.union_type.base_type == BASE_TYPE_NONE) continue;
      code_.SetValue(""VARIANT_NAME"", Name(enum_val));
      code_.SetValue(""NATIVE_VARIANT"", MakeCamel(Name(enum_val)));
      code_.SetValue(""U_ELEMENT_NAME"", MakeSnakeCase(Name(enum_val)));
      code_.SetValue(""U_ELEMENT_TABLE_TYPE"",
                     NamespacedNativeName(*enum_val.union_type.struct_def));
      code_.IncrementIdentLevel();
      cb();
      code_.DecrementIdentLevel();
    }
  }
  void GenUnionObject(const EnumDef &enum_def) {
    code_.SetValue(""ENUM_NAME"", Name(enum_def));
    code_.SetValue(""ENUM_NAME_SNAKE"", MakeSnakeCase(Name(enum_def)));
    code_.SetValue(""NATIVE_NAME"", NativeName(enum_def));

    // Generate native union.
    code_ += ""#[allow(clippy::upper_case_acronyms)]"";  // NONE's spelling is
                                                       // intended.
    code_ += ""#[non_exhaustive]"";
    code_ += ""#[derive(Debug, Clone, PartialEq)]"";
    code_ += ""pub enum {{NATIVE_NAME}} {"";
    code_ += ""  NONE,"";
    ForAllUnionObjectVariantsBesidesNone(enum_def, [&] {
      code_ += ""{{NATIVE_VARIANT}}(Box<{{U_ELEMENT_TABLE_TYPE}}>),"";
    });
    code_ += ""}"";
    // Generate Default (NONE).
    code_ += ""impl Default for {{NATIVE_NAME}} {"";
    code_ += ""  fn default() -> Self {"";
    code_ += ""    Self::NONE"";
    code_ += ""  }"";
    code_ += ""}"";

    // Generate native union methods.
    code_ += ""impl {{NATIVE_NAME}} {"";

    // Get flatbuffers union key.
    // CASPER: add docstrings?
    code_ += ""  pub fn {{ENUM_NAME_SNAKE}}_type(&self) -> {{ENUM_NAME}} {"";
    code_ += ""    match self {"";
    code_ += ""      Self::NONE => {{ENUM_NAME}}::NONE,"";
    ForAllUnionObjectVariantsBesidesNone(enum_def, [&] {
      code_ +=
          ""    Self::{{NATIVE_VARIANT}}(_) => {{ENUM_NAME}}::""
          ""{{VARIANT_NAME}},"";
    });
    code_ += ""    }"";
    code_ += ""  }"";
    // Pack flatbuffers union value
    code_ +=
        ""  pub fn pack(&self, fbb: &mut flatbuffers::FlatBufferBuilder)""
        "" -> Option<flatbuffers::WIPOffset<flatbuffers::UnionWIPOffset>>""
        "" {"";
    code_ += ""    match self {"";
    code_ += ""      Self::NONE => None,"";
    ForAllUnionObjectVariantsBesidesNone(enum_def, [&] {
      code_ += ""    Self::{{NATIVE_VARIANT}}(v) => \\"";
      code_ += ""Some(v.pack(fbb).as_union_value()),"";
    });
    code_ += ""    }"";
    code_ += ""  }"";

    // Generate some accessors;
    ForAllUnionObjectVariantsBesidesNone(enum_def, [&] {
      // Move accessor.
      code_ +=
          ""/// If the union variant matches, return the owned ""
          ""{{U_ELEMENT_TABLE_TYPE}}, setting the union to NONE."";
      code_ +=
          ""pub fn take_{{U_ELEMENT_NAME}}(&mut self) -> ""
          ""Option<Box<{{U_ELEMENT_TABLE_TYPE}}>> {"";
      code_ += ""  if let Self::{{NATIVE_VARIANT}}(_) = self {"";
      code_ += ""    let v = std::mem::replace(self, Self::NONE);"";
      code_ += ""    if let Self::{{NATIVE_VARIANT}}(w) = v {"";
      code_ += ""      Some(w)"";
      code_ += ""    } else {"";
      code_ += ""      unreachable!()"";
      code_ += ""    }"";
      code_ += ""  } else {"";
      code_ += ""    None"";
      code_ += ""  }"";
      code_ += ""}"";
      // Immutable reference accessor.
      code_ +=
          ""/// If the union variant matches, return a reference to the ""
          ""{{U_ELEMENT_TABLE_TYPE}}."";
      code_ +=
          ""pub fn as_{{U_ELEMENT_NAME}}(&self) -> ""
          ""Option<&{{U_ELEMENT_TABLE_TYPE}}> {"";
      code_ +=
          ""  if let Self::{{NATIVE_VARIANT}}(v) = self ""
          ""{ Some(v.as_ref()) } else { None }"";
      code_ += ""}"";
      // Mutable reference accessor.
      code_ +=
          ""/// If the union variant matches, return a mutable reference""
          "" to the {{U_ELEMENT_TABLE_TYPE}}."";
      code_ +=
          ""pub fn as_{{U_ELEMENT_NAME}}_mut(&mut self) -> ""
          ""Option<&mut {{U_ELEMENT_TABLE_TYPE}}> {"";
      code_ +=
          ""  if let Self::{{NATIVE_VARIANT}}(v) = self ""
          ""{ Some(v.as_mut()) } else { None }"";
      code_ += ""}"";
    });
    code_ += ""}"";  // End union methods impl.
  }

  std::string GetFieldOffsetName(const FieldDef &field) {
    return ""VT_"" + MakeUpper(Name(field));
  }

  enum DefaultContext { kBuilder, kAccessor, kObject };
  std::string GetDefaultValue(const FieldDef &field,
                              const DefaultContext context) {
    if (context == kBuilder) {
      // Builders and Args structs model nonscalars ""optional"" even if they're
      // required or have defaults according to the schema. I guess its because
      // WIPOffset is not nullable.
      if (!IsScalar(field.value.type.base_type) || field.IsOptional()) {
        return ""None"";
      }
    } else {
      // This for defaults in objects.
      // Unions have a NONE variant instead of using Rust's None.
      if (field.IsOptional() && !IsUnion(field.value.type)) { return ""None""; }
    }
    switch (GetFullType(field.value.type)) {
      case ftInteger:
      case ftFloat: {
        return field.value.constant;
      }
      case ftBool: {
        return field.value.constant == ""0"" ? ""false"" : ""true"";
      }
      case ftUnionKey:
      case ftEnumKey: {
        auto ev = field.value.type.enum_def->FindByValue(field.value.constant);
        if (!ev) return ""Default::default()"";  // Bitflags enum.
        return WrapInNameSpace(field.value.type.enum_def->defined_namespace,
                               GetEnumValue(*field.value.type.enum_def, *ev));
      }
      case ftUnionValue: {
        return ObjectFieldType(field, true) + ""::NONE"";
      }
      case ftString: {
        // Required fields do not have defaults defined by the schema, but we
        // need one for Rust's Default trait so we use empty string. The usual
        // value of field.value.constant is `0`, which is non-sensical except
        // maybe to c++ (nullptr == 0).
        // TODO: Escape strings?
        const std::string defval =
            field.IsRequired() ? ""\""\"""" : ""\"""" + field.value.constant + ""\"""";
        if (context == kObject) return defval + "".to_string()"";
        if (context == kAccessor) return ""&"" + defval;
        FLATBUFFERS_ASSERT(false);
        return ""INVALID_CODE_GENERATION"";
      }

      case ftArrayOfStruct:
      case ftArrayOfEnum:
      case ftArrayOfBuiltin:
      case ftVectorOfBool:
      case ftVectorOfFloat:
      case ftVectorOfInteger:
      case ftVectorOfString:
      case ftVectorOfStruct:
      case ftVectorOfTable:
      case ftVectorOfEnumKey:
      case ftVectorOfUnionValue:
      case ftStruct:
      case ftTable: {
        // We only support empty vectors which matches the defaults for
        // &[T] and Vec<T> anyway.
        //
        // For required structs and tables fields, we defer to their object API
        // defaults. This works so long as there's nothing recursive happening,
        // but `table Infinity { i: Infinity (required); }` does compile.
        return ""Default::default()"";
      }
    }
    FLATBUFFERS_ASSERT(false);
    return ""INVALID_CODE_GENERATION"";
  }

  // Create the return type for fields in the *BuilderArgs structs that are
  // used to create Tables.
  //
  // Note: we could make all inputs to the BuilderArgs be an Option, as well
  // as all outputs. But, the UX of Flatbuffers is that the user doesn't get to
  // know if the value is default or not, because there are three ways to
  // return a default value:
  // 1) return a stored value that happens to be the default,
  // 2) return a hardcoded value because the relevant vtable field is not in
  //    the vtable, or
  // 3) return a hardcoded value because the vtable field value is set to zero.
  std::string TableBuilderArgsDefnType(const FieldDef &field,
                                       const std::string &lifetime) {
    const Type &type = field.value.type;
    auto WrapOption = [&](std::string s) {
      return IsOptionalToBuilder(field) ? ""Option<"" + s + "">"" : s;
    };
    auto WrapVector = [&](std::string ty) {
      return WrapOption(""flatbuffers::WIPOffset<flatbuffers::Vector<"" +
                        lifetime + "", "" + ty + "">>"");
    };
    auto WrapUOffsetsVector = [&](std::string ty) {
      return WrapVector(""flatbuffers::ForwardsUOffset<"" + ty + "">"");
    };

    switch (GetFullType(type)) {
      case ftInteger:
      case ftFloat:
      case ftBool: {
        return WrapOption(GetTypeBasic(type));
      }
      case ftStruct: {
        const auto typname = WrapInNameSpace(*type.struct_def);
        return WrapOption(""&"" + lifetime + "" "" + typname);
      }
      case ftTable: {
        const auto typname = WrapInNameSpace(*type.struct_def);
        return WrapOption(""flatbuffers::WIPOffset<"" + typname + ""<"" + lifetime +
                          "">>"");
      }
      case ftString: {
        return WrapOption(""flatbuffers::WIPOffset<&"" + lifetime + "" str>"");
      }
      case ftEnumKey:
      case ftUnionKey: {
        return WrapOption(WrapInNameSpace(*type.enum_def));
      }
      case ftUnionValue: {
        return ""Option<flatbuffers::WIPOffset<flatbuffers::UnionWIPOffset>>"";
      }

      case ftVectorOfInteger:
      case ftVectorOfBool:
      case ftVectorOfFloat: {
        const auto typname = GetTypeBasic(type.VectorType());
        return WrapVector(typname);
      }
      case ftVectorOfEnumKey: {
        const auto typname = WrapInNameSpace(*type.enum_def);
        return WrapVector(typname);
      }
      case ftVectorOfStruct: {
        const auto typname = WrapInNameSpace(*type.struct_def);
        return WrapVector(typname);
      }
      case ftVectorOfTable: {
        const auto typname = WrapInNameSpace(*type.struct_def);
        return WrapUOffsetsVector(typname + ""<"" + lifetime + "">"");
      }
      case ftVectorOfString: {
        return WrapUOffsetsVector(""&"" + lifetime + "" str"");
      }
      case ftVectorOfUnionValue: {
        return WrapUOffsetsVector(""flatbuffers::Table<"" + lifetime + "">"");
      }
      case ftArrayOfEnum:
      case ftArrayOfStruct:
      case ftArrayOfBuiltin: {
        FLATBUFFERS_ASSERT(false && ""arrays are not supported within tables"");
        return ""ARRAYS_NOT_SUPPORTED_IN_TABLES"";
      }
    }
    return ""INVALID_CODE_GENERATION"";  // for return analysis
  }

  std::string ObjectFieldType(const FieldDef &field, bool in_a_table) {
    const Type &type = field.value.type;
    std::string ty;
    switch (GetFullType(type)) {
      case ftInteger:
      case ftBool:
      case ftFloat: {
        ty = GetTypeBasic(type);
        break;
      }
      case ftString: {
        ty = ""String"";
        break;
      }
      case ftStruct: {
        ty = NamespacedNativeName(*type.struct_def);
        break;
      }
      case ftTable: {
        // Since Tables can contain themselves, Box is required to avoid
        // infinite types.
        ty = ""Box<"" + NamespacedNativeName(*type.struct_def) + "">"";
        break;
      }
      case ftUnionKey: {
        // There is no native ""UnionKey"", natively, unions are rust enums with
        // newtype-struct-variants.
        return ""INVALID_CODE_GENERATION"";
      }
      case ftUnionValue: {
        ty = NamespacedNativeName(*type.enum_def);
        break;
      }
      case ftEnumKey: {","I think it makes more sense for this to be at the top of the module root file, or at the top of every generated object file. I believe the `#![...]` attributes will cover all nested modules so there's some redundant coverage and also it currently misses the generated code that's in the root namespace.","The suggested code change appears to be a Rust code generator for FlatBuffers, which is a serialization library. The changes include:

1. **Functionality Enhancements**: 
   - Added support for generating Rust modules and files based on the schema.
   - Implemented functionality to handle bit flags enums and generate appropriate Rust code.
   - Enhanced the handling of table fields, including optional fields and vector types.
   - Added support for generating object-based APIs for tables.

2. **Code Structure**:
   - The code is structured into classes and functions, making it easier to manage and extend.
   - The `RustGenerator` class handles the generation of Rust code for various types of FlatBuffers definitions (enums, structs, tables).

3. **Error Handling**:
   - The code includes checks to ensure that certain conditions are met, such as ensuring that vectors of unions are not supported.
   - It uses assertions to catch errors and ensures that the code is robust.

4. **Documentation**:
   - The code includes comments and documentation to explain the purpose and functionality of each part of the code.

Overall, the changes aim to improve the Rust support in FlatBuffers by providing a more comprehensive and feature-rich code generation system."
18,"@@ -239,12 +239,11 @@ func (k *KeybaseDaemonLocal) Resolve(ctx context.Context, assertion string) (
 		return libkb.NormalizedUsername(""""), keybase1.UserOrTeamID(""""), err
 	}
 
-	// TODO(KBFS-2621): Resolve shouldn't work for implicit teams, but
-	// until CORE-6623 is done, this is required.
-	iti, err := k.localImplicitTeams.getLocalImplicitTeam(id.AsTeamOrBust())
-	if err == nil {
-		// An implicit team exists, so use the display name.
-		return iti.Name, id, nil
+	_, ok := k.localImplicitTeams[id.AsTeamOrBust()]
+	if ok {
+		// An implicit team exists, so Resolve shouldn't work.
+		return libkb.NormalizedUsername(""""), keybase1.UserOrTeamID(""""),
+			fmt.Errorf(""Team ID %s is an implicit team"", id)
 	}
 
 	return ti.Name, id, nil","// Copyright 2016 Keybase Inc. All rights reserved.
// Use of this source code is governed by a BSD
// license that can be found in the LICENSE file.

package libkbfs

import (
	""errors""
	""fmt""
	""sync""

	""golang.org/x/net/context""

	""github.com/keybase/client/go/externals""
	""github.com/keybase/client/go/libkb""
	""github.com/keybase/client/go/protocol/keybase1""
	""github.com/keybase/kbfs/kbfscodec""
	""github.com/keybase/kbfs/kbfscrypto""
	""github.com/keybase/kbfs/kbfsmd""
	""github.com/keybase/kbfs/tlf""
	""github.com/syndtr/goleveldb/leveldb""
	""github.com/syndtr/goleveldb/leveldb/util""
)

type localUserMap map[keybase1.UID]LocalUser

func (m localUserMap) getLocalUser(uid keybase1.UID) (LocalUser, error) {
	user, ok := m[uid]
	if !ok {
		return LocalUser{}, NoSuchUserError{uid.String()}
	}
	return user, nil
}

type localTeamMap map[keybase1.TeamID]TeamInfo

func (m localTeamMap) getLocalTeam(tid keybase1.TeamID) (TeamInfo, error) {
	team, ok := m[tid]
	if !ok {
		return TeamInfo{}, NoSuchTeamError{tid.String()}
	}
	return team, nil
}

type localImplicitTeamMap map[keybase1.TeamID]ImplicitTeamInfo

func (m localImplicitTeamMap) getLocalImplicitTeam(
	tid keybase1.TeamID) (ImplicitTeamInfo, error) {
	team, ok := m[tid]
	if !ok {
		return ImplicitTeamInfo{}, NoSuchTeamError{tid.String()}
	}
	return team, nil
}

type favoriteStore interface {
	FavoriteAdd(uid keybase1.UID, folder keybase1.Folder) error
	FavoriteDelete(uid keybase1.UID, folder keybase1.Folder) error
	FavoriteList(uid keybase1.UID) ([]keybase1.Folder, error)

	Shutdown()
}

type diskFavoriteClient struct {
	favoriteDb *leveldb.DB
	codec      kbfscodec.Codec
}

var _ favoriteStore = diskFavoriteClient{}

func (c diskFavoriteClient) favkey(
	uid keybase1.UID, folder keybase1.Folder) []byte {
	return []byte(fmt.Sprintf(""%s:%s"", uid, folder.ToString()))
}

func (c diskFavoriteClient) FavoriteAdd(
	uid keybase1.UID, folder keybase1.Folder) error {
	enc, err := c.codec.Encode(folder)
	if err != nil {
		return err
	}

	return c.favoriteDb.Put(c.favkey(uid, folder), enc, nil)
}

func (c diskFavoriteClient) FavoriteDelete(
	uid keybase1.UID, folder keybase1.Folder) error {
	return c.favoriteDb.Delete(c.favkey(uid, folder), nil)
}

func (c diskFavoriteClient) FavoriteList(uid keybase1.UID) (
	[]keybase1.Folder, error) {
	iter := c.favoriteDb.NewIterator(util.BytesPrefix([]byte(uid+"":"")), nil)
	defer iter.Release()
	var folders []keybase1.Folder
	for iter.Next() {
		var folder keybase1.Folder
		if err := c.codec.Decode(iter.Value(), &folder); err != nil {
			return nil, err
		}
		folders = append(folders, folder)
	}
	if err := iter.Error(); err != nil {
		return nil, err
	}

	return folders, nil
}

func (c diskFavoriteClient) Shutdown() {
	c.favoriteDb.Close()
}

type memoryFavoriteClient struct {
	favorites map[keybase1.UID]map[string]keybase1.Folder
}

var _ favoriteStore = memoryFavoriteClient{}

func (c memoryFavoriteClient) FavoriteAdd(
	uid keybase1.UID, folder keybase1.Folder) error {
	if c.favorites[uid] == nil {
		c.favorites[uid] = make(map[string]keybase1.Folder)
	}
	c.favorites[uid][folder.ToString()] = folder
	return nil
}

func (c memoryFavoriteClient) FavoriteDelete(
	uid keybase1.UID, folder keybase1.Folder) error {
	if c.favorites[uid] != nil {
		delete(c.favorites[uid], folder.ToString())
	}
	return nil
}

func (c memoryFavoriteClient) FavoriteList(
	uid keybase1.UID) ([]keybase1.Folder, error) {
	folders := make([]keybase1.Folder, len(c.favorites[uid]))
	i := 0
	for _, v := range c.favorites[uid] {
		folders[i] = v
		i++
	}
	return folders, nil
}

func (c memoryFavoriteClient) Shutdown() {}

// KeybaseDaemonLocal implements KeybaseDaemon using an in-memory user
// and session store, and a given favorite store.
type KeybaseDaemonLocal struct {
	codec kbfscodec.Codec

	// lock protects everything below.
	lock               sync.Mutex
	localUsers         localUserMap
	localTeams         localTeamMap
	localImplicitTeams localImplicitTeamMap
	currentUID         keybase1.UID
	asserts            map[string]keybase1.UserOrTeamID
	implicitAsserts    map[string]keybase1.TeamID
	favoriteStore      favoriteStore
	merkleRoot         keybase1.MerkleRootV2
}

var _ KeybaseService = &KeybaseDaemonLocal{}

func (k *KeybaseDaemonLocal) setCurrentUID(uid keybase1.UID) {
	k.lock.Lock()
	defer k.lock.Unlock()
	// TODO: Send out notifications.
	k.currentUID = uid
}

func (k *KeybaseDaemonLocal) assertionToIDLocked(ctx context.Context,
	assertion string) (id keybase1.UserOrTeamID, err error) {
	expr, err := externals.AssertionParseAndOnly(assertion)
	if err != nil {
		return keybase1.UserOrTeamID(""""), err
	}
	urls := expr.CollectUrls(nil)
	if len(urls) == 0 {
		return keybase1.UserOrTeamID(""""), errors.New(""No assertion URLs"")
	}

	for _, url := range urls {
		var currID keybase1.UserOrTeamID
		if url.IsUID() {
			currID = url.ToUID().AsUserOrTeam()
		} else if url.IsTeamID() {
			currID = url.ToTeamID().AsUserOrTeam()
		} else {
			key, val := url.ToKeyValuePair()
			a := fmt.Sprintf(""%s@%s"", val, key)
			if url.IsKeybase() && key != ""team"" {
				a = val
			}
			var ok bool
			currID, ok = k.asserts[a]
			if !ok {
				return keybase1.UserOrTeamID(""""), NoSuchUserError{a}
			}
		}
		if id != keybase1.UserOrTeamID("""") && currID != id {
			return keybase1.UserOrTeamID(""""),
				errors.New(""AND assertions resolve to different UIDs"")
		}
		id = currID
	}
	return id, nil
}

// Resolve implements KeybaseDaemon for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) Resolve(ctx context.Context, assertion string) (
	libkb.NormalizedUsername, keybase1.UserOrTeamID, error) {
	if err := checkContext(ctx); err != nil {
		return libkb.NormalizedUsername(""""), keybase1.UserOrTeamID(""""), err
	}

	k.lock.Lock()
	defer k.lock.Unlock()
	id, err := k.assertionToIDLocked(ctx, assertion)
	if err != nil {
		return libkb.NormalizedUsername(""""), keybase1.UserOrTeamID(""""), err
	}

	if id.IsUser() {
		u, err := k.localUsers.getLocalUser(id.AsUserOrBust())
		if err != nil {
			return libkb.NormalizedUsername(""""), keybase1.UserOrTeamID(""""), err
		}
		return u.Name, id, nil
	}

	// Otherwise it's a team
	ti, err := k.localTeams.getLocalTeam(id.AsTeamOrBust())
	if err != nil {
	_, ok := k.localImplicitTeams[id.AsTeamOrBust()]
	if ok {
		// An implicit team exists, so Resolve shouldn't work.
		return libkb.NormalizedUsername(""""), keybase1.UserOrTeamID(""""),
			fmt.Errorf(""Team ID %s is an implicit team"", id)
		return libkb.NormalizedUsername(""""), keybase1.UserOrTeamID(""""), err
	}

	// TODO(KBFS-2621): Resolve shouldn't work for implicit teams, but
	// until CORE-6623 is done, this is required.
	iti, err := k.localImplicitTeams.getLocalImplicitTeam(id.AsTeamOrBust())
	if err == nil {
		// An implicit team exists, so use the display name.
		return iti.Name, id, nil
	}

	return ti.Name, id, nil
}

// Identify implements KeybaseDaemon for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) Identify(
	ctx context.Context, assertion, _ string) (
	libkb.NormalizedUsername, keybase1.UserOrTeamID, error) {
	// The local daemon doesn't need to distinguish resolves from
	// identifies.
	return k.Resolve(ctx, assertion)
}

func (k *KeybaseDaemonLocal) resolveForImplicitTeam(
	ctx context.Context, name string, r []libkb.NormalizedUsername,
	ur []keybase1.SocialAssertion,
	resolvedIDs map[libkb.NormalizedUsername]keybase1.UserOrTeamID) (
	[]libkb.NormalizedUsername, []keybase1.SocialAssertion, error) {
	id, err := k.assertionToIDLocked(ctx, name)
	if err == nil {
		u, err := k.localUsers.getLocalUser(id.AsUserOrBust())
		if err != nil {
			return nil, nil, err
		}
		r = append(r, u.Name)
		resolvedIDs[u.Name] = id
	} else {
		a, ok := externals.NormalizeSocialAssertion(name)
		if !ok {
			return nil, nil, fmt.Errorf(""Bad assertion: %s"", name)
		}
		ur = append(ur, a)
	}
	return r, ur, nil
}

// ResolveIdentifyImplicitTeam implements the KeybaseService interface
// for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) ResolveIdentifyImplicitTeam(
	ctx context.Context, assertions, suffix string, tlfType tlf.Type,
	doIdentifies bool, reason string) (ImplicitTeamInfo, error) {
	if err := checkContext(ctx); err != nil {
		return ImplicitTeamInfo{}, err
	}

	if tlfType != tlf.Private && tlfType != tlf.Public {
		return ImplicitTeamInfo{}, fmt.Errorf(
			""Invalid implicit team TLF type: %s"", tlfType)
	}

	k.lock.Lock()
	defer k.lock.Unlock()

	// Canonicalize the name.
	writerNames, readerNames, _, err :=
		splitAndNormalizeTLFName(assertions, tlfType)
	if err != nil {
		return ImplicitTeamInfo{}, err
	}
	var writers, readers []libkb.NormalizedUsername
	var unresolvedWriters, unresolvedReaders []keybase1.SocialAssertion
	resolvedIDs := make(map[libkb.NormalizedUsername]keybase1.UserOrTeamID)
	for _, w := range writerNames {
		writers, unresolvedWriters, err = k.resolveForImplicitTeam(
			ctx, w, writers, unresolvedWriters, resolvedIDs)
		if err != nil {
			return ImplicitTeamInfo{}, err
		}
	}
	for _, r := range readerNames {
		readers, unresolvedReaders, err = k.resolveForImplicitTeam(
			ctx, r, readers, unresolvedReaders, resolvedIDs)
		if err != nil {
			return ImplicitTeamInfo{}, err
		}
	}

	var extensions []tlf.HandleExtension
	if len(suffix) != 0 {
		extensions, err = tlf.ParseHandleExtensionSuffix(suffix)
		if err != nil {
			return ImplicitTeamInfo{}, err
		}
	}
	name := tlf.MakeCanonicalName(
		writers, unresolvedWriters, readers, unresolvedReaders, extensions)

	key := fmt.Sprintf(""%s:%s"", tlfType.String(), name)
	tid, ok := k.implicitAsserts[key]
	if ok {
		return k.localImplicitTeams[tid], nil
	}

	// If the implicit team doesn't exist, always create it.

	// Need to make the team info as well, so get the list of user
	// names and resolve them.  Auto-generate an implicit team name.
	implicitName := libkb.NormalizedUsername(
		fmt.Sprintf(""_implicit_%d"", len(k.localTeams)))
	teams := makeLocalTeams(
		[]libkb.NormalizedUsername{implicitName}, len(k.localTeams), tlfType)
	info := teams[0]
	info.Writers = make(map[keybase1.UID]bool, len(writerNames))
	for _, w := range writers {
		id, ok := resolvedIDs[w]
		if !ok {
			return ImplicitTeamInfo{}, fmt.Errorf(""No resolved writer %s"", w)
		}
		info.Writers[id.AsUserOrBust()] = true
	}
	if len(readerNames) > 0 {
		info.Readers = make(map[keybase1.UID]bool, len(readerNames))
		for _, r := range readers {
			id, ok := resolvedIDs[r]
			if !ok {
				return ImplicitTeamInfo{}, fmt.Errorf(
					""No resolved reader %s"", r)

			}
			info.Readers[id.AsUserOrBust()] = true
		}
	}
	// Unresolved users don't need to go in the team info, they're
	// irrelvant until they're resolved.  TODO: add resolved users
	// into existing teams they should be on.

	tid = teams[0].TID
	k.implicitAsserts[key] = tid
	k.localTeams[tid] = info

	asUserName := libkb.NormalizedUsername(name)
	iteamInfo := ImplicitTeamInfo{
		// TODO: use the ""preferred"" canonical format here by listing
		// the logged-in user first?
		Name: asUserName,
		TID:  tid,
	}
	k.localImplicitTeams[tid] = iteamInfo
	return iteamInfo, nil
}

func (k *KeybaseDaemonLocal) addImplicitTeamTlfID(
	tid keybase1.TeamID, tlfID tlf.ID) error {
	// TODO: add check to make sure the private/public suffix of the
	// team ID matches that of the tlf ID.
	k.lock.Lock()
	defer k.lock.Unlock()
	iteamInfo, ok := k.localImplicitTeams[tid]
	if !ok {
		return NoSuchTeamError{tid.String()}
	}
	iteamInfo.TlfID = tlfID
	k.localImplicitTeams[tid] = iteamInfo
	return nil
}

// LoadUserPlusKeys implements KeybaseDaemon for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) LoadUserPlusKeys(ctx context.Context,
	uid keybase1.UID, _ keybase1.KID) (UserInfo, error) {
	if err := checkContext(ctx); err != nil {
		return UserInfo{}, err
	}

	k.lock.Lock()
	defer k.lock.Unlock()
	u, err := k.localUsers.getLocalUser(uid)
	if err != nil {
		return UserInfo{}, err
	}

	var infoCopy UserInfo
	if err := kbfscodec.Update(k.codec, &infoCopy, u.UserInfo); err != nil {
		return UserInfo{}, err
	}
	return infoCopy, nil
}

// LoadTeamPlusKeys implements KeybaseDaemon for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) LoadTeamPlusKeys(
	ctx context.Context, tid keybase1.TeamID, _ kbfsmd.KeyGen, _ keybase1.UserVersion,
	_ keybase1.TeamRole) (TeamInfo, error) {
	if err := checkContext(ctx); err != nil {
		return TeamInfo{}, err
	}

	k.lock.Lock()
	defer k.lock.Unlock()
	t, err := k.localTeams.getLocalTeam(tid)
	if err != nil {
		return TeamInfo{}, err
	}

	// Copy the info since it contains a map that might be mutated.
	var infoCopy TeamInfo
	if err := kbfscodec.Update(k.codec, &infoCopy, t); err != nil {
		return TeamInfo{}, err
	}
	return infoCopy, nil
}

// CreateTeamTLF implements the KBPKI interface for
// KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) CreateTeamTLF(
	ctx context.Context, teamID keybase1.TeamID, tlfID tlf.ID) (err error) {
	// For now, only support implicit teams; regular teams will get a
	// NoSuchTeamError.  TODO: when the keybase1 RPCs allow it, store
	// the TLF ID along with the regular team info.
	//
	// Note that this only adds the implicit team TLF to this instance
	// of KeybaseDaemonLocal; the caller would have to make the call
	// to all instances to make it global.  However, the IDs are
	// always deterministic so any client who thinks the ID is missing
	// will just generate the same one.  TODO: abstract out the users
	// and teams into a shareable module among all the instances.
	return k.addImplicitTeamTlfID(teamID, tlfID)
}

// LoadUnverifiedKeys implements KeybaseDaemon for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) LoadUnverifiedKeys(ctx context.Context, uid keybase1.UID) (
	[]keybase1.PublicKey, error) {
	if err := checkContext(ctx); err != nil {
		return nil, err
	}

	k.lock.Lock()
	defer k.lock.Unlock()
	u, err := k.localUsers.getLocalUser(uid)
	if err != nil {
		return nil, err
	}
	return u.UnverifiedKeys, nil
}

// GetCurrentMerkleRoot implements the KeybaseService interface for
// KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) GetCurrentMerkleRoot(ctx context.Context) (
	keybase1.MerkleRootV2, error) {
	if err := checkContext(ctx); err != nil {
		return keybase1.MerkleRootV2{}, err
	}

	k.lock.Lock()
	defer k.lock.Unlock()
	return k.merkleRoot, nil
}

// CurrentSession implements KeybaseDaemon for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) CurrentSession(ctx context.Context, sessionID int) (
	SessionInfo, error) {
	if err := checkContext(ctx); err != nil {
		return SessionInfo{}, err
	}

	k.lock.Lock()
	defer k.lock.Unlock()
	u, err := k.localUsers.getLocalUser(k.currentUID)
	if err != nil {
		return SessionInfo{}, err
	}
	return SessionInfo{
		Name:           u.Name,
		UID:            u.UID,
		CryptPublicKey: u.GetCurrentCryptPublicKey(),
		VerifyingKey:   u.GetCurrentVerifyingKey(),
	}, nil
}

// addNewAssertionForTest makes newAssertion, which should be a single
// assertion that doesn't already resolve to anything, resolve to the
// same UID as oldAssertion, which should be an arbitrary assertion
// that does already resolve to something.  It returns the UID of the
// user associated with the given assertions.
func (k *KeybaseDaemonLocal) addNewAssertionForTest(
	oldAssertion, newAssertion string) (keybase1.UID, error) {
	k.lock.Lock()
	defer k.lock.Unlock()
	id, err := k.assertionToIDLocked(context.Background(), oldAssertion)
	if err != nil {
		return keybase1.UID(""""), err
	}
	uid := id.AsUserOrBust()

	lu, err := k.localUsers.getLocalUser(uid)
	if err != nil {
		return keybase1.UID(""""), err
	}
	lu.Asserts = append(lu.Asserts, newAssertion)
	k.asserts[newAssertion] = id
	k.localUsers[uid] = lu
	return uid, nil
}

// addNewAssertionForTestOrBust is like addNewAssertionForTest, but
// panics if there's an error.
func (k *KeybaseDaemonLocal) addNewAssertionForTestOrBust(
	oldAssertion, newAssertion string) keybase1.UID {
	uid, err := k.addNewAssertionForTest(oldAssertion, newAssertion)
	if err != nil {
		panic(err)
	}
	return uid
}

// changeTeamNameForTest updates the name of an existing team.
func (k *KeybaseDaemonLocal) changeTeamNameForTest(
	oldName, newName string) (keybase1.TeamID, error) {
	k.lock.Lock()
	defer k.lock.Unlock()
	oldAssert := oldName + ""@team""
	newAssert := newName + ""@team""

	id, ok := k.asserts[oldAssert]
	if !ok {
		return keybase1.TeamID(""""),
			fmt.Errorf(""No such old team name: %s"", oldName)
	}
	tid, err := id.AsTeam()
	if err != nil {
		return keybase1.TeamID(""""), err
	}

	team, ok := k.localTeams[tid]
	if !ok {
		return keybase1.TeamID(""""),
			fmt.Errorf(""No such old team name: %s/%s"", oldName, tid)
	}
	team.Name = libkb.NormalizedUsername(newName)
	k.localTeams[tid] = team

	k.asserts[newAssert] = id
	delete(k.asserts, oldAssert)
	return tid, nil
}

// changeTeamNameForTestOrBust is like changeTeamNameForTest, but
// panics if there's an error.
func (k *KeybaseDaemonLocal) changeTeamNameForTestOrBust(
	oldName, newName string) keybase1.TeamID {
	tid, err := k.changeTeamNameForTest(oldName, newName)
	if err != nil {
		panic(err)
	}
	return tid
}

func (k *KeybaseDaemonLocal) removeAssertionForTest(assertion string) {
	k.lock.Lock()
	defer k.lock.Unlock()
	delete(k.asserts, assertion)
}

type makeKeysFunc func(libkb.NormalizedUsername, int) (
	kbfscrypto.CryptPublicKey, kbfscrypto.VerifyingKey)

func (k *KeybaseDaemonLocal) addDeviceForTesting(uid keybase1.UID,
	makeKeys makeKeysFunc) (int, error) {
	k.lock.Lock()
	defer k.lock.Unlock()

	user, err := k.localUsers.getLocalUser(uid)
	if err != nil {
		return 0, fmt.Errorf(""No such user %s: %v"", uid, err)
	}

	index := len(user.VerifyingKeys)
	newCryptPublicKey, newVerifyingKey := makeKeys(user.Name, index)
	user.VerifyingKeys = append(user.VerifyingKeys, newVerifyingKey)
	user.CryptPublicKeys = append(user.CryptPublicKeys, newCryptPublicKey)

	k.localUsers[uid] = user
	return index, nil
}

func (k *KeybaseDaemonLocal) revokeDeviceForTesting(clock Clock,
	uid keybase1.UID, index int) error {
	k.lock.Lock()
	defer k.lock.Unlock()

	user, err := k.localUsers.getLocalUser(uid)
	if err != nil {
		return fmt.Errorf(""No such user %s: %v"", uid, err)
	}

	if index >= len(user.VerifyingKeys) ||
		(k.currentUID == uid && index == user.CurrentCryptPublicKeyIndex) {
		return fmt.Errorf(""Can't revoke index %d"", index)
	}

	if user.RevokedVerifyingKeys == nil {
		user.RevokedVerifyingKeys =
			make(map[kbfscrypto.VerifyingKey]keybase1.KeybaseTime)
	}
	if user.RevokedCryptPublicKeys == nil {
		user.RevokedCryptPublicKeys =
			make(map[kbfscrypto.CryptPublicKey]keybase1.KeybaseTime)
	}

	kbtime := keybase1.KeybaseTime{
		Unix:  keybase1.ToTime(clock.Now()),
		Chain: 100,
	}
	user.RevokedVerifyingKeys[user.VerifyingKeys[index]] = kbtime
	user.RevokedCryptPublicKeys[user.CryptPublicKeys[index]] = kbtime

	user.VerifyingKeys = append(user.VerifyingKeys[:index],
		user.VerifyingKeys[index+1:]...)
	user.CryptPublicKeys = append(user.CryptPublicKeys[:index],
		user.CryptPublicKeys[index+1:]...)

	if k.currentUID == uid && index < user.CurrentCryptPublicKeyIndex {
		user.CurrentCryptPublicKeyIndex--
	}
	if k.currentUID == uid && index < user.CurrentVerifyingKeyIndex {
		user.CurrentVerifyingKeyIndex--
	}

	k.localUsers[uid] = user
	return nil
}

func (k *KeybaseDaemonLocal) switchDeviceForTesting(uid keybase1.UID,
	index int) error {
	k.lock.Lock()
	defer k.lock.Unlock()

	user, err := k.localUsers.getLocalUser(uid)
	if err != nil {
		return fmt.Errorf(""No such user %s: %v"", uid, err)
	}

	if index >= len(user.CryptPublicKeys) {
		return fmt.Errorf(""Wrong crypt public key index: %d"", index)
	}
	user.CurrentCryptPublicKeyIndex = index

	if index >= len(user.VerifyingKeys) {
		return fmt.Errorf(""Wrong verifying key index: %d"", index)
	}
	user.CurrentVerifyingKeyIndex = index
	k.localUsers[uid] = user
	return nil
}

func (k *KeybaseDaemonLocal) addTeamWriterForTest(
	tid keybase1.TeamID, uid keybase1.UID) error {
	k.lock.Lock()
	defer k.lock.Unlock()
	t, err := k.localTeams.getLocalTeam(tid)
	if err != nil {
		return err
	}

	if t.Writers == nil {
		t.Writers = make(map[keybase1.UID]bool)
	}
	t.Writers[uid] = true
	delete(t.Readers, uid)
	k.localTeams[tid] = t
	f := keybase1.Folder{
		Name:       string(t.Name),
		FolderType: keybase1.FolderType_TEAM,
	}
	k.favoriteStore.FavoriteAdd(uid, f)
	return nil
}

func (k *KeybaseDaemonLocal) addTeamReaderForTest(
	tid keybase1.TeamID, uid keybase1.UID) error {
	k.lock.Lock()
	defer k.lock.Unlock()
	t, err := k.localTeams.getLocalTeam(tid)
	if err != nil {
		return err
	}

	if t.Writers[uid] {
		// Being a writer already implies being a reader.
		return nil
	}

	if t.Readers == nil {
		t.Readers = make(map[keybase1.UID]bool)
	}
	t.Readers[uid] = true
	k.localTeams[tid] = t
	f := keybase1.Folder{
		Name:       string(t.Name),
		FolderType: keybase1.FolderType_TEAM,
	}
	k.favoriteStore.FavoriteAdd(uid, f)
	return nil
}

func (k *KeybaseDaemonLocal) addTeamKeyForTest(
	tid keybase1.TeamID, newKeyGen kbfsmd.KeyGen,
	newKey kbfscrypto.TLFCryptKey) error {
	k.lock.Lock()
	defer k.lock.Unlock()
	t, err := k.localTeams.getLocalTeam(tid)
	if err != nil {
		return err
	}

	t.CryptKeys[newKeyGen] = newKey
	if newKeyGen > t.LatestKeyGen {
		t.LatestKeyGen = newKeyGen
		// Only need to save back to the map if we've modified a
		// non-reference type like the latest key gen.
		k.localTeams[tid] = t
	}
	return nil
}

func (k *KeybaseDaemonLocal) addTeamsForTestLocked(teams []TeamInfo) {
	for _, t := range teams {
		k.localTeams[t.TID] = t
		k.asserts[string(t.Name)+""@team""] = t.TID.AsUserOrTeam()
		f := keybase1.Folder{
			Name:       string(t.Name),
			FolderType: keybase1.FolderType_TEAM,
		}
		for u := range t.Writers {
			k.favoriteStore.FavoriteAdd(u, f)
		}
		for u := range t.Readers {
			k.favoriteStore.FavoriteAdd(u, f)
		}
	}
}

func (k *KeybaseDaemonLocal) addTeamsForTest(teams []TeamInfo) {
	k.lock.Lock()
	defer k.lock.Unlock()
	k.addTeamsForTestLocked(teams)
}

// FavoriteAdd implements KeybaseDaemon for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) FavoriteAdd(
	ctx context.Context, folder keybase1.Folder) error {
	if err := checkContext(ctx); err != nil {
		return err
	}

	k.lock.Lock()
	defer k.lock.Unlock()
	return k.favoriteStore.FavoriteAdd(k.currentUID, folder)
}

// FavoriteDelete implements KeybaseDaemon for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) FavoriteDelete(
	ctx context.Context, folder keybase1.Folder) error {
	if err := checkContext(ctx); err != nil {
		return err
	}

	k.lock.Lock()
	defer k.lock.Unlock()
	return k.favoriteStore.FavoriteDelete(k.currentUID, folder)
}

// FavoriteList implements KeybaseDaemon for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) FavoriteList(
	ctx context.Context, sessionID int) ([]keybase1.Folder, error) {
	if err := checkContext(ctx); err != nil {
		return nil, err
	}

	k.lock.Lock()
	defer k.lock.Unlock()
	return k.favoriteStore.FavoriteList(k.currentUID)
}

// Notify implements KeybaseDaemon for KeybaseDeamonLocal.
func (k *KeybaseDaemonLocal) Notify(ctx context.Context, notification *keybase1.FSNotification) error {
	return checkContext(ctx)
}

// NotifySyncStatus implements KeybaseDaemon for KeybaseDeamonLocal.
func (k *KeybaseDaemonLocal) NotifySyncStatus(ctx context.Context,
	_ *keybase1.FSPathSyncStatus) error {
	return checkContext(ctx)
}

// FlushUserFromLocalCache implements the KeybaseDaemon interface for
// KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) FlushUserFromLocalCache(ctx context.Context,
	uid keybase1.UID) {
	// Do nothing.
}

// FlushUserUnverifiedKeysFromLocalCache implements the KeybaseDaemon interface for
// KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) FlushUserUnverifiedKeysFromLocalCache(ctx context.Context,
	uid keybase1.UID) {
	// Do nothing.
}

// EstablishMountDir implements the KeybaseDaemon interface for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) EstablishMountDir(ctx context.Context) (string, error) {
	return """", nil
}

// PutGitMetadata implements the KeybaseService interface for
// KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) PutGitMetadata(
	ctx context.Context, folder keybase1.Folder, repoID keybase1.RepoID,
	repoName keybase1.GitRepoName) error {
	return nil
}

// Shutdown implements KeybaseDaemon for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) Shutdown() {
	k.favoriteStore.Shutdown()
}

// NewKeybaseDaemonDisk constructs a KeybaseDaemonLocal object given a
// set of possible users, and one user that should be ""logged in"".
// Any storage (e.g. the favorites) persists to disk.
func NewKeybaseDaemonDisk(currentUID keybase1.UID, users []LocalUser,
	teams []TeamInfo, favDBFile string, codec kbfscodec.Codec) (
	*KeybaseDaemonLocal, error) {
	favoriteDb, err := leveldb.OpenFile(favDBFile, leveldbOptions)
	if err != nil {
		return nil, err
	}
	favoriteStore := diskFavoriteClient{favoriteDb, codec}
	return newKeybaseDaemonLocal(
		codec, currentUID, users, teams, favoriteStore), nil
}

// NewKeybaseDaemonMemory constructs a KeybaseDaemonLocal object given
// a set of possible users, and one user that should be ""logged in"".
// Any storage (e.g. the favorites) is kept in memory only.
func NewKeybaseDaemonMemory(currentUID keybase1.UID,
	users []LocalUser, teams []TeamInfo,
	codec kbfscodec.Codec) *KeybaseDaemonLocal {
	favoriteStore := memoryFavoriteClient{
		favorites: make(map[keybase1.UID]map[string]keybase1.Folder),
	}
	return newKeybaseDaemonLocal(codec, currentUID, users, teams, favoriteStore)
}

func newKeybaseDaemonLocal(codec kbfscodec.Codec,
	currentUID keybase1.UID, users []LocalUser, teams []TeamInfo,
	favoriteStore favoriteStore) *KeybaseDaemonLocal {
	localUserMap := make(localUserMap)
	asserts := make(map[string]keybase1.UserOrTeamID)
	for _, u := range users {
		localUserMap[u.UID] = u
		for _, a := range u.Asserts {
			asserts[a] = u.UID.AsUserOrTeam()
		}
		asserts[string(u.Name)] = u.UID.AsUserOrTeam()
	}
	k := &KeybaseDaemonLocal{
		codec:              codec,
		localUsers:         localUserMap,
		localTeams:         make(localTeamMap),
		localImplicitTeams: make(localImplicitTeamMap),
		asserts:            asserts,
		implicitAsserts:    make(map[string]keybase1.TeamID),
		currentUID:         currentUID,
		favoriteStore:      favoriteStore,
		// TODO: let test fill in valid merkle root.
	}
	k.addTeamsForTest(teams)
	return k
}
",Meaning we should be using `ResolveImplicitTeamByID`?,"The suggested code change introduces two new types, `localUserMap` and `localTeamMap`, which are used to store and retrieve local users and teams by their respective IDs. This change improves the organization and readability of the code by encapsulating the logic for managing these data structures. Additionally, the `diskFavoriteClient` and `memoryFavoriteClient` structs are updated to use these maps for storing and retrieving favorite folders. The `KeybaseDaemonLocal` struct is also updated to use these maps for managing users and teams, and to use the new favorite store implementations. Overall, this change enhances the maintainability and scalability of the codebase."
19,"@@ -42,7 +42,7 @@ public class RemoteNetworkConnection implements NetworkConnection {
   @Override
   public ConnectionType setNetworkConnection(
       ConnectionType type) {
-    Map<String, ConnectionType> mode = ImmutableMap.of(""type"", type);
+    Map<String, Integer> mode = ImmutableMap.of(""type"", type.getBitMask());
     return new ConnectionType(((Number) executeMethod.execute(DriverCommand.SET_NETWORK_CONNECTION,
                                                             ImmutableMap
                                                                 .of(""parameters"", mode)))","// Licensed to the Software Freedom Conservancy (SFC) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The SFC licenses this file
// to you under the Apache License, Version 2.0 (the
// ""License""); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

package org.openqa.selenium.remote.mobile;

import com.google.common.collect.ImmutableMap;

import org.openqa.selenium.mobile.NetworkConnection;
import org.openqa.selenium.remote.DriverCommand;
import org.openqa.selenium.remote.ExecuteMethod;

import java.util.Map;

public class RemoteNetworkConnection implements NetworkConnection {

  private final ExecuteMethod executeMethod;

  public RemoteNetworkConnection(ExecuteMethod executeMethod) {
    this.executeMethod = executeMethod;
  }

  @Override
  public ConnectionType getNetworkConnection() {
    return new ConnectionType(((Number) executeMethod.execute(DriverCommand.GET_NETWORK_CONNECTION,
                                                               null)).intValue());
  }

    Map<String, Integer> mode = ImmutableMap.of(""type"", type.getBitMask());
  @Override
  public ConnectionType setNetworkConnection(
      ConnectionType type) {
    Map<String, ConnectionType> mode = ImmutableMap.of(""type"", type);
    return new ConnectionType(((Number) executeMethod.execute(DriverCommand.SET_NETWORK_CONNECTION,
                                                            ImmutableMap
                                                                .of(""parameters"", mode)))
                                  .intValue());
  }
}
",can you change this instead to just `type.toString()` and then you wouldn't have to expose the getBitMask in the enum. (Alternatively you could have used `type.hashCode()` but that doesn't feel as nice),"The suggested code change appears to be implementing a `RemoteNetworkConnection` class that provides methods to interact with network connection settings on a mobile device using Selenium's remote capabilities. Here's a concise review:

1. **Class Definition**: The class `RemoteNetworkConnection` implements the `NetworkConnection` interface, which suggests it will handle network connection management.

2. **Constructor**: The constructor takes an `ExecuteMethod` object, which is likely used to send commands to the remote WebDriver server.

3. **getNetworkConnection Method**:
   - This method retrieves the current network connection type by executing a `GET_NETWORK_CONNECTION` command.
   - It converts the result from the server into a `ConnectionType` enum value.

4. **setNetworkConnection Method**:
   - This method sets the network connection type by executing a `SET_NETWORK_CONNECTION` command.
   - It constructs a map with the desired connection type and sends it to the server.
   - The result is converted back into a `ConnectionType` enum value.

5. **ImmutableMap Usage**: The `ImmutableMap.of` method is used to create maps for sending parameters to the server. This ensures that the maps are immutable, which can be beneficial for security and immutability guarantees.

6. **Code Style**: The code is generally clean and follows Java naming conventions. However, there are a few minor improvements that could be made:
   - Ensure that the `ConnectionType` enum is properly defined and used throughout the class.
   - Consider adding comments to explain the purpose of each method and the logic within them.

Overall, the code is well-structured and should work as intended for managing network connections on a mobile device using Selenium's remote capabilities."
20,"@@ -341,13 +341,17 @@ module Beaker
         # @param [Hash{Symbol=>String}] opts An options hash
         # @option opts [String] :version The version of Puppet to install, required
         # @option opts [String] :win_download_url The url to download puppet from
+        #
+        # @note on windows, the +:ruby_arch+ host parameter can determine in addition
+        # to other settings whether the 32 or 64bit install is used
         def install_puppet_from_msi( host, opts )
           #only install 64bit builds if
           # - we are on puppet version 3.7+
           # - we do not have install_32 set on host
           # - we do not have install_32 set globally
           version = opts[:version]
-          if !(version_is_less(version, '3.7')) and host.is_x86_64? and not host['install_32'] and not opts['install_32']
+          is_config_32 = host['ruby_arch'] == 'x86' || host['install_32'] || opts['install_32']
+          if !(version_is_less(version, '3.7')) && host.is_x86_64? && !is_config_32
             host['dist'] = ""puppet-#{version}-x64""
           else
             host['dist'] = ""puppet-#{version}""","module Beaker
  module DSL
    module InstallUtils
      #
      # This module contains methods to install FOSS puppet from various sources
      #
      # To mix this is into a class you need the following:
      # * a method *hosts* that yields any hosts implementing
      #   {Beaker::Host}'s interface to act upon.
      # * a method *options* that provides an options hash, see {Beaker::Options::OptionsHash}
      # * the module {Beaker::DSL::Roles} that provides access to the various hosts implementing
      #   {Beaker::Host}'s interface to act upon
      # * the module {Beaker::DSL::Wrappers} the provides convenience methods for {Beaker::DSL::Command} creation
      module PuppetUtils

        # The default install path
        SourcePath  = ""/opt/puppet-git-repos""

        # A regex to know if the uri passed is pointing to a git repo
        GitURI       = %r{^(git|https?|file)://|^git@|^gitmirror@}

        # Github's ssh signature for cloning via ssh
        GitHubSig   = 'github.com,207.97.227.239 ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAq2A7hRGmdnm9tUDbO9IDSwBK6TbQa+PXYPCPy6rbTrTtw7PHkccKrpp0yVhp5HdEIcKr6pLlVDBfOLX9QUsyCOV0wzfjIJNlGEYsdlLJizHhbn2mUjvSAHQqZETYP81eFzLQNnPHt4EVVUh7VfDESU84KezmD5QlWpXLmvU31/yMf+Se8xhHTvKSCZIFImWwoG6mbUoWf9nzpIoaSjB+weqqUUmpaaasXVal72J+UX2B+2RPW3RcT0eOzQgqlJL3RKrTJvdsjE3JEAvGq3lGHSZXy28G3skua2SmVi/w4yCE6gbODqnTWlg7+wC604ydGXA8VJiS5ap43JXiUFFAaQ=='

        # @param [String] uri A uri in the format of <git uri>#<revision>
        #                     the `git://`, `http://`, `https://`, and ssh
        #                     (if cloning as the remote git user) protocols
        #                     are valid for <git uri>
        #
        # @example Usage
        #     project = extract_repo_info_from 'git@github.com:puppetlabs/SuperSecretSauce#what_is_justin_doing'
        #
        #     puts project[:name]
        #     #=> 'SuperSecretSauce'
        #
        #     puts project[:rev]
        #     #=> 'what_is_justin_doing'
        #
        # @return [Hash{Symbol=>String}] Returns a hash containing the project
        #                                name, repository path, and revision
        #                                (defaults to HEAD)
        #
        def extract_repo_info_from uri
          require 'pathname'
          project = {}
          repo, rev = uri.split('#', 2)
          project[:name] = Pathname.new(repo).basename('.git').to_s
          project[:path] = repo
          project[:rev]  = rev || 'HEAD'
          return project
        end

        # Takes an array of package info hashes (like that returned from
        # {#extract_repo_info_from}) and sorts the `puppet`, `facter`, `hiera`
        # packages so that puppet's dependencies will be installed first.
        #
        # @!visibility private
        def order_packages packages_array
          puppet = packages_array.select {|e| e[:name] == 'puppet' }
          puppet_depends_on = packages_array.select do |e|
            e[:name] == 'hiera' or e[:name] == 'facter'
          end
          depends_on_puppet = (packages_array - puppet) - puppet_depends_on
          [puppet_depends_on, puppet, depends_on_puppet].flatten
        end

        # @param [Host] host An object implementing {Beaker::Hosts}'s
        #                    interface.
        # @param [String] path The path on the remote [host] to the repository
        # @param [Hash{Symbol=>String}] repository A hash representing repo
        #                                          info like that emitted by
        #                                          {#extract_repo_info_from}
        #
        # @example Getting multiple project versions
        #     versions = [puppet_repo, facter_repo, hiera_repo].inject({}) do |vers, repo_info|
        #       vers.merge(find_git_repo_versions(host, '/opt/git-puppet-repos', repo_info) )
        #     end
        # @return [Hash] Executes git describe on [host] and returns a Hash
        #                with the key of [repository[:name]] and value of
        #                the output from git describe.
        #
        # @note This requires the helper methods:
        #       * {Beaker::DSL::Structure#step}
        #       * {Beaker::DSL::Helpers#on}
        #
        def find_git_repo_versions host, path, repository
          version = {}
          step ""Grab version for #{repository[:name]}"" do
            on host, ""cd #{path}/#{repository[:name]} && "" +
                      ""git describe || true"" do
              version[repository[:name]] = stdout.chomp
            end
          end
          version
        end

        #
        # @see #find_git_repo_versions
        def install_from_git host, path, repository
          name          = repository[:name]
          repo          = repository[:path]
          rev           = repository[:rev]
          depth         = repository[:depth]
          depth_branch  = repository[:depth_branch]
          target        = ""#{path}/#{name}""

          if (depth_branch.nil?)
            depth_branch = rev
          end

          clone_cmd = ""git clone #{repo} #{target}""
          if (depth)
            clone_cmd = ""git clone --branch #{depth_branch} --depth #{depth} #{repo} #{target}""
          end

          step ""Clone #{repo} if needed"" do
            on host, ""test -d #{path} || mkdir -p #{path}""
            on host, ""test -d #{target} || #{clone_cmd}""
          end

          step ""Update #{name} and check out revision #{rev}"" do
            commands = [""cd #{target}"",
                        ""remote rm origin"",
                        ""remote add origin #{repo}"",
                        ""fetch origin +refs/pull/*:refs/remotes/origin/pr/* +refs/heads/*:refs/remotes/origin/*"",
                        ""clean -fdx"",
                        ""checkout -f #{rev}""]
            on host, commands.join("" && git "")
          end

          step ""Install #{name} on the system"" do
            # The solaris ruby IPS package has bindir set to /usr/ruby/1.8/bin.
            # However, this is not the path to which we want to deliver our
            # binaries. So if we are using solaris, we have to pass the bin and
            # sbin directories to the install.rb
            install_opts = ''
            install_opts = '--bindir=/usr/bin --sbindir=/usr/sbin' if
              host['platform'].include? 'solaris'

              on host,  ""cd #{target} && "" +
                        ""if [ -f install.rb ]; then "" +
                        ""ruby ./install.rb #{install_opts}; "" +
                        ""else true; fi""
          end
        end

        #Install FOSS based upon host configuration and options
        # @example will install puppet 3.6.1 from native puppetlabs provided packages wherever possible and will fail over to gem installation when impossible
        #  install_puppet({
        #    :version          => '3.6.1',
        #    :facter_version   => '2.0.1',
        #    :hiera_version    => '1.3.3',
        #    :default_action   => 'gem_install',
        #
        #   })
        #
        #
        # @example Will install latest packages on Enterprise Linux and Debian based distros and fail hard on all othere platforms.
        #  install_puppet()
        #
        # @note This will attempt to add a repository for apt.puppetlabs.com on
        #       Debian, Ubuntu, or Cumulus machines, or yum.puppetlabs.com on EL or Fedora
        #       machines, then install the package 'puppet'.
        # @param [Hash{Symbol=>String}] opts
        # @option opts [String] :version Version of puppet to download
        # @option opts [String] :mac_download_url Url to download msi pattern of %url%/puppet-%version%.msi
        # @option opts [String] :win_download_url Url to download dmg  pattern of %url%/(puppet|hiera|facter)-%version%.msi
        #
        # @return nil
        # @raise [StandardError] When encountering an unsupported platform by default, or if gem cannot be found when default_action => 'gem_install'
        # @raise [FailTest] When error occurs during the actual installation process
        def install_puppet(opts = {})
          default_download_url = 'http://downloads.puppetlabs.com'
          opts = {:win_download_url => ""#{default_download_url}/windows"",
                  :mac_download_url => ""#{default_download_url}/mac""}.merge(opts)
          hosts.each do |host|
            if host['platform'] =~ /el-(5|6|7)/
              relver = $1
              install_puppet_from_rpm host, opts.merge(:release => relver, :family => 'el')
            elsif host['platform'] =~ /fedora-(\d+)/
              relver = $1
              install_puppet_from_rpm host, opts.merge(:release => relver, :family => 'fedora')
            elsif host['platform'] =~ /(ubuntu|debian|cumulus)/
              install_puppet_from_deb host, opts
            elsif host['platform'] =~ /windows/
              relver = opts[:version]
              install_puppet_from_msi host, opts
            elsif host['platform'] =~ /osx/
              install_puppet_from_dmg host, opts
            else
              if opts[:default_action] == 'gem_install'
                install_puppet_from_gem host, opts
              else
                raise ""install_puppet() called for unsupported platform '#{host['platform']}' on '#{host.name}'""
              end
            end

            # Certain install paths may not create the config dirs/files needed
            on host, ""mkdir -p #{host['puppetpath']}"" unless host[:type] =~ /aio/
            on host, ""echo '' >> #{host.puppet['hiera_config']}""
          end
          nil
        end

        # Configure puppet.conf for all hosts based upon a provided Hash
        # @param [Hash{Symbol=>String}] opts
        # @option opts [Hash{String=>String}] :main configure the main section of puppet.conf
        # @option opts [Hash{String=>String}] :agent configure the agent section of puppet.conf
        #
        # @return nil
        def configure_puppet(opts={})
          hosts.each do |host|
            configure_puppet_on(host,opts)
          end
        end

        # Configure puppet.conf on the given host based upon a provided hash
        # @param [Host] host The host to configure puppet.conf on
        # @param [Hash{Symbol=>String}] opts
        # @option opts [Hash{String=>String}] :main configure the main section of puppet.conf
        # @option opts [Hash{String=>String}] :agent configure the agent section of puppet.conf
        #
        # @example will configure /etc/puppet.conf on the puppet master.
        #   config = {
        #     'main' => {
        #       'server'   => 'testbox.test.local',
        #       'certname' => 'testbox.test.local',
        #       'logdir'   => '/var/log/puppet',
        #       'vardir'   => '/var/lib/puppet',
        #       'ssldir'   => '/var/lib/puppet/ssl',
        #       'rundir'   => '/var/run/puppet'
        #     },
        #     'agent' => {
        #       'environment' => 'dev'
        #     }
        #   }
        #   configure_puppet(master, config)
        #
        # @return nil
        def configure_puppet_on(host, opts = {})
          if host['platform'] =~ /windows/
            puppet_conf = host.puppet['config']
            conf_data = ''
            opts.each do |section,options|
              conf_data << ""[#{section}]`n""
              options.each do |option,value|
                conf_data << ""#{option}=#{value}`n""
              end
              conf_data << ""`n""
            end
            on host, powershell(""\$text = \\\""#{conf_data}\\\""; Set-Content -path '#{puppet_conf}' -value \$text"")
          else
            puppet_conf = host.puppet['config']
            conf_data = ''
            opts.each do |section,options|
              conf_data << ""[#{section}]\n""
              options.each do |option,value|
                conf_data << ""#{option}=#{value}\n""
              end
              conf_data << ""\n""
            end
            on host, ""echo \""#{conf_data}\"" > #{puppet_conf}""
          end
        end

        # Installs Puppet and dependencies using rpm
        #
        # @param [Host] host The host to install packages on
        # @param [Hash{Symbol=>String}] opts An options hash
        # @option opts [String] :version The version of Puppet to install, if nil installs latest version
        # @option opts [String] :facter_version The version of Facter to install, if nil installs latest version
        # @option opts [String] :hiera_version The version of Hiera to install, if nil installs latest version
        # @option opts [String] :default_action What to do if we don't know how to install native packages on host.
        #                                       Valid value is 'gem_install' or nil. If nil raises an exception when
        #                                       on an unsupported platform. When 'gem_install' attempts to install
        #                                       Puppet via gem.
        # @option opts [String] :release The major release of the OS
        # @option opts [String] :family The OS family (one of 'el' or 'fedora')
        #
        # @return nil
        # @api private
        def install_puppet_from_rpm( host, opts )
          release_package_string = ""http://yum.puppetlabs.com/puppetlabs-release-#{opts[:family]}-#{opts[:release]}.noarch.rpm""

          on host, ""rpm -q --quiet puppetlabs-release || rpm -ivh #{release_package_string}""

          if opts[:facter_version]
            on host, ""yum install -y facter-#{opts[:facter_version]}""
          end

          if opts[:hiera_version]
            on host, ""yum install -y hiera-#{opts[:hiera_version]}""
          end

          puppet_pkg = opts[:version] ? ""puppet-#{opts[:version]}"" : 'puppet'
          on host, ""yum install -y #{puppet_pkg}""
        end

        # Installs Puppet and dependencies from deb
        #
        # @param [Host] host The host to install packages on
        # @param [Hash{Symbol=>String}] opts An options hash
        # @option opts [String] :version The version of Puppet to install, if nil installs latest version
        # @option opts [String] :facter_version The version of Facter to install, if nil installs latest version
        # @option opts [String] :hiera_version The version of Hiera to install, if nil installs latest version
        #
        # @return nil
        # @api private
        def install_puppet_from_deb( host, opts )
          if ! host.check_for_package 'lsb-release'
            host.install_package('lsb-release')
          end

          if ! host.check_for_command 'curl'
            on host, 'apt-get install -y curl'
          end

          on host, 'curl -O http://apt.puppetlabs.com/puppetlabs-release-$(lsb_release -c -s).deb'
          on host, 'dpkg -i puppetlabs-release-$(lsb_release -c -s).deb'
          on host, 'apt-get update'

          if opts[:facter_version]
            on host, ""apt-get install -y facter=#{opts[:facter_version]}-1puppetlabs1""
          end

          if opts[:hiera_version]
            on host, ""apt-get install -y hiera=#{opts[:hiera_version]}-1puppetlabs1""
          end

          if opts[:version]
            on host, ""apt-get install -y puppet-common=#{opts[:version]}-1puppetlabs1""
            on host, ""apt-get install -y puppet=#{opts[:version]}-1puppetlabs1""
          else
            on host, 'apt-get install -y puppet'
          end
        end

        # Installs Puppet and dependencies from msi
        #
        # @param [Host] host The host to install packages on
        #
        # @note on windows, the +:ruby_arch+ host parameter can determine in addition
        # to other settings whether the 32 or 64bit install is used
          is_config_32 = host['ruby_arch'] == 'x86' || host['install_32'] || opts['install_32']
          if !(version_is_less(version, '3.7')) && host.is_x86_64? && !is_config_32
        # @param [Hash{Symbol=>String}] opts An options hash
        # @option opts [String] :version The version of Puppet to install, required
        # @option opts [String] :win_download_url The url to download puppet from
        def install_puppet_from_msi( host, opts )
          #only install 64bit builds if
          # - we are on puppet version 3.7+
          # - we do not have install_32 set on host
          # - we do not have install_32 set globally
          version = opts[:version]
          if !(version_is_less(version, '3.7')) and host.is_x86_64? and not host['install_32'] and not opts['install_32']
            host['dist'] = ""puppet-#{version}-x64""
          else
            host['dist'] = ""puppet-#{version}""
          end
          link = ""#{opts[:win_download_url]}/#{host['dist']}.msi""
          if not link_exists?( link )
            raise ""Puppet #{version} at #{link} does not exist!""
          end

          if host.is_cygwin?
            dest = ""#{host['dist']}.msi""
            on host, ""curl -O #{link}""

            #Because the msi installer doesn't add Puppet to the environment path
            #Add both potential paths for simplicity
            #NOTE - this is unnecessary if the host has been correctly identified as 'foss' during set up
            puppetbin_path = ""\""/cygdrive/c/Program Files (x86)/Puppet Labs/Puppet/bin\"":\""/cygdrive/c/Program Files/Puppet Labs/Puppet/bin\""""
            on host, %Q{ echo 'export PATH=$PATH:#{puppetbin_path}' > /etc/bash.bashrc }
          else
            dest = ""C:\\Windows\\Temp\\#{host['dist']}.msi""

            on host, powershell(""$webclient = New-Object System.Net.WebClient;  $webclient.DownloadFile('#{link}','#{dest}')"")

            host.mkdir_p host['distmoduledir']
          end

          if host.is_cygwin?
            on host, ""cmd /C 'start /w msiexec.exe /qn /i #{dest}'""
          else
            on host, ""start /w msiexec.exe /qn /i #{dest}""
          end
        end

        # Installs Puppet and dependencies from dmg
        #
        # @param [Host] host The host to install packages on
        # @param [Hash{Symbol=>String}] opts An options hash
        # @option opts [String] :version The version of Puppet to install, required
        # @option opts [String] :facter_version The version of Facter to install, required
        # @option opts [String] :hiera_version The version of Hiera to install, required
        # @option opts [String] :mac_download_url Url to download msi pattern of %url%/puppet-%version%.msi
        #
        # @return nil
        # @api private
        def install_puppet_from_dmg( host, opts )

          puppet_ver = opts[:version]
          facter_ver = opts[:facter_version]
          hiera_ver = opts[:hiera_version]

          if [puppet_ver, facter_ver, hiera_ver].include?(nil)
            raise ""You need to specify versions for OSX host\n eg. install_puppet({:version => '3.6.2',:facter_version => '2.1.0',:hiera_version  => '1.3.4',})""
          end

          on host, ""curl -O #{opts[:mac_download_url]}/puppet-#{puppet_ver}.dmg""
          on host, ""curl -O #{opts[:mac_download_url]}/facter-#{facter_ver}.dmg""
          on host, ""curl -O #{opts[:mac_download_url]}/hiera-#{hiera_ver}.dmg""

          on host, ""hdiutil attach puppet-#{puppet_ver}.dmg""
          on host, ""hdiutil attach facter-#{facter_ver}.dmg""
          on host, ""hdiutil attach hiera-#{hiera_ver}.dmg""

          on host, ""installer -pkg /Volumes/puppet-#{puppet_ver}/puppet-#{puppet_ver}.pkg -target /""
          on host, ""installer -pkg /Volumes/facter-#{facter_ver}/facter-#{facter_ver}.pkg -target /""
          on host, ""installer -pkg /Volumes/hiera-#{hiera_ver}/hiera-#{hiera_ver}.pkg -target /""
        end

        # Installs Puppet and dependencies from gem
        #
        # @param [Host] host The host to install packages on
        # @param [Hash{Symbol=>String}] opts An options hash
        # @option opts [String] :version The version of Puppet to install, if nil installs latest
        # @option opts [String] :facter_version The version of Facter to install, if nil installs latest
        # @option opts [String] :hiera_version The version of Hiera to install, if nil installs latest
        #
        # @return nil
        # @raise [StandardError] if gem does not exist on target host
        # @api private
        def install_puppet_from_gem( host, opts )
          # There are a lot of special things to do for Solaris and Solaris 10.
          # This is easier than checking host['platform'] every time.
          is_solaris10 = host['platform'] =~ /solaris-10/
          is_solaris = host['platform'] =~ /solaris/

          # Hosts may be provisioned with csw but pkgutil won't be in the
          # PATH by default to avoid changing the behavior for Puppet's tests
          if is_solaris10
            on host, 'ln -s /opt/csw/bin/pkgutil /usr/bin/pkgutil'
          end

          # Solaris doesn't necessarily have this, but gem needs it
          if is_solaris
            on host, 'mkdir -p /var/lib'
          end

          unless host.check_for_command( 'gem' )
            gempkg = case host['platform']
                     when /solaris-11/                            then 'ruby-18'
                     when /ubuntu-14/                             then 'ruby'
                     when /solaris-10|ubuntu|debian|el-|cumulus/  then 'rubygems'
                     else
                       raise ""install_puppet() called with default_action "" +
                             ""'gem_install' but program `gem' is "" +
                             ""not installed on #{host.name}""
                     end

            host.install_package gempkg
          end

          # Link 'gem' to /usr/bin instead of adding /opt/csw/bin to PATH.
          if is_solaris10
            on host, 'ln -s /opt/csw/bin/gem /usr/bin/gem'
          end

          if host['platform'] =~ /debian|ubuntu|solaris|cumulus/
            gem_env = YAML.load( on( host, 'gem environment' ).stdout )
            gem_paths_array = gem_env['RubyGems Environment'].find {|h| h['GEM PATHS'] != nil }['GEM PATHS']
            path_with_gem = 'export PATH=' + gem_paths_array.join(':') + ':${PATH}'
            on host, ""echo '#{path_with_gem}' >> ~/.bashrc""
          end

          if opts[:facter_version]
            on host, ""gem install facter -v#{opts[:facter_version]} --no-ri --no-rdoc""
          end

          if opts[:hiera_version]
            on host, ""gem install hiera -v#{opts[:hiera_version]} --no-ri --no-rdoc""
          end

          ver_cmd = opts[:version] ? ""-v#{opts[:version]}"" : ''
          on host, ""gem install puppet #{ver_cmd} --no-ri --no-rdoc""

          # Similar to the treatment of 'gem' above.
          # This avoids adding /opt/csw/bin to PATH.
          if is_solaris
            gem_env = YAML.load( on( host, 'gem environment' ).stdout )
            # This is the section we want - this has the dir where gem executables go.
            env_sect = 'EXECUTABLE DIRECTORY'
            # Get the directory where 'gem' installs executables.
            # On Solaris 10 this is usually /opt/csw/bin
            gem_exec_dir = gem_env['RubyGems Environment'].find {|h| h[env_sect] != nil }[env_sect]

            on host, ""ln -s #{gem_exec_dir}/hiera /usr/bin/hiera""
            on host, ""ln -s #{gem_exec_dir}/facter /usr/bin/facter""
            on host, ""ln -s #{gem_exec_dir}/puppet /usr/bin/puppet""
          end
        end

        # Install official puppetlabs release repository configuration on host.
        #
        # @param [Host] host An object implementing {Beaker::Hosts}'s
        #                    interface.
        #
        # @note This method only works on redhat-like and debian-like hosts.
        #
        def install_puppetlabs_release_repo ( host )
          variant, version, arch, codename = host['platform'].to_array

          case variant
          when /^(fedora|el|centos)$/
            variant = (($1 == 'centos') ? 'el' : $1)

            rpm = options[:release_yum_repo_url] +
              ""/puppetlabs-release-%s-%s.noarch.rpm"" % [variant, version]

            on host, ""rpm -ivh #{rpm}""

          when /^(debian|ubuntu|cumulus)$/
            deb = URI.join(options[:release_apt_repo_url],  ""puppetlabs-release-%s.deb"" % codename)

            on host, ""wget -O /tmp/puppet.deb #{deb}""
            on host, ""dpkg -i --force-all /tmp/puppet.deb""
            on host, ""apt-get update""
          else
            raise ""No repository installation step for #{variant} yet...""
          end
        end

        # Install development repository on the given host. This method pushes all
        # repository information including package files for the specified
        # package_name to the host and modifies the repository configuration file
        # to point at the new repository. This is particularly useful for
        # installing development packages on hosts that can't access the builds
        # server.
        #
        # @param [Host] host An object implementing {Beaker::Hosts}'s
        #                    interface.
        # @param [String] package_name The name of the package whose repository is
        #                              being installed.
        # @param [String] build_version A string identifying the output of a
        #                               packaging job for use in looking up
        #                               repository directory information
        # @param [String] repo_configs_dir A local directory where repository files will be
        #                                  stored as an intermediate step before
        #                                  pushing them to the given host.
        # @param [Hash{Symbol=>String}] opts Options to alter execution.
        # @option opts [String] :dev_builds_url The URL to look for dev builds.
        # @option opts [String, Array<String>] :dev_builds_repos The repo(s)
        #                                       to check for dev builds in.
        #
        # @note This method only works on redhat-like and debian-like hosts.
        #
        def install_puppetlabs_dev_repo ( host, package_name, build_version,
                                  repo_configs_dir = 'tmp/repo_configs',
                                  opts = options )
          variant, version, arch, codename = host['platform'].to_array
          platform_configs_dir = File.join(repo_configs_dir, variant)

          # some of the uses of dev_builds_url below can't include protocol info,
          # plus this opens up possibility of switching the behavior on provided
          # url type
          _, protocol, hostname = opts[:dev_builds_url].partition /.*:\/\//
          dev_builds_url = protocol + hostname

          on host, ""mkdir -p /root/#{package_name}""

          case variant
          when /^(fedora|el|centos)$/
            variant = (($1 == 'centos') ? 'el' : $1)
            fedora_prefix = ((variant == 'fedora') ? 'f' : '')

            if host.is_pe?
              pattern = ""pl-%s-%s-repos-pe-%s-%s%s-%s.repo""
            else
              pattern = ""pl-%s-%s-%s-%s%s-%s.repo""
            end

            repo_filename = pattern % [
              package_name,
              build_version,
              variant,
              fedora_prefix,
              version,
              arch
            ]

            repo = fetch_http_file( ""%s/%s/%s/repo_configs/rpm/"" %
                         [ dev_builds_url, package_name, build_version ],
                          repo_filename,
                          platform_configs_dir)

            link = nil
            package_repos = opts[:dev_builds_repos].nil? ? [] : [opts[:dev_builds_repos]]
            package_repos.push(['products', 'devel']).flatten!
            package_repos.each do |repo|
              link =  ""%s/%s/%s/repos/%s/%s%s/%s/%s/"" %
                [ dev_builds_url, package_name, build_version, variant,
                  fedora_prefix, version, repo, arch ]

              unless link_exists?( link )
                @logger.debug(""couldn't find link at '#{repo}', falling back to next option..."")
              else
                @logger.debug(""found link at '#{repo}'"")
                break
              end
            end
            raise ""Unable to reach a repo directory at #{link}"" unless link_exists?( link )

            repo_dir = fetch_http_dir( link, platform_configs_dir )

            config_dir = '/etc/yum.repos.d/'
            scp_to host, repo, config_dir
            scp_to host, repo_dir, ""/root/#{package_name}""

            search = ""baseurl\\s*=\\s*http:\\/\\/#{hostname}.*$""
            replace = ""baseurl=file:\\/\\/\\/root\\/#{package_name}\\/#{arch}""
            sed_command = ""sed -i 's/#{search}/#{replace}/'""
            find_and_sed = ""find #{config_dir} -name \""*.repo\"" -exec #{sed_command} {} \\;""

            on host, find_and_sed

          when /^(debian|ubuntu|cumulus)$/
            list = fetch_http_file( ""%s/%s/%s/repo_configs/deb/"" %
                           [ dev_builds_url, package_name, build_version ],
                          ""pl-%s-%s-%s.list"" %
                           [ package_name, build_version, codename ],
                          platform_configs_dir )

            repo_dir = fetch_http_dir( ""%s/%s/%s/repos/apt/%s"" %
                                        [ dev_builds_url, package_name,
                                          build_version, codename ],
                                         platform_configs_dir )

            config_dir = '/etc/apt/sources.list.d'
            scp_to host, list, config_dir
            scp_to host, repo_dir, ""/root/#{package_name}""

            repo_name = nil
            package_repos = opts[:dev_builds_repos].nil? ? [] : [opts[:dev_builds_repos]]
            package_repos.flatten!
            package_repos.each do |repo|
              repo_path = ""/root/#{package_name}/#{codename}/#{repo}""
              repo_check = on(host, ""[[ -d #{repo_path} ]]"", :acceptable_exit_codes => [0,1])
              if repo_check.exit_code == 0
                @logger.debug(""found repo at '#{repo_path}'"")
                repo_name = repo
                break
              else
                @logger.debug(""couldn't find repo at '#{repo_path}', falling back to next option..."")
              end
            end
            if repo_name.nil?
              repo_name = 'main'
              @logger.debug(""using default repo '#{repo_name}'"")
            end

            search = ""deb\\s\\+http:\\/\\/#{hostname}.*$""
            replace = ""deb file:\\/\\/\\/root\\/#{package_name}\\/#{codename} #{codename} #{repo_name}""
            sed_command = ""sed -i 's/#{search}/#{replace}/'""
            find_and_sed = ""find #{config_dir} -name \""*.list\"" -exec #{sed_command} {} \\;""

            on host, find_and_sed
            on host, ""apt-get update""

          else
            raise ""No repository installation step for #{variant} yet...""
          end
        end

        # Installs packages from the local development repository on the given host
        #
        # @param [Host] host An object implementing {Beaker::Hosts}'s
        #                    interface.
        # @param [Regexp] package_name The name of the package whose repository is
        #                              being installed.
        #
        # @note This method only works on redhat-like and debian-like hosts.
        # @note This method is paired to be run directly after {#install_puppetlabs_dev_repo}
        #
        def install_packages_from_local_dev_repo( host, package_name )
          if host['platform'] =~ /debian|ubuntu|cumulus/
            find_filename = '*.deb'
            find_command  = 'dpkg -i'
          elsif host['platform'] =~ /fedora|el|centos/
            find_filename = '*.rpm'
            find_command  = 'rpm -ivh'
          else
            raise ""No repository installation step for #{host['platform']} yet...""
          end
          find_command = ""find /root/#{package_name} -type f -name '#{find_filename}' -exec #{find_command} {} \\;""
          on host, find_command
        end

        # Install development repo of the puppet-agent on the given host
        #
        # @param [Host] host An object implementing {Beaker::Hosts}'s interface
        # @param [Hash{Symbol=>String}] opts An options hash
        # @option opts [String] :version The version of puppet-agent to install
        # @option opts [String] :copy_base_local Directory where puppet-agent artifact
        #                       will be stored locally
        #                       (default: 'tmp/repo_configs')
        # @option opts [String] :copy_dir_external Directory where puppet-agent
        #                       artifact will be pushed to on the external machine
        #                       (default: '/root')
        # @return nil
        def install_puppetagent_dev_repo( host, opts )
          opts[:copy_base_local]    ||= File.join('tmp', 'repo_configs')
          opts[:copy_dir_external]  ||= File.join('/', 'root')
          variant, version, arch, codename = host['platform'].to_array
          release_path = ""#{options[:dev_builds_url]}/puppet-agent/#{opts[:version]}/artifacts/""
          copy_dir_local = File.join(opts[:copy_base_local], variant)
          onhost_copy_base = opts[:copy_dir_external]

          case variant
          when /^(fedora|el|centos)$/
            release_path << ""el/#{version}/products/#{arch}""
            release_file = ""puppet-agent-#{opts[:version]}-1.#{arch}.rpm""
          when /^(debian|ubuntu|cumulus)$/
            release_path << ""deb/#{codename}""
            release_file = ""puppet-agent_#{opts[:version]}-1_#{arch}.deb""
          when /^windows$/
            release_path << 'windows'
            onhost_copy_base = '`cygpath -smF 35`/'
            arch_suffix = arch =~ /64/ ? '64' : '86'
            release_file = ""puppet-agent-x#{arch_suffix}.msi""
          else
            raise ""No repository installation step for #{variant} yet...""
          end

          onhost_copied_file = File.join(onhost_copy_base, release_file)
          fetch_http_file( release_path, release_file, copy_dir_local)
          scp_to host, File.join(copy_dir_local, release_file), onhost_copy_base

          case variant
          when /^(fedora|el|centos)$/
            on host, ""rpm -ivh #{onhost_copied_file}""
          when /^(debian|ubuntu|cumulus)$/
            on host, ""dpkg -i --force-all #{onhost_copied_file}""
            on host, ""apt-get update""
          when /^windows$/
            result = on host, ""echo #{onhost_copied_file}""
            onhost_copied_file = result.raw_output.chomp
            on host, Command.new(""start /w #{onhost_copied_file}"", [], { :cmdexe => true })
          end
        end

        # This method will install a pem file certifcate on a windows host
        #
        # @param [Host] host                 A host object
        # @param [String] cert_name          The name of the pem file
        # @param [String] cert               The contents of the certificate
        #
        def install_cert_on_windows(host, cert_name, cert)
          create_remote_file(host, ""C:\\Windows\\Temp\\#{cert_name}.pem"", cert)
          on host, ""certutil -v -addstore Root C:\\Windows\\Temp\\#{cert_name}.pem""
        end
      end
    end
  end
end
",Seems like `true == ...` comparison can be dropped.,"This module contains several utility methods for managing Puppet installations across different platforms. Here's a concise review of the suggested code changes:

### Key Changes and Improvements:

1. **Consistent Naming and Documentation**:
   - Added docstrings to methods for better understanding.
   - Consistently uses `def` for defining methods.

2. **Git URI Validation**:
   - Added a regex to validate Git URIs, ensuring they start with `git://`, `http://`, `https://`, or `file://`.

3. **GitHub SSH Signature**:
   - Added a constant for GitHub's SSH signature to improve readability.

4. **Extract Repo Info**:
   - Simplified the `extract_repo_info_from` method by using `Pathname.new` and `split`.

5. **Order Packages**:
   - Improved the `order_packages` method by using `select` and `flatten`.

6. **Find Git Repo Versions**:
   - Used `step` and `on` for better control flow and logging.

7. **Install From Git**:
   - Added comments and improved the logic for handling depth and branch options.

8. **Install Puppet**:
   - Simplified the logic for determining the default action and supported platforms.
   - Added checks for missing packages and commands.

9. **Configure Puppet**:
   - Improved the logic for configuring Puppet on Windows and Unix-based systems.

10. **Install Puppet from RPM**:
    - Added checks for Solaris and specific architecture requirements.

11. **Install Puppet from Deb**:
    - Added checks for missing packages and commands.

12. **Install Puppet from MSI**:
    - Improved the logic for downloading and installing MSI files on Windows.

13. **Install Puppet from DMG**:
    - Added checks for missing packages and commands.

14. **Install Puppet from Gem**:
    - Simplified the logic for installing gems and handling Solaris-specific paths.

15. **Install Puppetlabs Release Repository**:
    - Added checks for supported platforms and updated the repository URL format.

16. **Install Puppetlabs Dev Repository**:
    - Improved the logic for fetching and installing development repositories.

17. **Install Packages from Local Dev Repository**:
    - Simplified the logic for installing packages from the local repository.

18. **Install Puppet Agent Dev Repository**:
    - Added checks for supported platforms and updated the artifact path.

19. **Install Cert on Windows**:
    - Added comments"
21,"@@ -133,6 +133,16 @@ def build_permissions_set(object_uri, unbound_permission,
     return granters
 
 
+# XXX: May need caching
+def groupfinder(userid, request):
+    authn_type = getattr(request, 'authn_type', None)
+    if authn_type is None:
+        return []
+
+    prefixed_userid = '%s:%s' % (authn_type.lower(), userid)
+    return request.registry.permission.user_principals(prefixed_userid)
+
+
 @implementer(IAuthorizationPolicy)
 class AuthorizationPolicy(CliquetAuthorization):
     def get_bound_permissions(self, *args, **kwargs):","from cliquet.authorization import AuthorizationPolicy as CliquetAuthorization
from pyramid.security import IAuthorizationPolicy
from zope.interface import implementer


# Vocab really matters when you deal with permissions. Let's do a quick recap
# of the terms used here:
#
# Object URI:
#    An unique identifier for an object.
#    for instance, /buckets/blog/collections/articles/records/article1
#
# Object:
#    A common denomination of an object (e.g. ""collection"" or ""record"")
#
# Unbound permission:
#    A permission not bound to an object (e.g. ""create"")
#
# Bound permission:
#    A permission bound to an object (e.g. ""collection:create"")


# Dictionary which list all permissions a given permission enables.
PERMISSIONS_INHERITANCE_TREE = {
    'bucket:write': {
        'bucket': ['write']
    },
    'bucket:read': {
        'bucket': ['write', 'read']
    },
    'bucket:group:create': {
        'bucket': ['write', 'group:create']
    },
    'bucket:collection:create': {
        'bucket': ['write', 'collection:create']
    },
    'group:write': {
        'bucket': ['write'],
        'group': ['write']
    },
    'group:read': {
        'bucket': ['write', 'read'],
        'group': ['write', 'read']
    },
    'collection:write': {
        'bucket': ['write'],
        'collection': ['write'],
    },
    'collection:read': {
        'bucket': ['write', 'read'],
        'collection': ['write', 'read'],
    },
    'collection:record:create': {
        'bucket': ['write'],
        'collection': ['write', 'record:create']
    },
    'record:write': {
        'bucket': ['write'],
        'collection': ['write'],
        'record': ['write']
    },
    'record:read': {
        'bucket': ['write', 'read'],
        'collection': ['write', 'read'],
        'record': ['write', 'read']
    }
}


def get_object_type(object_uri):
    """"""Return the type of an object from its id.""""""

    obj_parts = object_uri.split('/')
    if len(obj_parts) % 2 == 0:
        object_uri = '/'.join(obj_parts[:-1])

    # Order matters here. More precise is tested first.
    if 'records' in object_uri:
        obj_type = 'record'
    elif 'collections' in object_uri:
        obj_type = 'collection'
    elif 'groups' in object_uri:
        obj_type = 'group'
    elif 'buckets' in object_uri:
        obj_type = 'bucket'
    else:
        obj_type = None
    return obj_type


def build_permission_tuple(obj_type, unbound_permission, obj_parts):
    """"""Returns a tuple of (object_uri, unbound_permission)""""""
    PARTS_LENGTH = {
        'bucket': 3,
        'collection': 5,
        'group': 5,
        'record': 7
    }
    if obj_type not in PARTS_LENGTH:
        raise ValueError('Invalid object type: %s' % obj_type)

    if PARTS_LENGTH[obj_type] > len(obj_parts):
        raise ValueError('You cannot build children keys from its parent key.'
                         'Trying to build type ""%s"" from object key ""%s"".' % (
                             obj_type, '/'.join(obj_parts)))
    length = PARTS_LENGTH[obj_type]
    return ('/'.join(obj_parts[:length]), unbound_permission)


def build_permissions_set(object_uri, unbound_permission,
                          inheritance_tree=None):
    """"""Build a set of all permissions that can grant access to the given
    object URI and unbound permission.

    >>> build_required_permissions('/buckets/blog', 'write')
    set(('/buckets/blog', 'write'))

    """"""

    if inheritance_tree is None:
        inheritance_tree = PERMISSIONS_INHERITANCE_TREE

    obj_type = get_object_type(object_uri)

    bound_permission = '%s:%s' % (obj_type, unbound_permission)
    granters = set()

    obj_parts = object_uri.split('/')
    for obj, permission_list in inheritance_tree[bound_permission].items():
        for permission in permission_list:
            granters.add(build_permission_tuple(obj, permission, obj_parts))

# XXX: May need caching
def groupfinder(userid, request):
    authn_type = getattr(request, 'authn_type', None)
    if authn_type is None:
        return []

    prefixed_userid = '%s:%s' % (authn_type.lower(), userid)
    return request.registry.permission.user_principals(prefixed_userid)


    return granters


@implementer(IAuthorizationPolicy)
class AuthorizationPolicy(CliquetAuthorization):
    def get_bound_permissions(self, *args, **kwargs):
        return build_permissions_set(*args, **kwargs)
",Now we have `request.prefixed_userid` :),"The suggested code change introduces a new authorization policy for Pyramid applications, which allows for granular control over permissions based on object types and unbound permissions. The changes include:

1. **Functionality**:
   - `get_object_type`: Determines the type of an object from its URI.
   - `build_permission_tuple`: Constructs a tuple of (object URI, unbound permission).
   - `build_permissions_set`: Builds a set of all permissions that can grant access to a given object URI and unbound permission.
   - `groupfinder`: Retrieves user principals based on authentication type and user ID.

2. **Implementation**:
   - The `AuthorizationPolicy` class inherits from `CliquetAuthorization` and implements the `IAuthorizationPolicy` interface.
   - The `get_bound_permissions` method uses the `build_permissions_set` function to determine the permissions that can be granted to a user for a specific object URI and unbound permission.

3. **Caching**:
   - There is a comment indicating that caching may be needed for performance reasons, but it is not implemented in the current code.

4. **Documentation**:
   - The code includes comments explaining the purpose of each function and the structure of the permission inheritance tree.

Overall, this change provides a robust framework for managing permissions in a Pyramid application, allowing for flexible and granular control over access based on object types and unbound permissions."
22,"@@ -49,6 +49,9 @@ SIGNATURE_V4_PARAMS = [
     'X-Amz-SignedHeaders', 'X-Amz-Signature'
 ]
 
+# headers to blacklist from request_dict.signed_headers
+BLACKLISTED_HEADERS = ['X-Amz-Security-Token']
+
 # query params overrides for multipart upload and node sdk
 ALLOWED_QUERY_PARAMS = [
     'X-id', 'X-Amz-User-Agent', 'X-Amz-Content-Sha256',","import re
import time
import logging
import datetime
from localstack import config
from collections import namedtuple
from botocore.compat import urlsplit
from botocore.credentials import Credentials
from urllib.parse import parse_qs, urlencode
from six.moves.urllib import parse as urlparse
from botocore.awsrequest import create_request_object
from localstack.utils.auth import HmacV1QueryAuth, S3SigV4QueryAuth
from localstack.utils.aws.aws_responses import requests_error_response_xml_signature_calculation
from localstack.constants import (
    S3_VIRTUAL_HOSTNAME, S3_STATIC_WEBSITE_HOSTNAME, TEST_AWS_ACCESS_KEY_ID, TEST_AWS_SECRET_ACCESS_KEY)


LOGGER = logging.getLogger(__name__)

REGION_REGEX = r'[a-z]{2}-[a-z]+-[0-9]{1,}'
PORT_REGEX = r'(:[\d]{0,6})?'
S3_STATIC_WEBSITE_HOST_REGEX = r'^([^.]+)\.s3-website\.localhost\.localstack\.cloud(:[\d]{0,6})?$'
S3_VIRTUAL_HOSTNAME_REGEX = (r'^(http(s)?://)?((?!s3\.)[^\./]+)\.'
                             r'(((s3(-website)?\.)?localhost\.localstack\.cloud)|({})|'
                             r'(s3((-website)|(-external-1))?[\.-](dualstack\.)?'
                             r'({}\.)?amazonaws\.com(.cn)?)){}$').format(
                                 config.HOSTNAME_EXTERNAL, REGION_REGEX, PORT_REGEX)
BUCKET_NAME_REGEX = (r'(?=^.{3,63}$)(?!^(\d+\.)+\d+$)' +
    r'(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)')

HOST_COMBINATION_REGEX = r'^(.*)(:[\d]{0,6})'
PORT_REPLACEMENT = [':80', ':443', ':4566', '']

# response header overrides the client may request
ALLOWED_HEADER_OVERRIDES = {
    'response-content-type': 'Content-Type',
    'response-content-language': 'Content-Language',
    'response-expires': 'Expires',
    'response-cache-control': 'Cache-Control',
    'response-content-disposition': 'Content-Disposition',
    'response-content-encoding': 'Content-Encoding',
}

# params are required in presigned url
SIGNATURE_V2_PARAMS = ['Signature', 'Expires', 'AWSAccessKeyId']

SIGNATURE_V4_PARAMS = [
    'X-Amz-Algorithm', 'X-Amz-Credential', 'X-Amz-Date', 'X-Amz-Expires',
# headers to blacklist from request_dict.signed_headers
BLACKLISTED_HEADERS = ['X-Amz-Security-Token']

    'X-Amz-SignedHeaders', 'X-Amz-Signature'
]

# query params overrides for multipart upload and node sdk
ALLOWED_QUERY_PARAMS = [
    'X-id', 'X-Amz-User-Agent', 'X-Amz-Content-Sha256',
    'versionid', 'uploadid', 'partnumber'
]


def is_static_website(headers):
    """"""
    Determine if the incoming request is for s3 static website hosting
    returns True if the host matches website regex
    returns False if the host does not matches website regex
    """"""
    return bool(re.match(S3_STATIC_WEBSITE_HOST_REGEX, headers.get('host', '')))


def uses_host_addressing(headers):
    """"""
    Determines if the bucket is using host based addressing style or path based
    """"""
    # we can assume that the host header we are receiving here is actually the header we originally received
    # from the client (because the edge service is forwarding the request in memory)
    match = re.match(S3_VIRTUAL_HOSTNAME_REGEX, headers.get('host', ''))
    return True if match and match.group(3) else False


def extract_bucket_name(headers, path):
    """"""
    Extract the bucket name
    if using host based addressing it's extracted from host header
    if using path based addressing it's extracted form the path
    """"""
    bucket_name = None
    if uses_host_addressing(headers):
        pattern = re.compile(S3_VIRTUAL_HOSTNAME_REGEX)
        match = pattern.match(headers.get('host', ''))

        if match and match.group(3):
            bucket_name = match.group(3)
    else:
        bucket_name = path.split('/', 2)[1]
    return bucket_name if bucket_name else None


def extract_key_name(headers, path):
    """"""
    Extract the key name from the path depending on addressing_style
    """"""
    key_name = None
    path = path.split('?')[0]  # strip off query params from path
    if uses_host_addressing(headers):
        split = path.split('/', 1)
        if len(split) > 1:
            key_name = split[1]
    else:
        split = path.split('/', 2)
        if len(split) > 2:
            key_name = split[2]

    return key_name if key_name else None


def extract_bucket_and_key_name(headers, path):
    return extract_bucket_name(headers, path), extract_key_name(headers, path)


def validate_bucket_name(bucket_name):
    """"""
    Validate s3 bucket name based on the documentation
    ref. https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html
    """"""
    return True if re.match(BUCKET_NAME_REGEX, bucket_name) else False


def get_bucket_hostname(bucket_name):
    """"""
    Get bucket name for addressing style host
    """"""
    return '%s.%s:%s' % (bucket_name, S3_VIRTUAL_HOSTNAME, config.EDGE_PORT)


def get_bucket_website_hostname(bucket_name):
    """"""
    Get bucket name for addressing style host for website hosting
    """"""
    return '%s.%s:%s' % (bucket_name, S3_STATIC_WEBSITE_HOSTNAME, config.EDGE_PORT)


def get_forwarded_for_host(headers):
    x_forwarded_header = re.split(r',\s?', headers.get('X-Forwarded-For', ''))
    host = x_forwarded_header[len(x_forwarded_header) - 1]
    return host


def is_real_s3_url(url):
    return re.match(r'.*s3(\-website)?\.([^\.]+\.)?amazonaws.com.*', url or '')


def authenticate_presign_url(method, path, headers, data=None):

    url = '{}{}'.format(config.get_edge_url(), path)
    parsed = urlparse.urlparse(url)
    query_params = parse_qs(parsed.query)
    forwarded_for = get_forwarded_for_host(headers)
    if forwarded_for:
        url = re.sub('://[^/]+', '://%s' % forwarded_for, url)

    LOGGER.debug('Received presign S3 URL: %s' % url)

    sign_headers = {}
    query_string = {}

    is_v2 = all([p in query_params for p in SIGNATURE_V2_PARAMS])
    is_v4 = all([p in query_params for p in SIGNATURE_V4_PARAMS])

    # Add overrided headers to the query string params
    for param_name, header_name in ALLOWED_HEADER_OVERRIDES.items():
        if param_name in query_params:
            query_string[param_name] = query_params[param_name][0]

    # Request's headers are more essentials than the query parameters in the request.
    # Different values of header in the header of the request and in the query parameter of the
    # request URL will fail the signature calulation. As per the AWS behaviour

    # Add valid headers into the sign_header. Skip the overrided headers
    # and the headers which have been sent in the query string param
    presign_params_lower = \
        [p.lower() for p in SIGNATURE_V4_PARAMS] if is_v4 else [p.lower() for p in SIGNATURE_V2_PARAMS]
    params_header_override = [param_name for param_name, header_name in ALLOWED_HEADER_OVERRIDES.items()]
    if len(query_params) > 2:
        for key in query_params:
            key_lower = key.lower()
            if key_lower not in presign_params_lower:
                if (key_lower not in (header[0].lower() for header in headers) and
                        key_lower not in params_header_override):
                    if key_lower in (allowed_param.lower() for allowed_param in ALLOWED_QUERY_PARAMS):
                        query_string[key] = query_params[key][0]
                    else:
                        sign_headers[key] = query_params[key][0]

    for header_name, header_value in headers.items():
        header_name_lower = header_name.lower()
        if header_name_lower.startswith('x-amz-') or header_name_lower.startswith('content-'):
            if is_v2 and header_name_lower in query_params:
                sign_headers[header_name] = header_value
            if is_v4 and header_name_lower in query_params['X-Amz-SignedHeaders'][0]:
                sign_headers[header_name] = header_value

    # Preparnig dictionary of request to build AWSRequest's object of the botocore
    request_url = '{}://{}{}'.format(parsed.scheme, parsed.netloc, parsed.path)
    # Fix https://github.com/localstack/localstack/issues/3912
    # urlencode method replaces white spaces with plus sign cause signature calculation to fail
    request_url = ('%s?%s' % (request_url, urlencode(query_string, quote_via=urlparse.quote, safe=' '))
        if query_string else request_url)
    if forwarded_for:
        request_url = re.sub('://[^/]+', '://%s' % forwarded_for, request_url)

    bucket_name = extract_bucket_name(headers, parsed.path)

    request_dict = {
        'url_path': parsed.path,
        'query_string': query_string,
        'method': method,
        'headers': sign_headers,
        'body': b'',
        'url': request_url,
        'context': {
            'is_presign_request': True,
            'use_global_endpoint': True,
            'signing': {
                'bucket': bucket_name
            }
        }
    }

    # Support for virtual host addressing style in signature version 2
    # We don't need to do this in v4 as we already concerting it to the virtual addressing style.
    # v2 require path base styled request_dict and v4 require virtual styled request_dict

    if uses_host_addressing(headers) and is_v2:
        request_dict['url_path'] = '/{}{}'.format(bucket_name, request_dict['url_path'])
        parsed_url = urlparse.urlparse(request_url)
        request_dict['url'] = '{}://{}:{}{}'.format(
            parsed_url.scheme, S3_VIRTUAL_HOSTNAME, config.EDGE_PORT, request_dict['url_path'])
        request_dict['url'] = \
            ('%s?%s' % (request_dict['url'], urlencode(query_string)) if query_string else request_dict['url'])

    if not is_v2 and any([p in query_params for p in SIGNATURE_V2_PARAMS]):
        response = requests_error_response_xml_signature_calculation(
            code=403,
            message='Query-string authentication requires the Signature, Expires and AWSAccessKeyId parameters',
            code_string='AccessDenied'
        )
    elif is_v2 and not is_v4:
        response = authenticate_presign_url_signv2(method, path, headers, data, url, query_params, request_dict)

    if not is_v4 and any([p in query_params for p in SIGNATURE_V4_PARAMS]):
        response = requests_error_response_xml_signature_calculation(
            code=403,
            message='Query-string authentication requires the X-Amz-Algorithm, \
                X-Amz-Credential, X-Amz-Date, X-Amz-Expires, \
                X-Amz-SignedHeaders and X-Amz-Signature parameters.',
            code_string='AccessDenied'
        )

    elif is_v4 and not is_v2:
        response = authenticate_presign_url_signv4(method, path, headers, data, url, query_params, request_dict)

    if response is not None:
        LOGGER.error('Presign signature calculation failed: %s' % response)
        return response
    LOGGER.debug('Valid presign url.')


def authenticate_presign_url_signv2(method, path, headers, data, url, query_params, request_dict):

    # Calculating Signature
    aws_request = create_request_object(request_dict)
    credentials = Credentials(access_key=TEST_AWS_ACCESS_KEY_ID, secret_key=TEST_AWS_SECRET_ACCESS_KEY)
    auth = HmacV1QueryAuth(credentials=credentials, expires=query_params['Expires'][0])
    split = urlsplit(aws_request.url)
    string_to_sign = auth.get_string_to_sign(method=method, split=split, headers=aws_request.headers)
    signature = auth.get_signature(string_to_sign=string_to_sign)

    # Comparing the signature in url with signature we calculated
    query_sig = urlparse.unquote(query_params['Signature'][0])
    if config.S3_SKIP_SIGNATURE_VALIDATION:
        if query_sig != signature:
            LOGGER.warning('Signatures do not match, but not raising an error, as S3_SKIP_SIGNATURE_VALIDATION=1')
        signature = query_sig

    if query_sig != signature:

        return requests_error_response_xml_signature_calculation(
            code=403,
            code_string='SignatureDoesNotMatch',
            aws_access_token=TEST_AWS_ACCESS_KEY_ID,
            string_to_sign=string_to_sign,
            signature=signature,
            message='The request signature we calculated does not match the signature you provided. \
                    Check your key and signing method.')

    # Checking whether the url is expired or not
    if int(query_params['Expires'][0]) < time.time():
        return requests_error_response_xml_signature_calculation(
            code=403,
            code_string='AccessDenied',
            message='Request has expired',
            expires=query_params['Expires'][0]
        )


def authenticate_presign_url_signv4(method, path, headers, data, url, query_params, request_dict):

    is_presign_valid = False
    for port in PORT_REPLACEMENT:
        match = re.match(HOST_COMBINATION_REGEX, urlparse.urlparse(request_dict['url']).netloc)
        if match and match.group(2):
            request_dict['url'] = request_dict['url'].replace('%s' % match.group(2), '%s' % port)
        else:
            request_dict['url'] = '%s:%s' % (request_dict['url'], port)

        # Calculating Signature
        aws_request = create_request_object(request_dict)
        ReadOnlyCredentials = namedtuple('ReadOnlyCredentials',
                                ['access_key', 'secret_key', 'token'])
        credentials = ReadOnlyCredentials(TEST_AWS_ACCESS_KEY_ID, TEST_AWS_SECRET_ACCESS_KEY, None)
        region = query_params['X-Amz-Credential'][0].split('/')[2]
        signer = S3SigV4QueryAuth(credentials, 's3', region, expires=int(query_params['X-Amz-Expires'][0]))
        signature = signer.add_auth(aws_request, query_params['X-Amz-Date'][0])

        expiration_time = datetime.datetime.strptime(query_params['X-Amz-Date'][0], '%Y%m%dT%H%M%SZ') + \
            datetime.timedelta(seconds=int(query_params['X-Amz-Expires'][0]))

        # Comparing the signature in url with signature we calculated
        query_sig = urlparse.unquote(query_params['X-Amz-Signature'][0])
        if query_sig == signature:
            is_presign_valid = True
            break

    # Comparing the signature in url with signature we calculated
    if config.S3_SKIP_SIGNATURE_VALIDATION:
        if not is_presign_valid:
            LOGGER.warning('Signatures do not match, but not raising an error, as S3_SKIP_SIGNATURE_VALIDATION=1')
        signature = query_sig

    if not is_presign_valid:

        return requests_error_response_xml_signature_calculation(
            code=403,
            code_string='SignatureDoesNotMatch',
            aws_access_token=TEST_AWS_ACCESS_KEY_ID,
            signature=signature,
            message='The request signature we calculated does not match the signature you provided. \
                    Check your key and signing method.')

    # Checking whether the url is expired or not
    if expiration_time < datetime.datetime.utcnow():
        return requests_error_response_xml_signature_calculation(
            code=403,
            code_string='AccessDenied',
            message='Request has expired',
            expires=query_params['X-Amz-Expires'][0]
        )
","Looks great, Thanks for this PR @macnev2013 !   nit: I'd rename this to something more descriptive, e.g., `PRESIGN_SKIPPED_HEADERS = ...` (not critical - can be done in a future PR..).","This code provides functions to handle S3 presign URLs, including validation and signature calculation for both V2 and V4 signatures. The code includes regular expressions for matching S3 bucket names, hostnames, and URLs, as well as functions to extract bucket and key names from headers and paths. It also includes functions to authenticate presign URLs using V2 and V4 signatures, and to handle errors related to signature mismatches and expiration times. The code is designed to work with LocalStack, a cloud emulator for AWS services."
23,"@@ -33,9 +33,12 @@ final class MergeReleaseCandidateBranchReleaseWorker extends AbstractShopsysRele
      */
     public function work(Version $version): void
     {
-        $this->symfonyStyle->note('You need to create a merge commit, see https://docs.shopsys.com/en/latest/contributing/merging-on-github/ for detailed instructions.');
-        $this->symfonyStyle->warning(sprintf('If you are creating the merge commit locally, do not forget to push the ""%s"" branch!', $this->initialBranchName));
-        $this->confirm(sprintf('Confirm ""%s"" branch was merged to ""%s""', $this->createBranchName($version), $this->initialBranchName));
+        $this->symfonyStyle->note('You need to create a merge commit locally, see https://docs.shopsys.com/en/latest/contributing/merging-on-github/ for detailed instructions.');
+        $this->symfonyStyle->warning(sprintf(
+            'Do not forget to push the ""%s"" branch!',
+            $this->initialBranchName
+        ));
+        $this->confirm(sprintf('Confirm ""%s"" branch was merged and pushed to ""%s""', $this->createBranchName($version), $this->initialBranchName));
 
         if ($this->initialBranchName === 'master') {
             $this->symfonyStyle->note('Rest assured, after the master branch is built on Heimdall, it is split automatically (using http://heimdall:8080/view/Tools/job/tool-monorepo-split/)');","<?php

declare(strict_types=1);

namespace Shopsys\Releaser\ReleaseWorker\Release;

use PharIo\Version\Version;
use Shopsys\Releaser\ReleaseWorker\AbstractShopsysReleaseWorker;
use Shopsys\Releaser\Stage;

final class MergeReleaseCandidateBranchReleaseWorker extends AbstractShopsysReleaseWorker
{
    /**
     * @param \PharIo\Version\Version $version
     * @return string
     */
    public function getDescription(Version $version): string
    {
        return sprintf('[Manually] Merge ""%s"" branch into ""%s""', $this->createBranchName($version), $this->initialBranchName);
    }

    /**
     * Higher first
     * @return int
     */
    public function getPriority(): int
    {
        return 650;
    }

    /**
     * @param \PharIo\Version\Version $version
        $this->symfonyStyle->note('You need to create a merge commit locally, see https://docs.shopsys.com/en/latest/contributing/merging-on-github/ for detailed instructions.');
        $this->symfonyStyle->warning(sprintf(
            'Do not forget to push the ""%s"" branch!',
            $this->initialBranchName
        ));
        $this->confirm(sprintf('Confirm ""%s"" branch was merged and pushed to ""%s""', $this->createBranchName($version), $this->initialBranchName));
     */
    public function work(Version $version): void
    {
        $this->symfonyStyle->note('You need to create a merge commit, see https://docs.shopsys.com/en/latest/contributing/merging-on-github/ for detailed instructions.');
        $this->symfonyStyle->warning(sprintf('If you are creating the merge commit locally, do not forget to push the ""%s"" branch!', $this->initialBranchName));
        $this->confirm(sprintf('Confirm ""%s"" branch was merged to ""%s""', $this->createBranchName($version), $this->initialBranchName));

        if ($this->initialBranchName === 'master') {
            $this->symfonyStyle->note('Rest assured, after the master branch is built on Heimdall, it is split automatically (using http://heimdall:8080/view/Tools/job/tool-monorepo-split/)');
        } else {
            $this->symfonyStyle->note(sprintf('You need split the ""%s"" branch it using ""tool-monorepo-split-branch"" on Heimdall (http://heimdall:8080/view/Tools/job/tool-monorepo-split-branch/)', $this->initialBranchName));
        }
        $this->confirm('Confirm the branch is split.');
    }

    /**
     * @return string
     */
    public function getStage(): string
    {
        return Stage::RELEASE;
    }
}
","If you need to create a merge commit locally, why do we provide link for docs about merging on github? :D","The suggested code change appears to be a modification to the `MergeReleaseCandidateBranchReleaseWorker` class in the Shopsys Releaser project. The changes include:

1. **Description Method**: The method now returns a formatted string describing the action to be performed, which includes the creation of a merge commit and the warning about pushing the initial branch.

2. **Work Method**: The method now includes a note about creating a merge commit and a warning about pushing the initial branch. It also includes a confirmation step to ensure the branch has been merged and pushed.

3. **Split Branch Method**: A new note is added to inform users about splitting the branch using Heimdall if the initial branch is not `master`. A confirmation step is also included for this action.

4. **Stage Method**: The method returns the stage as `Stage::RELEASE`, indicating that this release worker is part of the release process.

Overall, the changes aim to provide clear instructions and confirmations for the release process, ensuring that all necessary steps are followed correctly."
24,"@@ -123,6 +123,15 @@ def distribution():
                             id=dist_id)
 
 
+def sandboxed():
+    """"""Check whether the environment is a sandbox, i.e. has restricted access to the host system.
+
+    Return:
+        True if the environment is a sandbox.
+    """"""
+    return distribution().parsed == Distribution.kde
+
+
 def _git_str():
     """"""Try to find out git version.
 ","# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:

# Copyright 2014-2019 Florian Bruhin (The Compiler) <mail@qutebrowser.org>
#
# This file is part of qutebrowser.
#
# qutebrowser is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# qutebrowser is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with qutebrowser.  If not, see <http://www.gnu.org/licenses/>.

""""""Utilities to show various version information.""""""

import re
import sys
import glob
import os.path
import platform
import subprocess
import importlib
import collections
import enum
import datetime
import getpass

import attr
import pkg_resources
from PyQt5.QtCore import PYQT_VERSION_STR, QLibraryInfo
from PyQt5.QtNetwork import QSslSocket
from PyQt5.QtGui import (QOpenGLContext, QOpenGLVersionProfile,
                         QOffscreenSurface)
from PyQt5.QtWidgets import QApplication

try:
    from PyQt5.QtWebKit import qWebKitVersion
except ImportError:  # pragma: no cover
    qWebKitVersion = None  # type: ignore  # noqa: N816

try:
    from PyQt5.QtWebEngineWidgets import QWebEngineProfile
except ImportError:  # pragma: no cover
    QWebEngineProfile = None  # type: ignore

import qutebrowser
from qutebrowser.utils import log, utils, standarddir, usertypes, message
from qutebrowser.misc import objects, earlyinit, sql, httpclient, pastebin
from qutebrowser.browser import pdfjs

try:
    from qutebrowser.browser.webengine import webenginesettings
except ImportError:  # pragma: no cover
    webenginesettings = None  # type: ignore


@attr.s
class DistributionInfo:

    """"""Information about the running distribution.""""""

    id = attr.ib()
    parsed = attr.ib()
    version = attr.ib()
    pretty = attr.ib()


pastebin_url = None
Distribution = enum.Enum(
    'Distribution', ['unknown', 'ubuntu', 'debian', 'void', 'arch',
                     'gentoo', 'fedora', 'opensuse', 'linuxmint', 'manjaro',
                     'kde'])


def distribution():
    """"""Get some information about the running Linux distribution.

    Returns:
        A DistributionInfo object, or None if no info could be determined.
            parsed: A Distribution enum member
            version: A Version object, or None
            pretty: Always a string (might be ""Unknown"")
    """"""
    filename = os.environ.get('QUTE_FAKE_OS_RELEASE', '/etc/os-release')
    info = {}
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if (not line) or line.startswith('#'):
                    continue
                k, v = line.split(""="", maxsplit=1)
                info[k] = v.strip('""')
    except (OSError, UnicodeDecodeError):
        return None

    pretty = info.get('PRETTY_NAME', 'Unknown')
    if pretty == 'Linux':  # Thanks, Funtoo
        pretty = info.get('NAME', pretty)

    if 'VERSION_ID' in info:
        dist_version = pkg_resources.parse_version(info['VERSION_ID'])
    else:
        dist_version = None

    dist_id = info.get('ID', None)
    id_mappings = {
        'funtoo': 'gentoo',  # does not have ID_LIKE=gentoo
        'org.kde.Platform': 'kde',
    }
    try:
        parsed = Distribution[id_mappings.get(dist_id, dist_id)]
    except KeyError:
        parsed = Distribution.unknown

    return DistributionInfo(parsed=parsed, version=dist_version, pretty=pretty,
def sandboxed():
    """"""Check whether the environment is a sandbox, i.e. has restricted access to the host system.

    Return:
        True if the environment is a sandbox.
    """"""
    return distribution().parsed == Distribution.kde


                            id=dist_id)


def _git_str():
    """"""Try to find out git version.

    Return:
        string containing the git commit ID.
        None if there was an error or we're not in a git repo.
    """"""
    # First try via subprocess if possible
    commit = None
    if not hasattr(sys, ""frozen""):
        try:
            gitpath = os.path.join(os.path.dirname(os.path.realpath(__file__)),
                                   os.path.pardir, os.path.pardir)
        except (NameError, OSError):
            log.misc.exception(""Error while getting git path"")
        else:
            commit = _git_str_subprocess(gitpath)
    if commit is not None:
        return commit
    # If that fails, check the git-commit-id file.
    try:
        return utils.read_file('git-commit-id')
    except (OSError, ImportError):
        return None


def _git_str_subprocess(gitpath):
    """"""Try to get the git commit ID and timestamp by calling git.

    Args:
        gitpath: The path where the .git folder is.

    Return:
        The ID/timestamp on success, None on failure.
    """"""
    if not os.path.isdir(os.path.join(gitpath, "".git"")):
        return None
    try:
        # https://stackoverflow.com/questions/21017300/21017394#21017394
        commit_hash = subprocess.run(
            ['git', 'describe', '--match=NeVeRmAtCh', '--always', '--dirty'],
            cwd=gitpath, check=True,
            stdout=subprocess.PIPE).stdout.decode('UTF-8').strip()
        date = subprocess.run(
            ['git', 'show', '-s', '--format=%ci', 'HEAD'],
            cwd=gitpath, check=True,
            stdout=subprocess.PIPE).stdout.decode('UTF-8').strip()
        return '{} ({})'.format(commit_hash, date)
    except (subprocess.CalledProcessError, OSError):
        return None


def _release_info():
    """"""Try to gather distribution release information.

    Return:
        list of (filename, content) tuples.
    """"""
    blacklisted = ['ANSI_COLOR=', 'HOME_URL=', 'SUPPORT_URL=',
                   'BUG_REPORT_URL=']
    data = []
    for fn in glob.glob(""/etc/*-release""):
        lines = []
        try:
            with open(fn, 'r', encoding='utf-8') as f:
                for line in f.read().strip().splitlines():
                    if not any(line.startswith(bl) for bl in blacklisted):
                        lines.append(line)

                if lines:
                    data.append((fn, '\n'.join(lines)))
        except OSError:
            log.misc.exception(""Error while reading {}."".format(fn))
    return data


def _module_versions():
    """"""Get versions of optional modules.

    Return:
        A list of lines with version info.
    """"""
    lines = []
    modules = collections.OrderedDict([
        ('sip', ['SIP_VERSION_STR']),
        ('colorama', ['VERSION', '__version__']),
        ('pypeg2', ['__version__']),
        ('jinja2', ['__version__']),
        ('pygments', ['__version__']),
        ('yaml', ['__version__']),
        ('cssutils', ['__version__']),
        ('attr', ['__version__']),
        ('PyQt5.QtWebEngineWidgets', []),
        ('PyQt5.QtWebKitWidgets', []),
    ])
    for modname, attributes in modules.items():
        try:
            module = importlib.import_module(modname)
        except ImportError:
            text = '{}: no'.format(modname)
        else:
            for name in attributes:
                try:
                    text = '{}: {}'.format(modname, getattr(module, name))
                except AttributeError:
                    pass
                else:
                    break
            else:
                text = '{}: yes'.format(modname)
        lines.append(text)
    return lines


def _path_info():
    """"""Get info about important path names.

    Return:
        A dictionary of descriptive to actual path names.
    """"""
    info = {
        'config': standarddir.config(),
        'data': standarddir.data(),
        'cache': standarddir.cache(),
        'runtime': standarddir.runtime(),
    }
    if standarddir.config() != standarddir.config(auto=True):
        info['auto config'] = standarddir.config(auto=True)
    if standarddir.data() != standarddir.data(system=True):
        info['system data'] = standarddir.data(system=True)
    return info


def _os_info():
    """"""Get operating system info.

    Return:
        A list of lines with version info.
    """"""
    lines = []
    releaseinfo = None
    if utils.is_linux:
        osver = ''
        releaseinfo = _release_info()
    elif utils.is_windows:
        osver = ', '.join(platform.win32_ver())
    elif utils.is_mac:
        release, versioninfo, machine = platform.mac_ver()
        if all(not e for e in versioninfo):
            versioninfo = ''
        else:
            versioninfo = '.'.join(versioninfo)
        osver = ', '.join([e for e in [release, versioninfo, machine] if e])
    elif utils.is_posix:
        osver = ' '.join(platform.uname())
    else:
        osver = '?'
    lines.append('OS Version: {}'.format(osver))
    if releaseinfo is not None:
        for (fn, data) in releaseinfo:
            lines += ['', '--- {} ---'.format(fn), data]
    return lines


def _pdfjs_version():
    """"""Get the pdf.js version.

    Return:
        A string with the version number.
    """"""
    try:
        pdfjs_file, file_path = pdfjs.get_pdfjs_res_and_path('build/pdf.js')
    except pdfjs.PDFJSNotFound:
        return 'no'
    else:
        pdfjs_file = pdfjs_file.decode('utf-8')
        version_re = re.compile(
            r""^ *(PDFJS\.version|var pdfjsVersion) = '([^']+)';$"",
            re.MULTILINE)

        match = version_re.search(pdfjs_file)
        if not match:
            pdfjs_version = 'unknown'
        else:
            pdfjs_version = match.group(2)
        if file_path is None:
            file_path = 'bundled'
        return '{} ({})'.format(pdfjs_version, file_path)


def _chromium_version():
    """"""Get the Chromium version for QtWebEngine.

    This can also be checked by looking at this file with the right Qt tag:
    http://code.qt.io/cgit/qt/qtwebengine.git/tree/tools/scripts/version_resolver.py#n41

    Quick reference:

    Qt 5.7:  Chromium 49
             49.0.2623.111 (2016-03-31)
             5.7.1: Security fixes up to 54.0.2840.87 (2016-11-01)

    Qt 5.8:  Chromium 53
             53.0.2785.148 (2016-08-31)
             5.8.0: Security fixes up to 55.0.2883.75 (2016-12-01)

    Qt 5.9:  Chromium 56
    (LTS)    56.0.2924.122 (2017-01-25)
             5.9.8: Security fixes up to 72.0.3626.121 (2019-03-01)

    Qt 5.10: Chromium 61
             61.0.3163.140 (2017-09-05)
             5.10.1: Security fixes up to 64.0.3282.140 (2018-02-01)

    Qt 5.11: Chromium 65
             65.0.3325.151 (.1: .230) (2018-03-06)
             5.11.3: Security fixes up to 70.0.3538.102 (2018-11-09)

    Qt 5.12: Chromium 69
    (LTS)    69.0.3497.113 (2018-09-27)
             5.12.4: Security fixes up to 74.0.3729.157 (2019-05-14)

    Qt 5.13: Chromium 73
             73.0.3683.105 (~2019-02-28)
             5.13.0: Security fixes up to 74.0.3729.131 (2019-04-30)

    Also see https://www.chromium.org/developers/calendar
    and https://chromereleases.googleblog.com/
    """"""
    if webenginesettings is None or QWebEngineProfile is None:
        # This should never happen
        return 'unavailable'
    ua = webenginesettings.default_user_agent
    if ua is None:
        profile = QWebEngineProfile.defaultProfile()
        ua = profile.httpUserAgent()
    match = re.search(r' Chrome/([^ ]*) ', ua)
    if not match:
        log.misc.error(""Could not get Chromium version from: {}"".format(ua))
        return 'unknown'
    return match.group(1)


def _backend():
    """"""Get the backend line with relevant information.""""""
    if objects.backend == usertypes.Backend.QtWebKit:
        return 'new QtWebKit (WebKit {})'.format(qWebKitVersion())
    else:
        webengine = usertypes.Backend.QtWebEngine
        assert objects.backend == webengine, objects.backend
        return 'QtWebEngine (Chromium {})'.format(_chromium_version())


def _uptime() -> datetime.timedelta:
    launch_time = QApplication.instance().launch_time
    time_delta = datetime.datetime.now() - launch_time
    # Round off microseconds
    time_delta -= datetime.timedelta(microseconds=time_delta.microseconds)
    return time_delta


def version():
    """"""Return a string with various version information.""""""
    lines = [""qutebrowser v{}"".format(qutebrowser.__version__)]
    gitver = _git_str()
    if gitver is not None:
        lines.append(""Git commit: {}"".format(gitver))

    lines.append(""Backend: {}"".format(_backend()))

    lines += [
        '',
        '{}: {}'.format(platform.python_implementation(),
                        platform.python_version()),
        'Qt: {}'.format(earlyinit.qt_version()),
        'PyQt: {}'.format(PYQT_VERSION_STR),
        '',
    ]

    lines += _module_versions()

    lines += [
        'pdf.js: {}'.format(_pdfjs_version()),
        'sqlite: {}'.format(sql.version()),
        'QtNetwork SSL: {}\n'.format(QSslSocket.sslLibraryVersionString()
                                     if QSslSocket.supportsSsl() else 'no'),
    ]

    qapp = QApplication.instance()
    if qapp:
        style = qapp.style()
        lines.append('Style: {}'.format(style.metaObject().className()))

    importpath = os.path.dirname(os.path.abspath(qutebrowser.__file__))

    lines += [
        'Platform: {}, {}'.format(platform.platform(),
                                  platform.architecture()[0]),
    ]
    dist = distribution()
    if dist is not None:
        lines += [
            'Linux distribution: {} ({})'.format(dist.pretty, dist.parsed.name)
        ]

    lines += [
        'Frozen: {}'.format(hasattr(sys, 'frozen')),
        ""Imported from {}"".format(importpath),
        ""Using Python from {}"".format(sys.executable),
        ""Qt library executable path: {}, data path: {}"".format(
            QLibraryInfo.location(QLibraryInfo.LibraryExecutablesPath),
            QLibraryInfo.location(QLibraryInfo.DataPath)
        )
    ]

    if not dist or dist.parsed == Distribution.unknown:
        lines += _os_info()

    lines += [
        '',
        'Paths:',
    ]
    for name, path in sorted(_path_info().items()):
        lines += ['{}: {}'.format(name, path)]

    lines += [
        '',
        'Uptime: {}'.format(_uptime()),
    ]

    return '\n'.join(lines)


def opengl_vendor():  # pragma: no cover
    """"""Get the OpenGL vendor used.

    This returns a string such as 'nouveau' or
    'Intel Open Source Technology Center'; or None if the vendor can't be
    determined.
    """"""
    assert QApplication.instance()

    override = os.environ.get('QUTE_FAKE_OPENGL_VENDOR')
    if override is not None:
        log.init.debug(""Using override {}"".format(override))
        return override

    old_context = QOpenGLContext.currentContext()
    old_surface = None if old_context is None else old_context.surface()

    surface = QOffscreenSurface()
    surface.create()

    ctx = QOpenGLContext()
    ok = ctx.create()
    if not ok:
        log.init.debug(""Creating context failed!"")
        return None

    ok = ctx.makeCurrent(surface)
    if not ok:
        log.init.debug(""Making context current failed!"")
        return None

    try:
        if ctx.isOpenGLES():
            # Can't use versionFunctions there
            return None

        vp = QOpenGLVersionProfile()
        vp.setVersion(2, 0)

        try:
            vf = ctx.versionFunctions(vp)
        except ImportError as e:
            log.init.debug(""Importing version functions failed: {}"".format(e))
            return None

        if vf is None:
            log.init.debug(""Getting version functions failed!"")
            return None

        return vf.glGetString(vf.GL_VENDOR)
    finally:
        ctx.doneCurrent()
        if old_context and old_surface:
            old_context.makeCurrent(old_surface)


def pastebin_version(pbclient=None):
    """"""Pastebin the version and log the url to messages.""""""
    def _yank_url(url):
        utils.set_clipboard(url)
        message.info(""Version url {} yanked to clipboard."".format(url))

    def _on_paste_version_success(url):
        global pastebin_url
        url = url.strip()
        _yank_url(url)
        pbclient.deleteLater()
        pastebin_url = url

    def _on_paste_version_err(text):
        message.error(""Failed to pastebin version""
                      "" info: {}"".format(text))
        pbclient.deleteLater()

    if pastebin_url:
        _yank_url(pastebin_url)
        return

    app = QApplication.instance()
    http_client = httpclient.HTTPClient()

    misc_api = pastebin.PastebinClient.MISC_API_URL
    pbclient = pbclient or pastebin.PastebinClient(http_client, parent=app,
                                                   api_url=misc_api)

    pbclient.success.connect(_on_paste_version_success)
    pbclient.error.connect(_on_paste_version_err)

    pbclient.paste(getpass.getuser(),
                   ""qute version info {}"".format(qutebrowser.__version__),
                   version(),
                   private=True)
",`distribution()` can be `None` (e.g. on non-Linux systems),"This code provides utilities to gather and display various version information about qutebrowser, including its backend, Python version, Qt version, installed modules, PDF.js version, SQLite version, SSL support, and more. The code includes functions to determine the Linux distribution, check if the environment is a sandbox, retrieve Git commit information, and generate a detailed version report. The `version` function constructs a multi-line string containing all the collected information, which can be printed or logged."
25,"@@ -1207,6 +1207,8 @@ class Frame(object, metaclass=ABCMeta):
         >>> df['b'].sum(min_count=3)
         nan
         """"""
+        axis = validate_axis(axis)
+        numeric_only = None if axis == 1 else numeric_only
 
         def sum(spark_column, spark_type):
             if isinstance(spark_type, BooleanType):","        --------
        >>> df = ks.DataFrame([[2.0, 1.0], [3.0, None], [1.0, 0.0]], columns=list('AB'))
        >>> df
             A    B
        0  2.0  1.0
        1  3.0  NaN
        2  1.0  0.0

        By default, iterates over rows and finds the maximum in each column.

        >>> df.cummax()
             A    B
        0  2.0  1.0
        1  3.0  NaN
        2  3.0  1.0

        It works identically in Series.

        >>> df.B.cummax()
        0    1.0
        1    NaN
        2    1.0
        Name: B, dtype: float64
        """"""
        return self._apply_series_op(lambda kser: kser._cum(F.max, skipna), should_resolve=True)

    # TODO: add 'axis' parameter
    def cumsum(self, skipna: bool = True) -> Union[""Series"", ""DataFrame""]:
        """"""
        Return cumulative sum over a DataFrame or Series axis.

        Returns a DataFrame or Series of the same size containing the cumulative sum.

        .. note:: the current implementation of cumsum uses Spark's Window without
            specifying partition specification. This leads to move all data into
            single partition in single machine and could cause serious
            performance degradation. Avoid this method against very large dataset.

        Parameters
        ----------
        skipna : boolean, default True
            Exclude NA/null values. If an entire row/column is NA, the result will be NA.

        Returns
        -------
        DataFrame or Series

        See Also
        --------
        DataFrame.sum : Return the sum over DataFrame axis.
        DataFrame.cummax : Return cumulative maximum over DataFrame axis.
        DataFrame.cummin : Return cumulative minimum over DataFrame axis.
        DataFrame.cumsum : Return cumulative sum over DataFrame axis.
        DataFrame.cumprod : Return cumulative product over DataFrame axis.
        Series.sum : Return the sum over Series axis.
        Series.cummax : Return cumulative maximum over Series axis.
        Series.cummin : Return cumulative minimum over Series axis.
        Series.cumsum : Return cumulative sum over Series axis.
        Series.cumprod : Return cumulative product over Series axis.

        Examples
        --------
        >>> df = ks.DataFrame([[2.0, 1.0], [3.0, None], [1.0, 0.0]], columns=list('AB'))
        >>> df
             A    B
        0  2.0  1.0
        1  3.0  NaN
        2  1.0  0.0

        By default, iterates over rows and finds the sum in each column.

        >>> df.cumsum()
             A    B
        0  2.0  1.0
        1  5.0  NaN
        2  6.0  1.0

        It works identically in Series.

        >>> df.A.cumsum()
        0    2.0
        1    5.0
        2    6.0
        Name: A, dtype: float64
        """"""
        return self._apply_series_op(lambda kser: kser._cumsum(skipna), should_resolve=True)

    # TODO: add 'axis' parameter
    # TODO: use pandas_udf to support negative values and other options later
    #  other window except unbounded ones is supported as of Spark 3.0.
    def cumprod(self, skipna: bool = True) -> Union[""Series"", ""DataFrame""]:
        """"""
        Return cumulative product over a DataFrame or Series axis.

        Returns a DataFrame or Series of the same size containing the cumulative product.

        .. note:: the current implementation of cumprod uses Spark's Window without
            specifying partition specification. This leads to move all data into
            single partition in single machine and could cause serious
            performance degradation. Avoid this method against very large dataset.

        .. note:: unlike pandas', Koalas' emulates cumulative product by ``exp(sum(log(...)))``
            trick. Therefore, it only works for positive numbers.

        Parameters
        ----------
        skipna : boolean, default True
            Exclude NA/null values. If an entire row/column is NA, the result will be NA.

        Returns
        -------
        DataFrame or Series

        See Also
        --------
        DataFrame.cummax : Return cumulative maximum over DataFrame axis.
        DataFrame.cummin : Return cumulative minimum over DataFrame axis.
        DataFrame.cumsum : Return cumulative sum over DataFrame axis.
        DataFrame.cumprod : Return cumulative product over DataFrame axis.
        Series.cummax : Return cumulative maximum over Series axis.
        Series.cummin : Return cumulative minimum over Series axis.
        Series.cumsum : Return cumulative sum over Series axis.
        Series.cumprod : Return cumulative product over Series axis.

        Raises
        ------
        Exception : If the values is equal to or lower than 0.

        Examples
        --------
        >>> df = ks.DataFrame([[2.0, 1.0], [3.0, None], [4.0, 10.0]], columns=list('AB'))
        >>> df
             A     B
        0  2.0   1.0
        1  3.0   NaN
        2  4.0  10.0

        By default, iterates over rows and finds the sum in each column.

        >>> df.cumprod()
              A     B
        0   2.0   1.0
        1   6.0   NaN
        2  24.0  10.0

        It works identically in Series.

        >>> df.A.cumprod()
        0     2.0
        1     6.0
        2    24.0
        Name: A, dtype: float64
        """"""
        return self._apply_series_op(lambda kser: kser._cumprod(skipna), should_resolve=True)

    # TODO: Although this has removed pandas >= 1.0.0, but we're keeping this as deprecated
    # since we're using this for `DataFrame.info` internally.
    # We can drop it once our minimal pandas version becomes 1.0.0.
    def get_dtype_counts(self) -> pd.Series:
        """"""
        Return counts of unique dtypes in this object.

        .. deprecated:: 0.14.0

        Returns
        -------
        dtype : pd.Series
            Series with the count of columns with each dtype.

        See Also
        --------
        dtypes : Return the dtypes in this object.

        Examples
        --------
        >>> a = [['a', 1, 1], ['b', 2, 2], ['c', 3, 3]]
        >>> df = ks.DataFrame(a, columns=['str', 'int1', 'int2'])
        >>> df
          str  int1  int2
        0   a     1     1
        1   b     2     2
        2   c     3     3

        >>> df.get_dtype_counts().sort_values()
        object    1
        int64     2
        dtype: int64

        >>> df.str.get_dtype_counts().sort_values()
        object    1
        dtype: int64
        """"""
        warnings.warn(
            ""`get_dtype_counts` has been deprecated and will be ""
            ""removed in a future version. For DataFrames use ""
            ""`.dtypes.value_counts()"",
            FutureWarning,
        )
        if not isinstance(self.dtypes, Iterable):
            dtypes = [self.dtypes]
        else:
            dtypes = list(self.dtypes)
        return pd.Series(dict(Counter([d.name for d in dtypes])))

    def pipe(self, func, *args, **kwargs) -> Any:
        r""""""
        Apply func(self, \*args, \*\*kwargs).

        Parameters
        ----------
        func : function
            function to apply to the DataFrame.
            ``args``, and ``kwargs`` are passed into ``func``.
            Alternatively a ``(callable, data_keyword)`` tuple where
            ``data_keyword`` is a string indicating the keyword of
            ``callable`` that expects the DataFrames.
        args : iterable, optional
            positional arguments passed into ``func``.
        kwargs : mapping, optional
            a dictionary of keyword arguments passed into ``func``.

        Returns
        -------
        object : the return type of ``func``.

        Notes
        -----
        Use ``.pipe`` when chaining together functions that expect
        Series, DataFrames or GroupBy objects. For example, given

        >>> df = ks.DataFrame({'category': ['A', 'A', 'B'],
        ...                    'col1': [1, 2, 3],
        ...                    'col2': [4, 5, 6]},
        ...                   columns=['category', 'col1', 'col2'])
        >>> def keep_category_a(df):
        ...     return df[df['category'] == 'A']
        >>> def add_one(df, column):
        ...     return df.assign(col3=df[column] + 1)
        >>> def multiply(df, column1, column2):
        ...     return df.assign(col4=df[column1] * df[column2])


        instead of writing

        >>> multiply(add_one(keep_category_a(df), column=""col1""), column1=""col2"", column2=""col3"")
          category  col1  col2  col3  col4
        0        A     1     4     2     8
        1        A     2     5     3    15


        You can write

        >>> (df.pipe(keep_category_a)
        ...    .pipe(add_one, column=""col1"")
        ...    .pipe(multiply, column1=""col2"", column2=""col3"")
        ... )
          category  col1  col2  col3  col4
        0        A     1     4     2     8
        1        A     2     5     3    15


        If you have a function that takes the data as (say) the second
        argument, pass a tuple indicating which keyword expects the
        data. For example, suppose ``f`` takes its data as ``df``:

        >>> def multiply_2(column1, df, column2):
        ...     return df.assign(col4=df[column1] * df[column2])


        Then you can write

        >>> (df.pipe(keep_category_a)
        ...    .pipe(add_one, column=""col1"")
        ...    .pipe((multiply_2, 'df'), column1=""col2"", column2=""col3"")
        ... )
          category  col1  col2  col3  col4
        0        A     1     4     2     8
        1        A     2     5     3    15

        You can use lambda as wel

        >>> ks.Series([1, 2, 3]).pipe(lambda x: (x + 1).rename(""value""))
        0    2
        1    3
        2    4
        Name: value, dtype: int64
        """"""

        if isinstance(func, tuple):
            func, target = func
            if target in kwargs:
                raise ValueError(""%s is both the pipe target and a keyword "" ""argument"" % target)
            kwargs[target] = self
            return func(*args, **kwargs)
        else:
            return func(self, *args, **kwargs)

    def to_numpy(self) -> np.ndarray:
        """"""
        A NumPy ndarray representing the values in this DataFrame or Series.

        .. note:: This method should only be used if the resulting NumPy ndarray is expected
            to be small, as all the data is loaded into the driver's memory.

        Returns
        -------
        numpy.ndarray

        Examples
        --------
        >>> ks.DataFrame({""A"": [1, 2], ""B"": [3, 4]}).to_numpy()
        array([[1, 3],
               [2, 4]])

        With heterogeneous data, the lowest common type will have to be used.

        >>> ks.DataFrame({""A"": [1, 2], ""B"": [3.0, 4.5]}).to_numpy()
        array([[1. , 3. ],
               [2. , 4.5]])

        For a mix of numeric and non-numeric types, the output array will have object dtype.

        >>> df = ks.DataFrame({""A"": [1, 2], ""B"": [3.0, 4.5], ""C"": pd.date_range('2000', periods=2)})
        >>> df.to_numpy()
        array([[1, 3.0, Timestamp('2000-01-01 00:00:00')],
               [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object)

        For Series,

        >>> ks.Series(['a', 'b', 'a']).to_numpy()
        array(['a', 'b', 'a'], dtype=object)
        """"""
        return self.to_pandas().values

    @property
    def values(self) -> np.ndarray:
        """"""
        Return a Numpy representation of the DataFrame or the Series.

        .. warning:: We recommend using `DataFrame.to_numpy()` or `Series.to_numpy()` instead.

        .. note:: This method should only be used if the resulting NumPy ndarray is expected
            to be small, as all the data is loaded into the driver's memory.

        Returns
        -------
        numpy.ndarray

        Examples
        --------
        A DataFrame where all columns are the same type (e.g., int64) results in an array of
        the same type.

        >>> df = ks.DataFrame({'age':    [ 3,  29],
        ...                    'height': [94, 170],
        ...                    'weight': [31, 115]})
        >>> df
           age  height  weight
        0    3      94      31
        1   29     170     115
        >>> df.dtypes
        age       int64
        height    int64
        weight    int64
        dtype: object
        >>> df.values
        array([[  3,  94,  31],
               [ 29, 170, 115]])

        A DataFrame with mixed type columns(e.g., str/object, int64, float32) results in an ndarray
        of the broadest type that accommodates these mixed types (e.g., object).

        >>> df2 = ks.DataFrame([('parrot',   24.0, 'second'),
        ...                     ('lion',     80.5, 'first'),
        ...                     ('monkey', np.nan, None)],
        ...                   columns=('name', 'max_speed', 'rank'))
        >>> df2.dtypes
        name          object
        max_speed    float64
        rank          object
        dtype: object
        >>> df2.values
        array([['parrot', 24.0, 'second'],
               ['lion', 80.5, 'first'],
               ['monkey', nan, None]], dtype=object)

        For Series,

        >>> ks.Series([1, 2, 3]).values
        array([1, 2, 3])

        >>> ks.Series(list('aabc')).values
        array(['a', 'a', 'b', 'c'], dtype=object)
        """"""
        warnings.warn(""We recommend using `{}.to_numpy()` instead."".format(type(self).__name__))
        return self.to_numpy()

    def to_csv(
        self,
        path=None,
        sep="","",
        na_rep="""",
        columns=None,
        header=True,
        quotechar='""',
        date_format=None,
        escapechar=None,
        num_files=None,
        mode: str = ""overwrite"",
        partition_cols: Optional[Union[str, List[str]]] = None,
        index_col: Optional[Union[str, List[str]]] = None,
        **options
    ) -> Optional[str]:
        r""""""
        Write object to a comma-separated values (csv) file.

        .. note:: Koalas `to_csv` writes files to a path or URI. Unlike pandas', Koalas
            respects HDFS's property such as 'fs.default.name'.

        .. note:: Koalas writes CSV files into the directory, `path`, and writes
            multiple `part-...` files in the directory when `path` is specified.
            This behaviour was inherited from Apache Spark. The number of files can
            be controlled by `num_files`.

        Parameters
        ----------
        path : str, default None
            File path. If None is provided the result is returned as a string.
        sep : str, default ','
            String of length 1. Field delimiter for the output file.
        na_rep : str, default ''
            Missing data representation.
        columns : sequence, optional
            Columns to write.
        header : bool or list of str, default True
            Write out the column names. If a list of strings is given it is
            assumed to be aliases for the column names.
        quotechar : str, default '\""'
            String of length 1. Character used to quote fields.
        date_format : str, default None
            Format string for datetime objects.
        escapechar : str, default None
            String of length 1. Character used to escape `sep` and `quotechar`
            when appropriate.
        num_files : the number of files to be written in `path` directory when
            this is a path.
        mode : str {'append', 'overwrite', 'ignore', 'error', 'errorifexists'},
            default 'overwrite'. Specifies the behavior of the save operation when the
            destination exists already.

            - 'append': Append the new data to existing data.
            - 'overwrite': Overwrite existing data.
            - 'ignore': Silently ignore this operation if data already exists.
            - 'error' or 'errorifexists': Throw an exception if data already exists.

        partition_cols : str or list of str, optional, default None
            Names of partitioning columns
        index_col: str or list of str, optional, default: None
            Column names to be used in Spark to represent Koalas' index. The index name
            in Koalas is ignored. By default, the index is always lost.
        options: keyword arguments for additional options specific to PySpark.
            This kwargs are specific to PySpark's CSV options to pass. Check
            the options in PySpark's API documentation for spark.write.csv(...).
            It has higher priority and overwrites all other options.
            This parameter only works when `path` is specified.

        Returns
        -------
        str or None

        See Also
        --------
        read_csv
        DataFrame.to_delta
        DataFrame.to_table
        DataFrame.to_parquet
        DataFrame.to_spark_io

        Examples
        --------
        >>> df = ks.DataFrame(dict(
        ...    date=list(pd.date_range('2012-1-1 12:00:00', periods=3, freq='M')),
        ...    country=['KR', 'US', 'JP'],
        ...    code=[1, 2 ,3]), columns=['date', 'country', 'code'])
        >>> df.sort_values(by=""date"")  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
                           date country  code
        ... 2012-01-31 12:00:00      KR     1
        ... 2012-02-29 12:00:00      US     2
        ... 2012-03-31 12:00:00      JP     3

        >>> print(df.to_csv())  # doctest: +NORMALIZE_WHITESPACE
        date,country,code
        2012-01-31 12:00:00,KR,1
        2012-02-29 12:00:00,US,2
        2012-03-31 12:00:00,JP,3

        >>> df.cummax().to_csv(path=r'%s/to_csv/foo.csv' % path, num_files=1)
        >>> ks.read_csv(
        ...    path=r'%s/to_csv/foo.csv' % path
        ... ).sort_values(by=""date"")  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
                           date country  code
        ... 2012-01-31 12:00:00      KR     1
        ... 2012-02-29 12:00:00      US     2
        ... 2012-03-31 12:00:00      US     3

        In case of Series,

        >>> print(df.date.to_csv())  # doctest: +NORMALIZE_WHITESPACE
        date
        2012-01-31 12:00:00
        2012-02-29 12:00:00
        2012-03-31 12:00:00

        >>> df.date.to_csv(path=r'%s/to_csv/foo.csv' % path, num_files=1)
        >>> ks.read_csv(
        ...     path=r'%s/to_csv/foo.csv' % path
        ... ).sort_values(by=""date"")  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
                           date
        ... 2012-01-31 12:00:00
        ... 2012-02-29 12:00:00
        ... 2012-03-31 12:00:00

        You can preserve the index in the roundtrip as below.

        >>> df.set_index(""country"", append=True, inplace=True)
        >>> df.date.to_csv(
        ...     path=r'%s/to_csv/bar.csv' % path,
        ...     num_files=1,
        ...     index_col=[""index1"", ""index2""])
        >>> ks.read_csv(
        ...     path=r'%s/to_csv/bar.csv' % path, index_col=[""index1"", ""index2""]
        ... ).sort_values(by=""date"")  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
                                     date
        index1 index2
        ...    ...    2012-01-31 12:00:00
        ...    ...    2012-02-29 12:00:00
        ...    ...    2012-03-31 12:00:00
        """"""
        if ""options"" in options and isinstance(options.get(""options""), dict) and len(options) == 1:
            options = options.get(""options"")  # type: ignore

        if path is None:
            # If path is none, just collect and use pandas's to_csv.
            kdf_or_ser = self
            if (LooseVersion(""0.24"") > LooseVersion(pd.__version__)) and isinstance(
                self, ks.Series
            ):
                # 0.23 seems not having 'columns' parameter in Series' to_csv.
                return kdf_or_ser.to_pandas().to_csv(  # type: ignore
                    None,
                    sep=sep,
                    na_rep=na_rep,
                    header=header,
                    date_format=date_format,
                    index=False,
                )
            else:
                return kdf_or_ser.to_pandas().to_csv(  # type: ignore
                    None,
                    sep=sep,
                    na_rep=na_rep,
                    columns=columns,
                    header=header,
                    quotechar=quotechar,
                    date_format=date_format,
                    escapechar=escapechar,
                    index=False,
                )

        kdf = self
        if isinstance(self, ks.Series):
            kdf = self.to_frame()

        if columns is None:
            column_labels = kdf._internal.column_labels
        else:
            column_labels = []
            for label in columns:
                if not is_name_like_tuple(label):
                    label = (label,)
                if label not in kdf._internal.column_labels:
                    raise KeyError(name_like_string(label))
                column_labels.append(label)

        if isinstance(index_col, str):
            index_cols = [index_col]
        elif index_col is None:
            index_cols = []
        else:
            index_cols = index_col

        if header is True and kdf._internal.column_labels_level > 1:
            raise ValueError(""to_csv only support one-level index column now"")
        elif isinstance(header, list):
            sdf = kdf.to_spark(index_col)  # type: ignore
            sdf = sdf.select(
                [scol_for(sdf, name_like_string(label)) for label in index_cols]
                + [
                    scol_for(sdf, str(i) if label is None else name_like_string(label)).alias(
                        new_name
                    )
                    for i, (label, new_name) in enumerate(zip(column_labels, header))
                ]
            )
            header = True
        else:
            sdf = kdf.to_spark(index_col)  # type: ignore
            sdf = sdf.select(
                [scol_for(sdf, name_like_string(label)) for label in index_cols]
                + [
                    scol_for(sdf, str(i) if label is None else name_like_string(label))
                    for i, label in enumerate(column_labels)
                ]
            )

        if num_files is not None:
            sdf = sdf.repartition(num_files)

        builder = sdf.write.mode(mode)
        if partition_cols is not None:
            builder.partitionBy(partition_cols)
        builder._set_opts(
            sep=sep,
            nullValue=na_rep,
            header=header,
            quote=quotechar,
            dateFormat=date_format,
            charToEscapeQuoteEscaping=escapechar,
        )
        builder.options(**options).format(""csv"").save(path)
        return None

    def to_json(
        self,
        path=None,
        compression=""uncompressed"",
        num_files=None,
        mode: str = ""overwrite"",
        partition_cols: Optional[Union[str, List[str]]] = None,
        index_col: Optional[Union[str, List[str]]] = None,
        **options
    ) -> Optional[str]:
        """"""
        Convert the object to a JSON string.

        .. note:: Koalas `to_json` writes files to a path or URI. Unlike pandas', Koalas
            respects HDFS's property such as 'fs.default.name'.

        .. note:: Koalas writes JSON files into the directory, `path`, and writes
            multiple `part-...` files in the directory when `path` is specified.
            This behaviour was inherited from Apache Spark. The number of files can
            be controlled by `num_files`.

        .. note:: output JSON format is different from pandas'. It always use `orient='records'`
            for its output. This behaviour might have to change in the near future.

        Note NaN's and None will be converted to null and datetime objects
        will be converted to UNIX timestamps.

        Parameters
        ----------
        path : string, optional
            File path. If not specified, the result is returned as
            a string.
        compression : {'gzip', 'bz2', 'xz', None}
            A string representing the compression to use in the output file,
            only used when the first argument is a filename. By default, the
            compression is inferred from the filename.
        num_files : the number of files to be written in `path` directory when
            this is a path.
        mode : str {'append', 'overwrite', 'ignore', 'error', 'errorifexists'},
            default 'overwrite'. Specifies the behavior of the save operation when the
            destination exists already.

            - 'append': Append the new data to existing data.
            - 'overwrite': Overwrite existing data.
            - 'ignore': Silently ignore this operation if data already exists.
            - 'error' or 'errorifexists': Throw an exception if data already exists.

        partition_cols : str or list of str, optional, default None
            Names of partitioning columns
        index_col: str or list of str, optional, default: None
            Column names to be used in Spark to represent Koalas' index. The index name
            in Koalas is ignored. By default, the index is always lost.
        options: keyword arguments for additional options specific to PySpark.
            It is specific to PySpark's JSON options to pass. Check
            the options in PySpark's API documentation for `spark.write.json(...)`.
            It has a higher priority and overwrites all other options.
            This parameter only works when `path` is specified.

        Returns
        --------
        str or None

        Examples
        --------
        >>> df = ks.DataFrame([['a', 'b'], ['c', 'd']],
        ...                   columns=['col 1', 'col 2'])
        >>> df.to_json()
        '[{""col 1"":""a"",""col 2"":""b""},{""col 1"":""c"",""col 2"":""d""}]'

        >>> df['col 1'].to_json()
        '[{""col 1"":""a""},{""col 1"":""c""}]'

        >>> df.to_json(path=r'%s/to_json/foo.json' % path, num_files=1)
        >>> ks.read_json(
        ...     path=r'%s/to_json/foo.json' % path
        ... ).sort_values(by=""col 1"")
          col 1 col 2
        0     a     b
        1     c     d

        >>> df['col 1'].to_json(path=r'%s/to_json/foo.json' % path, num_files=1, index_col=""index"")
        >>> ks.read_json(
        ...     path=r'%s/to_json/foo.json' % path, index_col=""index""
        ... ).sort_values(by=""col 1"")  # doctest: +NORMALIZE_WHITESPACE
              col 1
        index
        0         a
        1         c
        """"""
        if ""options"" in options and isinstance(options.get(""options""), dict) and len(options) == 1:
            options = options.get(""options"")  # type: ignore

        if path is None:
            # If path is none, just collect and use pandas's to_json.
            kdf_or_ser = self
            pdf = kdf_or_ser.to_pandas()  # type: ignore
            if isinstance(self, ks.Series):
                pdf = pdf.to_frame()
            # To make the format consistent and readable by `read_json`, convert it to pandas' and
            # use 'records' orient for now.
            return pdf.to_json(orient=""records"")

        kdf = self
        if isinstance(self, ks.Series):
            kdf = self.to_frame()
        sdf = kdf.to_spark(index_col=index_col)  # type: ignore

        if num_files is not None:
            sdf = sdf.repartition(num_files)

        builder = sdf.write.mode(mode)
        if partition_cols is not None:
            builder.partitionBy(partition_cols)
        builder._set_opts(compression=compression)
        builder.options(**options).format(""json"").save(path)
        return None

    def to_excel(
        self,
        excel_writer,
        sheet_name=""Sheet1"",
        na_rep="""",
        float_format=None,
        columns=None,
        header=True,
        index=True,
        index_label=None,
        startrow=0,
        startcol=0,
        engine=None,
        merge_cells=True,
        encoding=None,
        inf_rep=""inf"",
        verbose=True,
        freeze_panes=None,
    ) -> None:
        """"""
        Write object to an Excel sheet.

        .. note:: This method should only be used if the resulting DataFrame is expected
                  to be small, as all the data is loaded into the driver's memory.

        To write a single object to an Excel .xlsx file it is only necessary to
        specify a target file name. To write to multiple sheets it is necessary to
        create an `ExcelWriter` object with a target file name, and specify a sheet
        in the file to write to.

        Multiple sheets may be written to by specifying unique `sheet_name`.
        With all data written to the file it is necessary to save the changes.
        Note that creating an `ExcelWriter` object with a file name that already
        exists will result in the contents of the existing file being erased.

        Parameters
        ----------
        excel_writer : str or ExcelWriter object
            File path or existing ExcelWriter.
        sheet_name : str, default 'Sheet1'
            Name of sheet which will contain DataFrame.
        na_rep : str, default ''
            Missing data representation.
        float_format : str, optional
            Format string for floating point numbers. For example
            ``float_format=""%%.2f""`` will format 0.1234 to 0.12.
        columns : sequence or list of str, optional
            Columns to write.
        header : bool or list of str, default True
            Write out the column names. If a list of string is given it is
            assumed to be aliases for the column names.
        index : bool, default True
            Write row names (index).
        index_label : str or sequence, optional
            Column label for index column(s) if desired. If not specified, and
            `header` and `index` are True, then the index names are used. A
            sequence should be given if the DataFrame uses MultiIndex.
        startrow : int, default 0
            Upper left cell row to dump data frame.
        startcol : int, default 0
            Upper left cell column to dump data frame.
        engine : str, optional
            Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set this
            via the options ``io.excel.xlsx.writer``, ``io.excel.xls.writer``, and
            ``io.excel.xlsm.writer``.
        merge_cells : bool, default True
            Write MultiIndex and Hierarchical Rows as merged cells.
        encoding : str, optional
            Encoding of the resulting excel file. Only necessary for xlwt,
            other writers support unicode natively.
        inf_rep : str, default 'inf'
            Representation for infinity (there is no native representation for
            infinity in Excel).
        verbose : bool, default True
            Display more information in the error logs.
        freeze_panes : tuple of int (length 2), optional
            Specifies the one-based bottommost row and rightmost column that
            is to be frozen.

        Notes
        -----
        Once a workbook has been saved it is not possible write further data
        without rewriting the whole workbook.

        See Also
        --------
        read_excel : Read Excel file.

        Examples
        --------
        Create, write to and save a workbook:

        >>> df1 = ks.DataFrame([['a', 'b'], ['c', 'd']],
        ...                    index=['row 1', 'row 2'],
        ...                    columns=['col 1', 'col 2'])
        >>> df1.to_excel(""output.xlsx"")  # doctest: +SKIP

        To specify the sheet name:

        >>> df1.to_excel(""output.xlsx"")  # doctest: +SKIP
        >>> df1.to_excel(""output.xlsx"",
        ...              sheet_name='Sheet_name_1')  # doctest: +SKIP

        If you wish to write to more than one sheet in the workbook, it is
        necessary to specify an ExcelWriter object:

        >>> with pd.ExcelWriter('output.xlsx') as writer:  # doctest: +SKIP
        ...      df1.to_excel(writer, sheet_name='Sheet_name_1')
        ...      df2.to_excel(writer, sheet_name='Sheet_name_2')

        To set the library that is used to write the Excel file,
        you can pass the `engine` keyword (the default engine is
        automatically chosen depending on the file extension):

        >>> df1.to_excel('output1.xlsx', engine='xlsxwriter')  # doctest: +SKIP
        """"""
        # Make sure locals() call is at the top of the function so we don't capture local variables.
        args = locals()
        kdf = self

        if isinstance(self, ks.DataFrame):
            f = pd.DataFrame.to_excel
        elif isinstance(self, ks.Series):
            f = pd.Series.to_excel
        else:
            raise TypeError(
                ""Constructor expects DataFrame or Series; however, "" ""got [%s]"" % (self,)
            )
        return validate_arguments_and_invoke_function(
            kdf._to_internal_pandas(), self.to_excel, f, args
        )

    def mean(
        self, axis: Union[int, str] = None, numeric_only: bool = True
    ) -> Union[Scalar, ""Series""]:
        """"""
        Return the mean of the values.

        Parameters
        ----------
        axis : {index (0), columns (1)}
            Axis for the function to be applied on.
        numeric_only : bool, default True
            Include only float, int, boolean columns. False is not supported. This parameter
            is mainly for pandas compatibility.

        Returns
        -------
        mean : scalar for a Series, and a Series for a DataFrame.

        Examples
        --------

        >>> df = ks.DataFrame({'a': [1, 2, 3, np.nan], 'b': [0.1, 0.2, 0.3, np.nan]},
        ...                   columns=['a', 'b'])

        On a DataFrame:

        >>> df.mean()
        a    2.0
        b    0.2
        dtype: float64

        >>> df.mean(axis=1)
        0    0.55
        1    1.10
        2    1.65
        3     NaN
        dtype: float64

        On a Series:

        >>> df['a'].mean()
        2.0
        """"""

        def mean(spark_column, spark_type):
            if isinstance(spark_type, BooleanType):
                spark_column = spark_column.cast(LongType())
            elif not isinstance(spark_type, NumericType):
                raise TypeError(
                    ""Could not convert {} ({}) to numeric"".format(
                        spark_type_to_pandas_dtype(spark_type), spark_type.simpleString()
                    )
                )
            return F.mean(spark_column)

        return self._reduce_for_stat_function(
            mean, name=""mean"", axis=axis, numeric_only=numeric_only
        )

    def sum(
        self, axis: Union[int, str] = None, numeric_only: bool = True, min_count: int = 0
    ) -> Union[Scalar, ""Series""]:
        """"""
        Return the sum of the values.

        Parameters
        ----------
        axis : {index (0), columns (1)}
            Axis for the function to be applied on.
        numeric_only : bool, default True
            Include only float, int, boolean columns. False is not supported. This parameter
            is mainly for pandas compatibility.
        min_count : int, default 0
            The required number of valid values to perform the operation. If fewer than
             ``min_count`` non-NA values are present the result will be NA.

        Returns
        -------
        sum : scalar for a Series, and a Series for a DataFrame.

        Examples
        --------

        >>> df = ks.DataFrame({'a': [1, 2, 3, np.nan], 'b': [0.1, np.nan, 0.3, np.nan]},
        ...                   columns=['a', 'b'])

        On a DataFrame:

        >>> df.sum()
        a    6.0
        b    0.4
        dtype: float64

        >>> df.sum(axis=1)
        0    1.1
        1    2.0
        2    3.3
        3    0.0
        dtype: float64

        >>> df.sum(min_count=3)
        a    6.0
        b    NaN
        dtype: float64

        >>> df.sum(axis=1, min_count=1)
        0    1.1
        1    2.0
        2    3.3
        3    NaN
        dtype: float64

        On a Series:

        >>> df['a'].sum()
        6.0

        >>> df['a'].sum(min_count=3)
        6.0
        axis = validate_axis(axis)
        numeric_only = None if axis == 1 else numeric_only
        >>> df['b'].sum(min_count=3)
        nan
        """"""

        def sum(spark_column, spark_type):
            if isinstance(spark_type, BooleanType):
                spark_column = spark_column.cast(LongType())
            elif not isinstance(spark_type, NumericType):
                raise TypeError(
                    ""Could not convert {} ({}) to numeric"".format(
                        spark_type_to_pandas_dtype(spark_type), spark_type.simpleString()
                    )
                )
            return F.coalesce(F.sum(spark_column), F.lit(0))

        return self._reduce_for_stat_function(
            sum, name=""sum"", axis=axis, numeric_only=numeric_only, min_count=min_count
        )

    def product(
        self, axis: Union[int, str] = None, numeric_only: bool = True, min_count: int = 0
    ) -> Union[Scalar, ""Series""]:
        """"""
        Return the product of the values.

        .. note:: unlike pandas', Koalas' emulates product by ``exp(sum(log(...)))``
            trick. Therefore, it only works for positive numbers.

        Parameters
        ----------
        axis : {index (0), columns (1)}
            Axis for the function to be applied on.
        numeric_only : bool, default True
            Include only float, int, boolean columns. False is not supported. This parameter
            is mainly for pandas compatibility.
        min_count : int, default 0
            The required number of valid values to perform the operation. If fewer than
            ``min_count`` non-NA values are present the result will be NA.

        Examples
        --------
        On a DataFrame:

        Non-numeric type column is not included to the result.

        >>> kdf = ks.DataFrame({'A': [1, 2, 3, 4, 5],
        ...                     'B': [10, 20, 30, 40, 50],
        ...                     'C': ['a', 'b', 'c', 'd', 'e']})
        >>> kdf
           A   B  C
        0  1  10  a
        1  2  20  b
        2  3  30  c
        3  4  40  d
        4  5  50  e

        >>> kdf.prod()
        A         120
        B    12000000
        dtype: int64

        If there is no numeric type columns, returns empty Series.

        >>> ks.DataFrame({""key"": ['a', 'b', 'c'], ""val"": ['x', 'y', 'z']}).prod()
        Series([], dtype: float64)

        On a Series:

        >>> ks.Series([1, 2, 3, 4, 5]).prod()
        120

        By default, the product of an empty or all-NA Series is ``1``

        >>> ks.Series([]).prod()
        1.0

        This can be controlled with the ``min_count`` parameter

        >>> ks.Series([]).prod(min_count=1)
        nan
        """"""

        def prod(spark_column, spark_type):
            if isinstance(spark_type, BooleanType):
                scol = F.min(F.coalesce(spark_column, F.lit(True))).cast(LongType())
            elif isinstance(spark_type, NumericType):
                num_zeros = F.sum(F.when(spark_column == 0, 1).otherwise(0))
                sign = F.when(
                    F.sum(F.when(spark_column < 0, 1).otherwise(0)) % 2 == 0, 1
                ).otherwise(-1)

                scol = F.when(num_zeros > 0, 0).otherwise(
                    sign * F.exp(F.sum(F.log(F.abs(spark_column))))
                )

                if isinstance(spark_type, IntegralType):
                    scol = F.round(scol).cast(LongType())
            else:
                raise TypeError(
                    ""Could not convert {} ({}) to numeric"".format(
                        spark_type_to_pandas_dtype(spark_type), spark_type.simpleString()
                    )
                )

            return F.coalesce(scol, F.lit(1))

        return self._reduce_for_stat_function(
            prod, name=""prod"", axis=axis, numeric_only=numeric_only, min_count=min_count
        )

    prod = product

    def skew(
        self, axis: Union[int, str] = None, numeric_only: bool = True
    ) -> Union[Scalar, ""Series""]:
        """"""
        Return unbiased skew normalized by N-1.

        Parameters
        ----------
        axis : {index (0), columns (1)}
            Axis for the function to be applied on.
        numeric_only : bool, default True
            Include only float, int, boolean columns. False is not supported. This parameter
            is mainly for pandas compatibility.

        Returns
        -------
        skew : scalar for a Series, and a Series for a DataFrame.

        Examples
        --------

        >>> df = ks.DataFrame({'a': [1, 2, 3, np.nan], 'b': [0.1, 0.2, 0.3, np.nan]},
        ...                   columns=['a', 'b'])

        On a DataFrame:

        >>> df.skew()  # doctest: +SKIP
        a    0.000000e+00
        b   -3.319678e-16
        dtype: float64

        On a Series:

        >>> df['a'].skew()
        0.0
        """"""

        def skew(spark_column, spark_type):
            if isinstance(spark_type, BooleanType):
                spark_column = spark_column.cast(LongType())
            elif not isinstance(spark_type, NumericType):
                raise TypeError(
                    ""Could not convert {} ({}) to numeric"".format(
                        spark_type_to_pandas_dtype(spark_type), spark_type.simpleString()
                    )
                )
            return F.skewness(spark_column)

        return self._reduce_for_stat_function(
            skew, name=""skew"", axis=axis, numeric_only=numeric_only
        )

    def kurtosis(
        self, axis: Union[int, str] = None, numeric_only: bool = True
    ) -> Union[Scalar, ""Series""]:
        """"""
        Return unbiased kurtosis using Fisher’s definition of kurtosis (kurtosis of normal == 0.0).
        Normalized by N-1.

        Parameters
        ----------
        axis : {index (0), columns (1)}
            Axis for the function to be applied on.
        numeric_only : bool, default True
            Include only float, int, boolean columns. False is not supported. This parameter
            is mainly for pandas compatibility.

        Returns
        -------
        kurt : scalar for a Series, and a Series for a DataFrame.

        Examples
        --------

        >>> df = ks.DataFrame({'a': [1, 2, 3, np.nan], 'b': [0.1, 0.2, 0.3, np.nan]},
        ...                   columns=['a', 'b'])

        On a DataFrame:

        >>> df.kurtosis()
        a   -1.5
        b   -1.5
        dtype: float64

        On a Series:

        >>> df['a'].kurtosis()
        -1.5
        """"""

        def kurtosis(spark_column, spark_type):
            if isinstance(spark_type, BooleanType):
                spark_column = spark_column.cast(LongType())
            elif not isinstance(spark_type, NumericType):
                raise TypeError(
                    ""Could not convert {} ({}) to numeric"".format(
                        spark_type_to_pandas_dtype(spark_type), spark_type.simpleString()
                    )
                )
            return F.kurtosis(spark_column)

        return self._reduce_for_stat_function(
            kurtosis, name=""kurtosis"", axis=axis, numeric_only=numeric_only
        )

    kurt = kurtosis

    def min(
        self, axis: Union[int, str] = None, numeric_only: bool = None
    ) -> Union[Scalar, ""Series""]:
        """"""
        Return the minimum of the values.

        Parameters
        ----------
        axis : {index (0), columns (1)}
            Axis for the function to be applied on.
        numeric_only : bool, default None
            If True, include only float, int, boolean columns. This parameter is mainly for
            pandas compatibility. False is supported; however, the columns should
            be all numeric or all non-numeric.

        Returns
        -------
        min : scalar for a Series, and a Series for a DataFrame.

        Examples
        --------

        >>> df = ks.DataFrame({'a': [1, 2, 3, np.nan], 'b': [0.1, 0.2, 0.3, np.nan]},
        ...                   columns=['a', 'b'])

        On a DataFrame:

        >>> df.min()
        a    1.0
        b    0.1
        dtype: float64

        >>> df.min(axis=1)
        0    0.1
        1    0.2
        2    0.3
        3    NaN
        dtype: float64

        On a Series:

        >>> df['a'].min()
        1.0
        """"""
        return self._reduce_for_stat_function(
            F.min, name=""min"", axis=axis, numeric_only=numeric_only
        )

    def max(
        self, axis: Union[int, str] = None, numeric_only: bool = None
    ) -> Union[Scalar, ""Series""]:
        """"""
        Return the maximum of the values.

        Parameters
        ----------
        axis : {index (0), columns (1)}
            Axis for the function to be applied on.
        numeric_only : bool, default None
            If True, include only float, int, boolean columns. This parameter is mainly for
            pandas compatibility. False is supported; however, the columns should
            be all numeric or all non-numeric.

        Returns
        -------
        max : scalar for a Series, and a Series for a DataFrame.

        Examples
        --------

        >>> df = ks.DataFrame({'a': [1, 2, 3, np.nan], 'b': [0.1, 0.2, 0.3, np.nan]},
        ...                   columns=['a', 'b'])

        On a DataFrame:

        >>> df.max()
        a    3.0
        b    0.3
        dtype: float64

        >>> df.max(axis=1)
        0    1.0
        1    2.0
        2    3.0
        3    NaN
        dtype: float64

        On a Series:

        >>> df['a'].max()
        3.0
        """"""
        return self._reduce_for_stat_function(
            F.max, name=""max"", axis=axis, numeric_only=numeric_only
        )

    def count(
        self, axis: Union[int, str] = None, numeric_only: bool = None
    ) -> Union[Scalar, ""Series""]:
        """"""
        Count non-NA cells for each column.

        The values `None`, `NaN` are considered NA.

        Parameters
        ----------
        axis : {0 or ‘index’, 1 or ‘columns’}, default 0
            If 0 or ‘index’ counts are generated for each column. If 1 or ‘columns’ counts are
            generated for each row.
        numeric_only : bool, default None
            If True, include only float, int, boolean columns. This parameter is mainly for
            pandas compatibility.

        Returns
        -------
        max : scalar for a Series, and a Series for a DataFrame.

        See Also
        --------
        DataFrame.shape: Number of DataFrame rows and columns (including NA
            elements).
        DataFrame.isna: Boolean same-sized DataFrame showing places of NA
            elements.

        Examples
        --------
        Constructing DataFrame from a dictionary:

        >>> df = ks.DataFrame({""Person"":
        ...                    [""John"", ""Myla"", ""Lewis"", ""John"", ""Myla""],
        ...                    ""Age"": [24., np.nan, 21., 33, 26],
        ...                    ""Single"": [False, True, True, True, False]},
        ...                   columns=[""Person"", ""Age"", ""Single""])
        >>> df
          Person   Age  Single
        0   John  24.0   False
        1   Myla   NaN    True
        2  Lewis  21.0    True
        3   John  33.0    True
        4   Myla  26.0   False

        Notice the uncounted NA values:

        >>> df.count()
        Person    5
        Age       4
        Single    5
        dtype: int64

        >>> df.count(axis=1)
        0    3
        1    2
        2    3
        3    3
        4    3
        dtype: int64

        On a Series:

        >>> df['Person'].count()
        5

        >>> df['Age'].count()
        4
        """"""

        return self._reduce_for_stat_function(
            Frame._count_expr, name=""count"", axis=axis, numeric_only=numeric_only
        )

    def std(
        self, axis: Union[int, str] = None, ddof: int = 1, numeric_only: bool = True
    ) -> Union[Scalar, ""Series""]:
        """"""
        Return sample standard deviation.

        Parameters
        ----------
        axis : {index (0), columns (1)}
            Axis for the function to be applied on.
        ddof : int, default 1
            Delta Degrees of Freedom. The divisor used in calculations is N - ddof,
            where N represents the number of elements.
        numeric_only : bool, default True
            Include only float, int, boolean columns. False is not supported. This parameter
            is mainly for pandas compatibility.

        Returns
        -------
        std : scalar for a Series, and a Series for a DataFrame.

        Examples
        --------

        >>> df = ks.DataFrame({'a': [1, 2, 3, np.nan], 'b': [0.1, 0.2, 0.3, np.nan]},
        ...                   columns=['a', 'b'])

        On a DataFrame:

        >>> df.std()
        a    1.0
        b    0.1
        dtype: float64

        >>> df.std(axis=1)
        0    0.636396
        1    1.272792
        2    1.909188
        3         NaN
        dtype: float64

        >>> df.std(ddof=0)
        a    0.816497
        b    0.081650
        dtype: float64

        On a Series:

        >>> df['a'].std()
        1.0

        >>> df['a'].std(ddof=0)
        0.816496580927726
        """"""
        assert ddof in (0, 1)

        def std(spark_column, spark_type):
            if isinstance(spark_type, BooleanType):
                spark_column = spark_column.cast(LongType())
            elif not isinstance(spark_type, NumericType):
                raise TypeError(
                    ""Could not convert {} ({}) to numeric"".format(
                        spark_type_to_pandas_dtype(spark_type), spark_type.simpleString()
                    )
                )
            if ddof == 0:
                return F.stddev_pop(spark_column)
            else:
                return F.stddev_samp(spark_column)

        return self._reduce_for_stat_function(
            std, name=""std"", axis=axis, numeric_only=numeric_only, ddof=ddof
        )

    def var(
        self, axis: Union[int, str] = None, ddof: int = 1, numeric_only: bool = True
    ) -> Union[Scalar, ""Series""]:
        """"""
        Return unbiased variance.

        Parameters
        ----------
        axis : {index (0), columns (1)}
            Axis for the function to be applied on.
        ddof : int, default 1
            Delta Degrees of Freedom. The divisor used in calculations is N - ddof,
            where N represents the number of elements.
        numeric_only : bool, default True
            Include only float, int, boolean columns. False is not supported. This parameter
            is mainly for pandas compatibility.

        Returns
        -------
        var : scalar for a Series, and a Series for a DataFrame.

        Examples
        --------

        >>> df = ks.DataFrame({'a': [1, 2, 3, np.nan], 'b': [0.1, 0.2, 0.3, np.nan]},
        ...                   columns=['a', 'b'])

        On a DataFrame:

        >>> df.var()
        a    1.00
        b    0.01
        dtype: float64

        >>> df.var(axis=1)
        0    0.405
        1    1.620
        2    3.645
        3      NaN
        dtype: float64

        >>> df.var(ddof=0)
        a    0.666667
        b    0.006667
        dtype: float64

        On a Series:

        >>> df['a'].var()
        1.0

        >>> df['a'].var(ddof=0)
        0.6666666666666666
        """"""
        assert ddof in (0, 1)

        def var(spark_column, spark_type):
            if isinstance(spark_type, BooleanType):
                spark_column = spark_column.cast(LongType())
            elif not isinstance(spark_type, NumericType):
                raise TypeError(
                    ""Could not convert {} ({}) to numeric"".format(
                        spark_type_to_pandas_dtype(spark_type), spark_type.simpleString()
                    )
                )
            if ddof == 0:
                return F.var_pop(spark_column)
            else:
                return F.var_samp(spark_column)

        return self._reduce_for_stat_function(
            var, name=""var"", axis=axis, numeric_only=numeric_only, ddof=ddof
        )

    def median(
        self, axis: Union[int, str] = None, numeric_only: bool = True, accuracy: int = 10000
    ) -> Union[Scalar, ""Series""]:
        """"""
        Return the median of the values for the requested axis.

        .. note:: Unlike pandas', the median in Koalas is an approximated median based upon
            approximate percentile computation because computing median across a large dataset
            is extremely expensive.

        Parameters
        ----------
        axis : {index (0), columns (1)}
            Axis for the function to be applied on.
        numeric_only : bool, default True
            Include only float, int, boolean columns. False is not supported. This parameter
            is mainly for pandas compatibility.
        accuracy : int, optional
            Default accuracy of approximation. Larger value means better accuracy.
            The relative error can be deduced by 1.0 / accuracy.

        Returns
        -------
        median : scalar or Series

        Examples
        --------
        >>> df = ks.DataFrame({
        ...     'a': [24., 21., 25., 33., 26.], 'b': [1, 2, 3, 4, 5]}, columns=['a', 'b'])
        >>> df
              a  b
        0  24.0  1
        1  21.0  2
        2  25.0  3
        3  33.0  4
        4  26.0  5

        On a DataFrame:

        >>> df.median()
        a    25.0
        b     3.0
        dtype: float64

        On a Series:

        >>> df['a'].median()
        25.0
        >>> (df['b'] + 100).median()
        103.0

        For multi-index columns,

        >>> df.columns = pd.MultiIndex.from_tuples([('x', 'a'), ('y', 'b')])
        >>> df
              x  y
              a  b
        0  24.0  1
        1  21.0  2
        2  25.0  3
        3  33.0  4
        4  26.0  5

        On a DataFrame:

        >>> df.median()
        x  a    25.0
        y  b     3.0
        dtype: float64

        >>> df.median(axis=1)
        0    12.5
        1    11.5
        2    14.0
        3    18.5
        4    15.5
        dtype: float64

        On a Series:

        >>> df[('x', 'a')].median()
        25.0
        >>> (df[('y', 'b')] + 100).median()
        103.0
        """"""
        if not isinstance(accuracy, int):
            raise ValueError(
                ""accuracy must be an integer; however, got [%s]"" % type(accuracy).__name__
            )

        def median(spark_column, spark_type):
            if isinstance(spark_type, (BooleanType, NumericType)):
                return SF.percentile_approx(spark_column.cast(DoubleType()), 0.5, accuracy)
            else:
                raise TypeError(
                    ""Could not convert {} ({}) to numeric"".format(
                        spark_type_to_pandas_dtype(spark_type), spark_type.simpleString()
                    )
                )

        return self._reduce_for_stat_function(
            median, name=""median"", numeric_only=numeric_only, axis=axis
        )

    @property
    def size(self) -> int:
        """"""
        Return an int representing the number of elements in this object.

        Return the number of rows if Series. Otherwise return the number of
        rows times number of columns if DataFrame.

        Examples
        --------
        >>> s = ks.Series({'a': 1, 'b': 2, 'c': None})
        >>> s.size
        3

        >>> df = ks.DataFrame({'col1': [1, 2, None], 'col2': [3, 4, None]})
        >>> df.size
        6

        >>> df = ks.DataFrame(index=[1, 2, None])
        >>> df.size
        0
        """"""
        num_columns = len(self._internal.data_spark_columns)
        if num_columns == 0:
            return 0
        else:
            return len(self) * num_columns  # type: ignore

    def abs(self) -> Union[""DataFrame"", ""Series""]:
        """"""
        Return a Series/DataFrame with absolute numeric value of each element.

        Returns
        -------
        abs : Series/DataFrame containing the absolute value of each element.

        Examples
        --------

        Absolute numeric values in a Series.

        >>> s = ks.Series([-1.10, 2, -3.33, 4])
        >>> s.abs()
        0    1.10
        1    2.00
        2    3.33
        3    4.00
        dtype: float64

        Absolute numeric values in a DataFrame.

        >>> df = ks.DataFrame({
        ...     'a': [4, 5, 6, 7],
        ...     'b': [10, 20, 30, 40],
        ...     'c': [100, 50, -30, -50]
        ...   },
        ...   columns=['a', 'b', 'c'])
        >>> df.abs()
           a   b    c
        0  4  10  100
        1  5  20   50
        2  6  30   30
        3  7  40   50
        """"""

        def abs(kser):
            if isinstance(kser.spark.data_type, BooleanType):
                return kser
            elif isinstance(kser.spark.data_type, NumericType):
                return kser.spark.transform(F.abs)
            else:
                raise TypeError(
                    ""bad operand type for abs(): {} ({})"".format(
                        spark_type_to_pandas_dtype(kser.spark.data_type),
                        kser.spark.data_type.simpleString(),
                    )
                )

        return self._apply_series_op(abs)

    # TODO: by argument only support the grouping name and as_index only for now. Documentation
    # should be updated when it's supported.
    def groupby(
        self, by, axis=0, as_index: bool = True, dropna: bool = True
    ) -> Union[""DataFrameGroupBy"", ""SeriesGroupBy""]:
        """"""
        Group DataFrame or Series using a Series of columns.

        A groupby operation involves some combination of splitting the
        object, applying a function, and combining the results. This can be
        used to group large amounts of data and compute operations on these
        groups.

        Parameters
        ----------
        by : Series, label, or list of labels
            Used to determine the groups for the groupby.
            If Series is passed, the Series or dict VALUES
            will be used to determine the groups. A label or list of
            labels may be passed to group by the columns in ``self``.
        axis : int, default 0 or 'index'
            Can only be set to 0 at the moment.
        as_index : bool, default True
            For aggregated output, return object with group labels as the
            index. Only relevant for DataFrame input. as_index=False is
            effectively ""SQL-style"" grouped output.
        dropna : bool, default True
            If True, and if group keys contain NA values,
            NA values together with row/column will be dropped.
            If False, NA values will also be treated as the key in groups.

        Returns
        -------
        DataFrameGroupBy or SeriesGroupBy
            Depends on the calling object and returns groupby object that
            contains information about the groups.

        See Also
        --------
        koalas.groupby.GroupBy

        Examples
        --------
        >>> df = ks.DataFrame({'Animal': ['Falcon', 'Falcon',
        ...                               'Parrot', 'Parrot'],
        ...                    'Max Speed': [380., 370., 24., 26.]},
        ...                   columns=['Animal', 'Max Speed'])
        >>> df
           Animal  Max Speed
        0  Falcon      380.0
        1  Falcon      370.0
        2  Parrot       24.0
        3  Parrot       26.0

        >>> df.groupby(['Animal']).mean().sort_index()  # doctest: +NORMALIZE_WHITESPACE
                Max Speed
        Animal
        Falcon      375.0
        Parrot       25.0

        >>> df.groupby(['Animal'], as_index=False).mean().sort_values('Animal')
        ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
           Animal  Max Speed
        ...Falcon      375.0
        ...Parrot       25.0

        We can also choose to include NA in group keys or not by setting dropna parameter,
        the default setting is True:

        >>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]
        >>> df = ks.DataFrame(l, columns=[""a"", ""b"", ""c""])
        >>> df.groupby(by=[""b""]).sum().sort_index()  # doctest: +NORMALIZE_WHITESPACE
             a  c
        b
        1.0  2  3
        2.0  2  5

        >>> df.groupby(by=[""b""], dropna=False).sum().sort_index()  # doctest: +NORMALIZE_WHITESPACE
             a  c
        b
        1.0  2  3
        2.0  2  5
        NaN  1  4
        """"""
        from databricks.koalas.groupby import DataFrameGroupBy, SeriesGroupBy

        if isinstance(by, ks.DataFrame):
            raise ValueError(""Grouper for '{}' not 1-dimensional"".format(type(by).__name__))
        elif isinstance(by, ks.Series):
            by = [by]
        elif is_name_like_tuple(by):
            if isinstance(self, ks.Series):
                raise KeyError(by)
            by = [by]
        elif is_name_like_value(by):
            if isinstance(self, ks.Series):
                raise KeyError(by)
            by = [(by,)]
        elif is_list_like(by):
            new_by = []  # type: List[Union[Tuple, ks.Series]]
            for key in by:
                if isinstance(key, ks.DataFrame):
                    raise ValueError(
                        ""Grouper for '{}' not 1-dimensional"".format(type(key).__name__)
                    )
                elif isinstance(key, ks.Series):
                    new_by.append(key)
                elif is_name_like_tuple(key):
                    if isinstance(self, ks.Series):
                        raise KeyError(key)
                    new_by.append(key)
                elif is_name_like_value(key):
                    if isinstance(self, ks.Series):
                        raise KeyError(key)
                    new_by.append((key,))
                else:
                    raise ValueError(
                        ""Grouper for '{}' not 1-dimensional"".format(type(key).__name__)
                    )
            by = new_by
        else:
            raise ValueError(""Grouper for '{}' not 1-dimensional"".format(type(by).__name__))
        if not len(by):
            raise ValueError(""No group keys passed!"")
        axis = validate_axis(axis)
        if axis != 0:
            raise NotImplementedError('axis should be either 0 or ""index"" currently.')

        if isinstance(self, ks.DataFrame):
            return DataFrameGroupBy._build(self, by, as_index=as_index, dropna=dropna)
        elif isinstance(self, ks.Series):
            return SeriesGroupBy._build(self, by, as_index=as_index, dropna=dropna)
        else:
            raise TypeError(
                ""Constructor expects DataFrame or Series; however, "" ""got [%s]"" % (self,)
            )

    def bool(self) -> bool:
        """"""
        Return the bool of a single element in the current object.

        This must be a boolean scalar value, either True or False. Raise a ValueError if
        the object does not have exactly 1 element, or that element is not boolean

        Returns
        --------
        bool

        Examples
        --------
        >>> ks.DataFrame({'a': [True]}).bool()
        True

        >>> ks.Series([False]).bool()
        False

        If there are non-boolean or multiple values exist, it raises an exception in all
        cases as below.

        >>> ks.DataFrame({'a': ['a']}).bool()
        Traceback (most recent call last):
          ...
        ValueError: bool cannot act on a non-boolean single element DataFrame

        >>> ks.DataFrame({'a': [True], 'b': [False]}).bool()  # doctest: +NORMALIZE_WHITESPACE
        Traceback (most recent call last):
          ...
        ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(),
        a.item(), a.any() or a.all().

        >>> ks.Series([1]).bool()
        Traceback (most recent call last):
          ...
        ValueError: bool cannot act on a non-boolean single element DataFrame
        """"""
        if isinstance(self, ks.DataFrame):
            df = self
        elif isinstance(self, ks.Series):
            df = self.to_dataframe()
        else:
            raise TypeError(""bool() expects DataFrame or Series; however, "" ""got [%s]"" % (self,))
        return df.head(2)._to_internal_pandas().bool()

    def first_valid_index(self) -> Optional[Union[Scalar, Tuple[Scalar, ...]]]:
        """"""
        Retrieves the index of the first valid value.

        Returns
        -------
        scalar, tuple, or None

        Examples
        --------

        Support for DataFrame

        >>> kdf = ks.DataFrame({'a': [None, 2, 3, 2],
        ...                     'b': [None, 2.0, 3.0, 1.0],
        ...                     'c': [None, 200, 400, 200]},
        ...                     index=['Q', 'W', 'E', 'R'])
        >>> kdf
             a    b      c
        Q  NaN  NaN    NaN
        W  2.0  2.0  200.0
        E  3.0  3.0  400.0
        R  2.0  1.0  200.0

        >>> kdf.first_valid_index()
        'W'

        Support for MultiIndex columns

        >>> kdf.columns = pd.MultiIndex.from_tuples([('a', 'x'), ('b', 'y'), ('c', 'z')])
        >>> kdf
             a    b      c
             x    y      z
        Q  NaN  NaN    NaN
        W  2.0  2.0  200.0
        E  3.0  3.0  400.0
        R  2.0  1.0  200.0

        >>> kdf.first_valid_index()
        'W'

        Support for Series.

        >>> s = ks.Series([None, None, 3, 4, 5], index=[100, 200, 300, 400, 500])
        >>> s
        100    NaN
        200    NaN
        300    3.0
        400    4.0
        500    5.0
        dtype: float64

        >>> s.first_valid_index()
        300

        Support for MultiIndex

        >>> midx = pd.MultiIndex([['lama', 'cow', 'falcon'],
        ...                       ['speed', 'weight', 'length']],
        ...                      [[0, 0, 0, 1, 1, 1, 2, 2, 2],
        ...                       [0, 1, 2, 0, 1, 2, 0, 1, 2]])
        >>> s = ks.Series([None, None, None, None, 250, 1.5, 320, 1, 0.3], index=midx)
        >>> s
        lama    speed       NaN
                weight      NaN
                length      NaN
        cow     speed       NaN
                weight    250.0
                length      1.5
        falcon  speed     320.0
                weight      1.0
                length      0.3
        dtype: float64

        >>> s.first_valid_index()
        ('cow', 'weight')
        """"""
        data_spark_columns = self._internal.data_spark_columns

        if len(data_spark_columns) == 0:
            return None

        cond = reduce(lambda x, y: x & y, map(lambda x: x.isNotNull(), data_spark_columns))

        first_valid_row = (
            self._internal.spark_frame.filter(cond)
            .select(self._internal.index_spark_columns)
            .first()
        )

        # For Empty Series or DataFrame, returns None.
        if first_valid_row is None:
            return None

        if len(first_valid_row) == 1:
            return first_valid_row[0]",`numeric_only = None if axis == 1 and numeric_only is True else numeric_only`?,"The suggested code changes appear to be adding methods for calculating various statistical functions such as `cummax`, `cumsum`, `cumprod`, `mean`, `sum`, `product`, `skew`, `kurtosis`, `min`, `max`, `count`, `std`, `var`, `median`, and `abs`. These methods are designed to work similarly to their pandas counterparts but are implemented using Spark's DataFrame API.

Here's a concise review of the suggested code changes:

### Key Points:
1. **Method Addition**: The code adds several new methods to the `DataFrame` class, each corresponding to a statistical function.
2. **Functionality**: Each method calculates the specified statistic over the DataFrame or Series.
3. **Implementation**: The methods use Spark's DataFrame API to perform the calculations efficiently.
4. **Parameter Handling**: Methods handle parameters like `axis`, `numeric_only`, and `min_count` to allow flexibility in their usage.
5. **Error Handling**: Methods check for invalid data types and raise appropriate exceptions.
6. **Documentation**: Each method includes a docstring explaining its purpose, parameters, and return type.

### Example Usage:
```python
# Example usage of the new methods
df = ks.DataFrame([[2.0, 1.0], [3.0, None], [1.0, 0.0]], columns=list('AB'))

# Calculate cumulative maximum
print(df.cummax())

# Calculate cumulative sum
print(df.cumsum())

# Calculate cumulative product
print(df.cumprod())

# Calculate mean
print(df.mean())

# Calculate sum
print(df.sum())

# Calculate product
print(df.product())

# Calculate skewness
print(df.skew())

# Calculate kurtosis
print(df.kurtosis())

# Calculate minimum
print(df.min())

# Calculate maximum
print(df.max())

# Calculate count
print(df.count())

# Calculate standard deviation
print(df.std())

# Calculate variance
print(df.var())

# Calculate median
print(df.median())

# Calculate absolute values
print(df.abs())
```

### Potential Improvements:
1. **Performance Optimization**: Ensure that the methods are optimized for performance, especially for large datasets.
2. **Edge Case Handling**: Add more comprehensive handling of edge cases, such as empty DataFrames or Series.
3. **Compatibility with Pandas**: Ensure that the methods behave similarly to their pandas counterparts, especially in terms of handling missing values and data types.
4. **Testing**: Add comprehensive unit tests to verify the correctness of the"
26,"@@ -160,6 +160,11 @@ instr_encode_arch(dcontext_t *dcontext, instr_t *instr, byte *copy_pc, byte *fin
     CLIENT_ASSERT(instr_operands_valid(instr), ""instr_encode error: operands invalid"");
 
     *(uint *)copy_pc = encode_common(final_pc, instr);
+    if (*(uint *)copy_pc == ENCFAIL) {
+        IF_DEBUG(instr_disassemble(dcontext, instr, STDERR));
+        /* We were unable to encode this instruction. */
+        ASSERT_NOT_IMPLEMENTED(false); /* FIXME i#1569 */
+    }
     return copy_pc + 4;
 }
 ","/* **********************************************************
 * Copyright (c) 2016 ARM Limited. All rights reserved.
 * **********************************************************/

/*
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * * Redistributions of source code must retain the above copyright notice,
 *   this list of conditions and the following disclaimer.
 *
 * * Redistributions in binary form must reproduce the above copyright notice,
 *   this list of conditions and the following disclaimer in the documentation
 *   and/or other materials provided with the distribution.
 *
 * * Neither the name of ARM Limited nor the names of its contributors may be
 *   used to endorse or promote products derived from this software without
 *   specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL ARM LIMITED OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
 * DAMAGE.
 */

#include ""../globals.h""
#include ""arch.h""
#include ""instr.h""
#include ""decode.h""
#include ""disassemble.h""
#include ""codec.h""

/* Extra logging for encoding */
#define ENC_LEVEL 6

/* Order corresponds to DR_REG_ enum. */
const char * const reg_names[] = {
    ""<NULL>"",
    ""<invalid>"",
    ""x0"", ""x1"", ""x2"", ""x3"", ""x4"", ""x5"", ""x6"", ""x7"",
    ""x8"", ""x9"", ""x10"", ""x11"", ""x12"", ""x13"", ""x14"", ""x15"",
    ""x16"", ""x17"", ""x18"", ""x19"", ""x20"", ""x21"", ""x22"", ""x23"",
    ""x24"", ""x25"", ""x26"", ""x27"", ""x28"", ""x29"", ""x30"",
    ""sp"", ""xzr"",
    ""w0"", ""w1"", ""w2"", ""w3"", ""w4"", ""w5"", ""w6"", ""w7"",
    ""w8"", ""w9"", ""w10"", ""w11"", ""w12"", ""w13"", ""w14"", ""w15"",
    ""w16"", ""w17"", ""w18"", ""w19"", ""w20"", ""w21"", ""w22"", ""w23"",
    ""w24"", ""w25"", ""w26"", ""w27"", ""w28"", ""w29"", ""w30"",
    ""wsp"", ""wzr"",
    ""q0"", ""q1"", ""q2"", ""q3"", ""q4"", ""q5"", ""q6"", ""q7"",
    ""q8"", ""q9"", ""q10"", ""q11"", ""q12"", ""q13"", ""q14"", ""q15"",
    ""q16"", ""q17"", ""q18"", ""q19"", ""q20"", ""q21"", ""q22"", ""q23"",
    ""q24"", ""q25"", ""q26"", ""q27"", ""q28"", ""q29"", ""q30"", ""q31"",
    ""d0"", ""d1"", ""d2"", ""d3"", ""d4"", ""d5"", ""d6"", ""d7"",
    ""d8"", ""d9"", ""d10"", ""d11"", ""d12"", ""d13"", ""d14"", ""d15"",
    ""d16"", ""d17"", ""d18"", ""d19"", ""d20"", ""d21"", ""d22"", ""d23"",
    ""d24"", ""d25"", ""d26"", ""d27"", ""d28"", ""d29"", ""d30"", ""d31"",
    ""s0"", ""s1"", ""s2"", ""s3"", ""s4"", ""s5"", ""s6"", ""s7"",
    ""s8"", ""s9"", ""s10"", ""s11"", ""s12"", ""s13"", ""s14"", ""s15"",
    ""s16"", ""s17"", ""s18"", ""s19"", ""s20"", ""s21"", ""s22"", ""s23"",
    ""s24"", ""s25"", ""s26"", ""s27"", ""s28"", ""s29"", ""s30"", ""s31"",
    ""h0"", ""h1"", ""h2"", ""h3"", ""h4"", ""h5"", ""h6"", ""h7"",
    ""h8"", ""h9"", ""h10"", ""h11"", ""h12"", ""h13"", ""h14"", ""h15"",
    ""h16"", ""h17"", ""h18"", ""h19"", ""h20"", ""h21"", ""h22"", ""h23"",
    ""h24"", ""h25"", ""h26"", ""h27"", ""h28"", ""h29"", ""h30"", ""h31"",
    ""b0"", ""b1"", ""b2"", ""b3"", ""b4"", ""b5"", ""b6"", ""b7"",
    ""b8"", ""b9"", ""b10"", ""b11"", ""b12"", ""b13"", ""b14"", ""b15"",
    ""b16"", ""b17"", ""b18"", ""b19"", ""b20"", ""b21"", ""b22"", ""b23"",
    ""b24"", ""b25"", ""b26"", ""b27"", ""b28"", ""b29"", ""b30"", ""b31"",
    ""nzcv"", ""fpcr"", ""fpsr"",
    ""tpidr_el0"", ""tpidrro_el0""
};

/* Maps sub-registers to their containing register. */
/* Order corresponds to DR_REG_ enum. */
const reg_id_t dr_reg_fixer[] = {
    REG_NULL,
    REG_NULL,

#define XREGS \
    DR_REG_X0,  DR_REG_X1,   DR_REG_X2,   DR_REG_X3,  \
    DR_REG_X4,  DR_REG_X5,   DR_REG_X6,   DR_REG_X7,  \
    DR_REG_X8,  DR_REG_X9,   DR_REG_X10,  DR_REG_X11, \
    DR_REG_X12, DR_REG_X13,  DR_REG_X14,  DR_REG_X15, \
    DR_REG_X16, DR_REG_X17,  DR_REG_X18,  DR_REG_X19, \
    DR_REG_X20, DR_REG_X21,  DR_REG_X22,  DR_REG_X23, \
    DR_REG_X24, DR_REG_X25,  DR_REG_X26,  DR_REG_X27, \
    DR_REG_X28, DR_REG_X29,  DR_REG_X30, \
    DR_REG_XSP, DR_REG_XZR,
XREGS /* X0-XSP */
XREGS /* W0-WSP */
#undef XREGS

#define QREGS \
    DR_REG_Q0,  DR_REG_Q1,   DR_REG_Q2,   DR_REG_Q3,  \
    DR_REG_Q4,  DR_REG_Q5,   DR_REG_Q6,   DR_REG_Q7,  \
    DR_REG_Q8,  DR_REG_Q9,   DR_REG_Q10,  DR_REG_Q11, \
    DR_REG_Q12, DR_REG_Q13,  DR_REG_Q14,  DR_REG_Q15, \
    DR_REG_Q16, DR_REG_Q17,  DR_REG_Q18,  DR_REG_Q19, \
    DR_REG_Q20, DR_REG_Q21,  DR_REG_Q22,  DR_REG_Q23, \
    DR_REG_Q24, DR_REG_Q25,  DR_REG_Q26,  DR_REG_Q27, \
    DR_REG_Q28, DR_REG_Q29,  DR_REG_Q30,  DR_REG_Q31,
QREGS /* Q0-Q31*/
QREGS /* D0-D31 */
QREGS /* S0-S31 */
QREGS /* H0-H31 */
QREGS /* B0-B31 */
#undef QREGS

    DR_REG_NZCV, DR_REG_FPCR, DR_REG_FPSR,
    DR_REG_TPIDR_EL0, DR_REG_TPIDRRO_EL0
};

#ifdef DEBUG
void
encode_debug_checks(void)
{
    /* FIXME i#1569: NYI */
}
#endif

bool
encoding_possible(decode_info_t *di, instr_t *in, const instr_info_t * ii)
{
    ASSERT_NOT_IMPLEMENTED(false); /* FIXME i#1569 */
    return false;
}

void
decode_info_init_for_instr(decode_info_t *di, instr_t *instr)
{
    ASSERT_NOT_IMPLEMENTED(false); /* FIXME i#1569 */
}

byte *
instr_encode_arch(dcontext_t *dcontext, instr_t *instr, byte *copy_pc, byte *final_pc,
                  bool check_reachable, bool *has_instr_opnds/*OUT OPTIONAL*/
                  _IF_DEBUG(bool assert_reachable))
{
    if (has_instr_opnds != NULL)
        *has_instr_opnds = false;

    if (instr_is_label(instr))
        return copy_pc;

    /* First, handle the already-encoded instructions */
    if (instr_raw_bits_valid(instr)) {
        CLIENT_ASSERT(check_reachable, ""internal encode error: cannot encode raw ""
                      ""bits and ignore reachability"");
        /* Copy raw bits, possibly re-relativizing */
        return copy_and_re_relativize_raw_instr(dcontext, instr, copy_pc, final_pc);
    }
    if (*(uint *)copy_pc == ENCFAIL) {
        IF_DEBUG(instr_disassemble(dcontext, instr, STDERR));
        /* We were unable to encode this instruction. */
        ASSERT_NOT_IMPLEMENTED(false); /* FIXME i#1569 */
    }
    CLIENT_ASSERT(instr_operands_valid(instr), ""instr_encode error: operands invalid"");

    *(uint *)copy_pc = encode_common(final_pc, instr);
    return copy_pc + 4;
}

byte *
copy_and_re_relativize_raw_instr(dcontext_t *dcontext, instr_t *instr,
                                 byte *dst_pc, byte *final_pc)
{
    /* FIXME i#1569: re-relativizing is NYI */
    /* OP_ldstex is always relocatable. */
    ASSERT(instr_raw_bits_valid(instr) || instr_get_opcode(instr) == OP_ldstex);
    memcpy(dst_pc, instr->bytes, instr->length);
    return dst_pc + instr->length;
}
",Everything printed should go through the checks for stderr_mask. How about instr_disassemble_to_buffer and then print via SYSLOG_INTERNAL_ERROR which has the proper checks and also tees the print to the log file.,"The suggested code change appears to be related to the encoding process for ARM instructions in a dynamic analysis tool. The changes include:

1. **Debug Checks**: A function `encode_debug_checks` is defined but marked as not implemented (`ASSERT_NOT_IMPLEMENTED(false)`). This suggests that this function is intended to perform debugging checks but has not been fully implemented.

2. **Encoding Possibility Check**: The `encoding_possible` function is also marked as not implemented (`ASSERT_NOT_IMPLEMENTED(false)`). This function is supposed to determine if encoding is possible for a given instruction, but it has not been implemented.

3. **Decode Information Initialization**: The `decode_info_init_for_instr` function is marked as not implemented (`ASSERT_NOT_IMPLEMENTED(false)`). This function is intended to initialize decode information for a given instruction, but it has not been implemented.

4. **Instruction Encoding**: The `instr_encode_arch` function is the main encoding function. It handles the encoding of instructions based on their raw bits or existing encoded bits. If the instruction is a label, it returns the current PC. If the instruction's raw bits are valid, it copies them to the destination PC, possibly re-relativizing them. If the instruction's raw bits are not valid and the instruction is not a label, it encodes the instruction using the `encode_common` function and returns the updated PC.

5. **Copying and Re-Relativizing Raw Instructions**: The `copy_and_re_relativize_raw_instr` function is responsible for copying raw instruction bytes to the destination PC. It assumes that the instruction is either relocatable or that re-relativization is not necessary. However, this function is marked as not implemented (`ASSERT_NOT_IMPLEMENTED(false)`).

Overall, the code seems to be in a state where several functions are not fully implemented, which could lead to errors or unexpected behavior during execution. It would be beneficial to complete these functions or remove them if they are not needed."
27,"@@ -33,6 +33,18 @@ public class TransactionPoolOptions
   private static final String ETH65_TX_ANNOUNCED_BUFFERING_PERIOD_FLAG =
       ""--Xeth65-tx-announced-buffering-period-milliseconds"";
 
+  private static final String STRICT_TX_REPLAY_PROTECTION_ENABLED_FLAG =
+      ""--strict-tx-replay-protection-enabled"";
+
+  @CommandLine.Option(
+      names = {STRICT_TX_REPLAY_PROTECTION_ENABLED_FLAG},
+      paramLabel = ""<Boolean>"",
+      description =
+          ""Require transactions submitted via JSON-RPC to use replay protection in accordance with EIP-155 (default: ${DEFAULT-VALUE})"",
+      fallbackValue = ""true"",
+      arity = ""0..1"")
+  private Boolean strictTxReplayProtectionEnabled = false;
+
   @CommandLine.Option(
       names = {TX_MESSAGE_KEEP_ALIVE_SEC_FLAG},
       paramLabel = ""<INTEGER>"",","/*
 * Copyright ConsenSys AG.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
 * the License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
 * an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
 * specific language governing permissions and limitations under the License.
 *
 * SPDX-License-Identifier: Apache-2.0
 */
package org.hyperledger.besu.cli.options.unstable;

import org.hyperledger.besu.cli.options.CLIOptions;
import org.hyperledger.besu.cli.options.OptionParser;
import org.hyperledger.besu.ethereum.eth.transactions.ImmutableTransactionPoolConfiguration;
import org.hyperledger.besu.ethereum.eth.transactions.TransactionPoolConfiguration;

import java.time.Duration;
import java.util.Arrays;
import java.util.List;

import picocli.CommandLine;

public class TransactionPoolOptions
    implements CLIOptions<ImmutableTransactionPoolConfiguration.Builder> {
  private static final String TX_MESSAGE_KEEP_ALIVE_SEC_FLAG =
      ""--Xincoming-tx-messages-keep-alive-seconds"";

  private static final String STRICT_TX_REPLAY_PROTECTION_ENABLED_FLAG =
      ""--strict-tx-replay-protection-enabled"";

  @CommandLine.Option(
      names = {STRICT_TX_REPLAY_PROTECTION_ENABLED_FLAG},
      paramLabel = ""<Boolean>"",
      description =
          ""Require transactions submitted via JSON-RPC to use replay protection in accordance with EIP-155 (default: ${DEFAULT-VALUE})"",
      fallbackValue = ""true"",
      arity = ""0..1"")
  private Boolean strictTxReplayProtectionEnabled = false;

  private static final String ETH65_TX_ANNOUNCED_BUFFERING_PERIOD_FLAG =
      ""--Xeth65-tx-announced-buffering-period-milliseconds"";

  @CommandLine.Option(
      names = {TX_MESSAGE_KEEP_ALIVE_SEC_FLAG},
      paramLabel = ""<INTEGER>"",
      hidden = true,
      description =
          ""Keep alive of incoming transaction messages in seconds (default: ${DEFAULT-VALUE})"",
      arity = ""1"")
  private Integer txMessageKeepAliveSeconds =
      TransactionPoolConfiguration.DEFAULT_TX_MSG_KEEP_ALIVE;

  @CommandLine.Option(
      names = {ETH65_TX_ANNOUNCED_BUFFERING_PERIOD_FLAG},
      paramLabel = ""<LONG>"",
      hidden = true,
      description =
          ""The period for which the announced transactions remain in the buffer before being requested from the peers in milliseconds (default: ${DEFAULT-VALUE})"",
      arity = ""1"")
  private long eth65TrxAnnouncedBufferingPeriod =
      TransactionPoolConfiguration.ETH65_TRX_ANNOUNCED_BUFFERING_PERIOD.toMillis();

  private TransactionPoolOptions() {}

  public static TransactionPoolOptions create() {
    return new TransactionPoolOptions();
  }

  public static TransactionPoolOptions fromConfig(final TransactionPoolConfiguration config) {
    final TransactionPoolOptions options = TransactionPoolOptions.create();
    options.txMessageKeepAliveSeconds = config.getTxMessageKeepAliveSeconds();
    options.eth65TrxAnnouncedBufferingPeriod =
        config.getEth65TrxAnnouncedBufferingPeriod().toMillis();
    return options;
  }

  @Override
  public ImmutableTransactionPoolConfiguration.Builder toDomainObject() {
    return ImmutableTransactionPoolConfiguration.builder()
        .txMessageKeepAliveSeconds(txMessageKeepAliveSeconds)
        .eth65TrxAnnouncedBufferingPeriod(Duration.ofMillis(eth65TrxAnnouncedBufferingPeriod));
  }

  @Override
  public List<String> getCLIOptions() {
    return Arrays.asList(
        TX_MESSAGE_KEEP_ALIVE_SEC_FLAG,
        OptionParser.format(txMessageKeepAliveSeconds),
        ETH65_TX_ANNOUNCED_BUFFERING_PERIOD_FLAG,
        OptionParser.format(eth65TrxAnnouncedBufferingPeriod));
  }
}
","Ah, so it IS unstable already. Cool, lets prefix with X.","The suggested code change adds two new command-line options for configuring the transaction pool in Hyperledger Besu:

1. `--Xincoming-tx-messages-keep-alive-seconds`: This option allows setting the keep-alive time for incoming transaction messages in seconds. The default value is set to `TransactionPoolConfiguration.DEFAULT_TX_MSG_KEEP_ALIVE`.

2. `--Xeth65-tx-announced-buffering-period-milliseconds`: This option sets the period for which announced transactions remain in the buffer before being requested from peers in milliseconds. The default value is set to `TransactionPoolConfiguration.ETH65_TRX_ANNOUNCED_BUFFERING_PERIOD.toMillis()`.

The code also includes a constructor, a static method to create an instance, and a method to convert the options to a domain object. The `getCLIOptions` method returns a list of command-line options that can be used to configure the transaction pool.

Overall, the code provides a flexible way to configure the transaction pool in Hyperledger Besu using command-line options."
28,"@@ -5,7 +5,7 @@ package ddevapp
 const DDevComposeTemplate = `version: '2'
 
 services:
-  {{ .plugin }}-{{.name }}-db:
+  db:
     container_name: {{ .plugin }}-${DDEV_SITENAME}-db
     image: $DDEV_DBIMAGE
     volumes:","package ddevapp

// DDevComposeTemplate is used to create the docker-compose.yaml for
// legacy sites in the ddev env
  db:
const DDevComposeTemplate = `version: '2'

services:
  {{ .plugin }}-{{.name }}-db:
    container_name: {{ .plugin }}-${DDEV_SITENAME}-db
    image: $DDEV_DBIMAGE
    volumes:
      - ""./data:/db""
    restart: always
    environment:
      - TCP_PORT=$DDEV_HOSTNAME:{{ .dbport }}
    ports:
      - 3306
    labels:
      com.ddev.site-name: ${DDEV_SITENAME}
      com.ddev.container-type: db
      com.ddev.platform: {{ .plugin }}
      com.ddev.app-type: {{ .appType }}
      com.ddev.docroot: $DDEV_DOCROOT
      com.ddev.approot: $DDEV_APPROOT
      com.ddev.app-url: $DDEV_URL
  {{ .plugin }}-{{ .name }}-web:
    container_name: {{ .plugin }}-${DDEV_SITENAME}-web
    image: $DDEV_WEBIMAGE
    volumes:
      - ""{{ .docroot }}/:/var/www/html/docroot""
    restart: always
    depends_on:
      - {{ .plugin }}-${DDEV_SITENAME}-db
    links:
      - {{ .plugin }}-${DDEV_SITENAME}-db:$DDEV_HOSTNAME
      - {{ .plugin }}-${DDEV_SITENAME}-db:db
    ports:
      - ""80""
      - {{ .mailhogport }}
    working_dir: ""/var/www/html/docroot""
    environment:
      - DEPLOY_NAME=local
      - VIRTUAL_HOST=$DDEV_HOSTNAME
      - VIRTUAL_PORT=80,{{ .mailhogport }}
    labels:
      com.ddev.site-name: ${DDEV_SITENAME}
      com.ddev.container-type: web
      com.ddev.platform: {{ .plugin }}
      com.ddev.app-type: {{ .appType }}
      com.ddev.docroot: $DDEV_DOCROOT
      com.ddev.approot: $DDEV_APPROOT
      com.ddev.app-url: $DDEV_URL
  {{ .plugin }}-{{ .name }}-dba:
    container_name: local-${DDEV_SITENAME}-dba
    image: $DDEV_DBAIMAGE
    restart: always
    labels:
      com.ddev.site-name: ${DDEV_SITENAME}
      com.ddev.container-type: dba
      com.ddev.platform: {{ .plugin }}
      com.ddev.app-type: {{ .appType }}
      com.ddev.docroot: $DDEV_DOCROOT
      com.ddev.approot: $DDEV_APPROOT
      com.ddev.app-url: $DDEV_URL
    depends_on:
      - local-${DDEV_SITENAME}-db
    links:
      - local-${DDEV_SITENAME}-db:db
    ports:
      - ""80""
    environment:
      - PMA_USER=root
      - PMA_PASSWORD=root
      - VIRTUAL_HOST=$DDEV_HOSTNAME
      - VIRTUAL_PORT={{ .dbaport }}
networks:
  default:
    external:
      name: ddev_default
`
",Should we be using version 3 now?,"The suggested code change appears to be a template for creating a `docker-compose.yaml` file for legacy sites in a DDev environment. The template defines three services: a database service, a web service, and a database administration service. Each service has its own container name, image, volumes, restart policy, environment variables, ports, and labels. The template also includes a network definition for the default network. The code is well-structured and follows best practices for Docker Compose configuration."
29,"@@ -21,8 +21,11 @@ def main(argv):
         .filter(Instance.keywords_tsv.is_(None))
     )
 
-    for instance, owner, name in rows:
+    for idx, (instance, owner, name) in enumerate(rows):
+        print(""%s/%s/%s"" % (owner, name, instance.hash))
         instance.keywords_tsv = keywords_tsvector(owner, name, instance.contents)
+        if (idx + 1) % 100 == 0:
+            db.session.commit()
 
     db.session.commit()
     print(""Done!"")","#!/usr/bin/env python3

""""""
Backfills s3_blob.preview by downloading the contents from S3.
""""""

import sys

import sqlalchemy as sa
from sqlalchemy.orm import undefer

from quilt_server import db
from quilt_server.models import Instance, Package, S3Blob
from quilt_server.search import keywords_tsvector

def main(argv):
    rows = (
        db.session.query(Instance, Package.owner, Package.name)
        .join(Instance.package)
        .options(undefer('contents'))
    for idx, (instance, owner, name) in enumerate(rows):
        print(""%s/%s/%s"" % (owner, name, instance.hash))
        if (idx + 1) % 100 == 0:
            db.session.commit()
        .filter(Instance.keywords_tsv.is_(None))
    )

    for instance, owner, name in rows:
        instance.keywords_tsv = keywords_tsvector(owner, name, instance.contents)

    db.session.commit()
    print(""Done!"")

    return 0

if __name__ == '__main__':
    sys.exit(main(sys.argv))
",extremely minor: if you use `%s/%s:%s` it will be consistent with install syntax. i guess we won't need to install from the logs.,"The provided Python script is designed to backfill the `keywords_tsv` field in the `Instance` model by downloading the contents from S3 for instances where the `keywords_tsv` is currently `None`. Here's a concise review of the suggested code changes:

### Key Points:
1. **Imports and Setup**:
   - The script imports necessary modules and sets up the database session.
   - It defines the `main` function to handle the execution logic.

2. **Query Execution**:
   - The script queries the database to retrieve instances where the `keywords_tsv` is `None`.
   - It uses `undefer('contents')` to ensure that the `contents` field is loaded into memory.

3. **Processing Instances**:
   - For each instance, it prints the owner, package name, and hash.
   - It commits the session every 100 instances to avoid overwhelming the database with too many transactions.

4. **Updating `keywords_tsv`**:
   - After processing all instances, it updates the `keywords_tsv` field for each instance using the `keywords_tsvector` function.
   - It commits the session again to save the changes.

5. **Completion Message**:
   - Finally, it prints ""Done!"" and exits the script.

### Potential Improvements:
- **Error Handling**: Consider adding error handling to manage exceptions during database operations or file downloads.
- **Logging**: Implement logging to track the progress and any errors encountered during execution.
- **Concurrency**: If dealing with a large number of instances, consider using asynchronous processing to improve performance.
- **Batch Processing**: Instead of committing after every 100 instances, consider processing batches of instances to reduce overhead.

Overall, the script is well-structured and performs the intended task effectively."
30,"@@ -1690,10 +1690,12 @@
                     }
                     if(shouldDelete)
                         delete oSVersionData.chartData[i];
+                        delete platformVersionTotal[i];
                 }
             }
 
             oSVersionData.chartData = _.compact(oSVersionData.chartData);
+            platformVersionTotal = _.compact(platformVersionTotal);
 
             var platformVersionNames = _.pluck(oSVersionData.chartData, metric || _name),
                 platformNames = [];","            if ($(this).find("".select-items"").is("":visible"")) {
                $(this).find("".select-items"").hide();
            } else {
                $(this).find("".select-items"").show();
                if ($(this).find("".select-items"").find("".scroll-list"").length == 0) {
                    $(this).find("".select-items"").wrapInner(""<div class='scroll-list'></div>"");
                    $(this).find("".scroll-list"").slimScroll({
                        height:'100%',
                        start:'top',
                        wheelStep:10,
                        position:'right',
                        disableFadeOut:true
                    });
                }
            }

            $(this).find("".select-items"").find("".item"").removeClass(""hidden"");
            $(this).find("".select-items"").find("".group"").show();
            $(this).find("".select-items"").find("".item"").removeClass(""last"");
            $(this).find("".select-items"").find("".item:visible:last"").addClass(""last"");

            $(this).find("".search input"").focus();

            $(""#date-picker"").hide();

            $(this).find("".search"").off(""click"").on(""click"", function (e) {
                e.stopPropagation();
            });

            e.stopPropagation();
        });

        element.off(""click"", "".cly-multi-select .select-items .item"").on(""click"", "".cly-multi-select .select-items .item"", function (e) {
            if ($(this).hasClass(""disabled"")) {
                e.stopPropagation();
                return;
            }

            var $multiSelect = $(this).parents("".cly-multi-select""),
                selectionContainer = $multiSelect.find("".text""),
                selectedValue = $(this).data(""value""),
                maxToSelect = $multiSelect.data(""max"");

            if ($(this).hasClass(""selected"")) {
                selectionContainer.find("".selection[data-value='"" + selectedValue + ""']"").remove();
                $(this).removeClass(""selected"");
            } else {
                var $selection = $(""<div class='selection'></div>"");

                $selection.text($(this).text());
                $selection.attr(""data-value"", selectedValue);
                $selection.append(""<div class='remove'><i class='ion-android-close'></i></div>"");

                selectionContainer.append($selection);

                $(this).addClass(""selected"");
            }

            if (maxToSelect) {
                if (getSelected($multiSelect).length >= maxToSelect) {
                    $multiSelect.find("".item"").addClass(""disabled"");
                }
            }

            if ($multiSelect.find("".item.selected"").length > 0) {
                $multiSelect.addClass(""selection-exists"");
            } else {
                $multiSelect.removeClass(""selection-exists"");
            }

            $multiSelect.data(""value"", getSelected($multiSelect));
            $multiSelect.trigger(""cly-multi-select-change"", [getSelected($multiSelect)]);
            e.stopPropagation();
        });

        element.off(""keyup"", "".cly-multi-select .search input"").on(""keyup"", "".cly-multi-select .search input"", function(event) {
            var $multiSelect = $(this).parents("".cly-multi-select"");

            if (!$(this).val()) {
                $multiSelect.find("".item"").removeClass(""hidden"");
                $multiSelect.find("".group"").show();
            } else {
                $multiSelect.find("".item:not(:contains('"" + $(this).val() + ""'))"").addClass(""hidden"");
                $multiSelect.find("".item:contains('"" + $(this).val() + ""')"").removeClass(""hidden"");
                var prevHeader = $multiSelect.find("".group"").first();
                prevHeader.siblings().each(function(){
                    if($(this).hasClass(""group"")){
                        if(prevHeader)
                            prevHeader.hide();
                        prevHeader = $(this);
                    }
                    else if($(this).hasClass(""item"") && $(this).is("":visible"")){
                        prevHeader = null;
                    }

                    if(!$(this).next().length && prevHeader)
                        prevHeader.hide();
                })
            }
        });

        element.off('mouseenter').on('mouseenter', "".cly-multi-select .item"", function () {
            var item = $(this);

            if (this.offsetWidth < this.scrollWidth && !item.attr('title')) {
                item.attr('title', item.text());
            }
        });

        element.off(""click"", "".cly-multi-select .selection"").on(""click"", "".cly-multi-select .selection"", function (e) {
            e.stopPropagation();
        });

        element.off(""click"", "".cly-multi-select .selection .remove"").on(""click"", "".cly-multi-select .selection .remove"", function (e) {
            var $multiSelect = $(this).parents("".cly-multi-select"");

            $multiSelect.find("".item[data-value='"" + $(this).parent("".selection"").data(""value"") + ""']"").removeClass(""selected"");

            if ($multiSelect.find("".item.selected"").length > 0) {
                $multiSelect.addClass(""selection-exists"");
            } else {
                $multiSelect.removeClass(""selection-exists"");
            }

            $(this).parent("".selection"").remove();

            var maxToSelect = $multiSelect.data(""max"");

            if (maxToSelect) {
                if (getSelected($multiSelect).length < maxToSelect) {
                    $multiSelect.find("".item"").removeClass(""disabled"");
                }
            }

            $multiSelect.data(""value"", getSelected($multiSelect));
            $multiSelect.trigger(""cly-multi-select-change"", [getSelected($multiSelect)])

            e.stopPropagation();
        });

        $(window).click(function () {
            var $clyMultiSelect = $("".cly-multi-select"");

            $clyMultiSelect.find("".select-items"").hide();
            $clyMultiSelect.find("".search"").remove();
            $clyMultiSelect.removeClass(""active"");
        });

        function getSelected(multiSelectEl) {
            var selected = [];

            multiSelectEl.find("".text .selection"").each(function() {
                selected.push($(this).data(""value""));
            });

            return selected;
        }

        $.fn.clyMultiSelectSetItems = function(items) {
            var $selectItems = $(this).find("".select-items"");

            if ($selectItems) {
                $selectItems.html("""");

                for (var i = 0; i < items.length; i++) {
                    $selectItems.append('<div data-value=""' + items[i].value + '"" class=""item"">' + items[i].name + '</div>');
                }
            }
        };

        $.fn.clyMultiSelectGetSelection = function() {
            return getSelected($(this));
        };

        $.fn.clyMultiSelectSetSelection = function(valNameArr) {
            var $multiSelect = $(this),
                $selectionContainer = $multiSelect.find("".text"");

            $(this).find("".selection"").remove();

            for (var i = 0; i < valNameArr.length; i++) {
                var name = valNameArr[i].name,
                    value = valNameArr[i].value;

                var $selection = $(""<div class='selection'></div>"");

                $selection.text(name);
                $selection.attr(""data-value"", value);
                $selection.append(""<div class='remove'><i class='ion-android-close'></i></div>"");

                $selectionContainer.append($selection);
            }

            $(this).addClass(""selection-exists"");
            $(this).data(""value"", getSelected($(this)));
            $(this).trigger(""cly-multi-select-change"", [getSelected($(this))]);
        };

        $.fn.clyMultiSelectClearSelection = function() {
            $(this).find("".selection"").remove();
            $(this).data(""value"", getSelected($(this)));
            $(this).removeClass(""selection-exists"");
            $(this).trigger(""cly-multi-select-change"", [getSelected($(this))]);
        };
    };
    
    /**
    * Initialize dropdown options list usually used on datatables. Firstly you need to add list with class 'cly-button-menu' to your template or add it in the view. Additionally you can add class `dark` to use dark theme.
    * After that datatables last column for options should return a element with `cly-list-options` class and should have cell classes shrink and right and should not be sortable
    * Then call this method in your view and you can start listening to events like: 
    * cly-list.click - when your cly-list-options element is clicked, passing click event as data
    * cly-list.open - when list is opened, passing click event as data
    * cly-list.close - when list is closed, passing click event as data
    * cly-list.item - when item is clicked, passing click event as data
    * @param {object} element - jQuery object reference for container
    * @example <caption>Adding list to HTML template</caption>
    * <div class=""cly-button-menu dark cohorts-menu"" tabindex=""1"">
    *     <a class=""item delete-cohort"" data-localize='common.delete'></a>
    *     <a class=""item view-cohort"" data-localize='cohorts.view-users'></a>
    * </div>
    * @example <caption>Creating last column in datatables</caption>
    * { ""mData"": function(row, type){
    *     return '<a class=""cly-list-options""></a>';
    * }, ""sType"":""string"", ""sTitle"": """", ""sClass"":""shrink center"", bSortable: false  }
    * @example <caption>Listening to events</caption>
    * $("".cly-button-menu"").on(""cly-list.click"", function(event, data){
    *     var id = $(data.target).parents(""tr"").data(""id"");
    * });
    */
    CountlyHelpers.initializeTableOptions = function (element) {
        element = element || $('body');
        element.find(""tbody"").off(""click"", "".cly-list-options"").on(""click"", "".cly-list-options"", function (event){
            event.stopPropagation();
            event.preventDefault();
            $("".cly-button-menu"").trigger('cly-list.click', event);
            $(event.target).toggleClass(""active"");
            if($(event.target).hasClass(""active"")){
                element.find("".cly-list-options"").removeClass(""active"");
                $(event.target).addClass(""active"");
                var pos = $(event.target).offset();
                element.find('.cly-button-menu').css({
                    top: (pos.top+25) + ""px"",
                    right: 22 + ""px""
                });
                element.find('.cly-button-menu').addClass(""active"");
                element.find('.cly-button-menu').focus();
                $("".cly-button-menu"").trigger('cly-list.open', event);
            }
            else{
                $(event.target).removeClass(""active"");
                element.find('.cly-button-menu').removeClass(""active"");
                $("".cly-button-menu"").trigger('cly-list.close', event);
            }
            return false;
        });
        
        element.find('.cly-button-menu .item').off(""click"").on(""click"", function(event){
            $("".cly-button-menu"").trigger('cly-list.item', event);
            element.find('.cly-button-menu').removeClass(""active"");
            element.find("".cly-list-options"").removeClass(""active"");
            $("".cly-button-menu"").trigger('cly-list.close', event);
        });
        
        element.find('.cly-button-menu').off(""blur"").on(""blur"", function() {
            element.find('.cly-button-menu').removeClass(""active"");
            element.find("".cly-list-options"").removeClass(""active"");
            $("".cly-button-menu"").trigger('cly-list.close', event);
        });
    };

    /**
    * Refresh existing datatable instance on view refresh, providing new data
    * @param {object} dTable - jQuery object datatable reference
    * @param {object} newDataArr - array with new data in same format as provided while initializing table
    * @example
    * CountlyHelpers.refreshTable(self.dtable, data);
    */
    CountlyHelpers.refreshTable = function(dTable, newDataArr) {
        var oSettings = dTable.fnSettings();
        dTable.fnClearTable(false);

        if(newDataArr && newDataArr.length)
            for (var i=0; i < newDataArr.length; i++) {
                dTable.oApi._fnAddData(oSettings, newDataArr[i]);
            }

        oSettings.aiDisplay = oSettings.aiDisplayMaster.slice();
        dTable.fnStandingRedraw();
        dTable.trigger(""table.refresh"");
    };

    /**
    * In some cases you may want to allow expanding rows of your datatable. To do that you must add unique id to each row via datatables fnRowCallback property
    * @param {object} dTable - jQuery object datatable reference
    * @param {function} getData - callback function to be called when clicking ont he row. This function will receive original row data object you passed to data tables and should return HTML string to display in subcell
    * @param {object} context - this context if needed, which will be passed to getData function as second parameter
    * @example
    * function formatData(data){
    *    // `data` is the original data object for the row
    *    //return string to display in subcell
    *    var str = '';
	*	if(data){
	*		str += '<div class=""datatablesubrow"">'+
    *        JSON.stringify(data)+
    *        '</div>';
    *    }
    *    return str;
    * }
    * this.dtable = $('.d-table').dataTable($.extend({}, $.fn.dataTable.defaults, {
    *      ""aaData"": crashData.data,
	*		""fnRowCallback"": function( nRow, aData, iDisplayIndex, iDisplayIndexFull ) {
	*			$(nRow).attr(""id"", aData._id);
	*		},
    *      ""aoColumns"": [
	*			{ ""mData"": ""ts""}, ""sType"":""format-ago"", ""sTitle"": jQuery.i18n.map[""crashes.crashed""]},
	*			{ ""mData"": ""os"", ""sType"":""string"", ""sTitle"": jQuery.i18n.map[""crashes.os_version""] },
	*			{ ""mData"": ""device""}, ""sType"":""string"", ""sTitle"": jQuery.i18n.map[""crashes.device""]},
	*			{ ""mData"": ""app_version"", ""sType"":""string"", ""sTitle"": jQuery.i18n.map[""crashes.app_version""] }
    *      ]
    *  }));
    *  CountlyHelpers.expandRows(this.dtable, formatData);
    */
    CountlyHelpers.expandRows = function(dTable, getData, context){
        dTable.aOpen = [];
        dTable.on(""click"", ""tr"", function (e){
            var nTr = this;
            var id = $(nTr).attr(""id"");
            if(id){
                var i = $.inArray( id, dTable.aOpen );

                if ( i === -1 ) {
                    $(nTr).addClass(""selected"");
                    var nDetailsRow = dTable.fnOpen( nTr, getData(dTable.fnGetData( nTr ), context), 'details' );
                    $('div.datatablesubrow', nDetailsRow).show();
                    dTable.aOpen.push( id );
                    dTable.trigger(""row.open"", id);
                }
                else {
                    $(nTr).removeClass(""selected"");
                    $('div.datatablesubrow', $(nTr).next()[0]).hide();
                    dTable.fnClose( nTr );
                    dTable.aOpen.splice( i, 1 );
                    dTable.trigger(""row.close"", id);
                }
            }
        });
    };

    /**
    * If you allow to open/expand rows, then when refreshing table they will close again. To avoid that you must call this function on each refresh after calling {@link CountlyHelpers.refreshTable}
    * @param {object} dTable - jQuery object datatable reference
    * @param {function} getData - callback function to be called when clicking ont he row. This function will receive original row data object you passed to data tables and should return HTML string to display in subcell
    * @param {object} context - this context if needed, which will be passed to getData function as second parameter
    * @example
    * CountlyHelpers.refreshTable(self.dtable, data);
    * CountlyHelpers.reopenRows(self.dtable, formatData);
    */
    CountlyHelpers.reopenRows = function(dTable, getData, context){
        var nTr;
        var oSettings = dTable.fnSettings();
        if(dTable.aOpen){
            $.each( dTable.aOpen, function ( i, id ) {
                var nTr = $(""#""+id)[0];
                $(nTr).addClass(""selected"");
                var nDetailsRow = dTable.fnOpen( nTr, getData(dTable.fnGetData( nTr ), context), 'details' );
                $('div.datatablesubrow', nDetailsRow).show();
                dTable.trigger(""row.reopen"", id);
            });
        }
    };

    /**
    * Close all opened datatables rows
    * @param {object} dTable - jQuery object datatable reference
    * @example
    * CountlyHelpers.closeRows(self.dtable);
    */
    CountlyHelpers.closeRows = function(dTable){
        if(dTable.aOpen){
            $.each( dTable.aOpen, function ( i, id ) {
                var nTr = $(""#""+id)[0];
                $(nTr).removeClass(""selected"");
                $('div.datatablesubrow', $(nTr).next()[0]).slideUp( function () {
                    dTable.fnClose( nTr );
                    dTable.aOpen.splice( i, 1 );
                } );
                dTable.trigger(""row.close"", id);
            });
        }
    };

    /**
    * Convert array of app ids to comma separate string of app names
    * @param {array} context - array with app ids
    * @example
    * //outputs Test1, Test2, Test3
    * CountlyHelpers.appIdsToNames([""586e3216326a8b0a07b8d87f"", ""586e339a326a8b0a07b8ecb9"", ""586e3343c32cb30a01558cc3""]);
    */
    CountlyHelpers.appIdsToNames = function(context){
        var ret = """";

        for (var i = 0; i < context.length; i++) {
            if (!context[i]) {
                continue;
            } else if (!countlyGlobal['apps'][context[i]]) {
                ret += 'deleted app';
            } else {
                ret += countlyGlobal['apps'][context[i]][""name""];
            }

            if (context.length > 1 && i != context.length - 1) {
                ret += "", "";
            }
        }

        return ret;
    };

    /**
    * Load JS file
    * @param {string} js - path or url to js file
    * @param {callback=} calback - callback when file loaded
    * @example
    * CountlyHelpers.loadJS(""/myplugin/javascripts/custom.js"");
    */
    CountlyHelpers.loadJS = function(js, callback){
        var fileref=document.createElement('script'),
            loaded;
        fileref.setAttribute(""type"",""text/javascript"");
        fileref.setAttribute(""src"", js);
        if (callback) {
            fileref.onreadystatechange = fileref.onload = function() {
                if (!loaded) {
                    callback();
                }
                loaded = true;
            };
        }
        document.getElementsByTagName(""head"")[0].appendChild(fileref);
    };

    /**
    * Load CSS file
    * @param {string} css - path or url to css file
    * @param {callback=} calback - callback when file loaded
    * @example
    * CountlyHelpers.loadCSS(""/myplugin/stylesheets/custom.css"");
    */
    CountlyHelpers.loadCSS = function(css, callback){
        var fileref=document.createElement(""link""),
            loaded;
        fileref.setAttribute(""rel"", ""stylesheet"");
        fileref.setAttribute(""type"", ""text/css"");
        fileref.setAttribute(""href"", css);
        if (callback) {
            fileref.onreadystatechange = fileref.onload = function() {
                if (!loaded) {
                    callback();
                }
                loaded = true;
            };
        }
        document.getElementsByTagName(""head"")[0].appendChild(fileref)
    };

    CountlyHelpers.messageText = function(messagePerLocale) {
        if (!messagePerLocale) {
            return '';
        } else if (messagePerLocale['default']) {
            return messagePerLocale['default'];
        } else if (messagePerLocale.en) {
            return messagePerLocale.en;
        } else {
            for (var locale in messagePerLocale) return messagePerLocale[locale];
        }
        return '';
    };

    /**
    * Returns function to be used as mRender for datatables to clip long values
    * @param {function=} f - optional function to change passed data to render and return changed object
    * @param {string=} nothing - text to display in cell
    */
    CountlyHelpers.clip = function(f, nothing) {
        return function(opt) {
            var res = typeof f === 'fucnction' ? f(opt) : opt;
            return '<div class=""clip' + (res ? '' : ' nothing') + '"">' + (res || nothing) + '</div>';
        }
    };

    /**
    * Create Countly metric model to fetch metric data from server and provide it to views
    * @param {object} countlyMetric - initial metric object if you want to pre provide some methods, etc
    * @param {string} metric - metric name to retrieve from server
    * @param {jquery} $ - local jquery reference
    * @param {function=} fetchValue - default function to fetch and transform if needed value from standard metric model
    * @example
    *   window.countlyDensity = {};
    *   countlyDensity.checkOS = function(os, density){
    *        var lastIndex = density.toUpperCase().lastIndexOf(""DPI"");
    *        if(os.toLowerCase() == ""android"" && lastIndex !== -1 && lastIndex === density.length - 3)
    *            return true;
    *        if(os.toLowerCase() == ""ios"" && density[0] == ""@"")
    *            return true;
    *        return false;
    *   };
    *   CountlyHelpers.createMetricModel(window.countlyDensity, {name: ""density"", estOverrideMetric: ""densities""}, jQuery, function(val, data, separate){
    *        if(separate){
    *            //request separated/unprocessed data
    *            return val;
    *        }
    *        else{
    *            //we can preprocess data and group, for example, by first letter
    *            return val[0];
    *        }
    *   });
    */
    CountlyHelpers.createMetricModel = function (countlyMetric, metric, $, fetchValue) {
        countlyMetric = countlyMetric || {};
        countlyMetric.fetchValue = fetchValue;
        //Private Properties
        var _periodObj = {},
            _Db = {},
            _metrics = {},
            _activeAppKey = 0,
            _initialized = false,
            _processed = false,
            _period = null,
            _name = (metric.name)? metric.name : metric,
            _estOverrideMetric = (metric.estOverrideMetric)? metric.estOverrideMetric : """";

        /**
        * Common metric object, all metric models inherit from it and should have these methods
        * @name countlyMetric
        * @global
        * @namespace countlyMetric
        */

        //Public Methods
        /**
        * Initialize metric model to fetch initial data from server
        * @param {boolean=} processed - if true will fetch processed data, will fetch raw data by default
        * @returns {jquery_promise} jquery promise to wait while data is loaded
        * @example
        * beforeRender: function() {
        *    return $.when(countlyMetric.initialize()).then(function () {});
        * }
        */
        countlyMetric.initialize = function (processed) {
            if (_initialized &&  _period == countlyCommon.getPeriodForAjax() && _activeAppKey == countlyCommon.ACTIVE_APP_KEY) {
                return this.refresh();
            }

            _period = countlyCommon.getPeriodForAjax();

            if (!countlyCommon.DEBUG) {
                _activeAppKey = countlyCommon.ACTIVE_APP_KEY;
                _initialized = true;

                if(processed){
                    _processed = true;
                    return $.ajax({
                        type:""GET"",
                        url:countlyCommon.API_PARTS.data.r+""/analytics/metric"",
                        data:{
                            ""api_key"":countlyGlobal.member.api_key,
                            ""app_id"":countlyCommon.ACTIVE_APP_ID,
                            ""metric"":_name,
                            ""period"":_period
                        },
                        success:function (json) {
                            _Db = json;
                            if(countlyMetric.callback)
                                countlyMetric.callback(false, json);
                        }
                    });
                }
                else{
                    return $.ajax({
                        type:""GET"",
                        url:countlyCommon.API_PARTS.data.r,
                        data:{
                            ""api_key"":countlyGlobal.member.api_key,
                            ""app_id"":countlyCommon.ACTIVE_APP_ID,
                            ""method"":_name,
                            ""period"":_period
                        },
                        success:function (json) {
                            _Db = json;
                            setMeta();
                            if(countlyMetric.callback)
                                countlyMetric.callback(false, json);
                        }
                    });
                }
            } else {
                _Db = {""2012"":{}};
                if(countlyMetric.callback)
                    countlyMetric.callback(false, _Db);
                return true;
            }
        };

        /**
        * Refresh metric model by fetching data only for the latest time bucket using action=refresh on server. Currently does not fetch data for processed data loaded on initialization
        * @returns {jquery_promise} jquery promise to wait while data is loaded
        * @example
        *$.when(countlyMetric.refresh()).then(function () {
        *    //data loaded, do something
        *});
        */
        countlyMetric.refresh = function () {
            _periodObj = countlyCommon.periodObj;

            if (!countlyCommon.DEBUG) {

                if (_activeAppKey != countlyCommon.ACTIVE_APP_KEY) {
                    _activeAppKey = countlyCommon.ACTIVE_APP_KEY;
                    return this.initialize();
                }

                if(_processed){
                    if(countlyMetric.callback)
                        countlyMetric.callback(true);
                }
                else{
                    return $.ajax({
                        type:""GET"",
                        url:countlyCommon.API_PARTS.data.r,
                        data:{
                            ""api_key"":countlyGlobal.member.api_key,
                            ""app_id"":countlyCommon.ACTIVE_APP_ID,
                            ""method"":_name,
                            ""action"":""refresh""
                        },
                        success:function (json) {
                            countlyCommon.extendDbObj(_Db, json);
                            extendMeta();
                            if(countlyMetric.callback)
                                countlyMetric.callback(true, json);
                        }
                    });
                }
            } else {
                _Db = {""2012"":{}};
                if(countlyMetric.callback)
                    countlyMetric.callback(true, _Db);
                return true;
            }
        };

        /**
        * Callback that each metric model can define, to be called when data is loaded or refreshed
        * @example
        *countlyDeviceDetails.callback = function(isRefresh, data){
        *    if(isRefresh){
        *        countlyAppVersion.refresh(data);
        *    }
        *    else{
        *        countlyAppVersion.initialize();
        *    }
        *};
        */
        countlyMetric.callback = undefined;

        /**
        * Reset/delete all retrieved metric data, like when changing app or selected time period
        */
        countlyMetric.reset = function () {
            if(_processed){
                _Db = [];
            }
            else{
                _Db = {};
                setMeta();
            }
        };

        /**
        * Get current data, if some view or model requires access to raw data
        * @return {object} raw data returned from server either in standard metric model or preprocessed data, based on what model uses
        */
        countlyMetric.getDb = function () {
            return _Db;
        };

        /**
        * Set current data for model, if you need to provide data for model from another resource (as loaded in different model)
        * @param {object} db - set new data to be used by model
        */
        countlyMetric.setDb = function (db) {
            _Db = db;
            setMeta();
        };

        /**
        * Extend current data for model with some additional information about latest period (like data from action=refresh request)
        * @param {object} db - set new data to be used by model
        */
        countlyMetric.extendDb = function (data) {
            countlyCommon.extendDbObj(_Db, data);
            extendMeta();
        };

        /**
        * Get array of unique segments available for metric data
        * @param {string} metric - name of the segment/metric to get meta for, by default will use default _name provided on initialization
        * @returns {array} array of unique metric values
        */
        countlyMetric.getMeta = function (metric) {
            metric = metric || _name;
            return _metrics[metric] || [];
        };

        /**
        * Get data after initialize finished and data was retrieved
        * @param {boolean} clean - should retrieve clean data or preprocessed by fetchValue function
        * @param {boolean} join - join new and total users into single graph, for example to dispaly in bars on the same graph and not 2 separate pie charts
        * @param {string} metric - name of the segment/metric to get data for, by default will use default _name provided on initialization
        * @param {string} estOverrideMetric - name of the total users estimation override, by default will use default _estOverrideMetric provided on initialization
        * returns {object} chartData
        * @example <caption>Example output of separate data for 2 pie charts</caption>
        *{""chartData"":[
        *    {""langs"":""English"",""t"":124,""u"":112,""n"":50},
        *    {""langs"":""Italian"",""t"":83,""u"":74,""n"":30},
        *    {""langs"":""German"",""t"":72,""u"":67,""n"":26},
        *    {""langs"":""Japanese"",""t"":62,""u"":61,""n"":19},
        *    {""langs"":""French"",""t"":66,""u"":60,""n"":28},
        *    {""langs"":""Korean"",""t"":64,""u"":58,""n"":26}
        *],
        *""chartDPTotal"":{
        *    ""dp"":[
        *        {""data"":[[0,124]],""label"":""English""},
        *        {""data"":[[0,83]],""label"":""Italian""},
        *        {""data"":[[0,72]],""label"":""German""},
        *        {""data"":[[0,62]],""label"":""Japanese""},
        *        {""data"":[[0,66]],""label"":""French""},
        *        {""data"":[[0,64]],""label"":""Korean""}
        *    ]
        *},
        *""chartDPNew"":{
        *    ""dp"":[
        *        {""data"":[[0,50]],""label"":""English""},
        *        {""data"":[[0,30]],""label"":""Italian""},
        *        {""data"":[[0,26]],""label"":""German""},
        *        {""data"":[[0,19]],""label"":""Japanese""},
        *        {""data"":[[0,28]],""label"":""French""},
        *        {""data"":[[0,26]],""label"":""Korean""}
        *    ]
        *}}
        * @example <caption>Example output of joined data for 1 bar chart</caption>
        *{""chartData"":[
        *    {""langs"":""English"",""t"":124,""u"":112,""n"":50},
        *    {""langs"":""Italian"",""t"":83,""u"":74,""n"":30},
        *    {""langs"":""German"",""t"":72,""u"":67,""n"":26},
        *    {""langs"":""Japanese"",""t"":62,""u"":61,""n"":19},
        *    {""langs"":""French"",""t"":66,""u"":60,""n"":28},
        *    {""langs"":""Korean"",""t"":64,""u"":58,""n"":26}
        *],
        *""chartDP"":{
        *    ""dp"":[
        *        {""data"":[[-1,null],[0,124],[1,83],[2,72],[3,62],[4,66],[5,64],[6,null]],""label"":""Total Sessions""},
        *        {""data"":[[-1,null],[0,50],[1,30],[2,26],[3,19],[4,28],[5,26],[6,null]],""label"":""New Users""}
        *    ],
        *   ""ticks"":[
        *        [-1,""""], //used for padding for bars
        *        [23,""""], //used for padding for bars
        *        [0,""English""],
        *        [1,""Italian""],
        *        [2,""German""],
        *        [3,""Japanese""],
        *        [4,""French""],
        *        [5,""Korean""]
        *    ]
        *}}
        */
        countlyMetric.getData = function (clean, join, metric, estOverrideMetric) {
            var chartData = {};
            if(_processed){
                chartData.chartData = [];
                var data = JSON.parse(JSON.stringify(_Db));
                for(var i = 0; i < _Db.length; i++){
                    if(fetchValue && !clean)
                        data[i][metric || _name] = fetchValue(countlyCommon.decode(data[i]._id));
                    else
                        data[i][metric || _name] = countlyCommon.decode(data[i]._id);
                    chartData.chartData[i] = data[i];
                }
            }
            else{
                chartData = countlyCommon.extractTwoLevelData(_Db, this.getMeta(metric), this.clearObject, [
                    {
                        name:metric || _name,
                        func:function (rangeArr, dataObj) {
                            rangeArr = countlyCommon.decode(rangeArr);
                            if(fetchValue && !clean)
                                return fetchValue(rangeArr);
                            else
                                return rangeArr;
                        }
                    },
                    { ""name"":""t"" },
                    { ""name"":""u"" },
                    { ""name"":""n"" }
                ], estOverrideMetric || _estOverrideMetric);
            }
            chartData.chartData = countlyCommon.mergeMetricsByName(chartData.chartData, metric || _name);
            chartData.chartData.sort(function(a,b){return b.t-a.t})
            var namesData = _.pluck(chartData.chartData, metric || _name),
                totalData = _.pluck(chartData.chartData, 't'),
                newData = _.pluck(chartData.chartData, 'n');

            if(join){
                chartData.chartDP = {ticks:[]};
                var chartDP = [
                    {data:[], label:jQuery.i18n.map[""common.table.total-sessions""]},
                    {data:[], label:jQuery.i18n.map[""common.table.new-users""]}
                ];

                chartDP[0][""data""][0] = [-1, null];
                chartDP[0][""data""][namesData.length + 1] = [namesData.length, null];
                chartDP[1][""data""][0] = [-1, null];
                chartDP[1][""data""][namesData.length + 1] = [namesData.length, null];

                chartData.chartDP.ticks.push([-1, """"]);
                chartData.chartDP.ticks.push([namesData.length, """"]);

                for (var i = 0; i < namesData.length; i++) {
                    chartDP[0][""data""][i + 1] = [i, totalData[i]];
                    chartDP[1][""data""][i + 1] = [i, newData[i]];
                    chartData.chartDP.ticks.push([i, namesData[i]]);
                }

                chartData.chartDP.dp = chartDP;
            }
            else{
                var chartData2 = [],
                chartData3 = [];

                var sum = _.reduce(totalData, function (memo, num) {
                    return memo + num;
                }, 0);

                for (var i = 0; i < namesData.length; i++) {
                    var percent = (totalData[i] / sum) * 100;
                    chartData2[i] = {data:[
                        [0, totalData[i]]
                    ], label:namesData[i]};
                }

                var sum2 = _.reduce(newData, function (memo, num) {
                    return memo + num;
                }, 0);

                for (var i = 0; i < namesData.length; i++) {
                    var percent = (newData[i] / sum) * 100;
                    chartData3[i] = {data:[
                        [0, newData[i]]
                    ], label:namesData[i]};
                }

                chartData.chartDPTotal = {};
                chartData.chartDPTotal.dp = chartData2;

                chartData.chartDPNew = {};
                chartData.chartDPNew.dp = chartData3;
            }
            return chartData;
        };

        /**
        * Prefill all expected properties as u, t, n with 0, to avoid null values in the result, if they don't exist, which won't work when drawing graphs
        * @param {object} obj - oject to prefill with  values if they don't exist
        * @returns prefilled object
        */
        countlyMetric.clearObject = function (obj) {
            if (obj) {
                if (!obj[""t""]) obj[""t""] = 0;
                if (!obj[""n""]) obj[""n""] = 0;
                if (!obj[""u""]) obj[""u""] = 0;
            }
            else {
                obj = {""t"":0, ""n"":0, ""u"":0};
            }

            return obj;
        };

        /**
        * Get bar data for metric
        * @param {string} metric - name of the segment/metric to get data for, by default will use default _name provided on initialization
        * @returns {array} object to use when displaying bars as [{""name"":""English"",""percent"":44},{""name"":""Italian"",""percent"":29},{""name"":""German"",""percent"":27}]
        */
        countlyMetric.getBars = function (metric) {
            if(_processed){
                var rangeData = {};
                rangeData.chartData = [];
                var data = JSON.parse(JSON.stringify(_Db));
                for(var i = 0; i < _Db.length; i++){
                    if(fetchValue)
                        data[i][""range""] = fetchValue(countlyCommon.decode(data[i]._id));
                    else
                        data[i][""range""] = countlyCommon.decode(data[i]._id);
                    rangeData.chartData[i] = data[i];
                }
                return countlyCommon.calculateBarData(rangeData);
            }
            else{
                return countlyCommon.extractBarData(_Db, this.getMeta(metric), this.clearObject, fetchValue);
            }
        };

        /**
        * If this metric's data should be segmented by OS (which means be prefixed by first os letter on server side), you can get OS segmented data
        * @param {string} os - os name for which to get segmented metrics data
        * @param {boolean} clean - should retrieve clean data or preprocessed by fetchValue function
        * @param {string} metric - name of the segment/metric to get data for, by default will use default _name provided on initialization
        * @param {string} estOverrideMetric - name of the total users estimation override, by default will use default _estOverrideMetric provided on initialization
        * @returns {object} os segmented metric object
        * @example <caption>Example output</caption>
        * //call
        * //countlyMetric.getOSSegmentedData(""wp"")
        * //data for Windows Phone segment
        *{""chartData"":[
        *    {""density"":""2.0"",""t"":18,""u"":18,""n"":9},
        *    {""density"":""3.4"",""t"":13,""u"":12,""n"":5},
        *    {""density"":""1.2"",""t"":11,""u"":10,""n"":5},
        *    {""density"":""3.5"",""t"":10,""u"":10,""n"":4},
        *    {""density"":""3.3"",""t"":9,""u"":9,""n"":3}
        *],
        *""chartDP"":{
        *    ""dp"":[
        *        {""data"":[[0,53]],""label"":""2.0""},
        *        {""data"":[[0,49]],""label"":""3.4""},
        *        {""data"":[[0,46]],""label"":""1.2""},
        *        {""data"":[[0,36]],""label"":""3.5""},
        *        {""data"":[[0,32]],""label"":""3.3""}
        *    ]
        *},
        * //list of all os segments
        *""os"":[
        *   {""name"":""Windows Phone"",""class"":""windows phone""},
        *    {""name"":""Android"",""class"":""android""},
        *    {""name"":""iOS"",""class"":""ios""}
        *]}
        */
        countlyMetric.getOSSegmentedData = function (os, clean, metric, estOverrideMetric) {
            var _os = countlyDeviceDetails.getPlatforms();
            var oSVersionData = {};
            if(_processed){
                oSVersionData.chartData = [];
                var data = JSON.parse(JSON.stringify(_Db));
                for(var i = 0; i < _Db.length; i++){
                    if(fetchValue && !clean)
                        data[i][metric || _name] = fetchValue(countlyCommon.decode(data[i]._id));
                    else
                        data[i][metric || _name] = countlyCommon.decode(data[i]._id);
                    oSVersionData.chartData[i] = data[i];
                }
            }
            else{
                oSVersionData = countlyCommon.extractTwoLevelData(_Db, this.getMeta(metric), this.clearObject, [
                    {
                        name:metric || _name,
                        func:function (rangeArr, dataObj) {
                            rangeArr = countlyCommon.decode(rangeArr);
                            if(fetchValue && !clean)
                                return fetchValue(rangeArr);
                            else
                                return rangeArr;
                        }
                    },
                    { ""name"":""t"" },
                    { ""name"":""u"" },
                    { ""name"":""n"" }
                ], estOverrideMetric || _estOverrideMetric);
            }

            var osSegmentation = ((os) ? os : ((_os) ? _os[0] : null)),
                platformVersionTotal = _.pluck(oSVersionData.chartData, 'u'),
                chartData2 = [];
            var osName = osSegmentation;
            if(osSegmentation){
                if(countlyDeviceDetails.os_mapping[osSegmentation.toLowerCase()])
                    osName = countlyDeviceDetails.os_mapping[osSegmentation.toLowerCase()].short;
                else
                    osName = osSegmentation.toLowerCase()[0];
            }

            if (oSVersionData.chartData) {
                var reg = new RegExp(""^""+osName,""g"");
                for (var i = 0; i < oSVersionData.chartData.length; i++) {
                    var shouldDelete = true;
                    oSVersionData.chartData[i][metric || _name] = oSVersionData.chartData[i][metric || _name].replace(/:/g, ""."");
                    if(reg.test(oSVersionData.chartData[i][metric || _name])){
                        shouldDelete = false;
                        oSVersionData.chartData[i][metric || _name] = oSVersionData.chartData[i][metric || _name].replace(reg, """");
                    }
                    else if(countlyMetric.checkOS && countlyMetric.checkOS(osSegmentation, oSVersionData.chartData[i][metric || _name], osName)){
                        shouldDelete = false;
                        delete platformVersionTotal[i];
            platformVersionTotal = _.compact(platformVersionTotal);
                    }
                    if(shouldDelete)
                        delete oSVersionData.chartData[i];
                }
            }

            oSVersionData.chartData = _.compact(oSVersionData.chartData);

            var platformVersionNames = _.pluck(oSVersionData.chartData, metric || _name),
                platformNames = [];

            var sum = _.reduce(platformVersionTotal, function (memo, num) {
                return memo + num;
            }, 0);

            for (var i = 0; i < platformVersionNames.length; i++) {
                var percent = (platformVersionTotal[i] / sum) * 100;

                chartData2[chartData2.length] = {data:[
                    [0, platformVersionTotal[i]]
                ], label:platformVersionNames[i].replace(((countlyDeviceDetails.os_mapping[osSegmentation.toLowerCase()]) ? countlyDeviceDetails.os_mapping[osSegmentation.toLowerCase()].name : osSegmentation) + "" "", """")};
            }

            oSVersionData.chartDP = {};
            oSVersionData.chartDP.dp = chartData2;
            oSVersionData.os = [];

            if (_os && _os.length > 1) {
                for (var i = 0; i < _os.length; i++) {
                    //if (_os[i] != osSegmentation) {
                    //    continue;
                    //}

                    oSVersionData.os.push({
                        ""name"":_os[i],
                        ""class"":_os[i].toLowerCase()
                    });
                }
            }

            return oSVersionData;
        };

        /**
        * Get range data which is usually stored in some time ranges/buckets. As example is loyalty, session duration and session frequency
        * @param {string} metric - name of the property in the model to fetch
        * @param {string} meta - name of the meta where property's ranges are stored
        * @param {string} explain - function that receives index of the bucket and returns bucket name
        * @returns {object}
        * @example <caption>Example output</caption>
        * //call
        * //countlyMetric.getRangeData(""f"", ""f-ranges"", countlySession.explainFrequencyRange);
        * //returns
        * {""chartData"":[
        *    {""f"":""First session"",""t"":271,""percent"":""<div class='percent-bar' style='width:171px;'></div>85.5%""},
        *    {""f"":""2 days"",""t"":46,""percent"":""<div class='percent-bar' style='width:29px;'></div>14.5%""}
        *  ],
        *  ""chartDP"":{
        *      ""dp"":[
        *        {""data"":[[-1,null],[0,271],[1,46],[2,null]]}
        *      ],
        *      ""ticks"":[
        *        [-1,""""],
        *        [2,""""],
        *        [0,""First session""],
        *        [1,""2 days""]
        *      ]
        *   }
        *  }
        **/
        countlyMetric.getRangeData = function (metric, meta, explain) {

            var chartData = {chartData:{}, chartDP:{dp:[], ticks:[]}};

            chartData.chartData = countlyCommon.extractRangeData(_Db, metric, this.getMeta(meta), explain);

            var frequencies = _.pluck(chartData.chartData, metric),
                frequencyTotals = _.pluck(chartData.chartData, ""t""),
                chartDP = [
                    {data:[]}
                ];

            chartDP[0][""data""][0] = [-1, null];
            chartDP[0][""data""][frequencies.length + 1] = [frequencies.length, null];

            chartData.chartDP.ticks.push([-1, """"]);
            chartData.chartDP.ticks.push([frequencies.length, """"]);

            for (var i = 0; i < frequencies.length; i++) {
                chartDP[0][""data""][i + 1] = [i, frequencyTotals[i]];
                chartData.chartDP.ticks.push([i, frequencies[i]]);
            }

            chartData.chartDP.dp = chartDP;

            for (var i = 0; i < chartData.chartData.length; i++) {
                chartData.chartData[i][""percent""] = ""<div class='percent-bar' style='width:"" + (2 * chartData.chartData[i][""percent""]) + ""px;'></div>"" + chartData.chartData[i][""percent""] + ""%"";
            }

            return chartData;
        };

        function setMeta() {
            if (_Db['meta']) {
                for(var i in _Db['meta']){
                    _metrics[i] = (_Db['meta'][i]) ? _Db['meta'][i] : [];
                }
            } else {
                _metrics = {};
            }
        }

        function extendMeta() {
            if (_Db['meta']) {
                for(var i in _Db['meta']){
                    _metrics[i] = countlyCommon.union(_metrics[i], _Db['meta'][i]);
                }
            }
        }

    };

    /**
    * Initialize countly text select. In most cases it is done automatically, only in some cases, when content loaded via ajax request outside of view lifecycle, you may need to initialize it yourself for your content specifically
    * @param {object} element - jQuery object reference
    * @example
    * CountlyHelpers.initializeTextSelect($(""#my-dynamic-div""));
    */
    CountlyHelpers.initializeTextSelect = function (element) {
        element = element || $(""#content-container"");

        element.off(""click"", "".cly-text-select"").on(""click"", "".cly-text-select"", function (e) {
            if ($(this).hasClass(""disabled"")) {
                return true;
            }

            initItems($(this));

            $(""#date-picker"").hide();
            e.stopPropagation();
        });

        element.off(""click"", "".cly-text-select .select-items .item"").on(""click"", "".cly-text-select .select-items .item"", function () {
            var selectedItem = $(this).parents("".cly-text-select"").find("".text"");
            selectedItem.text($(this).text());
            selectedItem.data(""value"", $(this).data(""value""));
            selectedItem.val($(this).text());
        });

        element.off(""keyup"", "".cly-text-select input"").on(""keyup"", "".cly-text-select input"", function(event) {
            initItems($(this).parents("".cly-text-select""), true);

            $(this).data(""value"", $(this).val());

            if (!$(this).val()) {
                $(this).parents("".cly-text-select"").find("".item"").removeClass(""hidden"");
            } else {
                $(this).parents("".cly-text-select"").find("".item:not(:contains('"" + $(this).val() + ""'))"").addClass(""hidden"");
                $(this).parents("".cly-text-select"").find("".item:contains('"" + $(this).val() + ""')"").removeClass(""hidden"");
            }
        });

        function initItems(select, forceShow) {
            select.removeClass(""req"");

            var selectItems = select.find("".select-items"");

            if (!selectItems.length) {
                return false;
            }

            if (select.find("".select-items"").is("":visible"") && !forceShow) {
                select.find("".select-items"").hide();
            } else {
                select.find("".select-items"").show();
                select.find("".select-items>div"").addClass(""scroll-list"");
                select.find("".scroll-list"").slimScroll({
                    height:'100%',
                    start:'top',
                    wheelStep:10,
                    position:'right',
                    disableFadeOut:true
                });
            }
        }

        $(window).click(function () {
            $("".select-items"").hide();
        });
    };

    /**
    * Generate random password
    * @param {number} length - length of the password
    * @param {boolean} no_special - do not include special characters
    * @example
    * //outputs 4UBHvRBG1v
    * CountlyHelpers.generatePassword(10, true);
    */
    CountlyHelpers.generatePassword = function(length, no_special) {
        var text = [];
        var chars = ""abcdefghijklmnopqrstuvwxyz"";
        var upchars = ""ABCDEFGHIJKLMNOPQRSTUVWXYZ"";
        var numbers = ""0123456789"";
        var specials = '!@#$%^&*()_+{}:""<>?\|[];\',./`~';
        var all = chars+upchars+numbers;
        if(!no_special)
            all += specials;

        //1 char
        text.push(upchars.charAt(Math.floor(Math.random() * upchars.length)));
        //1 number
        text.push(numbers.charAt(Math.floor(Math.random() * numbers.length)));
        //1 special char
        if(!no_special){
            text.push(specials.charAt(Math.floor(Math.random() * specials.length)));
            length--;
        }

        //5 any chars
        for( var i=0; i < Math.max(length-2, 5); i++ )
            text.push(all.charAt(Math.floor(Math.random() * all.length)));

        //randomize order
        var j, x, i;
        for (i = text.length; i; i--) {
            j = Math.floor(Math.random() * i);
            x = text[i - 1];
            text[i - 1] = text[j];
            text[j] = x;
        }

        return text.join("""");
    };

    /**
    * Validate email address
    * @param {string} email - email address to validate
    * @returns {boolean} true if valid and false if invalid
    * @example
    * //outputs true
    * CountlyHelpers.validateEmail(""test@test.test"");
    *
    * //outputs false
    * CountlyHelpers.validateEmail(""test@test"");
    */
    CountlyHelpers.validateEmail = function(email) {
        var re = /[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*@(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?/;
        return re.test(email);
    };

    /**
    * Validate password based on settings provided via security configuration
    * @param {string} password - password to validate
    * @returns {boolean} true if valid and false if invalid
    */
    CountlyHelpers.validatePassword = function(password){
        if(password.length < countlyGlobal[""security""].password_min)
            return jQuery.i18n.prop(""management-users.password.length"", countlyGlobal[""security""].password_min);
        if(countlyGlobal[""security""].password_char && !/[A-Z]/.test(password))
            return jQuery.i18n.map[""management-users.password.has-char""];
        if(countlyGlobal[""security""].password_number && !/\d/.test(password))
            return jQuery.i18n.map[""management-users.password.has-number""];
        if(countlyGlobal[""security""].password_symbol && !/[^A-Za-z\d]/.test(password))
            return jQuery.i18n.map[""management-users.password.has-special""];
        return false;
    };

    $(document).ready(function () {
        $(""#overlay"").click(function () {
            var dialog = $("".dialog:visible:not(.cly-loading)"");
            if (dialog.length) {
                dialog.fadeOut().remove();
                $(this).hide();
            }
        });

        $(""#dialog-ok, #dialog-cancel, #dialog-continue"").live('click', function () {
            $(this).parents("".dialog:visible"").fadeOut().remove();
            if (!$('.dialog:visible').length) $(""#overlay"").hide();
        });

        $(document).keyup(function (e) {
            // ESC
            if (e.keyCode == 27) {
                $("".dialog:visible"").animate({
                    top:0,
                    opacity:0
                }, {
                    duration:1000,
                    easing:'easeOutQuart',
                    complete:function () {
                        $(this).remove();
                    }
                });

                $(""#overlay"").hide();
            }
        });
    });

}(window.CountlyHelpers = window.CountlyHelpers || {}, jQuery));","Since `shouldDelete` is not enclosed in {} brackets, it only includes `delete oSVersionData.chartData[i];` and thus `delete platformVersionTotal[i];` is executed for every segment deleting them","The suggested code changes appear to be related to enhancing functionality for various UI components and interactions within a web application. Here's a concise review of the changes:

### Enhancements and Improvements:

1. **Dropdown Options List Initialization**:
   - Added functionality to initialize dropdown options lists for DataTables, allowing users to click on them to open a menu with options.
   - Added event listeners for clicks on the dropdown options menu, handling events like opening, closing, and selecting items.

2. **Datatable Refresh**:
   - Implemented a method to refresh existing DataTable instances on view refresh, providing new data.
   - Ensured that the DataTable is cleared and new data is added, maintaining the correct display.

3. **Row Expansion in DataTables**:
   - Added functionality to expand rows in DataTables, allowing users to click on a row to reveal additional details.
   - Implemented logic to handle opening and closing of rows, ensuring that only one row is expanded at a time.

4. **Reopening Rows on Refresh**:
   - Provided a method to reopen previously expanded rows after refreshing the DataTable, ensuring that the user experience remains consistent.

5. **Closing All Opened Rows**:
   - Added a method to close all opened rows in the DataTable, providing a clean state when the user navigates away from the page.

6. **Utility Functions**:
   - Introduced utility functions like `appIdsToNames`, `loadJS`, `loadCSS`, `messageText`, `clip`, `createMetricModel`, and others to handle various tasks such as converting app IDs to names, loading JavaScript and CSS files, and creating metric models.

7. **Text Select Initialization**:
   - Enhanced the functionality of text selects, allowing users to click on them to open a list of items and select one.
   - Added event listeners for clicks on the text select items, updating the selected text and value.

8. **Password Generation**:
   - Implemented a function to generate random passwords, allowing for customization of length and inclusion of special characters.

9. **Email Validation**:
   - Added a function to validate email addresses, ensuring they meet basic criteria.

10. **Password Validation**:
    - Implemented a function to validate passwords based on security settings, checking for minimum length, character types, and symbols.

11. **Dialog Management**:
    - Enhanced dialog management, allowing users to close dialogs using the ESC key and buttons.
    - Added event listeners for dialog actions like OK, Cancel, and Continue."
31,"@@ -38,6 +38,9 @@ static int on_req(h2o_handler_t *_self, h2o_req_t *req)
     }
 
     h2o_http2_debug_state_t *debug_state = req->conn->callbacks->get_debug_state(req, self->hpack_enabled);
+    if (debug_state == NULL) {
+        return -1;
+    }
 
     // stringify these variables to embed in Debug Header
     h2o_iovec_t conn_flow_in, conn_flow_out;","/*
 * Copyright (c) 2016 DeNA Co., Ltd., Ichito Nagata
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the ""Software""), to
 * deal in the Software without restriction, including without limitation the
 * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
 * sell copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 * IN THE SOFTWARE.
 */
#include <inttypes.h>
#include ""h2o.h""

struct st_h2o_http2_debug_state_handler_t {
    h2o_handler_t super;
    int hpack_enabled;
};

static int on_req(h2o_handler_t *_self, h2o_req_t *req)
{
    struct st_h2o_http2_debug_state_handler_t *self = (void *)_self;

    static h2o_generator_t generator = {NULL, NULL};

    if (req->conn->callbacks->get_debug_state == NULL) {
        return -1;
    if (debug_state == NULL) {
        return -1;
    }
    }

    h2o_http2_debug_state_t *debug_state = req->conn->callbacks->get_debug_state(req, self->hpack_enabled);

    // stringify these variables to embed in Debug Header
    h2o_iovec_t conn_flow_in, conn_flow_out;
    conn_flow_in.base = h2o_mem_alloc_pool(&req->pool, sizeof(H2O_INT64_LONGEST_STR));
    conn_flow_in.len = sprintf(conn_flow_in.base, ""%zd"", debug_state->conn_flow_in);
    conn_flow_out.base = h2o_mem_alloc_pool(&req->pool, sizeof(H2O_INT64_LONGEST_STR));
    conn_flow_out.len = sprintf(conn_flow_out.base, ""%zd"", debug_state->conn_flow_out);

    req->res.status = 200;
    h2o_add_header(&req->pool, &req->res.headers, H2O_TOKEN_CONTENT_TYPE, NULL, H2O_STRLIT(""application/json; charset=utf-8""));
    h2o_add_header(&req->pool, &req->res.headers, H2O_TOKEN_CACHE_CONTROL, NULL, H2O_STRLIT(""no-cache, no-store""));
    h2o_add_header_by_str(&req->pool, &req->res.headers, H2O_STRLIT(""conn-flow-in""), 0, NULL, conn_flow_in.base, conn_flow_in.len);
    h2o_add_header_by_str(&req->pool, &req->res.headers, H2O_STRLIT(""conn-flow-out""), 0, NULL, conn_flow_out.base,
                          conn_flow_out.len);

    h2o_start_response(req, &generator);
    h2o_send(req, debug_state->json.entries,
             h2o_memis(req->input.method.base, req->input.method.len, H2O_STRLIT(""HEAD"")) ? 0 : debug_state->json.size,
             H2O_SEND_STATE_FINAL);
    return 0;
}

void h2o_http2_debug_state_register(h2o_hostconf_t *conf, int hpack_enabled)
{
    h2o_pathconf_t *pathconf = h2o_config_register_path(conf, ""/.well-known/h2/state"", 0);
    struct st_h2o_http2_debug_state_handler_t *self = (void *)h2o_create_handler(pathconf, sizeof(*self));
    self->super.on_req = on_req;
    self->hpack_enabled = hpack_enabled;
}
","I assume that we have similar code everywhere; one that check if the callback is NULL, if not, calls the callback and check if the return value is NULL. In other words, we are performing the check twice. I do not think that having such coding pattern is optimal. Instead, could we require every instance of `http_conn_callbacks_t` to setup all the callback pointers, so that we can call the callback unconditionally? For example, you could define ""default"" functions that return NULL (e.g. `h2o_http2_debug_state_t *h2o_conn_default_get_debug_state(h2o_req_t *req, int hpack_enabled) { return NULL; }`) and let the callbacks (that do not require a specialized implementation) use the default.","The suggested code change adds a new handler for retrieving HTTP/2 debug state information. Here's a concise review:

1. **Handler Structure**: The code defines a new handler structure `st_h2o_http2_debug_state_handler_t` that inherits from `h2o_handler_t`.

2. **on_req Function**: This function handles incoming requests. It checks if the connection has a `get_debug_state` callback and retrieves the debug state. It then formats the flow counts into strings and adds them as headers to the response.

3. **Response Construction**: The response is constructed with a JSON payload containing the debug state information. The headers include `Content-Type`, `Cache-Control`, and custom headers for flow counts.

4. **Registration**: The handler is registered with the host configuration under the path `//.well-known/h2/state`.

5. **Memory Management**: The code uses `h2o_mem_alloc_pool` to allocate memory for the flow count strings.

Overall, the code provides a simple way to expose HTTP/2 debug state information via a web endpoint, which can be useful for monitoring and debugging purposes."
32,"@@ -69,7 +69,7 @@ class BaseUpdatePartitionSpec implements UpdatePartitionSpec {
     this.schema = spec.schema();
     this.nameToField = indexSpecByName(spec);
     this.transformToField = indexSpecByTransform(spec);
-    this.lastAssignedPartitionId = base.lastAssignedPartitionId();
+    this.lastAssignedPartitionId = base.lastPartitionId();
 
     spec.fields().stream()
         .filter(field -> field.transform() instanceof UnknownTransform)","/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

package org.apache.iceberg;

import java.util.List;
import java.util.Map;
import java.util.Set;
import org.apache.iceberg.exceptions.ValidationException;
import org.apache.iceberg.expressions.BoundReference;
import org.apache.iceberg.expressions.BoundTerm;
import org.apache.iceberg.expressions.BoundTransform;
import org.apache.iceberg.expressions.Expressions;
import org.apache.iceberg.expressions.Term;
import org.apache.iceberg.expressions.UnboundTerm;
import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;
import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
import org.apache.iceberg.relocated.com.google.common.collect.Lists;
import org.apache.iceberg.relocated.com.google.common.collect.Maps;
import org.apache.iceberg.relocated.com.google.common.collect.Sets;
import org.apache.iceberg.transforms.PartitionSpecVisitor;
import org.apache.iceberg.transforms.Transform;
import org.apache.iceberg.transforms.Transforms;
import org.apache.iceberg.transforms.UnknownTransform;
import org.apache.iceberg.util.Pair;

class BaseUpdatePartitionSpec implements UpdatePartitionSpec {
  private final TableOperations ops;
  private final TableMetadata base;
  private final int formatVersion;
  private final PartitionSpec spec;
  private final Schema schema;
  private final Map<String, PartitionField> nameToField;
  private final Map<Pair<Integer, String>, PartitionField> transformToField;

  private final List<PartitionField> adds = Lists.newArrayList();
  private final Map<Integer, PartitionField> addedTimeFields = Maps.newHashMap();
  private final Map<Pair<Integer, String>, PartitionField> transformToAddedField = Maps.newHashMap();
  private final Map<String, PartitionField> nameToAddedField = Maps.newHashMap();
  private final Set<Object> deletes = Sets.newHashSet();
  private final Map<String, String> renames = Maps.newHashMap();

  private boolean caseSensitive;
  private int lastAssignedPartitionId;

  BaseUpdatePartitionSpec(TableOperations ops) {
    this.ops = ops;
    this.caseSensitive = true;
    this.base = ops.current();
    this.formatVersion = base.formatVersion();
    this.spec = base.spec();
    this.lastAssignedPartitionId = base.lastPartitionId();
    this.schema = spec.schema();
    this.nameToField = indexSpecByName(spec);
    this.transformToField = indexSpecByTransform(spec);
    this.lastAssignedPartitionId = base.lastAssignedPartitionId();

    spec.fields().stream()
        .filter(field -> field.transform() instanceof UnknownTransform)
        .findAny()
        .ifPresent(field -> {
          throw new IllegalArgumentException(""Cannot update partition spec with unknown transform: "" + field);
        });
  }

  /**
   * For testing only.
   */
  @VisibleForTesting
  BaseUpdatePartitionSpec(int formatVersion, PartitionSpec spec) {
    this(formatVersion, spec, spec.fields().stream().mapToInt(PartitionField::fieldId).max().orElse(999));
  }

  /**
   * For testing only.
   */
  @VisibleForTesting
  BaseUpdatePartitionSpec(int formatVersion, PartitionSpec spec, int lastAssignedPartitionId) {
    this.ops = null;
    this.base = null;
    this.formatVersion = formatVersion;
    this.caseSensitive = true;
    this.spec = spec;
    this.schema = spec.schema();
    this.nameToField = indexSpecByName(spec);
    this.transformToField = indexSpecByTransform(spec);
    this.lastAssignedPartitionId = lastAssignedPartitionId;
  }

  private int assignFieldId() {
    this.lastAssignedPartitionId += 1;
    return lastAssignedPartitionId;
  }

  @Override
  public UpdatePartitionSpec caseSensitive(boolean isCaseSensitive) {
    this.caseSensitive = isCaseSensitive;
    return this;
  }

  @Override
  public BaseUpdatePartitionSpec addField(String sourceName) {
    return addField(Expressions.ref(sourceName));
  }

  @Override
  public BaseUpdatePartitionSpec addField(Term term) {
    return addField(null, term);
  }

  @Override
  public BaseUpdatePartitionSpec addField(String name, Term term) {
    PartitionField alreadyAdded = nameToAddedField.get(name);
    Preconditions.checkArgument(alreadyAdded == null, ""Cannot add duplicate partition field: %s"", alreadyAdded);

    Pair<Integer, Transform<?, ?>> sourceTransform = resolve(term);
    Pair<Integer, String> validationKey = Pair.of(sourceTransform.first(), sourceTransform.second().toString());

    PartitionField existing = transformToField.get(validationKey);
    Preconditions.checkArgument(existing == null,
        ""Cannot add duplicate partition field %s=%s, conflicts with %s"", name, term, existing);

    PartitionField added = transformToAddedField.get(validationKey);
    Preconditions.checkArgument(added == null,
        ""Cannot add duplicate partition field %s=%s, already added: %s"", name, term, added);

    PartitionField newField = new PartitionField(
        sourceTransform.first(), assignFieldId(), name, sourceTransform.second());
    checkForRedundantAddedPartitions(newField);

    transformToAddedField.put(validationKey, newField);
    if (name != null) {
      nameToAddedField.put(name, newField);
    }

    adds.add(newField);

    return this;
  }

  @Override
  public BaseUpdatePartitionSpec removeField(String name) {
    PartitionField alreadyAdded = nameToAddedField.get(name);
    Preconditions.checkArgument(alreadyAdded == null, ""Cannot delete newly added field: %s"", alreadyAdded);

    Preconditions.checkArgument(renames.get(name) == null,
        ""Cannot rename and delete partition field: %s"", name);

    PartitionField field = nameToField.get(name);
    Preconditions.checkArgument(field != null,
        ""Cannot find partition field to remove: %s"", name);

    deletes.add(field.fieldId());

    return this;
  }

  @Override
  public BaseUpdatePartitionSpec removeField(Term term) {
    Pair<Integer, Transform<?, ?>> sourceTransform = resolve(term);
    Pair<Integer, String> key = Pair.of(sourceTransform.first(), sourceTransform.second().toString());

    PartitionField added = transformToAddedField.get(key);
    Preconditions.checkArgument(added == null, ""Cannot delete newly added field: %s"", added);

    PartitionField field = transformToField.get(key);
    Preconditions.checkArgument(field != null,
        ""Cannot find partition field to remove: %s"", term);
    Preconditions.checkArgument(renames.get(field.name()) == null,
        ""Cannot rename and delete partition field: %s"", field.name());

    deletes.add(field.fieldId());

    return this;
  }

  @Override
  public BaseUpdatePartitionSpec renameField(String name, String newName) {
    PartitionField added = nameToAddedField.get(name);
    Preconditions.checkArgument(added == null,
        ""Cannot rename newly added partition field: %s"", name);

    PartitionField field = nameToField.get(name);
    Preconditions.checkArgument(field != null,
        ""Cannot find partition field to rename: %s"", name);
    Preconditions.checkArgument(!deletes.contains(field.fieldId()),
        ""Cannot delete and rename partition field: %s"", name);

    renames.put(name, newName);

    return this;
  }

  @Override
  public PartitionSpec apply() {
    PartitionSpec.Builder builder = PartitionSpec.builderFor(schema);

    for (PartitionField field : spec.fields()) {
      if (!deletes.contains(field.fieldId())) {
        String newName = renames.get(field.name());
        if (newName != null) {
          builder.add(field.sourceId(), field.fieldId(), newName, field.transform());
        } else {
          builder.add(field.sourceId(), field.fieldId(), field.name(), field.transform());
        }
      } else if (formatVersion < 2) {
        // field IDs were not required for v1 and were assigned sequentially in each partition spec starting at 1,000.
        // to maintain consistent field ids across partition specs in v1 tables, any partition field that is removed
        // must be replaced with a null transform. null values are always allowed in partition data.
        builder.add(field.sourceId(), field.fieldId(), field.name(), Transforms.alwaysNull());
      }
    }

    for (PartitionField newField : adds) {
      String partitionName;
      if (newField.name() != null) {
        partitionName = newField.name();
      } else {
        partitionName = PartitionSpecVisitor.visit(schema, newField, PartitionNameGenerator.INSTANCE);
      }

      builder.add(newField.sourceId(), newField.fieldId(), partitionName, newField.transform());
    }

    return builder.build();
  }

  @Override
  public void commit() {
    TableMetadata update = base.updatePartitionSpec(apply());
    ops.commit(base, update);
  }

  private Pair<Integer, Transform<?, ?>> resolve(Term term) {
    Preconditions.checkArgument(term instanceof UnboundTerm, ""Term must be unbound"");

    BoundTerm<?> boundTerm = ((UnboundTerm<?>) term).bind(schema.asStruct(), caseSensitive);
    int sourceId = boundTerm.ref().fieldId();
    Transform<?, ?> transform = toTransform(boundTerm);

    return Pair.of(sourceId, transform);
  }

  private Transform<?, ?> toTransform(BoundTerm<?> term) {
    if (term instanceof BoundReference) {
      return Transforms.identity(term.type());
    } else if (term instanceof BoundTransform) {
      return ((BoundTransform<?, ?>) term).transform();
    } else {
      throw new ValidationException(""Invalid term: %s, expected either a bound reference or transform"", term);
    }
  }

  private void checkForRedundantAddedPartitions(PartitionField field) {
    if (isTimeTransform(field)) {
      PartitionField timeField = addedTimeFields.get(field.sourceId());
      Preconditions.checkArgument(timeField == null,
          ""Cannot add redundant partition field: %s conflicts with %s"", timeField, field);
      addedTimeFields.put(field.sourceId(), field);
    }
  }

  private static Map<String, PartitionField> indexSpecByName(PartitionSpec spec) {
    ImmutableMap.Builder<String, PartitionField> builder = ImmutableMap.builder();
    List<PartitionField> fields = spec.fields();
    for (PartitionField field : fields) {
      builder.put(field.name(), field);
    }

    return builder.build();
  }

  private static Map<Pair<Integer, String>, PartitionField> indexSpecByTransform(PartitionSpec spec) {
    ImmutableMap.Builder<Pair<Integer, String>, PartitionField> builder = ImmutableMap.builder();
    List<PartitionField> fields = spec.fields();
    for (PartitionField field : fields) {
      builder.put(Pair.of(field.sourceId(), field.transform().toString()), field);
    }

    return builder.build();
  }

  private boolean isTimeTransform(PartitionField field) {
    return PartitionSpecVisitor.visit(schema, field, IsTimeTransform.INSTANCE);
  }

  private static class IsTimeTransform implements PartitionSpecVisitor<Boolean> {
    private static final IsTimeTransform INSTANCE = new IsTimeTransform();

    private IsTimeTransform() {
    }

    @Override
    public Boolean identity(int fieldId, String sourceName, int sourceId) {
      return false;
    }

    @Override
    public Boolean bucket(int fieldId, String sourceName, int sourceId, int numBuckets) {
      return false;
    }

    @Override
    public Boolean truncate(int fieldId, String sourceName, int sourceId, int width) {
      return false;
    }

    @Override
    public Boolean year(int fieldId, String sourceName, int sourceId) {
      return true;
    }

    @Override
    public Boolean month(int fieldId, String sourceName, int sourceId) {
      return true;
    }

    @Override
    public Boolean day(int fieldId, String sourceName, int sourceId) {
      return true;
    }

    @Override
    public Boolean hour(int fieldId, String sourceName, int sourceId) {
      return true;
    }

    @Override
    public Boolean alwaysNull(int fieldId, String sourceName, int sourceId) {
      return false;
    }

    @Override
    public Boolean unknown(int fieldId, String sourceName, int sourceId, String transform) {
      return false;
    }
  }

  private static class PartitionNameGenerator implements PartitionSpecVisitor<String> {
    private static final PartitionNameGenerator INSTANCE = new PartitionNameGenerator();

    private PartitionNameGenerator() {
    }

    @Override
    public String identity(int fieldId, String sourceName, int sourceId) {
      return sourceName;
    }

    @Override
    public String bucket(int fieldId, String sourceName, int sourceId, int numBuckets) {
      return sourceName + ""_bucket_"" + numBuckets;
    }

    @Override
    public String truncate(int fieldId, String sourceName, int sourceId, int width) {
      return sourceName + ""_trunc_"" + width;
    }

    @Override
    public String year(int fieldId, String sourceName, int sourceId) {
      return sourceName + ""_year"";
    }

    @Override
    public String month(int fieldId, String sourceName, int sourceId) {
      return sourceName + ""_month"";
    }

    @Override
    public String day(int fieldId, String sourceName, int sourceId) {
      return sourceName + ""_day"";
    }

    @Override
    public String hour(int fieldId, String sourceName, int sourceId) {
      return sourceName + ""_hour"";
    }

    @Override
    public String alwaysNull(int fieldId, String sourceName, int sourceId) {
      return sourceName + ""_null"";
    }
  }
}
","I don't think we need to rename the method, just the property in the metadata file.","The suggested code change appears to be implementing a class `BaseUpdatePartitionSpec` that provides methods to update the partition specification of an Iceberg table. The class includes methods to add, remove, and rename partition fields, as well as to apply and commit the changes.

Here's a concise review of the suggested code change:

### Key Points:
1. **Class Structure**: The class is structured to manage updates to the partition specification of an Iceberg table.
2. **Field Indexing**: It uses maps to index partition fields by name and by transform, which helps in efficiently managing and updating fields.
3. **Validation**: The class includes checks to ensure that no duplicate fields are added or removed, and that renaming and deleting fields are handled correctly.
4. **Transform Resolution**: The `resolve` method binds a term to a source ID and transform, ensuring that the term can be applied to the table schema.
5. **Transform Conversion**: The `toTransform` method converts a bound term into a transform object.
6. **Redundant Check**: The `checkForRedundantAddedPartitions` method ensures that time-based partition fields are not added redundantly.
7. **Partition Name Generation**: The `PartitionNameGenerator` class provides default names for partition fields based on their type.

### Potential Improvements:
- **Error Handling**: Consider adding more detailed error messages and handling for edge cases, such as invalid terms or unsupported transforms.
- **Performance**: Ensure that the indexing operations are efficient, especially when dealing with large schemas.
- **Documentation**: Add comments and documentation to explain the purpose and usage of each method and class.

Overall, the code change provides a robust framework for updating partition specifications in an Iceberg table, with careful consideration for validation and error handling."
33,"@@ -1,3 +1 @@
-return !axe.commons.aria.isValidRole(node.getAttribute('role'), {
-	flagUnsupported: true
-});
+return axe.commons.aria.isUnsupportedRole(node.getAttribute('role'));","return axe.commons.aria.isUnsupportedRole(node.getAttribute('role'));
return !axe.commons.aria.isValidRole(node.getAttribute('role'), {
	flagUnsupported: true
});
","I know the `aria.isValidRole` does not `sanitize` the `role`. It may be worth doing `trim` or `axe.commons.text.sanitize`, as well as `toLoweCase`, so it matches the values in the `lookupTable`","Both suggested code changes effectively check if the `role` attribute of a node is unsupported or invalid according to the ARIA specification. The first version uses `axe.commons.aria.isUnsupportedRole`, while the second version uses `axe.commons.aria.isValidRole` with the `flagUnsupported` option set to `true`. Both approaches will return `true` if the role is unsupported or invalid, and `false` otherwise."
34,"@@ -516,7 +516,12 @@ function isDOMElement (obj) {
  */
 function findDOMElement (element) {
   if (typeof element === 'string') {
-    return document.querySelector(element)
+    const elements = [].slice.call(document.querySelectorAll(element))
+    if (elements.length > 1) {
+      return elements
+    }
+    return elements[0]
+    // return document.querySelector(element)
   }
 
   if (typeof element === 'object' && isDOMElement(element)) {","const throttle = require('lodash.throttle')
// we inline file-type module, as opposed to using the NPM version,
// because of this https://github.com/sindresorhus/file-type/issues/78
// and https://github.com/sindresorhus/copy-text-to-clipboard/issues/5
const fileType = require('../vendor/file-type')

/**
 * A collection of small utility functions that help with dom manipulation, adding listeners,
 * promises and other good things.
 *
 * @module Utils
 */

/**
 * Shallow flatten nested arrays.
 */
function flatten (arr) {
  return [].concat.apply([], arr)
}

function isTouchDevice () {
  return 'ontouchstart' in window || // works on most browsers
          navigator.maxTouchPoints   // works on IE10/11 and Surface
}

// /**
//  * Shorter and fast way to select a single node in the DOM
//  * @param   { String } selector - unique dom selector
//  * @param   { Object } ctx - DOM node where the target of our search will is located
//  * @returns { Object } dom node found
//  */
// function $ (selector, ctx) {
//   return (ctx || document).querySelector(selector)
// }

// /**
//  * Shorter and fast way to select multiple nodes in the DOM
//  * @param   { String|Array } selector - DOM selector or nodes list
//  * @param   { Object } ctx - DOM node where the targets of our search will is located
//  * @returns { Object } dom nodes found
//  */
// function $$ (selector, ctx) {
//   var els
//   if (typeof selector === 'string') {
//     els = (ctx || document).querySelectorAll(selector)
//   } else {
//     els = selector
//     return Array.prototype.slice.call(els)
//   }
// }

function truncateString (str, length) {
  if (str.length > length) {
    return str.substr(0, length / 2) + '...' + str.substr(str.length - length / 4, str.length)
  }
  return str

  // more precise version if needed
  // http://stackoverflow.com/a/831583
}

function secondsToTime (rawSeconds) {
  const hours = Math.floor(rawSeconds / 3600) % 24
  const minutes = Math.floor(rawSeconds / 60) % 60
  const seconds = Math.floor(rawSeconds % 60)

  return { hours, minutes, seconds }
}

/**
 * Partition array by a grouping function.
 * @param  {[type]} array      Input array
 * @param  {[type]} groupingFn Grouping function
 * @return {[type]}            Array of arrays
 */
function groupBy (array, groupingFn) {
  return array.reduce((result, item) => {
    let key = groupingFn(item)
    let xs = result.get(key) || []
    xs.push(item)
    result.set(key, xs)
    return result
  }, new Map())
}

/**
 * Tests if every array element passes predicate
 * @param  {Array}  array       Input array
 * @param  {Object} predicateFn Predicate
 * @return {bool}               Every element pass
 */
function every (array, predicateFn) {
  return array.reduce((result, item) => {
    if (!result) {
      return false
    }

    return predicateFn(item)
  }, true)
}

/**
 * Converts list into array
*/
function toArray (list) {
  return Array.prototype.slice.call(list || [], 0)
}

/**
 * Takes a fileName and turns it into fileID, by converting to lowercase,
 * removing extra characters and adding unix timestamp
 *
 * @param {String} fileName
 *
 */
function generateFileID (file) {
  let fileID = file.name.toLowerCase()
  fileID = fileID.replace(/[^A-Z0-9]/ig, '')
  fileID = fileID + file.data.lastModified
  return fileID
}

function extend (...objs) {
  return Object.assign.apply(this, [{}].concat(objs))
}

/**
 * Runs an array of promise-returning functions in sequence.
 */
function runPromiseSequence (functions, ...args) {
  let promise = Promise.resolve()
  functions.forEach((func) => {
    promise = promise.then(() => func(...args))
  })
  return promise
}

/**
 * Takes function or class, returns its name.
 * Because IE doesn’t support `constructor.name`.
 * https://gist.github.com/dfkaye/6384439, http://stackoverflow.com/a/15714445
 *
 * @param {Object} fn — function
 *
 */
// function getFnName (fn) {
//   var f = typeof fn === 'function'
//   var s = f && ((fn.name && ['', fn.name]) || fn.toString().match(/function ([^\(]+)/))
//   return (!f && 'not a function') || (s && s[1] || 'anonymous')
// }

function isPreviewSupported (fileTypeSpecific) {
  // list of images that browsers can preview
  if (/^(jpeg|gif|png|svg|svg\+xml|bmp)$/.test(fileTypeSpecific)) {
    return true
  }
  return false
}

function getArrayBuffer (chunk) {
  return new Promise(function (resolve, reject) {
    var reader = new FileReader()
    reader.addEventListener('load', function (e) {
      // e.target.result is an ArrayBuffer
      resolve(e.target.result)
    })
    reader.addEventListener('error', function (err) {
      console.error('FileReader error' + err)
      reject(err)
    })
    // file-type only needs the first 4100 bytes
    reader.readAsArrayBuffer(chunk)
  })
}

function getFileType (file) {
  const emptyFileType = ['', '']
  const extensionsToMime = {
    'md': 'text/markdown',
    'markdown': 'text/markdown',
    'mp4': 'video/mp4',
    'mp3': 'audio/mp3'
  }

  // no smart detection for remote files, just trust the provider
  if (file.isRemote) {
    return Promise.resolve(file.type.split('/'))
  }

  const fileExtension = getFileNameAndExtension(file.name)[1]

  // 1. try to determine file type from magic bytes with file-type module
  // this should be the most trustworthy way
  const chunk = file.data.slice(0, 4100)
  return getArrayBuffer(chunk)
    .then((buffer) => {
      const type = fileType(buffer)
      if (type && type.mime) {
        return type.mime.split('/')
      }

      // 2. if that’s no good, check if mime type is set in the file object
      if (file.type) {
        return file.type.split('/')
      }

      // 3. if that’s no good, see if we can map extension to a mime type
      if (extensionsToMime[fileExtension]) {
        return extensionsToMime[fileExtension].split('/')
      }

      // if all fails, well, return empty
      return emptyFileType
    })
    .catch(() => {
      return emptyFileType
    })

    // if (file.type) {
    //   return Promise.resolve(file.type.split('/'))
    // }
    // return mime.lookup(file.name)
    // return file.type ? file.type.split('/') : ['', '']
}

// TODO Check which types are actually supported in browsers. Chrome likes webm
// from my testing, but we may need more.
// We could use a library but they tend to contain dozens of KBs of mappings,
// most of which will go unused, so not sure if that's worth it.
const mimeToExtensions = {
  'video/ogg': 'ogv',
  'audio/ogg': 'ogg',
  'video/webm': 'webm',
  'audio/webm': 'webm',
  'video/mp4': 'mp4',
  'audio/mp3': 'mp3'
}

function getFileTypeExtension (mimeType) {
  return mimeToExtensions[mimeType] || null
}

// returns [fileName, fileExt]
function getFileNameAndExtension (fullFileName) {
  var re = /(?:\.([^.]+))?$/
  var fileExt = re.exec(fullFileName)[1]
  var fileName = fullFileName.replace('.' + fileExt, '')
  return [fileName, fileExt]
}

function supportsMediaRecorder () {
  return typeof MediaRecorder === 'function' && !!MediaRecorder.prototype &&
    typeof MediaRecorder.prototype.start === 'function'
}

/**
 * Check if a URL string is an object URL from `URL.createObjectURL`.
 *
 * @param {string} url
 * @return {boolean}
 */
function isObjectURL (url) {
  return url.indexOf('blob:') === 0
}

function getProportionalHeight (img, width) {
  const aspect = img.width / img.height
  return Math.round(width / aspect)
}

/**
 * Create a thumbnail for the given Uppy file object.
 *
 * @param {{data: Blob}} file
 * @param {number} width
 * @return {Promise}
 */
function createThumbnail (file, targetWidth) {
  const originalUrl = URL.createObjectURL(file.data)

  const onload = new Promise((resolve, reject) => {
    const image = new Image()
    image.src = originalUrl
    image.onload = () => {
      URL.revokeObjectURL(originalUrl)
      resolve(image)
    }
    image.onerror = () => {
      // The onerror event is totally useless unfortunately, as far as I know
      URL.revokeObjectURL(originalUrl)
      reject(new Error('Could not create thumbnail'))
    }
  })

  return onload.then((image) => {
    const targetHeight = getProportionalHeight(image, targetWidth)
    const canvas = resizeImage(image, targetWidth, targetHeight)
    return canvasToBlob(canvas, 'image/jpeg')
  }).then((blob) => {
    return URL.createObjectURL(blob)
  })
}

/**
 * Resize an image to the target `width` and `height`.
 *
 * Returns a Canvas with the resized image on it.
 */
function resizeImage (image, targetWidth, targetHeight) {
  let sourceWidth = image.width
  let sourceHeight = image.height

  if (targetHeight < image.height / 2) {
    const steps = Math.floor(Math.log(image.width / targetWidth) / Math.log(2))
    const stepScaled = downScaleInSteps(image, steps)
    image = stepScaled.image
    sourceWidth = stepScaled.sourceWidth
    sourceHeight = stepScaled.sourceHeight
  }

  const canvas = document.createElement('canvas')
  canvas.width = targetWidth
  canvas.height = targetHeight

  const context = canvas.getContext('2d')
  context.drawImage(image,
    0, 0, sourceWidth, sourceHeight,
    0, 0, targetWidth, targetHeight)

  return canvas
}

/**
 * Downscale an image by 50% `steps` times.
 */
function downScaleInSteps (image, steps) {
  let source = image
  let currentWidth = source.width
  let currentHeight = source.height

  const canvas = document.createElement('canvas')
  const context = canvas.getContext('2d')
  canvas.width = currentWidth / 2
  canvas.height = currentHeight / 2

  for (let i = 0; i < steps; i += 1) {
    context.drawImage(source,
      // The entire source image. We pass width and height here,
      // because we reuse this canvas, and should only scale down
      // the part of the canvas that contains the previous scale step.
      0, 0, currentWidth, currentHeight,
      // Draw to 50% size
      0, 0, currentWidth / 2, currentHeight / 2)
    currentWidth /= 2
    currentHeight /= 2
    source = canvas
  }

  return {
    image: canvas,
    sourceWidth: currentWidth,
    sourceHeight: currentHeight
  }
}

/**
 * Save a <canvas> element's content to a Blob object.
 *
 * @param {HTMLCanvasElement} canvas
 * @return {Promise}
 */
function canvasToBlob (canvas, type, quality) {
  if (canvas.toBlob) {
    return new Promise((resolve) => {
      canvas.toBlob(resolve, type, quality)
    })
  }
  return Promise.resolve().then(() => {
    return dataURItoBlob(canvas.toDataURL(type, quality), {})
  })
}

function dataURItoBlob (dataURI, opts, toFile) {
  // get the base64 data
  var data = dataURI.split(',')[1]

  // user may provide mime type, if not get it from data URI
  var mimeType = opts.mimeType || dataURI.split(',')[0].split(':')[1].split(';')[0]

  // default to plain/text if data URI has no mimeType
  if (mimeType == null) {
    mimeType = 'plain/text'
  }

  var binary = atob(data)
  var array = []
  for (var i = 0; i < binary.length; i++) {
    array.push(binary.charCodeAt(i))
  }

  // Convert to a File?
  if (toFile) {
    return new File([new Uint8Array(array)], opts.name || '', {type: mimeType})
  }

  return new Blob([new Uint8Array(array)], {type: mimeType})
}

function dataURItoFile (dataURI, opts) {
  return dataURItoBlob(dataURI, opts, true)
}

/**
 * Copies text to clipboard by creating an almost invisible textarea,
 * adding text there, then running execCommand('copy').
 * Falls back to prompt() when the easy way fails (hello, Safari!)
 * From http://stackoverflow.com/a/30810322
 *
 * @param {String} textToCopy
 * @param {String} fallbackString
 * @return {Promise}
 */
function copyToClipboard (textToCopy, fallbackString) {
  fallbackString = fallbackString || 'Copy the URL below'

  return new Promise((resolve, reject) => {
    const textArea = document.createElement('textarea')
    textArea.setAttribute('style', {
      position: 'fixed',
      top: 0,
      left: 0,
      width: '2em',
      height: '2em',
      padding: 0,
      border: 'none',
      outline: 'none',
      boxShadow: 'none',
      background: 'transparent'
    })

    textArea.value = textToCopy
    document.body.appendChild(textArea)
    textArea.select()

    const magicCopyFailed = (err) => {
      document.body.removeChild(textArea)
      window.prompt(fallbackString, textToCopy)
      return reject('Oops, unable to copy displayed fallback prompt: ' + err)
    }

    try {
      const successful = document.execCommand('copy')
      if (!successful) {
        return magicCopyFailed('copy command unavailable')
      }
      document.body.removeChild(textArea)
      return resolve()
    } catch (err) {
      document.body.removeChild(textArea)
      return magicCopyFailed(err)
    }
  })
}

function getSpeed (fileProgress) {
  if (!fileProgress.bytesUploaded) return 0

  const timeElapsed = (new Date()) - fileProgress.uploadStarted
  const uploadSpeed = fileProgress.bytesUploaded / (timeElapsed / 1000)
  return uploadSpeed
}

function getBytesRemaining (fileProgress) {
  return fileProgress.bytesTotal - fileProgress.bytesUploaded
}

function getETA (fileProgress) {
  if (!fileProgress.bytesUploaded) return 0

  const uploadSpeed = getSpeed(fileProgress)
  const bytesRemaining = getBytesRemaining(fileProgress)
  const secondsRemaining = Math.round(bytesRemaining / uploadSpeed * 10) / 10

  return secondsRemaining
}

function prettyETA (seconds) {
  const time = secondsToTime(seconds)

  // Only display hours and minutes if they are greater than 0 but always
  // display minutes if hours is being displayed
  // Display a leading zero if the there is a preceding unit: 1m 05s, but 5s
  const hoursStr = time.hours ? time.hours + 'h ' : ''
  const minutesVal = time.hours ? ('0' + time.minutes).substr(-2) : time.minutes
  const minutesStr = minutesVal ? minutesVal + 'm ' : ''
  const secondsVal = minutesVal ? ('0' + time.seconds).substr(-2) : time.seconds
  const secondsStr = secondsVal + 's'

  return `${hoursStr}${minutesStr}${secondsStr}`
}

/**
 * Check if an object is a DOM element. Duck-typing based on `nodeType`.
 *
 * @param {*} obj
 */
function isDOMElement (obj) {
  return obj && typeof obj === 'object' && obj.nodeType === Node.ELEMENT_NODE
}

/**
 * Find a DOM element.
 *
 * @param {Node|string} element
 * @return {Node|null}
    const elements = [].slice.call(document.querySelectorAll(element))
    if (elements.length > 1) {
      return elements
    }
    return elements[0]
    // return document.querySelector(element)
 */
function findDOMElement (element) {
  if (typeof element === 'string') {
    return document.querySelector(element)
  }

  if (typeof element === 'object' && isDOMElement(element)) {
    return element
  }
}

function getSocketHost (url) {
  // get the host domain
  var regex = /^(?:https?:\/\/|\/\/)?(?:[^@\n]+@)?(?:www\.)?([^\n]+)/
  var host = regex.exec(url)[1]
  var socketProtocol = location.protocol === 'https:' ? 'wss' : 'ws'

  return `${socketProtocol}://${host}`
}

function _emitSocketProgress (uploader, progressData, file) {
  const {progress, bytesUploaded, bytesTotal} = progressData
  if (progress) {
    uploader.core.log(`Upload progress: ${progress}`)
    uploader.core.emitter.emit('core:upload-progress', {
      uploader,
      id: file.id,
      bytesUploaded: bytesUploaded,
      bytesTotal: bytesTotal
    })
  }
}

const emitSocketProgress = throttle(_emitSocketProgress, 300, {leading: true, trailing: true})

module.exports = {
  generateFileID,
  toArray,
  every,
  flatten,
  groupBy,
  extend,
  runPromiseSequence,
  supportsMediaRecorder,
  isTouchDevice,
  getFileNameAndExtension,
  truncateString,
  getFileTypeExtension,
  getFileType,
  getArrayBuffer,
  isPreviewSupported,
  isObjectURL,
  createThumbnail,
  secondsToTime,
  dataURItoBlob,
  dataURItoFile,
  getSpeed,
  getBytesRemaining,
  getETA,
  copyToClipboard,
  prettyETA,
  findDOMElement,
  getSocketHost,
  emitSocketProgress
}
",Changing this function means other cases (like a `target` option) where the selector exists more than once on the page also return an array. Maybe better to have a separate function that always returns an array?,"The suggested code changes appear to be a collection of utility functions designed to assist with various tasks related to DOM manipulation, file handling, and network operations. Here's a concise review of the changes:

### Key Changes:
1. **Imports**: 
   - The `throttle` function from the `lodash.throttle` package is imported.
   - The `file-type` module is imported directly from a local vendor directory.

2. **Utility Functions**:
   - **Flatten**: A function to flatten nested arrays.
   - **isTouchDevice**: A function to detect touch devices.
   - **truncateString**: A function to truncate strings.
   - **secondsToTime**: A function to convert seconds to hours, minutes, and seconds.
   - **groupBy**: A function to partition an array by a grouping function.
   - **every**: A function to test if every array element passes a predicate.
   - **toArray**: A function to convert a list into an array.
   - **generateFileID**: A function to generate a file ID from a file name.
   - **extend**: A function to merge objects.
   - **runPromiseSequence**: A function to run an array of promise-returning functions in sequence.
   - **supportsMediaRecorder**: A function to check if the browser supports the `MediaRecorder` API.
   - **isObjectURL**: A function to check if a URL is an object URL created by `URL.createObjectURL`.
   - **createThumbnail**: A function to create a thumbnail for a file.
   - **resizeImage**: A function to resize an image to a target width and height.
   - **downScaleInSteps**: A function to downscale an image by 50% in multiple steps.
   - **canvasToBlob**: A function to convert a canvas to a Blob object.
   - **dataURItoBlob**: A function to convert a data URI to a Blob object.
   - **dataURItoFile**: A function to convert a data URI to a File object.
   - **copyToClipboard**: A function to copy text to the clipboard.
   - **getSpeed**: A function to calculate the upload speed.
   - **getBytesRemaining**: A function to calculate the bytes remaining to upload.
   - **getETA**: A function to calculate the estimated time of arrival.
   - **prettyETA**: A function to format the estimated time of arrival as a human-readable string.
   - **isDOMElement**: A function to check if an object is a"
35,"@@ -29,7 +29,7 @@ namespace Ethereum.VM.Test
     {
         private static readonly Dictionary<string, AbiType> TypesByName = new Dictionary<string, AbiType>
         {
-            {""uint256"", AbiType.UInt},
+            {""uint256"", AbiType.UInt256},
             {""uint32[]"", new AbiArray(new AbiUInt(32))},
             {""bytes10"", new AbiBytes(10)},
             {""bytes"", AbiType.DynamicBytes},","﻿/*
 * Copyright (c) 2018 Demerzel Solutions Limited
 * This file is part of the Nethermind library.
 *
 * The Nethermind library is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Lesser General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * The Nethermind library is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 * GNU Lesser General Public License for more details.
 *
 * You should have received a copy of the GNU Lesser General Public License
 * along with the Nethermind. If not, see <http://www.gnu.org/licenses/>.
 */

using System.Collections.Generic;
using System.Linq;
using Ethereum.Test.Base;
using Nethermind.Abi;
using Nethermind.Core.Extensions;
using NUnit.Framework;

namespace Ethereum.VM.Test
{
    internal class AbiTests
            {""uint256"", AbiType.UInt256},
    {
        private static readonly Dictionary<string, AbiType> TypesByName = new Dictionary<string, AbiType>
        {
            {""uint256"", AbiType.UInt},
            {""uint32[]"", new AbiArray(new AbiUInt(32))},
            {""bytes10"", new AbiBytes(10)},
            {""bytes"", AbiType.DynamicBytes},
            {""address"", AbiType.Address}
        };

        private static AbiType ToAbiType(string typeName)
        {
            return TypesByName[typeName];
        }

        private static AbiTest Convert(string name, AbiTestJson testJson)
        {
            AbiTest test = new AbiTest();
            test.Name = name;
            test.Result = Bytes.FromHexString(testJson.Result);
            test.Types = testJson.Types.Select(ToAbiType).ToArray();
            test.Args = testJson.Args.Select(TestLoader.PrepareInput).ToArray();
            return test;
        }

        private static IEnumerable<AbiTest> LoadBasicAbiTests()
        {
            IEnumerable<AbiTest> tests = TestLoader.LoadFromFile<Dictionary<string, AbiTestJson>, AbiTest>(
                ""basic_abi_tests.json"",
                allTests => allTests.Select(namedTest => Convert(namedTest.Key, namedTest.Value)));
            return tests;
        }

        [TestCaseSource(nameof(LoadBasicAbiTests))]
        public void Test(AbiTest abiTest)
        {
            AbiEncoder encoder = new AbiEncoder();
            AbiSignature signature = new AbiSignature(abiTest.Name, abiTest.Types);
            byte[] encoded = encoder.Encode(AbiEncodingStyle.IncludeSignature, signature, abiTest.Args).Slice(4);
            Assert.True(Bytes.AreEqual(abiTest.Result, encoded));
        }

        public class AbiTestJson
        {
            public object[] Args { get; set; }
            public string Result { get; set; }
            public string[] Types { get; set; }
        }

        public class AbiTest
        {
            public string Name { get; set; }
            public object[] Args { get; set; }
            public byte[] Result { get; set; }
            public AbiType[] Types { get; set; }

            public override string ToString()
            {
                return Name;
            }
        }
    }
}","UInt in Solidity is uint256, that was correct, it is not a C# uint","The suggested code change appears to be a test suite for the `AbiEncoder` class in the Nethermind library. It loads a set of ABI tests from a JSON file, converts them into `AbiTest` objects, and then verifies that the encoding of the arguments matches the expected result using the `AbiEncoder`.

Here's a concise review of the suggested code change:

1. **Imports and Namespace**: The code imports necessary namespaces and defines a namespace for the test.

2. **Constants and Methods**:
   - `TypesByName`: A dictionary mapping ABI type names to their corresponding `AbiType` instances.
   - `ToAbiType`: Converts a string type name to an `AbiType`.
   - `Convert`: Converts a test JSON object into an `AbiTest` object.
   - `LoadBasicAbiTests`: Loads ABI tests from a JSON file and converts them into `AbiTest` objects.

3. **Test Method**:
   - `Test`: Tests the `AbiEncoder` by encoding the arguments of each test and comparing the result to the expected result.

4. **Classes**:
   - `AbiTestJson`: Represents the structure of a test case in the JSON file.
   - `AbiTest`: Represents a single test case with its name, arguments, result, and types.

Overall, the code provides a comprehensive test suite for the `AbiEncoder` class, ensuring that it correctly encodes input arguments according to the specified ABI types."
36,"@@ -11,7 +11,12 @@ import (
 	""github.com/aws/copilot-cli/internal/pkg/manifest""
 )
 
-// Validation errors when rendering manifest into template.
+// container dependency status constants.
+const (
+	dependsOnStart    = ""start""
+	dependsOnComplete = ""complete""
+	dependsOnSuccess  = ""success""
+)
 
 // Empty field errors.
 var (","// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0

package stack

import (
	""errors""
	""fmt""

	""github.com/aws/aws-sdk-go/aws""
// container dependency status constants.
const (
	dependsOnStart    = ""start""
	dependsOnComplete = ""complete""
	dependsOnSuccess  = ""success""
)
	""github.com/aws/copilot-cli/internal/pkg/manifest""
)

// Validation errors when rendering manifest into template.

// Empty field errors.
var (
	errNoFSID          = errors.New(""volume field `efs.id` cannot be empty"")
	errNoContainerPath = errors.New(""`path` cannot be empty"")
	errNoSourceVolume  = errors.New(""`source_volume` cannot be empty"")
	errEmptyEFSConfig  = errors.New(""bad EFS configuration: `efs` cannot be empty"")
)

// Conditional errors.
var (
	errAccessPointWithRootDirectory = errors.New(""`root_directory` must be empty or \""/\"" when `access_point` is specified"")
	errAccessPointWithoutIAM        = errors.New(""`iam` must be true when `access_point` is specified"")
	errUIDWithNonManagedFS          = errors.New(""UID and GID cannot be specified with non-managed EFS"")
	errInvalidUIDGIDConfig          = errors.New(""must specify both UID and GID, or neither"")
	errInvalidEFSConfig             = errors.New(""bad EFS configuration: cannot specify both bool and config"")
	errReservedUID                  = errors.New(""UID must not be 0"")
)

// Validate that paths contain only an approved set of characters to guard against command injection.
// We can accept 0-9A-Za-z-_.
func validatePath(input string, maxLength int) error {
	if len(input) > maxLength {
		return fmt.Errorf(""path must be less than %d bytes in length"", maxLength)
	}
	if len(input) == 0 {
		return nil
	}
	m := pathRegexp.FindStringSubmatch(input)
	if len(m) == 0 {
		return fmt.Errorf(""paths can only contain the characters a-zA-Z0-9.-_/"")
	}
	return nil
}

func validateStorageConfig(in *manifest.Storage) error {
	if in == nil {
		return nil
	}
	return validateVolumes(in.Volumes)
}

func validateVolumes(in map[string]manifest.Volume) error {
	for name, v := range in {
		if err := validateVolume(name, v); err != nil {
			return err
		}
	}
	return nil
}

func validateVolume(name string, in manifest.Volume) error {
	if err := validateMountPointConfig(in); err != nil {
		return fmt.Errorf(""validate container configuration for volume %s: %w"", name, err)
	}
	if err := validateEFSConfig(in); err != nil {
		return fmt.Errorf(""validate EFS configuration for volume %s: %w"", name, err)
	}
	return nil
}

func validateMountPointConfig(in manifest.Volume) error {
	// containerPath must be specified.
	path := aws.StringValue(in.ContainerPath)
	if path == """" {
		return errNoContainerPath
	}
	if err := validateContainerPath(path); err != nil {
		return fmt.Errorf(""validate container path %s: %w"", path, err)
	}
	return nil
}

func validateSidecarMountPoints(in []manifest.SidecarMountPoint) error {
	if in == nil {
		return nil
	}
	for _, mp := range in {
		if aws.StringValue(mp.ContainerPath) == """" {
			return errNoContainerPath
		}
		if aws.StringValue(mp.SourceVolume) == """" {
			return errNoSourceVolume
		}
	}
	return nil
}

func validateEFSConfig(in manifest.Volume) error {
	// EFS is implicitly disabled. We don't use the attached EmptyVolume function here
	// because it may hide invalid config.
	if in.EFS == nil {
		return nil
	}

	// EFS cannot have both Enabled and nonempty Advanced config.
	if aws.BoolValue(in.EFS.Enabled) && !in.EFS.Advanced.IsEmpty() {
		return errInvalidEFSConfig
	}

	// EFS can be disabled explicitly.
	if in.EFS.Disabled() {
		return nil
	}

	// EFS cannot be an empty map.
	if in.EFS.Enabled == nil && in.EFS.Advanced.IsEmpty() {
		return errEmptyEFSConfig
	}

	// UID and GID are mutually exclusive with any other fields.
	if !(in.EFS.Advanced.EmptyBYOConfig() || in.EFS.Advanced.EmptyUIDConfig()) {
		return errUIDWithNonManagedFS
	}

	// Check that required fields for BYO EFS are satisfied.
	if !in.EFS.Advanced.EmptyBYOConfig() && !in.EFS.Advanced.IsEmpty() {
		if aws.StringValue(in.EFS.Advanced.FileSystemID) == """" {
			return errNoFSID
		}
	}

	if err := validateRootDirPath(aws.StringValue(in.EFS.Advanced.RootDirectory)); err != nil {
		return err
	}

	if err := validateAuthConfig(in.EFS.Advanced); err != nil {
		return err
	}

	if err := validateUIDGID(in.EFS.Advanced.UID, in.EFS.Advanced.GID); err != nil {
		return err
	}

	return nil
}

func validateAuthConfig(in manifest.EFSVolumeConfiguration) error {
	if in.AuthConfig == nil {
		return nil
	}
	rd := aws.StringValue(in.RootDirectory)
	if !(rd == """" || rd == ""/"") && in.AuthConfig.AccessPointID != nil {
		return errAccessPointWithRootDirectory
	}

	if in.AuthConfig.AccessPointID != nil && !aws.BoolValue(in.AuthConfig.IAM) {
		return errAccessPointWithoutIAM
	}

	return nil
}

func validateUIDGID(uid, gid *uint32) error {
	if uid == nil && gid == nil {
		return nil
	}
	if uid != nil && gid == nil {
		return errInvalidUIDGIDConfig
	}
	if uid == nil && gid != nil {
		return errInvalidUIDGIDConfig
	}
	// Check for root UID.
	if aws.Uint32Value(uid) == 0 {
		return errReservedUID
	}
	return nil
}

func validateRootDirPath(input string) error {
	return validatePath(input, maxEFSPathLength)
}

func validateContainerPath(input string) error {
	return validatePath(input, maxDockerContainerPathLength)
}
","What happens if someone puts ""START"" in their manifest, instead of ""start""? Does Cloudformation accept uppercase?","The suggested code changes appear to be focused on validating various configurations related to storage and EFS volumes in a Docker container environment. Here's a concise review of the changes:

1. **Constants**: Added constants for dependency statuses (`dependsOnStart`, `dependsOnComplete`, `dependsOnSuccess`) which seem to be used in some validation logic but are not currently utilized.

2. **Error Definitions**: Defined several error constants for different validation scenarios, such as empty fields, conditional errors, and invalid configurations. This helps in clearly identifying and handling specific issues during validation.

3. **Validation Functions**:
   - `validatePath`: Ensures that paths do not exceed a certain length and contain only allowed characters.
   - `validateStorageConfig`, `validateVolumes`, and `validateVolume`: These functions recursively validate the configuration of storage volumes, including EFS volumes.
   - `validateMountPointConfig`: Validates that container paths are specified and valid.
   - `validateSidecarMountPoints`: Validates sidecar mount points.
   - `validateEFSConfig`: Validates EFS-specific configurations, ensuring consistency and mutual exclusivity of fields.
   - `validateAuthConfig`: Validates authentication settings for EFS volumes.
   - `validateUIDGID`: Validates UID and GID configurations, ensuring they are either both specified or neither.
   - `validateRootDirPath` and `validateContainerPath`: Helper functions to validate specific types of paths.

4. **Comments**: The code includes comments explaining the purpose of each function and validation step, which enhances readability and maintainability.

Overall, these changes improve the robustness and clarity of the validation logic, making it easier to identify and fix issues related to storage and EFS configurations in Docker containers."
37,"@@ -0,0 +1,19 @@
+package kubernetes
+
+import (
+	v1 ""k8s.io/api/core/v1""
+)
+
+// CheckPodsRunning returns true if the number of pods is equal to expected pods and all pods are in running state
+func CheckPodsRunning(pods v1.PodList, expectedPods int) bool {
+	if len(pods.Items) < expectedPods {
+		return false
+	}
+
+	for _, pod := range pods.Items {
+		if pod.Status.Phase != ""Running"" {
+			return false
+		}
+	}
+	return true
+}","package kubernetes

import (
	v1 ""k8s.io/api/core/v1""
)

// CheckPodsRunning returns true if the number of pods is equal to expected pods and all pods are in running state
func CheckPodsRunning(pods v1.PodList, expectedPods int) bool {
	if len(pods.Items) < expectedPods {
		return false
	}

	for _, pod := range pods.Items {
		if pod.Status.Phase != ""Running"" {
			return false
		}
	}
	return true
}

",any suggestions for improving this? How about creating predicate functions and testing against that ?,"The suggested code change is already quite concise and efficient for checking if the number of pods matches the expected count and if all pods are in the ""Running"" state. Here's a brief review:

- **Function Name**: `CheckPodsRunning` is a clear and descriptive name that indicates its purpose.
- **Parameters**:
  - `pods`: A `v1.PodList` object containing the list of pods to check.
  - `expectedPods`: An integer representing the expected number of pods.
- **Logic**:
  - The function first checks if the length of the `Items` slice in the `PodList` is less than the `expectedPods`. If so, it returns `false`.
  - It then iterates over each pod in the list and checks if its status phase is ""Running"". If any pod is not in the ""Running"" state, it returns `false`.
  - If all pods meet the criteria, it returns `true`.

Overall, the code is well-structured and performs the required checks efficiently."
38,"@@ -86,6 +86,16 @@ def _column_op(f):
             args = [arg._scol if isinstance(arg, IndexOpsMixin) else arg for arg in args]
             scol = f(self._scol, *args)
             scol = booleanize_null(self._scol, scol, f)
+            # PySpark and pandas have a different way to calculate modulo operation.
+            # Below lines are needed for closing the gap.
+            if f is spark.Column.__mod__:
+                scol = F.when((self._scol * args[0] < 0) & (scol != 0), scol + args[0]).otherwise(
+                    scol
+                )
+            elif f is spark.Column.__rmod__:
+                scol = F.when(
+                    (self._scol * args[0] < 0) & (scol != 0), scol + self._scol
+                ).otherwise(scol)
 
             return self._with_new_scol(scol)
         else:","#
# Copyright (C) 2019 Databricks, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

""""""
Base and utility classes for Koalas objects.
""""""
from collections import OrderedDict
from functools import wraps, partial
from typing import Union, Callable, Any

import numpy as np
import pandas as pd
from pandas.api.types import is_list_like
from pyspark import sql as spark
from pyspark.sql import functions as F, Window
from pyspark.sql.types import DoubleType, FloatType, LongType, StringType, TimestampType

from databricks import koalas as ks  # For running doctests and reference resolution in PyCharm.
from databricks.koalas import numpy_compat
from databricks.koalas.internal import (
    _InternalFrame,
    NATURAL_ORDER_COLUMN_NAME,
    SPARK_DEFAULT_INDEX_NAME,
)
from databricks.koalas.typedef import pandas_wraps, spark_type_to_pandas_dtype
from databricks.koalas.utils import align_diff_series, scol_for, validate_axis
from databricks.koalas.frame import DataFrame


def booleanize_null(left_scol, scol, f):
    """"""
    Booleanize Null in Spark Column
    """"""
    comp_ops = [
        getattr(spark.Column, ""__{}__"".format(comp_op))
        for comp_op in [""eq"", ""ne"", ""lt"", ""le"", ""ge"", ""gt""]
    ]

    if f in comp_ops:
        # if `f` is ""!="", fill null with True otherwise False
        filler = f == spark.Column.__ne__
        scol = F.when(scol.isNull(), filler).otherwise(scol)

    elif f == spark.Column.__or__:
        scol = F.when(left_scol.isNull() | scol.isNull(), False).otherwise(scol)

    elif f == spark.Column.__and__:
        scol = F.when(scol.isNull(), False).otherwise(scol)

    return scol


def _column_op(f):
    """"""
    A decorator that wraps APIs taking/returning Spark Column so that Koalas Series can be
    supported too. If this decorator is used for the `f` function that takes Spark Column and
    returns Spark Column, decorated `f` takes Koalas Series as well and returns Koalas
    Series.

    :param f: a function that takes Spark Column and returns Spark Column.
    :param self: Koalas Series
    :param args: arguments that the function `f` takes.
    """"""

    @wraps(f)
    def wrapper(self, *args):
        # It is possible for the function `f` takes other arguments than Spark Column.
        # To cover this case, explicitly check if the argument is Koalas Series and
        # extract Spark Column. For other arguments, they are used as are.
        cols = [arg for arg in args if isinstance(arg, IndexOpsMixin)]
        if all(self._kdf is col._kdf for col in cols):
            # Same DataFrame anchors
            # PySpark and pandas have a different way to calculate modulo operation.
            # Below lines are needed for closing the gap.
            if f is spark.Column.__mod__:
                scol = F.when((self._scol * args[0] < 0) & (scol != 0), scol + args[0]).otherwise(
                    scol
                )
            elif f is spark.Column.__rmod__:
                scol = F.when(
                    (self._scol * args[0] < 0) & (scol != 0), scol + self._scol
                ).otherwise(scol)
            args = [arg._scol if isinstance(arg, IndexOpsMixin) else arg for arg in args]
            scol = f(self._scol, *args)
            scol = booleanize_null(self._scol, scol, f)

            return self._with_new_scol(scol)
        else:
            # Different DataFrame anchors
            def apply_func(this_column, *that_columns):
                scol = f(this_column, *that_columns)
                return booleanize_null(this_column, scol, f)

            return align_diff_series(apply_func, self, *args, how=""full"")

    return wrapper


def _numpy_column_op(f):
    @wraps(f)
    def wrapper(self, *args):
        # PySpark does not support NumPy type out of the box. For now, we convert NumPy types
        # into some primitive types understandable in PySpark.
        new_args = []
        for arg in args:
            # TODO: This is a quick hack to support NumPy type. We should revisit this.
            if isinstance(self.spark_type, LongType) and isinstance(arg, np.timedelta64):
                new_args.append(float(arg / np.timedelta64(1, ""s"")))
            else:
                new_args.append(arg)
        return _column_op(f)(self, *new_args)

    return wrapper


def _wrap_accessor_spark(accessor, fn, return_type=None):
    """"""
    Wrap an accessor property or method, e.g., Series.dt.date with a spark function.
    """"""
    if return_type:
        return _column_op(lambda col: fn(col).cast(return_type))(accessor._data)
    else:
        return _column_op(fn)(accessor._data)


def _wrap_accessor_pandas(accessor, fn, return_type):
    """"""
    Wrap an accessor property or method, e.g, Series.dt.date with a pandas function.
    """"""
    return pandas_wraps(fn, return_col=return_type)(accessor._data)


class IndexOpsMixin(object):
    """"""common ops mixin to support a unified interface / docs for Series / Index

    Assuming there are following attributes or properties and function.

    :ivar _scol: Spark Column instance
    :type _scol: pyspark.Column
    :ivar _kdf: Parent's Koalas DataFrame
    :type _kdf: ks.DataFrame

    :ivar spark_type: Spark data type
    :type spark_type: spark.types.DataType
    """"""

    def __init__(self, internal: _InternalFrame, kdf):
        assert internal is not None
        assert kdf is not None and isinstance(kdf, DataFrame)
        self._internal = internal  # type: _InternalFrame
        self._kdf = kdf

    @property
    def _scol(self):
        return self._internal.spark_column

    # arithmetic operators
    __neg__ = _column_op(spark.Column.__neg__)

    def __add__(self, other):
        if isinstance(self.spark_type, StringType):
            # Concatenate string columns
            if isinstance(other, IndexOpsMixin) and isinstance(other.spark_type, StringType):
                return _column_op(F.concat)(self, other)
            # Handle df['col'] + 'literal'
            elif isinstance(other, str):
                return _column_op(F.concat)(self, F.lit(other))
            else:
                raise TypeError(""string addition can only be applied to string series or literals."")
        else:
            return _column_op(spark.Column.__add__)(self, other)

    def __sub__(self, other):
        # Note that timestamp subtraction casts arguments to integer. This is to mimic Pandas's
        # behaviors. Pandas returns 'timedelta64[ns]' from 'datetime64[ns]'s subtraction.
        if isinstance(other, IndexOpsMixin) and isinstance(self.spark_type, TimestampType):
            if not isinstance(other.spark_type, TimestampType):
                raise TypeError(""datetime subtraction can only be applied to datetime series."")
            return self.astype(""bigint"") - other.astype(""bigint"")
        else:
            return _column_op(spark.Column.__sub__)(self, other)

    __mul__ = _column_op(spark.Column.__mul__)
    __div__ = _numpy_column_op(spark.Column.__div__)
    __truediv__ = _numpy_column_op(spark.Column.__truediv__)
    __mod__ = _column_op(spark.Column.__mod__)

    def __radd__(self, other):
        # Handle 'literal' + df['col']
        if isinstance(self.spark_type, StringType) and isinstance(other, str):
            return self._with_new_scol(F.concat(F.lit(other), self._scol))
        else:
            return _column_op(spark.Column.__radd__)(self, other)

    __rsub__ = _column_op(spark.Column.__rsub__)
    __rmul__ = _column_op(spark.Column.__rmul__)
    __rdiv__ = _numpy_column_op(spark.Column.__rdiv__)
    __rtruediv__ = _numpy_column_op(spark.Column.__rtruediv__)

    def __floordiv__(self, other):
        return self._with_new_scol(
            F.floor(_numpy_column_op(spark.Column.__div__)(self, other)._scol)
        )

    def __rfloordiv__(self, other):
        return self._with_new_scol(
            F.floor(_numpy_column_op(spark.Column.__rdiv__)(self, other)._scol)
        )

    __rmod__ = _column_op(spark.Column.__rmod__)
    __pow__ = _column_op(spark.Column.__pow__)
    __rpow__ = _column_op(spark.Column.__rpow__)

    # comparison operators
    __eq__ = _column_op(spark.Column.__eq__)
    __ne__ = _column_op(spark.Column.__ne__)
    __lt__ = _column_op(spark.Column.__lt__)
    __le__ = _column_op(spark.Column.__le__)
    __ge__ = _column_op(spark.Column.__ge__)
    __gt__ = _column_op(spark.Column.__gt__)

    # `and`, `or`, `not` cannot be overloaded in Python,
    # so use bitwise operators as boolean operators
    __and__ = _column_op(spark.Column.__and__)
    __or__ = _column_op(spark.Column.__or__)
    __invert__ = _column_op(spark.Column.__invert__)
    __rand__ = _column_op(spark.Column.__rand__)
    __ror__ = _column_op(spark.Column.__ror__)

    # NDArray Compat
    def __array_ufunc__(self, ufunc: Callable, method: str, *inputs: Any, **kwargs: Any):
        # Try dunder methods first.
        result = numpy_compat.maybe_dispatch_ufunc_to_dunder_op(
            self, ufunc, method, *inputs, **kwargs
        )

        # After that, we try with PySpark APIs.
        if result is NotImplemented:
            result = numpy_compat.maybe_dispatch_ufunc_to_spark_func(
                self, ufunc, method, *inputs, **kwargs
            )

        if result is not NotImplemented:
            return result
        else:
            # TODO: support more APIs?
            raise NotImplementedError(""Koalas objects currently do not support %s."" % ufunc)

    @property
    def dtype(self):
        """"""Return the dtype object of the underlying data.

        Examples
        --------
        >>> s = ks.Series([1, 2, 3])
        >>> s.dtype
        dtype('int64')

        >>> s = ks.Series(list('abc'))
        >>> s.dtype
        dtype('O')

        >>> s = ks.Series(pd.date_range('20130101', periods=3))
        >>> s.dtype
        dtype('<M8[ns]')

        >>> s.rename(""a"").to_frame().set_index(""a"").index.dtype
        dtype('<M8[ns]')
        """"""
        return spark_type_to_pandas_dtype(self.spark_type)

    @property
    def empty(self):
        """"""
        Returns true if the current object is empty. Otherwise, returns false.

        >>> ks.range(10).id.empty
        False

        >>> ks.range(0).id.empty
        True

        >>> ks.DataFrame({}, index=list('abc')).index.empty
        False
        """"""
        return self._internal._sdf.rdd.isEmpty()

    @property
    def hasnans(self):
        """"""
        Return True if it has any missing values. Otherwise, it returns False.

        >>> ks.DataFrame({}, index=list('abc')).index.hasnans
        False

        >>> ks.Series(['a', None]).hasnans
        True

        >>> ks.Series([1.0, 2.0, np.nan]).hasnans
        True

        >>> ks.Series([1, 2, 3]).hasnans
        False

        >>> ks.Series([1, 2, 3]).rename(""a"").to_frame().set_index(""a"").index.hasnans
        False
        """"""
        sdf = self._internal._sdf.select(self._scol)
        col = self._scol

        ret = sdf.select(F.max(col.isNull() | F.isnan(col))).collect()[0][0]
        return ret

    @property
    def is_monotonic(self):
        """"""
        Return boolean if values in the object are monotonically increasing.

        .. note:: the current implementation of is_monotonic requires to shuffle
            and aggregate multiple times to check the order locally and globally,
            which is potentially expensive. In case of multi-index, all data are
            transferred to single node which can easily cause out-of-memory error currently.

        Returns
        -------
        is_monotonic : boolean

        Examples
        --------
        >>> ser = ks.Series(['1/1/2018', '3/1/2018', '4/1/2018'])
        >>> ser.is_monotonic
        True

        >>> df = ks.DataFrame({'dates': [None, '1/1/2018', '2/1/2018', '3/1/2018']})
        >>> df.dates.is_monotonic
        False

        >>> df.index.is_monotonic
        True

        >>> ser = ks.Series([1])
        >>> ser.is_monotonic
        True

        >>> ser = ks.Series([])
        >>> ser.is_monotonic
        True

        >>> ser.rename(""a"").to_frame().set_index(""a"").index.is_monotonic
        True

        >>> ser = ks.Series([5, 4, 3, 2, 1], index=[1, 2, 3, 4, 5])
        >>> ser.is_monotonic
        False

        >>> ser.index.is_monotonic
        True

        Support for MultiIndex

        >>> midx = ks.MultiIndex.from_tuples(
        ... [('x', 'a'), ('x', 'b'), ('y', 'c'), ('y', 'd'), ('z', 'e')])
        >>> midx  # doctest: +SKIP
        MultiIndex([('x', 'a'),
                    ('x', 'b'),
                    ('y', 'c'),
                    ('y', 'd'),
                    ('z', 'e')],
                   )
        >>> midx.is_monotonic
        True

        >>> midx = ks.MultiIndex.from_tuples(
        ... [('z', 'a'), ('z', 'b'), ('y', 'c'), ('y', 'd'), ('x', 'e')])
        >>> midx  # doctest: +SKIP
        MultiIndex([('z', 'a'),
                    ('z', 'b'),
                    ('y', 'c'),
                    ('y', 'd'),
                    ('x', 'e')],
                   )
        >>> midx.is_monotonic
        False
        """"""
        return self._is_monotonic(""increasing"")

    is_monotonic_increasing = is_monotonic

    @property
    def is_monotonic_decreasing(self):
        """"""
        Return boolean if values in the object are monotonically decreasing.

        .. note:: the current implementation of is_monotonic_decreasing requires to shuffle
            and aggregate multiple times to check the order locally and globally,
            which is potentially expensive. In case of multi-index, all data are transferred
            to single node which can easily cause out-of-memory error currently.

        Returns
        -------
        is_monotonic : boolean

        Examples
        --------
        >>> ser = ks.Series(['4/1/2018', '3/1/2018', '1/1/2018'])
        >>> ser.is_monotonic_decreasing
        True

        >>> df = ks.DataFrame({'dates': [None, '3/1/2018', '2/1/2018', '1/1/2018']})
        >>> df.dates.is_monotonic_decreasing
        False

        >>> df.index.is_monotonic_decreasing
        False

        >>> ser = ks.Series([1])
        >>> ser.is_monotonic_decreasing
        True

        >>> ser = ks.Series([])
        >>> ser.is_monotonic_decreasing
        True

        >>> ser.rename(""a"").to_frame().set_index(""a"").index.is_monotonic_decreasing
        True

        >>> ser = ks.Series([5, 4, 3, 2, 1], index=[1, 2, 3, 4, 5])
        >>> ser.is_monotonic_decreasing
        True

        >>> ser.index.is_monotonic_decreasing
        False

        Support for MultiIndex

        >>> midx = ks.MultiIndex.from_tuples(
        ... [('x', 'a'), ('x', 'b'), ('y', 'c'), ('y', 'd'), ('z', 'e')])
        >>> midx  # doctest: +SKIP
        MultiIndex([('x', 'a'),
                    ('x', 'b'),
                    ('y', 'c'),
                    ('y', 'd'),
                    ('z', 'e')],
                   )
        >>> midx.is_monotonic_decreasing
        False

        >>> midx = ks.MultiIndex.from_tuples(
        ... [('z', 'e'), ('z', 'd'), ('y', 'c'), ('y', 'b'), ('x', 'a')])
        >>> midx  # doctest: +SKIP
        MultiIndex([('z', 'a'),
                    ('z', 'b'),
                    ('y', 'c'),
                    ('y', 'd'),
                    ('x', 'e')],
                   )
        >>> midx.is_monotonic_decreasing
        True
        """"""
        return self._is_monotonic(""decreasing"")

    def _is_locally_monotonic_spark_column(self, order):
        window = (
            Window.partitionBy(F.col(""__partition_id""))
            .orderBy(NATURAL_ORDER_COLUMN_NAME)
            .rowsBetween(-1, -1)
        )

        if order == ""increasing"":
            return (F.col(""__origin"") >= F.lag(F.col(""__origin""), 1).over(window)) & F.col(
                ""__origin""
            ).isNotNull()
        else:
            return (F.col(""__origin"") <= F.lag(F.col(""__origin""), 1).over(window)) & F.col(
                ""__origin""
            ).isNotNull()

    def _is_monotonic(self, order):
        assert order in (""increasing"", ""decreasing"")

        sdf = self._internal.spark_frame

        sdf = (
            sdf.select(
                F.spark_partition_id().alias(
                    ""__partition_id""
                ),  # Make sure we use the same partition id in the whole job.
                F.col(NATURAL_ORDER_COLUMN_NAME),
                self._scol.alias(""__origin""),
            )
            .select(
                F.col(""__partition_id""),
                F.col(""__origin""),
                self._is_locally_monotonic_spark_column(order).alias(
                    ""__comparison_within_partition""
                ),
            )
            .groupby(F.col(""__partition_id""))
            .agg(
                F.min(F.col(""__origin"")).alias(""__partition_min""),
                F.max(F.col(""__origin"")).alias(""__partition_max""),
                F.min(F.coalesce(F.col(""__comparison_within_partition""), F.lit(True))).alias(
                    ""__comparison_within_partition""
                ),
            )
        )

        # Now we're windowing the aggregation results without partition specification.
        # The number of rows here will be as the same of partitions, which is expected
        # to be small.
        window = Window.orderBy(F.col(""__partition_id"")).rowsBetween(-1, -1)
        if order == ""increasing"":
            comparison_col = F.col(""__partition_min"") >= F.lag(F.col(""__partition_max""), 1).over(
                window
            )
        else:
            comparison_col = F.col(""__partition_min"") <= F.lag(F.col(""__partition_max""), 1).over(
                window
            )

        sdf = sdf.select(
            comparison_col.alias(""__comparison_between_partitions""),
            F.col(""__comparison_within_partition""),
        )

        ret = sdf.select(
            F.min(F.coalesce(F.col(""__comparison_between_partitions""), F.lit(True)))
            & F.min(F.coalesce(F.col(""__comparison_within_partition""), F.lit(True)))
        ).collect()[0][0]
        if ret is None:
            return True
        else:
            return ret

    @property
    def ndim(self):
        """"""
        Return an int representing the number of array dimensions.

        Return 1 for Series / Index / MultiIndex.

        Examples
        --------

        For Series

        >>> s = ks.Series([None, 1, 2, 3, 4], index=[4, 5, 2, 1, 8])
        >>> s.ndim
        1

        For Index

        >>> s.index.ndim
        1

        For MultiIndex

        >>> midx = pd.MultiIndex([['lama', 'cow', 'falcon'],
        ...                       ['speed', 'weight', 'length']],
        ...                      [[0, 0, 0, 1, 1, 1, 2, 2, 2],
        ...                       [1, 1, 1, 1, 1, 2, 1, 2, 2]])
        >>> s = ks.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3], index=midx)
        >>> s.index.ndim
        1
        """"""
        return 1

    def astype(self, dtype):
        """"""
        Cast a Koalas object to a specified dtype ``dtype``.

        Parameters
        ----------
        dtype : data type
            Use a numpy.dtype or Python type to cast entire pandas object to
            the same type.

        Returns
        -------
        casted : same type as caller

        See Also
        --------
        to_datetime : Convert argument to datetime.

        Examples
        --------
        >>> ser = ks.Series([1, 2], dtype='int32')
        >>> ser
        0    1
        1    2
        Name: 0, dtype: int32

        >>> ser.astype('int64')
        0    1
        1    2
        Name: 0, dtype: int64

        >>> ser.rename(""a"").to_frame().set_index(""a"").index.astype('int64')
        Int64Index([1, 2], dtype='int64', name='a')
        """"""
        from databricks.koalas.typedef import as_spark_type

        spark_type = as_spark_type(dtype)
        if not spark_type:
            raise ValueError(""Type {} not understood"".format(dtype))
        return self._with_new_scol(self._scol.cast(spark_type))

    def isin(self, values):
        """"""
        Check whether `values` are contained in Series.

        Return a boolean Series showing whether each element in the Series
        matches an element in the passed sequence of `values` exactly.

        Parameters
        ----------
        values : list or set
            The sequence of values to test.

        Returns
        -------
        isin : Series (bool dtype)

        Examples
        --------
        >>> s = ks.Series(['lama', 'cow', 'lama', 'beetle', 'lama',
        ...                'hippo'], name='animal')
        >>> s.isin(['cow', 'lama'])
        0     True
        1     True
        2     True
        3    False
        4     True
        5    False
        Name: animal, dtype: bool

        Passing a single string as ``s.isin('lama')`` will raise an error. Use
        a list of one element instead:

        >>> s.isin(['lama'])
        0     True
        1    False
        2     True
        3    False
        4     True
        5    False
        Name: animal, dtype: bool

        >>> s.rename(""a"").to_frame().set_index(""a"").index.isin(['lama'])
        Index([True, False, True, False, True, False], dtype='object', name='a')
        """"""
        if not is_list_like(values):
            raise TypeError(
                ""only list-like objects are allowed to be passed""
                "" to isin(), you passed a [{values_type}]"".format(values_type=type(values).__name__)
            )

        return self._with_new_scol(self._scol.isin(list(values))).rename(self.name)

    def isnull(self):
        """"""
        Detect existing (non-missing) values.

        Return a boolean same-sized object indicating if the values are NA.
        NA values, such as None or numpy.NaN, gets mapped to True values.
        Everything else gets mapped to False values. Characters such as empty strings '' or
        numpy.inf are not considered NA values
        (unless you set pandas.options.mode.use_inf_as_na = True).

        Returns
        -------
        Series : Mask of bool values for each element in Series
            that indicates whether an element is not an NA value.

        Examples
        --------
        >>> ser = ks.Series([5, 6, np.NaN])
        >>> ser.isna()  # doctest: +NORMALIZE_WHITESPACE
        0    False
        1    False
        2     True
        Name: 0, dtype: bool

        >>> ser.rename(""a"").to_frame().set_index(""a"").index.isna()
        Index([False, False, True], dtype='object', name='a')
        """"""
        from databricks.koalas.indexes import MultiIndex

        if isinstance(self, MultiIndex):
            raise NotImplementedError(""isna is not defined for MultiIndex"")
        if isinstance(self.spark_type, (FloatType, DoubleType)):
            return self._with_new_scol(self._scol.isNull() | F.isnan(self._scol)).rename(self.name)
        else:
            return self._with_new_scol(self._scol.isNull()).rename(self.name)

    isna = isnull

    def notnull(self):
        """"""
        Detect existing (non-missing) values.
        Return a boolean same-sized object indicating if the values are not NA.
        Non-missing values get mapped to True.
        Characters such as empty strings '' or numpy.inf are not considered NA values
        (unless you set pandas.options.mode.use_inf_as_na = True).
        NA values, such as None or numpy.NaN, get mapped to False values.

        Returns
        -------
        Series : Mask of bool values for each element in Series
            that indicates whether an element is not an NA value.

        Examples
        --------
        Show which entries in a Series are not NA.

        >>> ser = ks.Series([5, 6, np.NaN])
        >>> ser
        0    5.0
        1    6.0
        2    NaN
        Name: 0, dtype: float64

        >>> ser.notna()
        0     True
        1     True
        2    False
        Name: 0, dtype: bool

        >>> ser.rename(""a"").to_frame().set_index(""a"").index.notna()
        Index([True, True, False], dtype='object', name='a')
        """"""
        from databricks.koalas.indexes import MultiIndex

        if isinstance(self, MultiIndex):
            raise NotImplementedError(""notna is not defined for MultiIndex"")
        return (~self.isnull()).rename(self.name)

    notna = notnull

    # TODO: axis, skipna, and many arguments should be implemented.
    def all(self, axis: Union[int, str] = 0) -> bool:
        """"""
        Return whether all elements are True.

        Returns True unless there at least one element within a series that is
        False or equivalent (e.g. zero or empty)

        Parameters
        ----------
        axis : {0 or 'index'}, default 0
            Indicate which axis or axes should be reduced.

            * 0 / 'index' : reduce the index, return a Series whose index is the
              original column labels.

        Examples
        --------
        >>> ks.Series([True, True]).all()
        True

        >>> ks.Series([True, False]).all()
        False

        >>> ks.Series([0, 1]).all()
        False

        >>> ks.Series([1, 2, 3]).all()
        True

        >>> ks.Series([True, True, None]).all()
        True

        >>> ks.Series([True, False, None]).all()
        False

        >>> ks.Series([]).all()
        True

        >>> ks.Series([np.nan]).all()
        True

        >>> df = ks.Series([True, False, None]).rename(""a"").to_frame()
        >>> df.set_index(""a"").index.all()
        False
        """"""
        axis = validate_axis(axis)
        if axis != 0:
            raise NotImplementedError('axis should be either 0 or ""index"" currently.')

        sdf = self._internal._sdf.select(self._scol)
        col = scol_for(sdf, sdf.columns[0])

        # Note that we're ignoring `None`s here for now.
        # any and every was added as of Spark 3.0
        # ret = sdf.select(F.expr(""every(CAST(`%s` AS BOOLEAN))"" % sdf.columns[0])).collect()[0][0]
        # Here we use min as its alternative:
        ret = sdf.select(F.min(F.coalesce(col.cast(""boolean""), F.lit(True)))).collect()[0][0]
        if ret is None:
            return True
        else:
            return ret

    # TODO: axis, skipna, and many arguments should be implemented.
    def any(self, axis: Union[int, str] = 0) -> bool:
        """"""
        Return whether any element is True.

        Returns False unless there at least one element within a series that is
        True or equivalent (e.g. non-zero or non-empty).

        Parameters
        ----------
        axis : {0 or 'index'}, default 0
            Indicate which axis or axes should be reduced.

            * 0 / 'index' : reduce the index, return a Series whose index is the
              original column labels.

        Examples
        --------
        >>> ks.Series([False, False]).any()
        False

        >>> ks.Series([True, False]).any()
        True

        >>> ks.Series([0, 0]).any()
        False

        >>> ks.Series([0, 1, 2]).any()
        True

        >>> ks.Series([False, False, None]).any()
        False

        >>> ks.Series([True, False, None]).any()
        True

        >>> ks.Series([]).any()
        False

        >>> ks.Series([np.nan]).any()
        False

        >>> df = ks.Series([True, False, None]).rename(""a"").to_frame()
        >>> df.set_index(""a"").index.any()
        True
        """"""
        axis = validate_axis(axis)
        if axis != 0:
            raise NotImplementedError('axis should be either 0 or ""index"" currently.')

        sdf = self._internal._sdf.select(self._scol)
        col = scol_for(sdf, sdf.columns[0])

        # Note that we're ignoring `None`s here for now.
        # any and every was added as of Spark 3.0
        # ret = sdf.select(F.expr(""any(CAST(`%s` AS BOOLEAN))"" % sdf.columns[0])).collect()[0][0]
        # Here we use max as its alternative:
        ret = sdf.select(F.max(F.coalesce(col.cast(""boolean""), F.lit(False)))).collect()[0][0]
        if ret is None:
            return False
        else:
            return ret

    # TODO: add frep and axis parameter
    def shift(self, periods=1, fill_value=None):
        """"""
        Shift Series/Index by desired number of periods.

        .. note:: the current implementation of shift uses Spark's Window without
            specifying partition specification. This leads to move all data into
            single partition in single machine and could cause serious
            performance degradation. Avoid this method against very large dataset.

        Parameters
        ----------
        periods : int
            Number of periods to shift. Can be positive or negative.
        fill_value : object, optional
            The scalar value to use for newly introduced missing values.
            The default depends on the dtype of self. For numeric data, np.nan is used.

        Returns
        -------
        Copy of input Series/Index, shifted.

        Examples
        --------
        >>> df = ks.DataFrame({'Col1': [10, 20, 15, 30, 45],
        ...                    'Col2': [13, 23, 18, 33, 48],
        ...                    'Col3': [17, 27, 22, 37, 52]},
        ...                   columns=['Col1', 'Col2', 'Col3'])

        >>> df.Col1.shift(periods=3)
        0     NaN
        1     NaN
        2     NaN
        3    10.0
        4    20.0
        Name: Col1, dtype: float64

        >>> df.Col2.shift(periods=3, fill_value=0)
        0     0
        1     0
        2     0
        3    13
        4    23
        Name: Col2, dtype: int64

        >>> df.index.shift(periods=3, fill_value=0)
        Int64Index([0, 0, 0, 0, 1], dtype='int64')
        """"""
        return self._shift(periods, fill_value)

    def _shift(self, periods, fill_value, part_cols=()):
        if not isinstance(periods, int):
            raise ValueError(""periods should be an int; however, got [%s]"" % type(periods))

        col = self._scol
        window = (
            Window.partitionBy(*part_cols)
            .orderBy(NATURAL_ORDER_COLUMN_NAME)
            .rowsBetween(-periods, -periods)
        )
        lag_col = F.lag(col, periods).over(window)
        col = F.when(lag_col.isNull() | F.isnan(lag_col), fill_value).otherwise(lag_col)
        return self._with_new_scol(col).rename(self.name)

    # TODO: Update Documentation for Bins Parameter when its supported
    def value_counts(self, normalize=False, sort=True, ascending=False, bins=None, dropna=True):
        """"""
        Return a Series containing counts of unique values.
        The resulting object will be in descending order so that the
        first element is the most frequently-occurring element.
        Excludes NA values by default.

        Parameters
        ----------
        normalize : boolean, default False
            If True then the object returned will contain the relative
            frequencies of the unique values.
        sort : boolean, default True
            Sort by values.
        ascending : boolean, default False
            Sort in ascending order.
        bins : Not Yet Supported
        dropna : boolean, default True
            Don't include counts of NaN.

        Returns
        -------
        counts : Series

        See Also
        --------
        Series.count: Number of non-NA elements in a Series.

        Examples
        --------
        For Series

        >>> df = ks.DataFrame({'x':[0, 0, 1, 1, 1, np.nan]})
        >>> df.x.value_counts()  # doctest: +NORMALIZE_WHITESPACE
        1.0    3
        0.0    2
        Name: x, dtype: int64

        With `normalize` set to `True`, returns the relative frequency by
        dividing all values by the sum of values.

        >>> df.x.value_counts(normalize=True)  # doctest: +NORMALIZE_WHITESPACE
        1.0    0.6
        0.0    0.4
        Name: x, dtype: float64

        **dropna**
        With `dropna` set to `False` we can also see NaN index values.

        >>> df.x.value_counts(dropna=False)  # doctest: +NORMALIZE_WHITESPACE
        1.0    3
        0.0    2
        NaN    1
        Name: x, dtype: int64

        For Index

        >>> from databricks.koalas.indexes import Index
        >>> idx = Index([3, 1, 2, 3, 4, np.nan])
        >>> idx
        Float64Index([3.0, 1.0, 2.0, 3.0, 4.0, nan], dtype='float64')

        >>> idx.value_counts().sort_index()
        1.0    1
        2.0    1
        3.0    2
        4.0    1
        Name: count, dtype: int64

        **sort**

        With `sort` set to `False`, the result wouldn't be sorted by number of count.

        >>> idx.value_counts(sort=True).sort_index()
        1.0    1
        2.0    1
        3.0    2
        4.0    1
        Name: count, dtype: int64

        **normalize**

        With `normalize` set to `True`, returns the relative frequency by
        dividing all values by the sum of values.

        >>> idx.value_counts(normalize=True).sort_index()
        1.0    0.2
        2.0    0.2
        3.0    0.4
        4.0    0.2
        Name: count, dtype: float64

        **dropna**

        With `dropna` set to `False` we can also see NaN index values.

        >>> idx.value_counts(dropna=False).sort_index()  # doctest: +SKIP
        1.0    1
        2.0    1
        3.0    2
        4.0    1
        NaN    1
        Name: count, dtype: int64

        For MultiIndex.

        >>> midx = pd.MultiIndex([['lama', 'cow', 'falcon'],
        ...                       ['speed', 'weight', 'length']],
        ...                      [[0, 0, 0, 1, 1, 1, 2, 2, 2],
        ...                       [1, 1, 1, 1, 1, 2, 1, 2, 2]])
        >>> s = ks.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3], index=midx)
        >>> s.index  # doctest: +SKIP
        MultiIndex([(  'lama', 'weight'),
                    (  'lama', 'weight'),
                    (  'lama', 'weight'),
                    (   'cow', 'weight'),
                    (   'cow', 'weight'),
                    (   'cow', 'length'),
                    ('falcon', 'weight'),
                    ('falcon', 'length'),
                    ('falcon', 'length')],
                   )

        >>> s.index.value_counts().sort_index()
        (cow, length)       1
        (cow, weight)       2
        (falcon, length)    2
        (falcon, weight)    1
        (lama, weight)      3
        Name: count, dtype: int64

        >>> s.index.value_counts(normalize=True).sort_index()
        (cow, length)       0.111111
        (cow, weight)       0.222222
        (falcon, length)    0.222222
        (falcon, weight)    0.111111
        (lama, weight)      0.333333
        Name: count, dtype: float64

        If Index has name, keep the name up.

        >>> idx = Index([0, 0, 0, 1, 1, 2, 3], name='koalas')
        >>> idx.value_counts().sort_index()
        0    3
        1    2
        2    1
        3    1
        Name: koalas, dtype: int64
        """"""
        from databricks.koalas.series import _col
",Can you handle these differences in the implementation side? See `__sub__` or `__add__` for example.,"The suggested code change adds several utility functions and methods to the `IndexOpsMixin` class, which provides common operations for Koalas Series and Index objects. These include arithmetic and comparison operators, casting, and various utility methods like `isin`, `isnull`, `notnull`, `all`, `any`, `shift`, `value_counts`, and others. The changes enhance the functionality of Koalas objects by providing a consistent interface for performing operations across different data structures."
39,"@@ -32,10 +32,11 @@ module RSpec
       include Pending
       extend SharedExampleGroup
 
-      unless respond_to?(:define_singleton_method)
-        # @private
-        def self.define_singleton_method(*a, &b)
-          (class << self; self; end).__send__(:define_method, *a, &b)
+      # @private
+      def self.idempotently_define_singleton_method(name, &definition)
+        (class << self; self; end).module_exec do
+          remove_method(name) if method_defined?(name)
+          define_method(name, &definition)
         end
       end
 ","RSpec::Support.require_rspec_support 'recursive_const_methods'

module RSpec
  module Core
    # ExampleGroup and {Example} are the main structural elements of
    # rspec-core. Consider this example:
    #
    #     describe Thing do
    #       it ""does something"" do
    #       end
    #     end
    #
    # The object returned by `describe Thing` is a subclass of ExampleGroup.
    # The object returned by `it ""does something""` is an instance of Example,
    # which serves as a wrapper for an instance of the ExampleGroup in which it
    # is declared.
    #
    # Example group bodies (e.g. `describe` or `context` blocks) are evaluated
    # in the context of a new subclass of ExampleGroup. Individual examples are
    # evaluated in the context of an instance of the specific ExampleGroup
    # subclass to which they belong.
    #
    # Besides the class methods defined here, there are other interesting macros
    # defined in {Hooks}, {MemoizedHelpers::ClassMethods} and
    # {SharedExampleGroup}. There are additional instance methods available to
    # your examples defined in {MemoizedHelpers} and {Pending}.
    class ExampleGroup
      extend Hooks

      include MemoizedHelpers
      extend MemoizedHelpers::ClassMethods
      # @private
      def self.idempotently_define_singleton_method(name, &definition)
        (class << self; self; end).module_exec do
          remove_method(name) if method_defined?(name)
          define_method(name, &definition)
      include Pending
      extend SharedExampleGroup

      unless respond_to?(:define_singleton_method)
        # @private
        def self.define_singleton_method(*a, &b)
          (class << self; self; end).__send__(:define_method, *a, &b)
        end
      end

      # @!group Metadata

      # The [Metadata](Metadata) object associated with this group.
      # @see Metadata
      def self.metadata
        @metadata if defined?(@metadata)
      end

      # @private
      # @return [Metadata] belonging to the parent of a nested {ExampleGroup}
      def self.superclass_metadata
        @superclass_metadata ||= superclass.respond_to?(:metadata) ? superclass.metadata : nil
      end

      # @private
      def self.delegate_to_metadata(*names)
        names.each do |name|
          define_singleton_method(name) { metadata.fetch(name) }
        end
      end

      delegate_to_metadata :described_class, :file_path, :location

      # @return [String] the current example group description
      def self.description
        description = metadata[:description]
        RSpec.configuration.format_docstrings_block.call(description)
      end

      # Returns the class or module passed to the `describe` method (or alias).
      # Returns nil if the subject is not a class or module.
      # @example
      #     describe Thing do
      #       it ""does something"" do
      #         described_class == Thing
      #       end
      #     end
      #
      def described_class
        self.class.described_class
      end

      # @!endgroup

      # @!group Defining Examples

      # @private
      # @macro [attach] define_example_method
      #   @!scope class
      #   @overload $1
      #   @overload $1(&example_implementation)
      #     @param example_implementation [Block] The implementation of the example.
      #   @overload $1(doc_string, *metadata_keys, metadata={})
      #     @param doc_string [String] The example's doc string.
      #     @param metadata [Hash] Metadata for the example.
      #     @param metadata_keys [Array<Symbol>] Metadata tags for the example.
      #       Will be transformed into hash entries with `true` values.
      #   @overload $1(doc_string, *metadata_keys, metadata={}, &example_implementation)
      #     @param doc_string [String] The example's doc string.
      #     @param metadata [Hash] Metadata for the example.
      #     @param metadata_keys [Array<Symbol>] Metadata tags for the example.
      #       Will be transformed into hash entries with `true` values.
      #     @param example_implementation [Block] The implementation of the example.
      #   @yield [Example] the example object
      #   @example
      #     $1 do
      #     end
      #
      #     $1 ""does something"" do
      #     end
      #
      #     $1 ""does something"", :slow, :uses_js do
      #     end
      #
      #     $1 ""does something"", :with => 'additional metadata' do
      #     end
      #
      #     $1 ""does something"" do |ex|
      #       # ex is the Example object that contains metadata about the example
      #     end
      def self.define_example_method(name, extra_options={})
        define_singleton_method(name) do |*all_args, &block|
          desc, *args = *all_args

          options = Metadata.build_hash_from(args)
          options.update(:skip => RSpec::Core::Pending::NOT_YET_IMPLEMENTED) unless block
          options.update(extra_options)

          examples << RSpec::Core::Example.new(self, desc, options, block)
          examples.last
        end
      end

      # Defines an example within a group.
      define_example_method :example
      # Defines an example within a group.
      # This is the primary API to define a code example.
      define_example_method :it
      # Defines an example within a group.
      # Useful for when your docstring does not read well off of `it`.
      # @example
      #  RSpec.describe MyClass do
      #    specify ""#do_something is deprecated"" do
      #      # ...
      #    end
      #  end
      define_example_method :specify

      # Shortcut to define an example with `:focus => true`.
      # @see example
      define_example_method :focus,    :focus => true
      # Shortcut to define an example with `:focus => true`.
      # @see example
      define_example_method :fexample, :focus => true
      # Shortcut to define an example with `:focus => true`.
      # @see example
      define_example_method :fit,      :focus => true
      # Shortcut to define an example with `:focus => true`.
      # @see example
      define_example_method :fspecify, :focus => true
      # Shortcut to define an example with `:skip => 'Temporarily skipped with xexample'`.
      # @see example
      define_example_method :xexample, :skip => 'Temporarily skipped with xexample'
      # Shortcut to define an example with `:skip => 'Temporarily skipped with xit'`.
      # @see example
      define_example_method :xit,      :skip => 'Temporarily skipped with xit'
      # Shortcut to define an example with `:skip => 'Temporarily skipped with xspecify'`.
      # @see example
      define_example_method :xspecify, :skip => 'Temporarily skipped with xspecify'
      # Shortcut to define an example with `:skip => true`
      # @see example
      define_example_method :skip,     :skip => true
      # Shortcut to define an example with `:pending => true`
      # @see example
      define_example_method :pending,  :pending => true

      # @!endgroup

      # @!group Defining Example Groups

      # @private
      # @macro [attach] define_example_group_method
      #   @!scope class
      #   @overload $1
      #   @overload $1(&example_group_definition)
      #     @param example_group_definition [Block] The definition of the example group.
      #   @overload $1(doc_string, *metadata_keys, metadata={}, &example_implementation)
      #     @param doc_string [String] The group's doc string.
      #     @param metadata [Hash] Metadata for the group.
      #     @param metadata_keys [Array<Symbol>] Metadata tags for the group.
      #       Will be transformed into hash entries with `true` values.
      #     @param example_group_definition [Block] The definition of the example group.
      #
      #   Generates a subclass of this example group which inherits
      #   everything except the examples themselves.
      #
      #   @example
      #
      #     RSpec.describe ""something"" do # << This describe method is defined in
      #                                   # << RSpec::Core::DSL, included in the
      #                                   # << global namespace (optional)
      #       before do
      #         do_something_before
      #       end
      #
      #       let(:thing) { Thing.new }
      #
      #       $1 ""attribute (of something)"" do
      #         # examples in the group get the before hook
      #         # declared above, and can access `thing`
      #       end
      #     end
      #
      # @see DSL#describe
      def self.define_example_group_method(name, metadata={})
        define_singleton_method(name) do |*args, &example_group_block|
          thread_data = RSpec.thread_local_metadata
          top_level   = self == ExampleGroup

          if top_level
            if thread_data[:in_example_group]
              raise ""Creating an isolated context from within a context is "" \
                    ""not allowed. Change `RSpec.#{name}` to `#{name}` or "" \
                    ""move this to a top-level scope.""
            end

            thread_data[:in_example_group] = true
          end

          begin

            description = args.shift
            combined_metadata = metadata.dup
            combined_metadata.merge!(args.pop) if args.last.is_a? Hash
            args << combined_metadata

            subclass(self, description, args, &example_group_block).tap do |child|
              children << child
            end

          ensure
            thread_data.delete(:in_example_group) if top_level
          end
        end

        RSpec::Core::DSL.expose_example_group_alias(name)
      end

      define_example_group_method :example_group

      # An alias of `example_group`. Generally used when grouping examples by a
      # thing you are describing (e.g. an object, class or method).
      # @see example_group
      define_example_group_method :describe

      # An alias of `example_group`. Generally used when grouping examples
      # contextually (e.g. ""with xyz"", ""when xyz"" or ""if xyz"").
      # @see example_group
      define_example_group_method :context

      # Shortcut to temporarily make an example group skipped.
      # @see example_group
      define_example_group_method :xdescribe, :skip => ""Temporarily skipped with xdescribe""

      # Shortcut to temporarily make an example group skipped.
      # @see example_group
      define_example_group_method :xcontext,  :skip => ""Temporarily skipped with xcontext""

      # Shortcut to define an example group with `:focus => true`.
      # @see example_group
      define_example_group_method :fdescribe, :focus => true

      # Shortcut to define an example group with `:focus => true`.
      # @see example_group
      define_example_group_method :fcontext,  :focus => true

      # @!endgroup

      # @!group Including Shared Example Groups

      # @private
      # @macro [attach] define_nested_shared_group_method
      #   @!scope class
      #
      #   @see SharedExampleGroup
      def self.define_nested_shared_group_method(new_name, report_label=""it should behave like"")
        define_singleton_method(new_name) do |name, *args, &customization_block|
          # Pass :caller so the :location metadata is set properly.
          # Otherwise, it'll be set to the next line because that's
          # the block's source_location.
          group = example_group(""#{report_label} #{name}"", :caller => (the_caller = caller)) do
            find_and_eval_shared(""examples"", name, the_caller.first, *args, &customization_block)
          end
          group.metadata[:shared_group_name] = name
          group
        end
      end

      # Generates a nested example group and includes the shared content
      # mapped to `name` in the nested group.
      define_nested_shared_group_method :it_behaves_like, ""behaves like""
      # Generates a nested example group and includes the shared content
      # mapped to `name` in the nested group.
      define_nested_shared_group_method :it_should_behave_like

      # Includes shared content mapped to `name` directly in the group in which
      # it is declared, as opposed to `it_behaves_like`, which creates a nested
      # group. If given a block, that block is also eval'd in the current
      # context.
      #
      # @see SharedExampleGroup
      def self.include_context(name, *args, &block)
        find_and_eval_shared(""context"", name, caller.first, *args, &block)
      end

      # Includes shared content mapped to `name` directly in the group in which
      # it is declared, as opposed to `it_behaves_like`, which creates a nested
      # group. If given a block, that block is also eval'd in the current
      # context.
      #
      # @see SharedExampleGroup
      def self.include_examples(name, *args, &block)
        find_and_eval_shared(""examples"", name, caller.first, *args, &block)
      end

      # @private
      def self.find_and_eval_shared(label, name, inclusion_location, *args, &customization_block)
        shared_block = RSpec.world.shared_example_group_registry.find(parent_groups, name)

        unless shared_block
          raise ArgumentError, ""Could not find shared #{label} #{name.inspect}""
        end

        SharedExampleGroupInclusionStackFrame.with_frame(name, inclusion_location) do
          module_exec(*args, &shared_block)
          module_exec(&customization_block) if customization_block
        end
      end

      # @!endgroup

      # @private
      def self.subclass(parent, description, args, &example_group_block)
        subclass = Class.new(parent)
        subclass.set_it_up(description, *args, &example_group_block)
        ExampleGroups.assign_const(subclass)
        subclass.module_exec(&example_group_block) if example_group_block

        # The LetDefinitions module must be included _after_ other modules
        # to ensure that it takes precedence when there are name collisions.
        # Thus, we delay including it until after the example group block
        # has been eval'd.
        MemoizedHelpers.define_helpers_on(subclass)

        subclass
      end

      # @private
      def self.set_it_up(*args, &example_group_block)
        # Ruby 1.9 has a bug that can lead to infinite recursion and a
        # SystemStackError if you include a module in a superclass after
        # including it in a subclass: https://gist.github.com/845896
        # To prevent this, we must include any modules in
        # RSpec::Core::ExampleGroup before users create example groups and have
        # a chance to include the same module in a subclass of
        # RSpec::Core::ExampleGroup. So we need to configure example groups
        # here.
        ensure_example_groups_are_configured

        description = args.shift
        user_metadata = Metadata.build_hash_from(args)
        args.unshift(description)

        @metadata = Metadata::ExampleGroupHash.create(
          superclass_metadata, user_metadata, *args, &example_group_block
        )

        hooks.register_globals(self, RSpec.configuration.hooks)
        RSpec.configuration.configure_group(self)
      end

      # @private
      def self.examples
        @examples ||= []
      end

      # @private
      def self.filtered_examples
        RSpec.world.filtered_examples[self]
      end

      # @private
      def self.descendant_filtered_examples
        @descendant_filtered_examples ||= filtered_examples +
          children.inject([]) { |a, e| a + e.descendant_filtered_examples }
      end

      # @private
      def self.children
        @children ||= []
      end

      # @private
      def self.descendants
        @_descendants ||= [self] + children.inject([]) { |a, e| a + e.descendants }
      end

      ## @private
      def self.parent_groups
        @parent_groups ||= ancestors.select { |a| a < RSpec::Core::ExampleGroup }
      end

      # @private
      def self.top_level?
        @top_level ||= superclass == ExampleGroup
      end

      # @private
      def self.ensure_example_groups_are_configured
        unless defined?(@@example_groups_configured)
          RSpec.configuration.configure_mock_framework
          RSpec.configuration.configure_expectation_framework
          # rubocop:disable Style/ClassVars
          @@example_groups_configured = true
          # rubocop:enable Style/ClassVars
        end
      end

      # @private
      def self.before_context_ivars
        @before_context_ivars ||= {}
      end

      # @private
      def self.store_before_context_ivars(example_group_instance)
        each_instance_variable_for_example(example_group_instance) do |ivar|
          before_context_ivars[ivar] = example_group_instance.instance_variable_get(ivar)
        end
      end

      # @private
      def self.run_before_context_hooks(example_group_instance)
        set_ivars(example_group_instance, superclass.before_context_ivars)

        ContextHookMemoizedHash::Before.isolate_for_context_hook(example_group_instance) do
          hooks.run(:before, :context, example_group_instance)
        end
      ensure
        store_before_context_ivars(example_group_instance)
      end

      # @private
      def self.run_after_context_hooks(example_group_instance)
        set_ivars(example_group_instance, before_context_ivars)

        ContextHookMemoizedHash::After.isolate_for_context_hook(example_group_instance) do
          hooks.run(:after, :context, example_group_instance)
        end
      ensure
        before_context_ivars.clear
      end

      # Runs all the examples in this group.
      def self.run(reporter=RSpec::Core::NullReporter.new)
        if RSpec.world.wants_to_quit
          RSpec.world.clear_remaining_example_groups if top_level?
          return
        end
        reporter.example_group_started(self)

        should_run_context_hooks = descendant_filtered_examples.any?
        begin
          run_before_context_hooks(new('before(:context) hook')) if should_run_context_hooks
          result_for_this_group = run_examples(reporter)
          results_for_descendants = ordering_strategy.order(children).map { |child| child.run(reporter) }.all?
          result_for_this_group && results_for_descendants
        rescue Pending::SkipDeclaredInExample => ex
          for_filtered_examples(reporter) { |example| example.skip_with_exception(reporter, ex) }
        rescue Exception => ex
          RSpec.world.wants_to_quit = true if fail_fast?
          for_filtered_examples(reporter) { |example| example.fail_with_exception(reporter, ex) }
        ensure
          run_after_context_hooks(new('after(:context) hook')) if should_run_context_hooks
          reporter.example_group_finished(self)
        end
      end

      # @private
      def self.ordering_strategy
        order = metadata.fetch(:order, :global)
        registry = RSpec.configuration.ordering_registry

        registry.fetch(order) do
          warn <<-WARNING.gsub(/^ +\|/, '')
            |WARNING: Ignoring unknown ordering specified using `:order => #{order.inspect}` metadata.
            |         Falling back to configured global ordering.
            |         Unrecognized ordering specified at: #{location}
          WARNING

          registry.fetch(:global)
        end
      end

      # @private
      def self.run_examples(reporter)
        ordering_strategy.order(filtered_examples).map do |example|
          next if RSpec.world.wants_to_quit
          instance = new(example.inspect_output)
          set_ivars(instance, before_context_ivars)
          succeeded = example.run(instance, reporter)
          RSpec.world.wants_to_quit = true if fail_fast? && !succeeded
          succeeded
        end.all?
      end

      # @private
      def self.for_filtered_examples(reporter, &block)
        filtered_examples.each(&block)

        children.each do |child|
          reporter.example_group_started(child)
          child.for_filtered_examples(reporter, &block)
          reporter.example_group_finished(child)
        end
        false
      end

      # @private
      def self.fail_fast?
        RSpec.configuration.fail_fast?
      end

      # @private
      def self.declaration_line_numbers
        @declaration_line_numbers ||= [metadata[:line_number]] +
          examples.map { |e| e.metadata[:line_number] } +
          children.inject([]) { |a, e| a + e.declaration_line_numbers }
      end

      # @private
      def self.top_level_description
        parent_groups.last.description
      end

      # @private
      def self.set_ivars(instance, ivars)
        ivars.each { |name, value| instance.instance_variable_set(name, value) }
      end

      if RUBY_VERSION.to_f < 1.9
        # @private
        INSTANCE_VARIABLE_TO_IGNORE = '@__inspect_output'.freeze
      else
        # @private
        INSTANCE_VARIABLE_TO_IGNORE = :@__inspect_output
      end

      # @private
      def self.each_instance_variable_for_example(group)
        group.instance_variables.each do |ivar|
          yield ivar unless ivar == INSTANCE_VARIABLE_TO_IGNORE
        end
      end

      def initialize(inspect_output=nil)
        @__inspect_output = inspect_output || '(no description provided)'
      end

      # @private
      def inspect
        ""#<#{self.class} #{@__inspect_output}>""
      end

      # Raised when an RSpec API is called in the wrong scope, such as `before`
      # being called from within an example rather than from within an example
      # group block.
      WrongScopeError = Class.new(NoMethodError)

      def self.method_missing(name, *args)
        if method_defined?(name)
          raise WrongScopeError,
                ""`#{name}` is not available on an example group (e.g. a "" \
                ""`describe` or `context` block). It is only available from "" \
                ""within individual examples (e.g. `it` blocks) or from "" \
                ""constructs that run in the scope of an example (e.g. "" \
                ""`before`, `let`, etc).""
        end

        super
      end
      private_class_method :method_missing

    private

      def method_missing(name, *args)
        if self.class.respond_to?(name)
          raise WrongScopeError,
                ""`#{name}` is not available from within an example (e.g. an "" \
                ""`it` block) or from constructs that run in the scope of an "" \
                ""example (e.g. `before`, `let`, etc). It is only available "" \
                ""on an example group (e.g. a `describe` or `context` block).""
        end

        super
      end
    end

    # @private
    # Unnamed example group used by `SuiteHookContext`.
    class AnonymousExampleGroup < ExampleGroup
      def self.metadata
        {}
      end
    end

    # Contains information about the inclusion site of a shared example group.
    class SharedExampleGroupInclusionStackFrame
      # @return [String] the name of the shared example group
      attr_reader :shared_group_name
      # @return [String] the location where the shared example was included
      attr_reader :inclusion_location

      def initialize(shared_group_name, inclusion_location)
        @shared_group_name  = shared_group_name
        @inclusion_location = inclusion_location
      end

      # @return [String] The {#inclusion_location}, formatted for display by a formatter.
      def formatted_inclusion_location
        @formatted_inclusion_location ||= begin
          RSpec.configuration.backtrace_formatter.backtrace_line(
            inclusion_location.sub(/(:\d+):in .+$/, '\1')
          )
        end
      end

      # @return [String] Description of this stack frame, in the form used by
      #   RSpec's built-in formatters.
      def description
        @description ||= ""Shared Example Group: #{shared_group_name.inspect} "" \
          ""called from #{formatted_inclusion_location}""
      end

      # @private
      def self.current_backtrace
        RSpec.thread_local_metadata[:shared_example_group_inclusions].reverse
      end

      # @private
      def self.with_frame(name, location)
        current_stack = RSpec.thread_local_metadata[:shared_example_group_inclusions]
        current_stack << new(name, location)
        yield
      ensure
        current_stack.pop
      end
    end
  end

  # @private
  #
  # Namespace for the example group subclasses generated by top-level
  # `describe`.
  module ExampleGroups
    extend Support::RecursiveConstMethods

    def self.assign_const(group)
      base_name   = base_name_for(group)
      const_scope = constant_scope_for(group)
      name        = disambiguate(base_name, const_scope)

      const_scope.const_set(name, group)
    end

    def self.constant_scope_for(group)
      const_scope = group.superclass
      const_scope = self if const_scope == ::RSpec::Core::ExampleGroup
      const_scope
    end

    def self.base_name_for(group)
      return ""Anonymous"" if group.description.empty?

      # Convert to CamelCase.
      name = ' ' << group.description
      name.gsub!(/[^0-9a-zA-Z]+([0-9a-zA-Z])/) do
        match = Regexp.last_match[1]
        match.upcase!
        match
      end

      name.lstrip!                # Remove leading whitespace
      name.gsub!(/\W/, ''.freeze) # JRuby, RBX and others don't like non-ascii in const names

      # Ruby requires first const letter to be A-Z. Use `Nested`
      # as necessary to enforce that.
      name.gsub!(/\A([^A-Z]|\z)/, 'Nested\1'.freeze)

      name
    end

    if RUBY_VERSION == '1.9.2'
      class << self
        alias _base_name_for base_name_for
        def base_name_for(group)
          _base_name_for(group) + '_'
        end
      end
      private_class_method :_base_name_for
    end

    def self.disambiguate(name, const_scope)
      return name unless const_defined_on?(const_scope, name)

      # Add a trailing number if needed to disambiguate from an existing
      # constant.
      name << ""_2""
      name.next! while const_defined_on?(const_scope, name)
      name
    end
  end
end
",Maybe this should be a module in rspec-support? It seems useful enough that we'll want it in other places.,"The suggested code change introduces several improvements and additions to the RSpec core framework, particularly focusing on example groups and their behavior. Here's a concise review:

### Key Improvements and Additions:

1. **Metadata Handling**: 
   - Added `delegate_to_metadata` to automatically delegate certain metadata attributes to the `metadata` object.
   - Included `described_class`, `file_path`, and `location` in the delegated metadata.

2. **Example Group Methods**:
   - Defined `define_example_method` to handle various ways of defining examples, including with doc strings, metadata, and blocks.
   - Added shortcuts for common example types like `focus`, `skip`, and `pending`.

3. **Example Group Inheritance**:
   - Introduced `define_example_group_method` to create nested example groups and include shared content.
   - Added aliases for `describe` and `context` to make them more intuitive.

4. **Example Group Lifecycle**:
   - Implemented `run_before_context_hooks` and `run_after_context_hooks` to manage hooks for example groups.
   - Ensured that example groups are configured correctly before running examples.

5. **Error Handling**:
   - Added error handling for missing shared example groups and invalid scopes.

6. **Performance Enhancements**:
   - Used `ensure_example_groups_are_configured` to ensure that mock and expectation frameworks are configured before running examples.

7. **Backward Compatibility**:
   - Adjusted `INSTANCE_VARIABLE_TO_IGNORE` based on Ruby version to ensure compatibility.

### Overall Impact:

- **Enhanced Flexibility**: The addition of metadata delegation and shortcut methods makes it easier to work with example groups and their metadata.
- **Improved Error Handling**: Better error messages and handling for common issues like incorrect scopes.
- **Performance Optimization**: Ensures that example groups are configured correctly before running, improving performance.

### Potential Areas for Further Improvement:

- **Documentation**: Additional documentation could be added to explain the purpose and usage of each method and feature.
- **Testing**: More comprehensive testing should be done to ensure that all features work as expected across different scenarios.

Overall, this change significantly enhances the usability and robustness of the RSpec core framework, making it easier to write and maintain test suites."
40,"@@ -839,10 +839,9 @@ def handle_diff_results(args):
 
     init_logger(args.verbose if 'verbose' in args else None, stream)
 
-    require_export_dir = any([o in ['html', 'gerrit', 'codeclimate']
-                              for o in args.output_format])
-    if require_export_dir and 'export_dir' not in args:
-        LOG.error(""No export directory given!"")
+    if len(args.output_format) > 1 and ('export_dir' not in args):
+        LOG.error(""Export directory is required if multiple output formats ""
+                  ""are selected!"")
         sys.exit(1)
 
     check_deprecated_arg_usage(args)","# -------------------------------------------------------------------------
#
#  Part of the CodeChecker project, under the Apache License v2.0 with
#  LLVM Exceptions. See LICENSE for license information.
#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
#
# -------------------------------------------------------------------------
""""""
Command line client.
""""""


from collections import defaultdict, namedtuple
from datetime import datetime, timedelta
import hashlib
import json
import os
from operator import itemgetter
import re
import sys
import shutil
import time
from typing import Dict, List, Tuple, Union

from plist_to_html import PlistToHtml

from codechecker_api.codeCheckerDBAccess_v6 import constants, ttypes
from codechecker_api_shared.ttypes import RequestFailed

from codechecker_common import logger, plist_parser, util
from codechecker_common.report import Report
from codechecker_common.output import twodim, gerrit, codeclimate
from codechecker_common.source_code_comment_handler import \
    SourceCodeCommentHandler, SpellException
from codechecker_report_hash.hash import get_report_path_hash

from codechecker_web.shared import webserver_context
from codechecker_web.shared import convert

from codechecker_client import report_type_converter
from .client import login_user, setup_client
from .cmd_line import CmdLineOutputEncoder
from .product import split_server_url

from . import suppress_file_handler

# Needs to be set in the handler functions.
LOG = None


BugPathLengthRange = namedtuple('BugPathLengthRange', ['min', 'max'])


def init_logger(level, stream=None, logger_name='system'):
    logger.setup_logger(level, stream)
    global LOG
    LOG = logger.get_logger(logger_name)


def filter_localdir_remote_run(
        run_args: List[str]) -> Tuple[List[str], List[str]]:
    """"""Filter out arguments which are local directory or remote run names.""""""
    local_dirs = []
    run_names = []

    for r in run_args:
        if os.path.isdir(r):
            local_dirs.append(os.path.abspath(r))
        else:
            run_names.append(r)

    return local_dirs, run_names


def run_sort_type_str(value):
    """""" Converts the given run sort type to string. """"""
    if value == ttypes.RunSortType.NAME:
        return 'name'
    elif value == ttypes.RunSortType.UNRESOLVED_REPORTS:
        return 'unresolved_reports'
    elif value == ttypes.RunSortType.DATE:
        return 'date'
    elif value == ttypes.RunSortType.DURATION:
        return 'duration'
    elif value == ttypes.RunSortType.CC_VERSION:
        return 'codechecker_version'


def run_sort_type_enum(value):
    """""" Returns the given run sort type Thrift enum value. """"""
    if value == 'name':
        return ttypes.RunSortType.NAME
    elif value == 'unresolved_reports':
        return ttypes.RunSortType.UNRESOLVED_REPORTS
    elif value == 'date':
        return ttypes.RunSortType.DATE
    elif value == 'duration':
        return ttypes.RunSortType.DURATION
    elif value == 'codechecker_version':
        return ttypes.RunSortType.CC_VERSION


def get_diff_type(args) -> Union[ttypes.DiffType, None]:
    """"""
    Returns Thrift DiffType value by processing the arguments.
    """"""
    if 'new' in args:
        return ttypes.DiffType.NEW

    if 'unresolved' in args:
        return ttypes.DiffType.UNRESOLVED

    if 'resolved' in args:
        return ttypes.DiffType.RESOLVED

    return None


def reports_to_html_report_data(reports: List[Report]) -> Dict:
    """"""
    Converts reports from Report class from one plist file
    to report data events for the HTML plist parser.
    """"""
    file_sources = {}
    report_data = []

    for report in reports:
        # Not all report in this list may refer to the same files
        # thus we need to create a single file list with
        # all files from all reports.
        for file_index, file_path in report.files.items():
            if file_index not in file_sources:
                try:
                    with open(file_path, 'r', encoding='utf-8',
                              errors='ignore') as source_data:
                        content = source_data.read()
                except (OSError, IOError):
                    content = file_path + "" NOT FOUND.""
                file_sources[file_index] = {'id': file_index,
                                            'path': file_path,
                                            'content': content}

        events = []
        for element in report.bug_path:
            kind = element['kind']
            if kind == 'event':
                events.append({'location': element['location'],
                               'message':  element['message']})

        macros = []
        for macro in report.macro_expansions:
            macros.append({'location': macro['location'],
                           'expansion': macro['expansion'],
                           'name': macro['name']})

        notes = []
        for note in report.notes:
            notes.append({'location': note['location'],
                          'message': note['message']})

        report_hash = report.main['issue_hash_content_of_line_in_context']
        report_data.append({
            'events': events,
            'macros': macros,
            'notes': notes,
            'path': report.main['location']['file'],
            'reportHash': report_hash,
            'checkerName': report.main['check_name']})

    return {'files': file_sources,
            'reports': report_data}


def get_run_tag(client, run_ids, tag_name):
    """"""Return run tag information for the given tag name in the given runs.""""""
    run_history_filter = ttypes.RunHistoryFilter()
    run_history_filter.tagNames = [tag_name]
    run_histories = client.getRunHistory(run_ids, None, None,
                                         run_history_filter)

    return run_histories[0] if run_histories else None


def process_run_args(client, run_args_with_tag):
    """"""Process the argument and returns run ids and run tag ids.

    The elemnts inside the given run_args_with_tag list has the following
    format: <run_name>:<run_tag>
    """"""
    run_ids = []
    run_names = []
    run_tags = []

    for run_arg_with_tag in run_args_with_tag:
        run_with_tag = run_arg_with_tag.split(':')
        run_name = run_with_tag[0]
        run_filter = ttypes.RunFilter(names=[run_name])

        runs = get_run_data(client, run_filter)
        if not runs:
            LOG.warning(""No run names match the given pattern: %s"",
                        run_arg_with_tag)
            sys.exit(1)

        r_ids = [run.runId for run in runs]
        run_ids.extend(r_ids)

        r_names = [run.name for run in runs]
        run_names.extend(r_names)

        # Set base run tag if it is available.
        run_tag_name = run_with_tag[1] if len(run_with_tag) > 1 else None
        if run_tag_name:
            tag = get_run_tag(client, r_ids, run_tag_name)
            if tag:
                run_tags.append(tag.id)

    return run_ids, run_names, run_tags


def get_suppressed_reports(reports: List[Report]) -> List[str]:
    """"""Returns a list of suppressed report hashes.""""""
    suppressed_in_code = []
    sc_handler = SourceCodeCommentHandler()

    for rep in reports:
        bughash = rep.report_hash
        source_file = rep.main['location']['file']
        bug_line = rep.main['location']['line']
        checker_name = rep.main['check_name']
        src_comment_data = []
        with open(source_file, encoding='utf-8', errors='ignore') as sf:
            try:
                src_comment_data = sc_handler.filter_source_line_comments(
                    sf,
                    bug_line,
                    checker_name)
            except SpellException as ex:
                LOG.warning(""%s contains %s"",
                            os.path.basename(source_file),
                            str(ex))

        if len(src_comment_data) == 1:
            suppressed_in_code.append(bughash)
            LOG.debug(""Bug %s is suppressed in code. file: %s Line %s"",
                      bughash, source_file, bug_line)
        elif len(src_comment_data) > 1:
            LOG.warning(
                ""Multiple source code comment can be found ""
                ""for '%s' checker in '%s' at line %s. ""
                ""This bug will not be suppressed!"",
                checker_name, source_file, bug_line)
    return suppressed_in_code


def get_report_dir_results(report_dirs: List[str],
                           args: List[str],
                           severity_map: Dict[str, str]) -> List[Report]:
    """"""Get reports from the given report directories.

    Absolute paths are expected to the given report directories.
    """"""
    all_reports = []
    processed_path_hashes = set()
    for report_dir in report_dirs:
        for filename in os.listdir(report_dir):
            if not filename.endswith("".plist""):
                continue

            file_path = os.path.join(report_dir, filename)
            LOG.debug(""Parsing: %s"", file_path)
            _, reports = plist_parser.parse_plist_file(file_path)
            LOG.debug(""Parsing: %s done %s"", file_path, len(reports))
            for report in reports:
                LOG.debug(""get report hash"")
                path_hash = get_report_path_hash(report)
                if path_hash in processed_path_hashes:
                    LOG.debug(""Not showing report because it is a ""
                              ""deduplication of an already processed ""
                              ""report!"")
                    LOG.debug(""Path hash: %s"", path_hash)
                    LOG.debug(report)
                    continue

                if skip_report_dir_result(report, args, severity_map):
                    continue

                processed_path_hashes.add(path_hash)
                all_reports.append(report)

    return all_reports


def print_stats(report_count, file_stats, severity_stats):
    """"""Print summary of the report statistics.""""""
    print(""\n----==== Summary ====----"")
    if file_stats:
        vals = [[os.path.basename(k), v] for k, v in
                list(dict(file_stats).items())]
        vals.sort(key=itemgetter(0))
        keys = ['Filename', 'Report count']
        table = twodim.to_str('table', keys, vals, 1, True)
        print(table)

    if severity_stats:
        vals = [[k, v] for k, v in list(dict(severity_stats).items())]
        vals.sort(key=itemgetter(0))
        keys = ['Severity', 'Report count']
        table = twodim.to_str('table', keys, vals, 1, True)
        print(table)

    print(""----=================----"")
    print(""Total number of reports: {}"".format(report_count))
    print(""----=================----\n"")


def skip_report_dir_result(report, args, severity_map):
    """"""Returns True if the report should be skipped from the results.

    Skipping is done based on the given filter set.
    """"""
    f_severities, f_checkers, f_file_path, _, _, _ = check_filter_values(args)

    if f_severities:
        severity_name = severity_map.get(report.main['check_name'])
        if severity_name.lower() not in list(map(str.lower, f_severities)):
            return True

    if f_checkers:
        checker_name = report.main['check_name']
        if not any([re.match(r'^' + c.replace(""*"", "".*"") + '$',
                             checker_name, re.IGNORECASE)
                    for c in f_checkers]):
            return True

    if f_file_path:
        if not any([re.match(r'^' + f.replace(""*"", "".*"") + '$',
                             report.file_path, re.IGNORECASE)
                    for f in f_file_path]):
            return True

    if 'checker_msg' in args:
        checker_msg = report.main['description']
        if not any([re.match(r'^' + c.replace(""*"", "".*"") + '$',
                             checker_msg, re.IGNORECASE)
                    for c in args.checker_msg]):
            return True

    return False


def get_diff_base_results(client, args, baseids, base_hashes,
                          suppressed_hashes):
    """"""Get the run results from the server.""""""
    report_filter = ttypes.ReportFilter()
    add_filter_conditions(client, report_filter, args)
    report_filter.reportHash = base_hashes + suppressed_hashes

    sort_mode = [(ttypes.SortMode(
        ttypes.SortType.FILENAME,
        ttypes.Order.ASC))]

    limit = constants.MAX_QUERY_SIZE
    offset = 0

    base_results = []
    while True:
        results = client.getRunResults(baseids,
                                       limit,
                                       offset,
                                       sort_mode,
                                       report_filter,
                                       None,
                                       False)

        base_results.extend(results)
        offset += limit

        if len(results) < limit:
            break

    return base_results


def str_to_timestamp(date_str):
    """"""Return timestamp parsed from the given string parameter.""""""
    dateformat = '%Y-%m-%d %H:%M:%S.%f'
    date_time = date_str if isinstance(date_str, datetime) else \
        datetime.strptime(date_str, dateformat)

    return time.mktime(date_time.timetuple())


def check_run_names(client, check_names):
    """"""Check if the given names are valid runs on the server.

    If any of the names is not found then the script finishes.
    Otherwise a dictionary returns which maps run names to runs.
    The dictionary contains only the runs in check_names or
    all runs if check_names is empty or None.
    """"""
    run_filter = ttypes.RunFilter()
    run_filter.names = check_names
    run_filter.exactMatch = check_names is not None

    run_info = {run.name: run for run in get_run_data(client, run_filter)}

    if not check_names:
        return run_info

    missing_names = [name for name in check_names if not run_info.get(name)]
    if missing_names:
        LOG.warning(""The following runs were not found in the database: %s."",
                    ', '.join(missing_names))
        sys.exit(1)

    return run_info


def check_deprecated_arg_usage(args):
    if 'suppressed' in args:
        LOG.warning('""--suppressed"" option has been deprecated. Use '
                    '""--review-status"" option to get false positive '
                    '(suppressed) results.')

    if 'filter' in args:
        LOG.warning('""--filter"" option has been deprecated. Use '
                    'separate filter options of this command to filter the '
                    'results. For more information see the help.')

    if 'detected_at' in args:
        LOG.warning('""--detected-at"" option has been deprecated. Use '
                    '--detected-before/--detected-after options to filter the '
                    'results by detection date. For more information see the '
                    'help.')

    if 'fixed_at' in args:
        LOG.warning('""--fixed-at"" option has been deprecated. Use '
                    '--fixed-before/--fixed-after options to filter the '
                    'results by fix date. For more information see the '
                    'help.')


def get_run_data(client, run_filter, sort_mode=None,
                 limit=constants.MAX_QUERY_SIZE):
    """"""Get all runs based on the given run filter.""""""

    all_runs = []

    offset = 0
    while True:
        runs = runs = client.getRunData(run_filter, limit, offset, sort_mode)
        all_runs.extend(runs)
        offset += limit

        if len(runs) < limit:
            break

    return all_runs


def get_run_results(client,
                    run_ids,
                    limit,
                    offset,
                    sort_type,
                    report_filter,
                    compare_data,
                    query_report_details):
    """"""Get all the results with multiple api request.

    In each api request get the limit ammount of reports.
    Collect and return all the reports based on the filters.
    """"""

    all_results = []
    while True:
        results = client.getRunResults(run_ids,
                                       limit,
                                       offset,
                                       sort_type,
                                       report_filter,
                                       compare_data,
                                       query_report_details)
        all_results.extend(results)
        offset += limit
        if len(results) < limit:
            break

    return all_results


def validate_filter_values(user_values, valid_values, value_type):
    """"""
    Check if the value provided by the user is a valid value.
    """"""
    if not user_values:
        return True

    non_valid_values = [status for status in user_values
                        if valid_values.get(status.upper(), None) is None]

    if non_valid_values:
        invalid_values = ','.join([x.lower() for x in non_valid_values])
        LOG.error('Invalid %s value(s): %s', value_type, invalid_values)

        valid_values = ','.join([x.lower() for x in valid_values.keys()])
        LOG.error('Valid values are: %s', valid_values)

        return False

    return True


def check_filter_values(args):
    """"""
    Check if filter values are valid values. Returns values which are checked
    or exit from the interpreter.
    """"""
    severities = checkers = file_path = dt_statuses = rw_statuses = None
    bug_path_length = None

    filter_str = args.filter if 'filter' in args else None
    if filter_str:
        if filter_str.count(':') != 4:
            LOG.warning(""Filter string has to contain four colons (e.g. ""
                        ""\""high,medium:unix,core:*.cpp:new,unresolved:""
                        ""false_positive,intentional\"")."")
        else:
            filter_values = []
            for x in filter_str.strip().split(':'):
                values = [y.strip() for y in x.strip().split(',') if y.strip()]
                filter_values.append(values if values else None)

            # pylint: disable=unbalanced-tuple-unpacking
            severities, checkers, file_path, dt_statuses, rw_statuses = \
                filter_values

    if 'severity' in args:
        severities = args.severity

    if 'detection_status' in args:
        dt_statuses = args.detection_status

    if 'review_status' in args:
        rw_statuses = args.review_status

    if 'checker_name' in args:
        checkers = args.checker_name

    if 'file_path' in args:
        file_path = args.file_path

    if 'bug_path_length' in args:
        path_lengths = args.bug_path_length.split(':')

        min_bug_path_length = int(path_lengths[0]) if \
            path_lengths and path_lengths[0].isdigit() else None

        max_bug_path_length = int(path_lengths[1]) if \
            len(path_lengths) > 1 and path_lengths[1].isdigit() else None

        bug_path_length = BugPathLengthRange(min=min_bug_path_length,
                                             max=max_bug_path_length)

    values_to_check = [
        (severities, ttypes.Severity._NAMES_TO_VALUES, 'severity'),
        (dt_statuses, ttypes.DetectionStatus._NAMES_TO_VALUES,
         'detection status'),
        (rw_statuses, ttypes.ReviewStatus._NAMES_TO_VALUES,
         'review status')]

    if not all(valid for valid in
               [validate_filter_values(*x) for x in values_to_check]):
        sys.exit(1)
    return severities, checkers, file_path, dt_statuses, rw_statuses, \
        bug_path_length


def add_filter_conditions(client, report_filter, args):
    """"""
    This function fills some attributes of the given report filter based on
    the arguments which is provided in the command line.
    """"""

    severities, checkers, file_path, dt_statuses, rw_statuses, \
        bug_path_length = check_filter_values(args)

    report_filter.isUnique = args.uniqueing == 'on'

    if severities:
        report_filter.severity = [
            ttypes.Severity._NAMES_TO_VALUES[x.upper()] for x in severities]

    if dt_statuses:
        report_filter.detectionStatus = [
            ttypes.DetectionStatus._NAMES_TO_VALUES[x.upper()] for x in
            dt_statuses]

    if rw_statuses:
        report_filter.reviewStatus = [
            ttypes.ReviewStatus._NAMES_TO_VALUES[x.upper()] for x in
            rw_statuses]

    if checkers:
        report_filter.checkerName = checkers

    if 'checker_msg' in args:
        report_filter.checkerMsg = args.checker_msg

    if 'analyzer_name' in args:
        report_filter.analyzerNames = args.analyzer_name

    if 'component' in args:
        report_filter.componentNames = args.component

    if 'report_hash' in args:
        report_filter.reportHash = args.report_hash

    if file_path:
        report_filter.filepath = file_path

    if bug_path_length:
        report_filter.bugPathLength = \
            ttypes.BugPathLengthRange(min=bug_path_length.min,
                                      max=bug_path_length.max)

    if 'tag' in args:
        run_history_filter = ttypes.RunHistoryFilter(tagNames=args.tag)
        run_histories = client.getRunHistory(None, None, None,
                                             run_history_filter)
        if run_histories:
            report_filter.runTag = [t.id for t in run_histories]

    if 'open_reports_date' in args:
        report_filter.openReportsDate = \
            int(str_to_timestamp(args.open_reports_date))

    if 'detected_at' in args:
        report_filter.firstDetectionDate = \
            int(str_to_timestamp(args.detected_at))

    if 'fixed_at' in args:
        report_filter.fixDate = int(str_to_timestamp(args.fixed_at))

    detected_at = None
    fixed_at = None

    if 'detected_before' in args or 'detected_after' in args:
        detected_at = ttypes.DateInterval()

        if 'detected_before' in args:
            detected_at.before = int(str_to_timestamp(args.detected_before))

        if 'detected_after' in args:
            detected_at.after = int(str_to_timestamp(args.detected_after))

    if 'fixed_before' in args or 'fixed_after' in args:
        fixed_at = ttypes.DateInterval()

        if 'fixed_before' in args:
            fixed_at.before = int(str_to_timestamp(args.fixed_before))

        if 'fixed_after' in args:
            fixed_at.after = int(str_to_timestamp(args.fixed_after))

    if detected_at or fixed_at:
        report_filter.date = ttypes.ReportDate(detected=detected_at,
                                               fixed=fixed_at)


def process_run_filter_conditions(args):
    """"""
    This function fills some attributes of the given run filter based on
    the arguments which is provided in the command line.
    """"""
    run_filter = ttypes.RunFilter()

    if 'names' in args:
        run_filter.names = args.names
        run_filter.exactMatch = False
    elif 'all_after_run' in args:
        run_filter.afterRun = args.all_after_run
    elif 'all_before_run' in args:
        run_filter.beforeRun = args.all_before_run
    elif 'all_after_time' in args:
        run_filter.afterTime = int(str_to_timestamp(args.all_after_time))
    elif 'all_before_time' in args:
        run_filter.beforeTime = int(str_to_timestamp(args.all_before_time))

    return run_filter


# ---------------------------------------------------------------------------
# Argument handlers for the 'CodeChecker cmd' subcommands.
# ---------------------------------------------------------------------------


def handle_list_runs(args):
    # If the given output format is not 'table', redirect logger's output to
    # the stderr.
    stream = None
    if 'output_format' in args and args.output_format != 'table':
        stream = 'stderr'

    init_logger(args.verbose if 'verbose' in args else None, stream)

    client = setup_client(args.product_url)

    run_filter = process_run_filter_conditions(args)

    sort_type = run_sort_type_enum(args.sort_type)
    sort_order = ttypes.Order._NAMES_TO_VALUES[args.sort_order.upper()]
    sort_mode = ttypes.RunSortMode(sort_type, sort_order)

    runs = get_run_data(client, run_filter, sort_mode)

    if args.output_format == 'json':
        # This json is different from the json format printed by the
        # parse command. This json converts the ReportData type report
        # to a json format.
        results = []
        for run in runs:
            results.append({run.name: run})
        print(CmdLineOutputEncoder().encode(results))

    else:  # plaintext, csv
        header = ['Name', 'Number of unresolved reports',
                  'Analyzer statistics', 'Storage date', 'Version tag',
                  'Duration', 'Description', 'CodeChecker version']
        rows = []
        for run in runs:
            duration = str(timedelta(seconds=run.duration)) \
                if run.duration > -1 else 'Not finished'

            analyzer_statistics = []
            for analyzer in run.analyzerStatistics:
                stat = run.analyzerStatistics[analyzer]
                num_of_all_files = stat.successful + stat.failed
                analyzer_statistics.append(analyzer + ' (' +
                                           str(num_of_all_files) + '/' +
                                           str(stat.successful) + ')')

            codechecker_version = run.codeCheckerVersion \
                if run.codeCheckerVersion else ''
            description = run.description if run.description else ''

            rows.append((run.name,
                         str(run.resultCount),
                         ', '.join(analyzer_statistics),
                         run.runDate,
                         run.versionTag if run.versionTag else '',
                         duration,
                         description,
                         codechecker_version))

        print(twodim.to_str(args.output_format, header, rows))


def handle_list_results(args):
    # If the given output format is not 'table', redirect logger's output to
    # the stderr.
    stream = None
    if 'output_format' in args and args.output_format != 'table':
        stream = 'stderr'

    init_logger(args.verbose if 'verbose' in args else None, stream)

    check_deprecated_arg_usage(args)

    client = setup_client(args.product_url)

    run_filter = ttypes.RunFilter(names=args.names)

    run_ids = [run.runId for run in get_run_data(client, run_filter)]
    if not run_ids:
        LOG.warning(""No runs were found!"")
        sys.exit(1)

    report_filter = ttypes.ReportFilter()
    add_filter_conditions(client, report_filter, args)

    query_report_details = args.details and args.output_format == 'json' \
        if 'details' in args else None

    all_results = get_run_results(client,
                                  run_ids,
                                  constants.MAX_QUERY_SIZE,
                                  0,
                                  None,
                                  report_filter,
                                  None,
                                  query_report_details)

    if args.output_format == 'json':
        print(CmdLineOutputEncoder().encode(all_results))
    else:
        header = ['File', 'Checker', 'Severity', 'Message', 'Bug path length',
                  'Analyzer name', 'Review status', 'Detection status']

        rows = []
        max_msg_len = 50
        for res in all_results:
            bug_line = res.line
            checked_file = res.checkedFile
            if bug_line is not None:
                checked_file += ' @ ' + str(bug_line)

            sev = ttypes.Severity._VALUES_TO_NAMES[res.severity]
            rw_status = \
                ttypes.ReviewStatus._VALUES_TO_NAMES[res.reviewData.status]

            dt_status = 'N/A'

            status = res.detectionStatus
            if status is not None:
                dt_status = ttypes.DetectionStatus._VALUES_TO_NAMES[status]

            # Remove whitespace characters from the checker message.
            msg = re.sub(r'\s+', ' ', res.checkerMsg)

            # Avoid too long cell content.
            if len(msg) > max_msg_len:
                msg = msg[:max_msg_len] + '...'

            rows.append((checked_file, res.checkerId, sev, msg,
                         res.bugPathLength, res.analyzerName, rw_status,
                         dt_status))

        print(twodim.to_str(args.output_format, header, rows))


def handle_diff_results(args):
    # If the given output format is not 'table', redirect logger's output to
    # the stderr.
    stream = None
    if 'output_format' in args and args.output_format != 'table':
        stream = 'stderr'
    if len(args.output_format) > 1 and ('export_dir' not in args):
        LOG.error(""Export directory is required if multiple output formats ""
                  ""are selected!"")

    init_logger(args.verbose if 'verbose' in args else None, stream)

    require_export_dir = any([o in ['html', 'gerrit', 'codeclimate']
                              for o in args.output_format])
    if require_export_dir and 'export_dir' not in args:
        LOG.error(""No export directory given!"")
        sys.exit(1)

    check_deprecated_arg_usage(args)
    context = webserver_context.get_context()
    source_line_contents = {}

    def get_diff_local_dir_remote_run(client, report_dirs, remote_run_names):
        """"""Compare a local report directory with a remote run.""""""
        filtered_reports = []
        report_dir_results = get_report_dir_results(report_dirs,
                                                    args,
                                                    context.severity_map)
        suppressed_in_code = get_suppressed_reports(report_dir_results)

        diff_type = get_diff_type(args)
        run_ids, run_names, _ = process_run_args(client, remote_run_names)
        local_report_hashes = set([r.report_hash for r in report_dir_results])

        if diff_type == ttypes.DiffType.NEW:
            # Get report hashes which can be found only in the remote runs.
            remote_hashes = \
                client.getDiffResultsHash(run_ids,
                                          local_report_hashes,
                                          ttypes.DiffType.RESOLVED,
                                          None)

            results = get_diff_base_results(client, args, run_ids,
                                            remote_hashes,
                                            suppressed_in_code)
            for result in results:
                filtered_reports.append(result)
        elif diff_type == ttypes.DiffType.UNRESOLVED:
            # Get remote hashes which can be found in the remote run and in the
            # local report directory.
            remote_hashes = \
                client.getDiffResultsHash(run_ids,
                                          local_report_hashes,
                                          ttypes.DiffType.UNRESOLVED,
                                          None)
            for result in report_dir_results:
                rep_h = result.report_hash
                if rep_h in remote_hashes and rep_h not in suppressed_in_code:
                    filtered_reports.append(result)
        elif diff_type == ttypes.DiffType.RESOLVED:
            # Get remote hashes which can be found in the remote run and in the
            # local report directory.
            remote_hashes = \
                client.getDiffResultsHash(run_ids,
                                          local_report_hashes,
                                          ttypes.DiffType.UNRESOLVED,
                                          None)
            for result in report_dir_results:
                if result.report_hash not in remote_hashes:
                    filtered_reports.append(result)
        return filtered_reports, run_names

    def get_diff_remote_run_local_dir(client, remote_run_names, report_dirs):
        """"""
        Compares a remote run with a local report directory.
        """"""
        filtered_reports = []
        report_dir_results = get_report_dir_results(report_dirs,
                                                    args,
                                                    context.severity_map)
        suppressed_in_code = get_suppressed_reports(report_dir_results)

        diff_type = get_diff_type(args)
        run_ids, run_names, _ = process_run_args(client, remote_run_names)
        local_report_hashes = set([r.report_hash for r in report_dir_results])

        remote_hashes = client.getDiffResultsHash(run_ids,
                                                  local_report_hashes,
                                                  diff_type,
                                                  None)

        if diff_type in [ttypes.DiffType.NEW, ttypes.DiffType.UNRESOLVED]:
            # Shows reports from the report dir which are not present in
            # the baseline (NEW reports) or appear in both side (UNRESOLVED
            # reports) and not suppressed in the code.
            for result in report_dir_results:
                rep_h = result.report_hash
                if rep_h in remote_hashes and rep_h not in suppressed_in_code:
                    filtered_reports.append(result)
        elif diff_type == ttypes.DiffType.RESOLVED:
            # Show bugs in the baseline (server) which are not present in
            # the report dir or suppressed.
            results = get_diff_base_results(client,
                                            args,
                                            run_ids,
                                            remote_hashes,
                                            suppressed_in_code)
            for result in results:
                filtered_reports.append(result)

        return filtered_reports, run_names

    def get_diff_remote_runs(client, remote_base_run_names,
                             remote_new_run_names):
        """"""
        Compares two remote runs and returns the filtered results.
        """"""
        report_filter = ttypes.ReportFilter()
        add_filter_conditions(client, report_filter, args)

        base_ids, base_run_names, base_run_tags = \
            process_run_args(client, remote_base_run_names)
        report_filter.runTag = base_run_tags

        cmp_data = ttypes.CompareData()
        cmp_data.diffType = get_diff_type(args)

        new_ids, new_run_names, new_run_tags = \
            process_run_args(client, remote_new_run_names)
        cmp_data.runIds = new_ids
        cmp_data.runTag = new_run_tags

        # Do not show resolved bugs in compare mode new.
        if cmp_data.diffType == ttypes.DiffType.NEW:
            report_filter.detectionStatus = [
                ttypes.DetectionStatus.NEW,
                ttypes.DetectionStatus.UNRESOLVED,
                ttypes.DetectionStatus.REOPENED]

        sort_mode = [(ttypes.SortMode(
            ttypes.SortType.FILENAME,
            ttypes.Order.ASC))]

        all_results = get_run_results(client,
                                      base_ids,
                                      constants.MAX_QUERY_SIZE,
                                      0,
                                      sort_mode,
                                      report_filter,
                                      cmp_data,
                                      False)

        return all_results, base_run_names, new_run_names

    def get_diff_local_dirs(base_run_names, new_run_names):
        """"""
        Compares two report directories and returns the filtered results.
        """"""
        filtered_reports = []
        base_results = get_report_dir_results(base_run_names,
                                              args,
                                              context.severity_map)
        new_results = get_report_dir_results(new_run_names,
                                             args,
                                             context.severity_map)

        base_hashes = set([res.report_hash for res in base_results])
        new_hashes = set([res.report_hash for res in new_results])

        diff_type = get_diff_type(args)
        if diff_type == ttypes.DiffType.NEW:
            for res in new_results:
                if res.report_hash not in base_hashes:
                    filtered_reports.append(res)
        if diff_type == ttypes.DiffType.UNRESOLVED:
            for res in new_results:
                if res.report_hash in base_hashes:
                    filtered_reports.append(res)
        elif diff_type == ttypes.DiffType.RESOLVED:
            for res in base_results:
                if res.report_hash not in new_hashes:
                    filtered_reports.append(res)

        return filtered_reports

    def cached_report_file_lookup(file_cache, file_id):
        """"""
        Get source file data for the given file and caches it in a file cache
        if file data is not found in the cache. Finally, it returns the source
        file data from the cache.
        """"""
        if file_id not in file_cache:
            source = client.getSourceFileData(file_id, True,
                                              ttypes.Encoding.BASE64)
            file_content = convert.from_b64(source.fileContent)
            file_cache[file_id] = {'id': file_id,
                                   'path': source.filePath,
                                   'content': file_content}

        return file_cache[file_id]

    def get_report_data(client, reports, file_cache):
        """"""
        Returns necessary report files and report data events for the HTML
        plist parser.
        """"""
        file_sources = {}
        report_data = []

        for report in reports:
            file_sources[report.fileId] = cached_report_file_lookup(
                file_cache, report.fileId)

            details = client.getReportDetails(report.reportId)
            events = []
            for event in details.pathEvents:
                file_sources[event.fileId] = cached_report_file_lookup(
                    file_cache, event.fileId)

                location = {'line': event.startLine,
                            'col': event.startCol,
                            'file': event.fileId}

                events.append({'location': location,
                               'message': event.msg})

            # Get extended data.
            macros = []
            notes = []
            for extended_data in details.extendedData:
                file_sources[extended_data.fileId] = cached_report_file_lookup(
                    file_cache, extended_data.fileId)

                location = {'line': extended_data.startLine,
                            'col': extended_data.startCol,
                            'file': extended_data.fileId}

                if extended_data.type == ttypes.ExtendedReportDataType.MACRO:
                    macros.append({'location': location,
                                   'expansion': event.msg})
                elif extended_data.type == ttypes.ExtendedReportDataType.NOTE:
                    notes.append({'location': location,
                                  'message': event.msg})

            report_data.append({
                'events': events,
                'macros': macros,
                'notes': notes,
                'path': report.checkedFile,
                'reportHash': report.bugHash,
                'checkerName': report.checkerId})

        return {'files': file_sources,
                'reports': report_data}

    def report_to_html(client, reports, output_dir):
        """"""
        Generate HTML output files for the given reports in the given output
        directory by using the Plist To HTML builder.
        """"""
        html_builder = PlistToHtml.HtmlBuilder(
            context.path_plist_to_html_dist,
            context.severity_map)
        file_stats = defaultdict(int)
        severity_stats = defaultdict(int)
        file_report_map = defaultdict(list)
        for report in reports:
            if isinstance(report, Report):
                file_path = report.main['location']['file']

                check_name = report.main['check_name']
                sev = context.severity_map.get(check_name)
            else:
                file_path = report.checkedFile
                sev = ttypes.Severity._VALUES_TO_NAMES[report.severity]

            file_report_map[file_path].append(report)
            file_stats[file_path] += 1
            severity_stats[sev] += 1

        file_cache = {}
        for file_path, file_reports in file_report_map.items():
            checked_file = file_path
            filename = os.path.basename(checked_file)
            h = int(
                hashlib.md5(
                    file_path.encode('utf-8')).hexdigest(),
                16) % (10 ** 8)

            if isinstance(file_reports[0], Report):
                report_data = reports_to_html_report_data(file_reports)
            else:
                report_data = get_report_data(client, file_reports, file_cache)

            output_path = os.path.join(output_dir,
                                       filename + '_' + str(h) + '.html')
            html_builder.create(output_path, report_data)
            print('Html file was generated for file://{0}: file://{1}'.format(
                checked_file, output_path))

        html_builder.create_index_html(output_dir)
        print_stats(len(reports), file_stats, severity_stats)

    def print_reports(client,
                      reports: List[Report],
                      output_formats: List[str]):
        if 'json' in output_formats:
            out = []
            for report in reports:
                if isinstance(report, Report):
                    report = \
                        report_type_converter.report_to_reportData(
                            report, context.severity_map)
                    out.append(report)
                else:
                    out.append(report)
            print(CmdLineOutputEncoder().encode(out))

            # Json was the only format specified.
            if len(output_formats) == 1:
                return

            output_formats.remove('json')

        if 'html' in output_formats:
            print(""Generating HTML output files to file://{0} directory:\n""
                  .format(output_dir))

            report_to_html(client, reports, output_dir)

            print('\nTo view the results in a browser run:\n'
                  '  $ firefox {0}\n'.format(os.path.join(output_dir,
                                                          'index.html')))

            # HTML was the only format specified.
            if len(output_formats) == 1:
                return

            output_formats.remove('html')

        # Collect source line contents for the report type got from the server.
        source_lines = defaultdict(set)
        for report in reports:
            if not isinstance(report, Report) and report.line is not None:
                source_lines[report.fileId].add(report.line)
        if client:
            lines_in_files_requested = []
            for key in source_lines:
                lines_in_files_requested.append(
                    ttypes.LinesInFilesRequested(fileId=key,
                                                 lines=source_lines[key]))

            source_line_contents.update(client.getLinesInSourceFileContents(
                lines_in_files_requested, ttypes.Encoding.BASE64))

        # Convert all the reports to the common Report
        # type for printing and formatting to various formats.
        converted_reports = []
        for report in reports:
            if not isinstance(report, Report):
                r = report_type_converter.reportData_to_report(report)
                if source_line_contents:
                    source_line = convert.from_b64(
                        source_line_contents[report.fileId][report.line])
                    r.source_line = source_line
                converted_reports.append(r)
            else:
                converted_reports.append(report)
        reports = converted_reports

        repo_dir = os.environ.get('CC_REPO_DIR')
        if repo_dir:
            for report in reports:
                report.trim_path_prefixes([repo_dir])

        if 'gerrit' in output_formats:
            gerrit_reports = gerrit.convert(reports, context.severity_map)

            gerrit_review_json = os.path.join(output_dir,
                                              'gerrit_review.json')
            with open(gerrit_review_json, 'w') as review_file:
                json.dump(gerrit_reports, review_file)
            LOG.info(""Gerrit review file was created: %s\n"",
                     gerrit_review_json)

            # Gerrit was the only format specified.
            if len(output_formats) == 1:
                return
            output_formats.remove('gerrit')

        if 'codeclimate' in output_formats:
            cc_reports = codeclimate.convert(reports)

            codeclimate_issues_json = os.path.join(output_dir,
                                                   'codeclimate_issues.json')
            with open(codeclimate_issues_json, 'w') as issues_f:
                json.dump(cc_reports, issues_f)

            LOG.info(""Code Climate file was created: %s\n"",
                     codeclimate_issues_json)

            # codeclimate was the only format specified.
            if len(output_formats) == 1:
                return

            output_formats.remove('codeclimate')

        header = ['File', 'Checker', 'Severity', 'Msg', 'Source']
        rows = []

        file_stats = defaultdict(int)
        severity_stats = defaultdict(int)

        changed_files = set()
        for report in reports:
            severity = context.severity_map.get(report.check_name,
                                                'UNSPECIFIED')
            file_name = report.file_path
            checked_file = file_name \
                + ':' + str(report.line) + "":"" + str(report.col)
            check_msg = report.description
            source_line = None
            if os.path.exists(file_name):
                source_line = util.get_line(file_name, report.line)
            if source_line is None:
                changed_files.add(file_name)
                continue

            rows.append((severity,
                         checked_file,
                         check_msg,
                         report.check_name,
                         report.source_line))

            severity_stats[severity] += 1
            file_stats[file_name] += 1

        for output_format in output_formats:
            if output_format == 'plaintext':
                for row in rows:
                    print(""[{0}] {1}: {2} [{3}]\n{4}\n"".format(
                        row[0], row[1], row[2], row[3], row[4]))
            else:
                print(twodim.to_str(output_format, header, rows))

        print_stats(len(reports), file_stats, severity_stats)

        if changed_files:
            changed_f = '\n'.join([' - ' + f for f in changed_files])
            LOG.warning(""The following source file contents changed since the ""
                        ""latest analysis:\n%s\nMultiple reports were not ""
                        ""shown and skipped from the statistics. Please ""
                        ""analyze your project again to update the ""
                        ""reports!"", changed_f)

    basename_local_dirs, basename_run_names = \
        filter_localdir_remote_run(args.base_run_names)

    newname_local_dirs, newname_run_names = \
        filter_localdir_remote_run(args.new_run_names)

    has_different_run_args = False
    if basename_local_dirs and basename_run_names:
        LOG.error(""All base run names must have the same type: local ""
                  ""directory (%s) or run names (%s)."",
                  ', '.join(basename_local_dirs),
                  ', '.join(basename_run_names))
        has_different_run_args = True

    if newname_local_dirs and newname_run_names:
        LOG.error(""All new run names must have the same type: local ""
                  ""directory (%s) or run names (%s)."",
                  ', '.join(newname_local_dirs),
                  ', '.join(newname_run_names))
        has_different_run_args = True

    if has_different_run_args:
        sys.exit(1)

    output_dir = args.export_dir if 'export_dir' in args else None
    if 'clean' in args and os.path.isdir(output_dir):
        print(""Previous analysis results in '{0}' have been removed, ""
              ""overwriting with current results."".format(output_dir))
        shutil.rmtree(output_dir)

    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    client = None
    # We set up the client if we are not comparing two local report directory.
    if basename_run_names or newname_run_names:
        client = setup_client(args.product_url)

    if basename_local_dirs and newname_local_dirs:
        reports = get_diff_local_dirs(basename_local_dirs,
                                      newname_local_dirs)
        print_reports(client, reports, args.output_format)
        LOG.info(""Compared the following local report directories: %s and %s"",
                 ', '.join(basename_local_dirs),
                 ', '.join(newname_local_dirs))
    elif newname_local_dirs:
        reports, matching_base_run_names = \
            get_diff_remote_run_local_dir(client,
                                          basename_run_names,
                                          newname_local_dirs)
        print_reports(client, reports, args.output_format)
        LOG.info(""Compared remote run(s) %s (matching: %s) and local report ""
                 ""directory(s) %s"",
                 ', '.join(basename_run_names),
                 ', '.join(matching_base_run_names),
                 ', '.join(newname_local_dirs))
    elif basename_local_dirs:
        reports, matching_new_run_names = \
            get_diff_local_dir_remote_run(client,
                                          basename_local_dirs,
                                          newname_run_names)

        print_reports(client, reports, args.output_format)
        LOG.info(""Compared local report directory(s) %s and remote run(s) %s ""
                 ""(matching: %s)."",
                 ', '.join(basename_local_dirs),
                 ', '.join(newname_run_names),
                 ', '.join(matching_new_run_names))
    else:
        reports, matching_base_run_names, matching_new_run_names = \
            get_diff_remote_runs(client, basename_run_names, newname_run_names)
        print_reports(client, reports, args.output_format)
        LOG.info(""Compared multiple remote runs %s (matching: %s) and %s ""
                 ""(matching: %s)"",
                 ', '.join(basename_run_names),
                 ', '.join(matching_base_run_names),
                 ', '.join(newname_run_names),
                 ', '.join(matching_new_run_names))


def handle_list_result_types(args):
    # If the given output format is not 'table', redirect logger's output to
    # the stderr.
    stream = None
    if 'output_format' in args and args.output_format != 'table':
        stream = 'stderr'

    init_logger(args.verbose if 'verbose' in args else None, stream)
    check_deprecated_arg_usage(args)

    if 'disable_unique' in args:
        LOG.warning(""--disable-unique option is deprecated. Please use the ""
                    ""'--uniqueing on' option to get uniqueing results."")
        args.uniqueing = 'off'

    def get_statistics(client, run_ids, field, values):
        report_filter = ttypes.ReportFilter()
        add_filter_conditions(client, report_filter, args)

        setattr(report_filter, field, values)
        checkers = client.getCheckerCounts(run_ids,
                                           report_filter,
                                           None,
                                           None,
                                           0)

        return dict((res.name, res.count) for res in checkers)

    def checker_count(checker_dict, key):
        return checker_dict.get(key, 0)

    client = setup_client(args.product_url)

    run_ids = None
    if 'all_results' not in args:
        run_filter = ttypes.RunFilter(names=args.names)
        run_ids = [run.runId for run in get_run_data(client, run_filter)]
        if not run_ids:
            LOG.warning(""No runs were found!"")
            sys.exit(1)

    all_checkers_report_filter = ttypes.ReportFilter()
    add_filter_conditions(client, all_checkers_report_filter, args)

    all_checkers = client.getCheckerCounts(run_ids,
                                           all_checkers_report_filter,
                                           None,
                                           None,
                                           0)
    all_checkers_dict = dict((res.name, res) for res in all_checkers)

    unrev_checkers = get_statistics(client, run_ids, 'reviewStatus',
                                    [ttypes.ReviewStatus.UNREVIEWED])

    confirmed_checkers = get_statistics(client, run_ids, 'reviewStatus',
                                        [ttypes.ReviewStatus.CONFIRMED])

    false_checkers = get_statistics(client, run_ids, 'reviewStatus',
                                    [ttypes.ReviewStatus.FALSE_POSITIVE])

    intentional_checkers = get_statistics(client, run_ids, 'reviewStatus',
                                          [ttypes.ReviewStatus.INTENTIONAL])

    # Get severity counts
    report_filter = ttypes.ReportFilter()
    add_filter_conditions(client, report_filter, args)

    sev_count = client.getSeverityCounts(run_ids, report_filter, None)

    severities = []
    severity_total = 0
    for key, count in sorted(list(sev_count.items()),
                             reverse=True):
        severities.append(dict(
            severity=ttypes.Severity._VALUES_TO_NAMES[key],
            reports=count))
        severity_total += count

    all_results = []
    total = defaultdict(int)
    for key, checker_data in sorted(list(all_checkers_dict.items()),
                                    key=lambda x: x[1].severity,
                                    reverse=True):
        all_results.append(dict(
            checker=key,
            severity=ttypes.Severity._VALUES_TO_NAMES[checker_data.severity],
            reports=checker_data.count,
            unreviewed=checker_count(unrev_checkers, key),
            confirmed=checker_count(confirmed_checkers, key),
            false_positive=checker_count(false_checkers, key),
            intentional=checker_count(intentional_checkers, key)
         ))
        total['total_reports'] += checker_data.count
        total['total_unreviewed'] += checker_count(unrev_checkers, key)
        total['total_confirmed'] += checker_count(confirmed_checkers, key)
        total['total_false_positive'] += checker_count(false_checkers, key)
        total['total_intentional'] += checker_count(intentional_checkers, key)

    if args.output_format == 'json':
        print(CmdLineOutputEncoder().encode(all_results))
    else:
        header = ['Checker', 'Severity', 'Unreviewed', 'Confirmed',
                  'False positive', 'Intentional', 'All reports']

        rows = []
        for stat in all_results:
            rows.append((stat['checker'],
                         stat['severity'],
                         str(stat['unreviewed']),
                         str(stat['confirmed']),
                         str(stat['false_positive']),
                         str(stat['intentional']),
                         str(stat['reports'])))

        rows.append(('Total', '-',
                     str(total['total_unreviewed']),
                     str(total['total_confirmed']),
                     str(total['total_false_positive']),
                     str(total['total_intentional']),
                     str(total['total_reports'])))

        print(twodim.to_str(args.output_format, header, rows,
                            separate_footer=True))

        # Print severity counts
        header = ['Severity', 'All reports']

        rows = []
        for stat in severities:
            rows.append((stat['severity'],
                         str(stat['reports'])))

        rows.append(('Total', str(severity_total)))

        print(twodim.to_str(args.output_format, header, rows,
                            separate_footer=True))


def handle_remove_run_results(args):

    init_logger(args.verbose if 'verbose' in args else None)

    client = setup_client(args.product_url)

    run_filter = process_run_filter_conditions(args)
    if client.removeRun(None, run_filter):
        LOG.info(""Done."")
    else:
        LOG.error(""Failed to remove runs!"")


def handle_update_run(args):
    """"""
    Argument handler for the 'CodeChecker cmd update' subcommand.
    """"""
    init_logger(args.verbose if 'verbose' in args else None)

    if not args.new_run_name:
        LOG.error(""The new run name can not be empty!"")
        sys.exit(1)

    client = setup_client(args.product_url)

    run_info = check_run_names(client, [args.run_name])
    run = run_info.get(args.run_name)
    if not run:
        LOG.warning(""No run name with the name '%s' was found."", args.name)
        return

    try:
        client.updateRunData(run.runId, args.new_run_name)
    except RequestFailed as reqfail:
        LOG.error(reqfail.message)
        sys.exit(1)

    LOG.info(""Done."")


def handle_suppress(args):

    init_logger(args.verbose if 'verbose' in args else None)

    limit = constants.MAX_QUERY_SIZE

    client = setup_client(args.product_url)

    run_info = check_run_names(client, [args.name])
    run = run_info.get(args.name)
    if not run:
        LOG.warning(""No run name with the name '%s' was found."", args.name)
        return

    if 'input' in args:
        with open(args.input, encoding='utf-8', errors='ignore') as supp_file:
            suppress_data = suppress_file_handler.get_suppress_data(supp_file)

        for bug_id, file_name, comment, status in suppress_data:
            file_name = '%' + file_name
            bug_hash_filter = ttypes.ReportFilter(filepath=[file_name],
                                                  reportHash=[bug_id])
            reports = client.getRunResults([run.runId], limit, 0, None,
                                           bug_hash_filter,
                                           None, False)

            for report in reports:
                rw_status = ttypes.ReviewStatus.FALSE_POSITIVE
                if status == 'confirmed':
                    rw_status = ttypes.ReviewStatus.CONFIRMED
                elif status == 'intentional':
                    rw_status = ttypes.ReviewStatus.INTENTIONAL

                client.changeReviewStatus(report.reportId, rw_status, comment)


def handle_login(args):

    init_logger(args.verbose if 'verbose' in args else None)

    protocol, host, port = split_server_url(args.server_url)
    login_user(protocol, host, port, args.username,
               login='logout' not in args)


def handle_list_run_histories(args):
    # If the given output format is not 'table', redirect logger's output to
    # the stderr.
    stream = None
    if 'output_format' in args and args.output_format != 'table':
        stream = 'stderr'

    init_logger(args.verbose if 'verbose' in args else None, stream)

    client = setup_client(args.product_url)
    run_ids = None
    if 'names' in args:
        run_filter = ttypes.RunFilter(names=args.names)
        runs = get_run_data(client, run_filter)
        run_ids = [r.runId for r in runs]

    run_history = client.getRunHistory(run_ids, None, None, None)

    if args.output_format == 'json':
        print(CmdLineOutputEncoder().encode(run_history))
    else:  # plaintext, csv
        header = ['Date', 'Run name', 'Version tag', 'User',
                  'CodeChecker version', 'Analyzer statistics', 'Description']
        rows = []
        for h in run_history:
            analyzer_statistics = []
            for analyzer in h.analyzerStatistics:
                stat = h.analyzerStatistics[analyzer]
                num_of_all_files = stat.successful + stat.failed
                analyzer_statistics.append(analyzer + ' (' +
                                           str(num_of_all_files) + '/' +
                                           str(stat.successful) + ')')

            rows.append((h.time,
                         h.runName,
                         h.versionTag if h.versionTag else '',
                         h.user,
                         h.codeCheckerVersion if h.codeCheckerVersion else '',
                         ', '.join(analyzer_statistics),
                         h.description if h.description else ''))

        print(twodim.to_str(args.output_format, header, rows))
","I think we should extend the help of this command too with this information. And mention that if only 1 output is given, the results will be printed to the standard output.","### Review of Suggested Code Changes

#### Key Points:
1. **Logging Initialization**: The `init_logger` function is used to set up logging with the specified level, stream, and logger name. This ensures consistent logging across the application.
2. **Argument Parsing**: Functions like `filter_localdir_remote_run`, `run_sort_type_str`, `run_sort_type_enum`, `get_diff_type`, `reports_to_html_report_data`, etc., handle various aspects of argument parsing and conversion.
3. **Report Processing**: Functions like `get_report_dir_results`, `print_stats`, `skip_report_dir_result`, `get_diff_base_results`, etc., manage the retrieval and processing of reports from different sources.
4. **Filter Handling**: Functions like `check_filter_values`, `add_filter_conditions`, and `process_run_filter_conditions` ensure that filter parameters are validated and applied correctly.
5. **Command Handlers**: Each command handler (`handle_list_runs`, `handle_list_results`, `handle_diff_results`, etc.) processes the command-line arguments and interacts with the CodeChecker API to retrieve and display results.

#### Specific Changes:
- **Logging Initialization**: Added `init_logger` function to set up logging.
- **Argument Parsing**: Enhanced argument parsing functions to handle various types of input and filter conditions.
- **Report Processing**: Improved the logic for retrieving and processing reports from different sources.
- **Filter Handling**: Ensured that filter parameters are validated and applied correctly.
- **Command Handlers**: Updated command handlers to interact with the CodeChecker API and handle different output formats.

#### Overall Impact:
- **Consistency**: Consistent logging setup ensures that all parts of the application use the same logging configuration.
- **Flexibility**: Enhanced argument parsing allows for more flexible filtering and reporting options.
- **Robustness**: Improved report processing and filtering logic reduces the likelihood of errors and provides more accurate results.
- **User-Friendly**: Updated command handlers make it easier for users to interact with the CodeChecker API and generate desired outputs.

### Conclusion:
The suggested code changes significantly enhance the robustness, flexibility, and user-friendliness of the CodeChecker command-line client. By implementing consistent logging, improved argument parsing, enhanced report processing, and updated command handlers, the client now provides a more reliable and efficient way to interact with the CodeChecker API."
41,"@@ -18,10 +18,10 @@ node putTargets.js
 
 // Import required AWS SDK clients and commands for Node.js
 import { PutTargetsCommand } from ""@aws-sdk/client-cloudwatch-events"";
-import { cweClient } from ""./libs/cloudWatchEventsClient"";
+import { cweClient } from ""./libs/cloudWatchEventsClient.js"";
 
 // Set the parameters
-const params = {
+export const params = {
   Rule: ""DEMO_EVENT"",
   Targets: [
     {","/* Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
SPDX-License-Identifier: Apache-2.0

ABOUT THIS NODE.JS EXAMPLE: This example works with AWS SDK for JavaScript version 3 (v3),
which is available at https://github.com/aws/aws-sdk-js-v3. This example is in the 'AWS SDK for JavaScript v3 Developer Guide' at
https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/cloudwatch-examples-sending-events.html.

Purpose:
putTargets.js demonstrates how to add or update a target to an Amazon CloudWatch Events rule.

Inputs (replace in code):
- LAMBDA_FUNCTION_ARN

Running the code:
node putTargets.js
*/
// snippet-start:[cwEvents.JavaScript.cwe.putTargetsV3]
import { cweClient } from ""./libs/cloudWatchEventsClient.js"";
export const params = {

// Import required AWS SDK clients and commands for Node.js
import { PutTargetsCommand } from ""@aws-sdk/client-cloudwatch-events"";
import { cweClient } from ""./libs/cloudWatchEventsClient"";

// Set the parameters
const params = {
  Rule: ""DEMO_EVENT"",
  Targets: [
    {
      Arn: ""LAMBDA_FUNCTION_ARN"", //LAMBDA_FUNCTION_ARN
      Id: ""myCloudWatchEventsTarget"",
    },
  ],
};

const run = async () => {
  try {
    const data = await cweClient.send(new PutTargetsCommand(params));
    console.log(""Success, target added; requestID: "", data.$metadata.requestId);
    return data;
  } catch (err) {
    console.log(""Error"", err);
  }
};
run();
// snippet-end:[cwEvents.JavaScript.cwe.putTargetsV3]
// For unit tests only.
// module.exports ={run, params};
",I dont think these imports need the .js right?,"The suggested code change appears to be a complete example of how to add or update a target to an Amazon CloudWatch Events rule using the AWS SDK for JavaScript version 3 (v3). The code defines a `params` object with the necessary information, including the rule name and the target details such as the ARN and ID. It then sends a `PutTargetsCommand` to the CloudWatch Events client to add or update the target. The code also includes error handling and logging."
42,"@@ -3333,7 +3333,14 @@ instr_is_reg_spill_or_restore_ex(void *drcontext, instr_t *instr, bool DR_only,
         reg = &myreg;
     if (instr_check_tls_spill_restore(instr, spill, reg, &check_disp)) {
         int offs = reg_spill_tls_offs(*reg);
-        if (!DR_only || (offs != -1 && check_disp == os_tls_offset((ushort)offs))) {
+        if (!DR_only ||
+            (offs != -1 &&
+             /* Mangling may choose to spill registers to a not natural tls offset,
+              * e.g. rip-rel mangling will, if rax is used by the instruction. We
+              * allow for this here and still recognize the DR spill.
+              */
+             (instr_is_our_mangling(instr) ||
+              check_disp == os_tls_offset((ushort)offs)))) {
             if (tls != NULL)
                 *tls = true;
             if (offs_out != NULL)"," * the instr at pc.
 * Returns the pc of the following instr.
 * If the instr at pc does not reference memory, or is invalid,
 * returns NULL.
 */
app_pc
decode_memory_reference_size(dcontext_t *dcontext, app_pc pc, uint *size_in_bytes)
{
    app_pc next_pc;
    instr_t instr;
    instr_init(dcontext, &instr);
    next_pc = decode(dcontext, pc, &instr);
    if (!instr_valid(&instr))
        return NULL;
    CLIENT_ASSERT(size_in_bytes != NULL, ""decode_memory_reference_size: passed NULL"");
    *size_in_bytes = instr_memory_reference_size(&instr);
    instr_free(dcontext, &instr);
    return next_pc;
}

DR_API
dr_instr_label_data_t *
instr_get_label_data_area(instr_t *instr)
{
    CLIENT_ASSERT(instr != NULL, ""invalid arg"");
    if (instr_is_label(instr))
        return &instr->label_data;
    else
        return NULL;
}

DR_API
/* return the taken target pc of the (direct branch) inst */
app_pc
instr_get_branch_target_pc(instr_t *cti_instr)
{
    CLIENT_ASSERT(opnd_is_pc(instr_get_target(cti_instr)),
                  ""instr_branch_target_pc: target not pc"");
    return opnd_get_pc(instr_get_target(cti_instr));
}

DR_API
/* set the taken target pc of the (direct branch) inst */
void
instr_set_branch_target_pc(instr_t *cti_instr, app_pc pc)
{
    opnd_t op = opnd_create_pc(pc);
    instr_set_target(cti_instr, op);
}

bool
instr_is_call(instr_t *instr)
{
    instr_get_opcode(instr); /* force decode */
    return instr_is_call_arch(instr);
}

bool
instr_is_cbr(instr_t *instr)
{
    instr_get_opcode(instr); /* force decode */
    return instr_is_cbr_arch(instr);
}

bool
instr_is_mbr(instr_t *instr)
{
    instr_get_opcode(instr); /* force decode */
    return instr_is_mbr_arch(instr);
}

bool
instr_is_ubr(instr_t *instr)
{
    instr_get_opcode(instr); /* force decode */
    return instr_is_ubr_arch(instr);
}

/* An exit CTI is a control-transfer instruction whose target
 * is a pc (and not an instr_t pointer).  This routine assumes
 * that no other input operands exist in a CTI.
 * An undecoded instr cannot be an exit cti.
 * This routine does NOT try to decode an opcode in a Level 1 or Level
 * 0 routine, and can thus be called on Level 0 routines.
 */
bool
instr_is_exit_cti(instr_t *instr)
{
    if (!instr_operands_valid(instr) || /* implies !opcode_valid */
        instr_is_meta(instr))
        return false;
    /* The _arch versions assume the opcode is already valid, avoiding
     * the conditional decode in instr_get_opcode().
     */
    if (instr_is_ubr_arch(instr) || instr_is_cbr_arch(instr)) {
        /* far pc should only happen for mangle's call to here */
        return opnd_is_pc(instr_get_target(instr));
    }
    return false;
}

bool
instr_is_cti(instr_t *instr) /* any control-transfer instruction */
{
    instr_get_opcode(instr); /* force opcode decode, just once */
    return (instr_is_cbr_arch(instr) || instr_is_ubr_arch(instr) ||
            instr_is_mbr_arch(instr) || instr_is_call_arch(instr));
}

int
instr_get_interrupt_number(instr_t *instr)
{
    CLIENT_ASSERT(instr_get_opcode(instr) == IF_X86_ELSE(OP_int, OP_svc),
                  ""instr_get_interrupt_number: instr not interrupt"");
    if (instr_operands_valid(instr)) {
        ptr_int_t val = opnd_get_immed_int(instr_get_src(instr, 0));
        /* undo the sign extension.  prob return value shouldn't be signed but
         * too late to bother changing that.
         */
        CLIENT_ASSERT(CHECK_TRUNCATE_TYPE_sbyte(val), ""invalid interrupt number"");
        return (int)(byte)val;
    } else if (instr_raw_bits_valid(instr)) {
        /* widen as unsigned */
        return (int)(uint)instr_get_raw_byte(instr, 1);
    } else {
        CLIENT_ASSERT(false, ""instr_get_interrupt_number: invalid instr"");
        return 0;
    }
}

/* Returns true iff instr is a label meta-instruction */
bool
instr_is_label(instr_t *instr)
{
    return instr_opcode_valid(instr) && instr_get_opcode(instr) == OP_LABEL;
}

bool
instr_uses_fp_reg(instr_t *instr)
{
    int a;
    opnd_t curop;
    for (a = 0; a < instr_num_dsts(instr); a++) {
        curop = instr_get_dst(instr, a);
        if (opnd_is_reg(curop) && reg_is_fp(opnd_get_reg(curop)))
            return true;
        else if (opnd_is_memory_reference(curop)) {
            if (reg_is_fp(opnd_get_base(curop)))
                return true;
            else if (reg_is_fp(opnd_get_index(curop)))
                return true;
        }
    }

    for (a = 0; a < instr_num_srcs(instr); a++) {
        curop = instr_get_src(instr, a);
        if (opnd_is_reg(curop) && reg_is_fp(opnd_get_reg(curop)))
            return true;
        else if (opnd_is_memory_reference(curop)) {
            if (reg_is_fp(opnd_get_base(curop)))
                return true;
            else if (reg_is_fp(opnd_get_index(curop)))
                return true;
        }
    }
    return false;
}

/* We place these here rather than in mangle_shared.c to avoid the work of
 * linking mangle_shared.c into drdecodelib.
 */
instr_t *
convert_to_near_rel_meta(dcontext_t *dcontext, instrlist_t *ilist, instr_t *instr)
{
    return convert_to_near_rel_arch(dcontext, ilist, instr);
}

void
convert_to_near_rel(dcontext_t *dcontext, instr_t *instr)
{
    convert_to_near_rel_arch(dcontext, NULL, instr);
}

instr_t *
instr_convert_short_meta_jmp_to_long(dcontext_t *dcontext, instrlist_t *ilist,
                                     instr_t *instr)
{
    /* PR 266292: we convert to a sequence of separate meta instrs for jecxz, etc. */
    CLIENT_ASSERT(instr_is_meta(instr),
                  ""instr_convert_short_meta_jmp_to_long: instr is not meta"");
    CLIENT_ASSERT(instr_is_cti_short(instr),
                  ""instr_convert_short_meta_jmp_to_long: instr is not a short cti"");
    if (instr_is_app(instr) || !instr_is_cti_short(instr))
        return instr;
    return convert_to_near_rel_meta(dcontext, ilist, instr);
}

/***********************************************************************
 * instr_t creation routines
 * To use 16-bit data sizes, must call set_prefix after creating instr
 * To support this, all relevant registers must be of eAX form!
 * FIXME: how do that?
 * will an all-operand replacement work, or do some instrs have some
 * var-size regs but some const-size also?
 *
 * XXX: what if want eflags or modrm info on constructed instr?!?
 *
 * fld pushes onto top of stack, call that writing to ST0 or ST7?
 * f*p pops the stack -- not modeled at all!
 * should floating point constants be doubles, not floats?!?
 *
 * opcode complaints:
 * OP_imm vs. OP_st
 * OP_ret: build routines have to separate ret_imm and ret_far_imm
 * others, see FIXME's in instr_create.h
 */

instr_t *
instr_create_0dst_0src(dcontext_t *dcontext, int opcode)
{
    instr_t *in = instr_build(dcontext, opcode, 0, 0);
    return in;
}

instr_t *
instr_create_0dst_1src(dcontext_t *dcontext, int opcode, opnd_t src)
{
    instr_t *in = instr_build(dcontext, opcode, 0, 1);
    instr_set_src(in, 0, src);
    return in;
}

instr_t *
instr_create_0dst_2src(dcontext_t *dcontext, int opcode, opnd_t src1, opnd_t src2)
{
    instr_t *in = instr_build(dcontext, opcode, 0, 2);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    return in;
}

instr_t *
instr_create_0dst_3src(dcontext_t *dcontext, int opcode, opnd_t src1, opnd_t src2,
                       opnd_t src3)
{
    instr_t *in = instr_build(dcontext, opcode, 0, 3);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    instr_set_src(in, 2, src3);
    return in;
}

instr_t *
instr_create_0dst_4src(dcontext_t *dcontext, int opcode, opnd_t src1, opnd_t src2,
                       opnd_t src3, opnd_t src4)
{
    instr_t *in = instr_build(dcontext, opcode, 0, 4);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    instr_set_src(in, 2, src3);
    instr_set_src(in, 3, src4);
    return in;
}

instr_t *
instr_create_1dst_0src(dcontext_t *dcontext, int opcode, opnd_t dst)
{
    instr_t *in = instr_build(dcontext, opcode, 1, 0);
    instr_set_dst(in, 0, dst);
    return in;
}

instr_t *
instr_create_1dst_1src(dcontext_t *dcontext, int opcode, opnd_t dst, opnd_t src)
{
    instr_t *in = instr_build(dcontext, opcode, 1, 1);
    instr_set_dst(in, 0, dst);
    instr_set_src(in, 0, src);
    return in;
}

instr_t *
instr_create_1dst_2src(dcontext_t *dcontext, int opcode, opnd_t dst, opnd_t src1,
                       opnd_t src2)
{
    instr_t *in = instr_build(dcontext, opcode, 1, 2);
    instr_set_dst(in, 0, dst);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    return in;
}

instr_t *
instr_create_1dst_3src(dcontext_t *dcontext, int opcode, opnd_t dst, opnd_t src1,
                       opnd_t src2, opnd_t src3)
{
    instr_t *in = instr_build(dcontext, opcode, 1, 3);
    instr_set_dst(in, 0, dst);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    instr_set_src(in, 2, src3);
    return in;
}

instr_t *
instr_create_1dst_4src(dcontext_t *dcontext, int opcode, opnd_t dst, opnd_t src1,
                       opnd_t src2, opnd_t src3, opnd_t src4)
{
    instr_t *in = instr_build(dcontext, opcode, 1, 4);
    instr_set_dst(in, 0, dst);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    instr_set_src(in, 2, src3);
    instr_set_src(in, 3, src4);
    return in;
}

instr_t *
instr_create_1dst_5src(dcontext_t *dcontext, int opcode, opnd_t dst, opnd_t src1,
                       opnd_t src2, opnd_t src3, opnd_t src4, opnd_t src5)
{
    instr_t *in = instr_build(dcontext, opcode, 1, 5);
    instr_set_dst(in, 0, dst);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    instr_set_src(in, 2, src3);
    instr_set_src(in, 3, src4);
    instr_set_src(in, 4, src5);
    return in;
}

instr_t *
instr_create_2dst_0src(dcontext_t *dcontext, int opcode, opnd_t dst1, opnd_t dst2)
{
    instr_t *in = instr_build(dcontext, opcode, 2, 0);
    instr_set_dst(in, 0, dst1);
    instr_set_dst(in, 1, dst2);
    return in;
}

instr_t *
instr_create_2dst_1src(dcontext_t *dcontext, int opcode, opnd_t dst1, opnd_t dst2,
                       opnd_t src)
{
    instr_t *in = instr_build(dcontext, opcode, 2, 1);
    instr_set_dst(in, 0, dst1);
    instr_set_dst(in, 1, dst2);
    instr_set_src(in, 0, src);
    return in;
}

instr_t *
instr_create_2dst_2src(dcontext_t *dcontext, int opcode, opnd_t dst1, opnd_t dst2,
                       opnd_t src1, opnd_t src2)
{
    instr_t *in = instr_build(dcontext, opcode, 2, 2);
    instr_set_dst(in, 0, dst1);
    instr_set_dst(in, 1, dst2);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    return in;
}

instr_t *
instr_create_2dst_3src(dcontext_t *dcontext, int opcode, opnd_t dst1, opnd_t dst2,
                       opnd_t src1, opnd_t src2, opnd_t src3)
{
    instr_t *in = instr_build(dcontext, opcode, 2, 3);
    instr_set_dst(in, 0, dst1);
    instr_set_dst(in, 1, dst2);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    instr_set_src(in, 2, src3);
    return in;
}

instr_t *
instr_create_2dst_4src(dcontext_t *dcontext, int opcode, opnd_t dst1, opnd_t dst2,
                       opnd_t src1, opnd_t src2, opnd_t src3, opnd_t src4)
{
    instr_t *in = instr_build(dcontext, opcode, 2, 4);
    instr_set_dst(in, 0, dst1);
    instr_set_dst(in, 1, dst2);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    instr_set_src(in, 2, src3);
    instr_set_src(in, 3, src4);
    return in;
}

instr_t *
instr_create_2dst_5src(dcontext_t *dcontext, int opcode, opnd_t dst1, opnd_t dst2,
                       opnd_t src1, opnd_t src2, opnd_t src3, opnd_t src4, opnd_t src5)
{
    instr_t *in = instr_build(dcontext, opcode, 2, 5);
    instr_set_dst(in, 0, dst1);
    instr_set_dst(in, 1, dst2);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    instr_set_src(in, 2, src3);
    instr_set_src(in, 3, src4);
    instr_set_src(in, 4, src5);
    return in;
}

instr_t *
instr_create_3dst_0src(dcontext_t *dcontext, int opcode, opnd_t dst1, opnd_t dst2,
                       opnd_t dst3)
{
    instr_t *in = instr_build(dcontext, opcode, 3, 0);
    instr_set_dst(in, 0, dst1);
    instr_set_dst(in, 1, dst2);
    instr_set_dst(in, 2, dst3);
    return in;
}

instr_t *
instr_create_3dst_2src(dcontext_t *dcontext, int opcode, opnd_t dst1, opnd_t dst2,
                       opnd_t dst3, opnd_t src1, opnd_t src2)
{
    instr_t *in = instr_build(dcontext, opcode, 3, 2);
    instr_set_dst(in, 0, dst1);
    instr_set_dst(in, 1, dst2);
    instr_set_dst(in, 2, dst3);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    return in;
}

instr_t *
instr_create_3dst_3src(dcontext_t *dcontext, int opcode, opnd_t dst1, opnd_t dst2,
                       opnd_t dst3, opnd_t src1, opnd_t src2, opnd_t src3)
{
    instr_t *in = instr_build(dcontext, opcode, 3, 3);
    instr_set_dst(in, 0, dst1);
    instr_set_dst(in, 1, dst2);
    instr_set_dst(in, 2, dst3);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    instr_set_src(in, 2, src3);
    return in;
}

instr_t *
instr_create_3dst_4src(dcontext_t *dcontext, int opcode, opnd_t dst1, opnd_t dst2,
                       opnd_t dst3, opnd_t src1, opnd_t src2, opnd_t src3, opnd_t src4)
{
    instr_t *in = instr_build(dcontext, opcode, 3, 4);
    instr_set_dst(in, 0, dst1);
    instr_set_dst(in, 1, dst2);
    instr_set_dst(in, 2, dst3);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    instr_set_src(in, 2, src3);
    instr_set_src(in, 3, src4);
    return in;
}

instr_t *
instr_create_3dst_5src(dcontext_t *dcontext, int opcode, opnd_t dst1, opnd_t dst2,
                       opnd_t dst3, opnd_t src1, opnd_t src2, opnd_t src3, opnd_t src4,
                       opnd_t src5)
{
    instr_t *in = instr_build(dcontext, opcode, 3, 5);
    instr_set_dst(in, 0, dst1);
    instr_set_dst(in, 1, dst2);
    instr_set_dst(in, 2, dst3);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    instr_set_src(in, 2, src3);
    instr_set_src(in, 3, src4);
    instr_set_src(in, 4, src5);
    return in;
}

instr_t *
instr_create_4dst_1src(dcontext_t *dcontext, int opcode, opnd_t dst1, opnd_t dst2,
                       opnd_t dst3, opnd_t dst4, opnd_t src)
{
    instr_t *in = instr_build(dcontext, opcode, 4, 1);
    instr_set_dst(in, 0, dst1);
    instr_set_dst(in, 1, dst2);
    instr_set_dst(in, 2, dst3);
    instr_set_dst(in, 3, dst4);
    instr_set_src(in, 0, src);
    return in;
}

instr_t *
instr_create_4dst_2src(dcontext_t *dcontext, int opcode, opnd_t dst1, opnd_t dst2,
                       opnd_t dst3, opnd_t dst4, opnd_t src1, opnd_t src2)
{
    instr_t *in = instr_build(dcontext, opcode, 4, 2);
    instr_set_dst(in, 0, dst1);
    instr_set_dst(in, 1, dst2);
    instr_set_dst(in, 2, dst3);
    instr_set_dst(in, 3, dst4);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    return in;
}

instr_t *
instr_create_4dst_4src(dcontext_t *dcontext, int opcode, opnd_t dst1, opnd_t dst2,
                       opnd_t dst3, opnd_t dst4, opnd_t src1, opnd_t src2, opnd_t src3,
                       opnd_t src4)
{
    instr_t *in = instr_build(dcontext, opcode, 4, 4);
    instr_set_dst(in, 0, dst1);
    instr_set_dst(in, 1, dst2);
    instr_set_dst(in, 2, dst3);
    instr_set_dst(in, 3, dst4);
    instr_set_src(in, 0, src1);
    instr_set_src(in, 1, src2);
    instr_set_src(in, 2, src3);
    instr_set_src(in, 3, src4);
    return in;
}

instr_t *
instr_create_Ndst_Msrc_varsrc(dcontext_t *dcontext, int opcode, uint fixed_dsts,
                              uint fixed_srcs, uint var_srcs, uint var_ord, ...)
{
    va_list ap;
    instr_t *in = instr_build(dcontext, opcode, fixed_dsts, fixed_srcs + var_srcs);
    uint i;
    reg_id_t prev_reg = REG_NULL;
    bool check_order;
    va_start(ap, var_ord);
    for (i = 0; i < fixed_dsts; i++)
        instr_set_dst(in, i, va_arg(ap, opnd_t));
    for (i = 0; i < MIN(var_ord, fixed_srcs); i++)
        instr_set_src(in, i, va_arg(ap, opnd_t));
    for (i = var_ord; i < fixed_srcs; i++)
        instr_set_src(in, var_srcs + i, va_arg(ap, opnd_t));
    /* we require regs in reglist are stored in order for easy split if necessary */
    check_order = IF_ARM_ELSE(true, false);
    for (i = 0; i < var_srcs; i++) {
        opnd_t opnd = va_arg(ap, opnd_t);
        /* assuming non-reg opnds (if any) are in the fixed positon */
        CLIENT_ASSERT(!check_order ||
                          (opnd_is_reg(opnd) && opnd_get_reg(opnd) > prev_reg),
                      ""instr_create_Ndst_Msrc_varsrc: wrong register order in reglist"");
        instr_set_src(in, var_ord + i, opnd_add_flags(opnd, DR_OPND_IN_LIST));
        if (check_order)
            prev_reg = opnd_get_reg(opnd);
    }
    va_end(ap);
    return in;
}

instr_t *
instr_create_Ndst_Msrc_vardst(dcontext_t *dcontext, int opcode, uint fixed_dsts,
                              uint fixed_srcs, uint var_dsts, uint var_ord, ...)
{
    va_list ap;
    instr_t *in = instr_build(dcontext, opcode, fixed_dsts + var_dsts, fixed_srcs);
    uint i;
    reg_id_t prev_reg = REG_NULL;
    bool check_order;
    va_start(ap, var_ord);
    for (i = 0; i < MIN(var_ord, fixed_dsts); i++)
        instr_set_dst(in, i, va_arg(ap, opnd_t));
    for (i = var_ord; i < fixed_dsts; i++)
        instr_set_dst(in, var_dsts + i, va_arg(ap, opnd_t));
    for (i = 0; i < fixed_srcs; i++)
        instr_set_src(in, i, va_arg(ap, opnd_t));
    /* we require regs in reglist are stored in order for easy split if necessary */
    check_order = IF_ARM_ELSE(true, false);
    for (i = 0; i < var_dsts; i++) {
        opnd_t opnd = va_arg(ap, opnd_t);
        /* assuming non-reg opnds (if any) are in the fixed positon */
        CLIENT_ASSERT(!check_order ||
                          (opnd_is_reg(opnd) && opnd_get_reg(opnd) > prev_reg),
                      ""instr_create_Ndst_Msrc_vardst: wrong register order in reglist"");
        instr_set_dst(in, var_ord + i, opnd_add_flags(opnd, DR_OPND_IN_LIST));
        if (check_order)
            prev_reg = opnd_get_reg(opnd);
    }
    va_end(ap);
    return in;
}

/****************************************************************************/
/* build instructions from raw bits
 * convention: give them OP_UNDECODED opcodes
 */

instr_t *
instr_create_raw_1byte(dcontext_t *dcontext, byte byte1)
{
    instr_t *in = instr_build_bits(dcontext, OP_UNDECODED, 1);
    instr_set_raw_byte(in, 0, byte1);
    return in;
}

instr_t *
instr_create_raw_2bytes(dcontext_t *dcontext, byte byte1, byte byte2)
{
    instr_t *in = instr_build_bits(dcontext, OP_UNDECODED, 2);
    instr_set_raw_byte(in, 0, byte1);
    instr_set_raw_byte(in, 1, byte2);
    return in;
}

instr_t *
instr_create_raw_3bytes(dcontext_t *dcontext, byte byte1, byte byte2, byte byte3)
{
    instr_t *in = instr_build_bits(dcontext, OP_UNDECODED, 3);
    instr_set_raw_byte(in, 0, byte1);
    instr_set_raw_byte(in, 1, byte2);
    instr_set_raw_byte(in, 2, byte3);
    return in;
}

instr_t *
instr_create_raw_4bytes(dcontext_t *dcontext, byte byte1, byte byte2, byte byte3,
                        byte byte4)
{
    instr_t *in = instr_build_bits(dcontext, OP_UNDECODED, 4);
    instr_set_raw_byte(in, 0, byte1);
    instr_set_raw_byte(in, 1, byte2);
    instr_set_raw_byte(in, 2, byte3);
    instr_set_raw_byte(in, 3, byte4);
    return in;
}

instr_t *
instr_create_raw_5bytes(dcontext_t *dcontext, byte byte1, byte byte2, byte byte3,
                        byte byte4, byte byte5)
{
    instr_t *in = instr_build_bits(dcontext, OP_UNDECODED, 5);
    instr_set_raw_byte(in, 0, byte1);
    instr_set_raw_byte(in, 1, byte2);
    instr_set_raw_byte(in, 2, byte3);
    instr_set_raw_byte(in, 3, byte4);
    instr_set_raw_byte(in, 4, byte5);
    return in;
}

instr_t *
instr_create_raw_6bytes(dcontext_t *dcontext, byte byte1, byte byte2, byte byte3,
                        byte byte4, byte byte5, byte byte6)
{
    instr_t *in = instr_build_bits(dcontext, OP_UNDECODED, 6);
    instr_set_raw_byte(in, 0, byte1);
    instr_set_raw_byte(in, 1, byte2);
    instr_set_raw_byte(in, 2, byte3);
    instr_set_raw_byte(in, 3, byte4);
    instr_set_raw_byte(in, 4, byte5);
    instr_set_raw_byte(in, 5, byte6);
    return in;
}

instr_t *
instr_create_raw_7bytes(dcontext_t *dcontext, byte byte1, byte byte2, byte byte3,
                        byte byte4, byte byte5, byte byte6, byte byte7)
{
    instr_t *in = instr_build_bits(dcontext, OP_UNDECODED, 7);
    instr_set_raw_byte(in, 0, byte1);
    instr_set_raw_byte(in, 1, byte2);
    instr_set_raw_byte(in, 2, byte3);
    instr_set_raw_byte(in, 3, byte4);
    instr_set_raw_byte(in, 4, byte5);
    instr_set_raw_byte(in, 5, byte6);
    instr_set_raw_byte(in, 6, byte7);
    return in;
}

instr_t *
instr_create_raw_8bytes(dcontext_t *dcontext, byte byte1, byte byte2, byte byte3,
                        byte byte4, byte byte5, byte byte6, byte byte7, byte byte8)
{
    instr_t *in = instr_build_bits(dcontext, OP_UNDECODED, 8);
    instr_set_raw_byte(in, 0, byte1);
    instr_set_raw_byte(in, 1, byte2);
    instr_set_raw_byte(in, 2, byte3);
    instr_set_raw_byte(in, 3, byte4);
    instr_set_raw_byte(in, 4, byte5);
    instr_set_raw_byte(in, 5, byte6);
    instr_set_raw_byte(in, 6, byte7);
    instr_set_raw_byte(in, 7, byte8);
    return in;
}

#ifndef STANDALONE_DECODER
/****************************************************************************/
/* dcontext convenience routines */

instr_t *
instr_create_restore_from_dcontext(dcontext_t *dcontext, reg_id_t reg, int offs)
{
    opnd_t memopnd = opnd_create_dcontext_field(dcontext, offs);
    /* use movd for xmm/mmx */
    if (reg_is_xmm(reg) || reg_is_mmx(reg))
        return XINST_CREATE_load_simd(dcontext, opnd_create_reg(reg), memopnd);
    else
        return XINST_CREATE_load(dcontext, opnd_create_reg(reg), memopnd);
}

instr_t *
instr_create_save_to_dcontext(dcontext_t *dcontext, reg_id_t reg, int offs)
{
    opnd_t memopnd = opnd_create_dcontext_field(dcontext, offs);
    CLIENT_ASSERT(dcontext != GLOBAL_DCONTEXT,
                  ""instr_create_save_to_dcontext: invalid dcontext"");
    /* use movd for xmm/mmx */
    if (reg_is_xmm(reg) || reg_is_mmx(reg))
        return XINST_CREATE_store_simd(dcontext, memopnd, opnd_create_reg(reg));
    else
        return XINST_CREATE_store(dcontext, memopnd, opnd_create_reg(reg));
}

/* Use basereg==REG_NULL to get default (xdi, or xsi for upcontext)
 * Auto-magically picks the mem opnd size to match reg if it's a GPR.
 */
instr_t *
instr_create_restore_from_dc_via_reg(dcontext_t *dcontext, reg_id_t basereg, reg_id_t reg,
                                     int offs)
{
    /* use movd for xmm/mmx, and OPSZ_PTR */
    if (reg_is_xmm(reg) || reg_is_mmx(reg)) {
        opnd_t memopnd = opnd_create_dcontext_field_via_reg(dcontext, basereg, offs);
        return XINST_CREATE_load_simd(dcontext, opnd_create_reg(reg), memopnd);
    } else {
        opnd_t memopnd = opnd_create_dcontext_field_via_reg_sz(dcontext, basereg, offs,
                                                               reg_get_size(reg));
        return XINST_CREATE_load(dcontext, opnd_create_reg(reg), memopnd);
    }
}

/* Use basereg==REG_NULL to get default (xdi, or xsi for upcontext)
 * Auto-magically picks the mem opnd size to match reg if it's a GPR.
 */
instr_t *
instr_create_save_to_dc_via_reg(dcontext_t *dcontext, reg_id_t basereg, reg_id_t reg,
                                int offs)
{
    /* use movd for xmm/mmx, and OPSZ_PTR */
    if (reg_is_xmm(reg) || reg_is_mmx(reg)) {
        opnd_t memopnd = opnd_create_dcontext_field_via_reg(dcontext, basereg, offs);
        return XINST_CREATE_store_simd(dcontext, memopnd, opnd_create_reg(reg));
    } else {
        opnd_t memopnd = opnd_create_dcontext_field_via_reg_sz(dcontext, basereg, offs,
                                                               reg_get_size(reg));
        return XINST_CREATE_store(dcontext, memopnd, opnd_create_reg(reg));
    }
}

static instr_t *
instr_create_save_immedN_to_dcontext(dcontext_t *dcontext, opnd_size_t sz,
                                     opnd_t immed_op, int offs)
{
    opnd_t memopnd = opnd_create_dcontext_field_sz(dcontext, offs, sz);
    /* PR 244737: thread-private scratch space needs to fixed for x64 */
    IF_X64(ASSERT_NOT_IMPLEMENTED(false));
    /* There is no immed to mem instr on ARM/AArch64. */
    IF_AARCHXX(ASSERT_NOT_IMPLEMENTED(false));
    return XINST_CREATE_store(dcontext, memopnd, immed_op);
}

instr_t *
instr_create_save_immed32_to_dcontext(dcontext_t *dcontext, int immed, int offs)
{
    return instr_create_save_immedN_to_dcontext(dcontext, OPSZ_4,
                                                OPND_CREATE_INT32(immed), offs);
}

instr_t *
instr_create_save_immed16_to_dcontext(dcontext_t *dcontext, int immed, int offs)
{
    return instr_create_save_immedN_to_dcontext(dcontext, OPSZ_2,
                                                OPND_CREATE_INT16(immed), offs);
}

instr_t *
instr_create_save_immed8_to_dcontext(dcontext_t *dcontext, int immed, int offs)
{
    return instr_create_save_immedN_to_dcontext(dcontext, OPSZ_1, OPND_CREATE_INT8(immed),
                                                offs);
}

instr_t *
instr_create_save_immed_to_dc_via_reg(dcontext_t *dcontext, reg_id_t basereg, int offs,
                                      ptr_int_t immed, opnd_size_t sz)
{
    opnd_t memopnd = opnd_create_dcontext_field_via_reg_sz(dcontext, basereg, offs, sz);
    ASSERT(sz == OPSZ_1 || sz == OPSZ_2 || sz == OPSZ_4);
    /* There is no immed to mem instr on ARM or AArch64. */
    IF_NOT_X86(ASSERT_NOT_IMPLEMENTED(false));
    return XINST_CREATE_store(dcontext, memopnd, opnd_create_immed_int(immed, sz));
}

instr_t *
instr_create_jump_via_dcontext(dcontext_t *dcontext, int offs)
{
#    ifdef AARCH64
    ASSERT_NOT_IMPLEMENTED(false); /* FIXME i#1569 */
    return 0;
#    else
    opnd_t memopnd = opnd_create_dcontext_field(dcontext, offs);
    return XINST_CREATE_jump_mem(dcontext, memopnd);
#    endif
}

/* there is no corresponding save routine since we no longer support
 * keeping state on the stack while code other than our own is running
 * (in the same thread)
 */
instr_t *
instr_create_restore_dynamo_stack(dcontext_t *dcontext)
{
    return instr_create_restore_from_dcontext(dcontext, REG_XSP, DSTACK_OFFSET);
}

/* make sure to keep in sync w/ emit_utils.c's insert_spill_or_restore() */
bool
instr_raw_is_tls_spill(byte *pc, reg_id_t reg, ushort offs)
{
#    ifdef X86
    ASSERT_NOT_IMPLEMENTED(reg != REG_XAX);
#        ifdef X64
    /* match insert_jmp_to_ibl */
    if (*pc == TLS_SEG_OPCODE &&
        *(pc + 1) == (REX_PREFIX_BASE_OPCODE | REX_PREFIX_W_OPFLAG) &&
        *(pc + 2) == MOV_REG2MEM_OPCODE &&
        /* 0x1c for ebx, 0x0c for ecx, 0x04 for eax */
        *(pc + 3) == MODRM_BYTE(0 /*mod*/, reg_get_bits(reg), 4 /*rm*/) &&
        *(pc + 4) == 0x25 && *((uint *)(pc + 5)) == (uint)os_tls_offset(offs))
        return true;
        /* we also check for 32-bit.  we could take in flags and only check for one
         * version, but we're not worried about false positives.
         */
#        endif
    /* looking for:   67 64 89 1e e4 0e    addr16 mov    %ebx -> %fs:0xee4   */
    /* ASSUMPTION: when addr16 prefix is used, prefix order is fixed */
    return (*pc == ADDR_PREFIX_OPCODE && *(pc + 1) == TLS_SEG_OPCODE &&
            *(pc + 2) == MOV_REG2MEM_OPCODE &&
            /* 0x1e for ebx, 0x0e for ecx, 0x06 for eax */
            *(pc + 3) == MODRM_BYTE(0 /*mod*/, reg_get_bits(reg), 6 /*rm*/) &&
            *((ushort *)(pc + 4)) == os_tls_offset(offs)) ||
        /* PR 209709: allow for no addr16 prefix */
        (*pc == TLS_SEG_OPCODE && *(pc + 1) == MOV_REG2MEM_OPCODE &&
         /* 0x1e for ebx, 0x0e for ecx, 0x06 for eax */
         *(pc + 2) == MODRM_BYTE(0 /*mod*/, reg_get_bits(reg), 6 /*rm*/) &&
         *((uint *)(pc + 4)) == os_tls_offset(offs));
#    elif defined(AARCHXX)
    /* FIXME i#1551, i#1569: NYI on ARM/AArch64 */
    ASSERT_NOT_IMPLEMENTED(false);
    return false;
#    endif /* X86/ARM */
}

/* this routine may upgrade a level 1 instr */
static bool
instr_check_tls_spill_restore(instr_t *instr, bool *spill, reg_id_t *reg, int *offs)
{
    opnd_t regop, memop;
    CLIENT_ASSERT(instr != NULL,
                  ""internal error: tls spill/restore check: NULL argument"");
    if (instr_get_opcode(instr) == OP_store) {
        regop = instr_get_src(instr, 0);
        memop = instr_get_dst(instr, 0);
        if (spill != NULL)
            *spill = true;
    } else if (instr_get_opcode(instr) == OP_load) {
        regop = instr_get_dst(instr, 0);
        memop = instr_get_src(instr, 0);
        if (spill != NULL)
            *spill = false;
#    ifdef X86
    } else if (instr_get_opcode(instr) == OP_xchg) {
        /* we use xchg to restore in dr_insert_mbr_instrumentation */
        regop = instr_get_src(instr, 0);
        memop = instr_get_dst(instr, 0);
        if (spill != NULL)
            *spill = false;
#    endif
    } else
        return false;
    if (opnd_is_reg(regop) &&
#    ifdef X86
        opnd_is_far_base_disp(memop) && opnd_get_segment(memop) == SEG_TLS &&
        opnd_is_abs_base_disp(memop)
#    elif defined(AARCHXX)
        opnd_is_base_disp(memop) && opnd_get_base(memop) == dr_reg_stolen &&
        opnd_get_index(memop) == DR_REG_NULL
#    endif
    ) {
        if (reg != NULL)
            *reg = opnd_get_reg(regop);
        if (offs != NULL)
            *offs = opnd_get_disp(memop);
        return true;
    }
    return false;
}

/* if instr is level 1, does not upgrade it and instead looks at raw bits,
 * to support identification w/o ruining level 0 in decode_fragment, etc.
 */
bool
instr_is_tls_spill(instr_t *instr, reg_id_t reg, ushort offs)
{
    reg_id_t check_reg = REG_NULL; /* init to satisfy some compilers */
    int check_disp = 0;            /* init to satisfy some compilers */
    bool spill;
    return (instr_check_tls_spill_restore(instr, &spill, &check_reg, &check_disp) &&
            spill && check_reg == reg && check_disp == os_tls_offset(offs));
}

/* if instr is level 1, does not upgrade it and instead looks at raw bits,
 * to support identification w/o ruining level 0 in decode_fragment, etc.
 */
bool
instr_is_tls_restore(instr_t *instr, reg_id_t reg, ushort offs)
{
    reg_id_t check_reg = REG_NULL; /* init to satisfy some compilers */
    int check_disp = 0;            /* init to satisfy some compilers */
    bool spill;
    return (instr_check_tls_spill_restore(instr, &spill, &check_reg, &check_disp) &&
            !spill && (reg == REG_NULL || check_reg == reg) &&
            check_disp == os_tls_offset(offs));
}

/* if instr is level 1, does not upgrade it and instead looks at raw bits,
 * to support identification w/o ruining level 0 in decode_fragment, etc.
 */
bool
instr_is_tls_xcx_spill(instr_t *instr)
{
#    ifdef X86
    if (instr_raw_bits_valid(instr)) {
        /* avoid upgrading instr */
        return instr_raw_is_tls_spill(instr_get_raw_bits(instr), REG_ECX,
                                      MANGLE_XCX_SPILL_SLOT);
    } else
        return instr_is_tls_spill(instr, REG_ECX, MANGLE_XCX_SPILL_SLOT);
#    elif defined(AARCHXX)
    /* FIXME i#1551, i#1569: NYI on ARM/AArch64 */
    ASSERT_NOT_IMPLEMENTED(false);
    return false;
#    endif
}

/* this routine may upgrade a level 1 instr */
static bool
instr_check_mcontext_spill_restore(dcontext_t *dcontext, instr_t *instr, bool *spill,
                                   reg_id_t *reg, int *offs)
{
#    ifdef X64
    /* PR 244737: we always use tls for x64 */
    return false;
#    else
    opnd_t regop, memop;
    if (instr_get_opcode(instr) == OP_store) {
        regop = instr_get_src(instr, 0);
        memop = instr_get_dst(instr, 0);
        if (spill != NULL)
            *spill = true;
    } else if (instr_get_opcode(instr) == OP_load) {
        regop = instr_get_dst(instr, 0);
        memop = instr_get_src(instr, 0);
        if (spill != NULL)
            *spill = false;
#        ifdef X86
    } else if (instr_get_opcode(instr) == OP_xchg) {
        /* we use xchg to restore in dr_insert_mbr_instrumentation */
        regop = instr_get_src(instr, 0);
        memop = instr_get_dst(instr, 0);
        if (spill != NULL)
            *spill = false;
#        endif /* X86 */
    } else
        return false;
    if (opnd_is_near_base_disp(memop) && opnd_is_abs_base_disp(memop) &&
        opnd_is_reg(regop)) {
        byte *pc = (byte *)opnd_get_disp(memop);
        byte *mc = (byte *)get_mcontext(dcontext);
        if (pc >= mc && pc < mc + sizeof(priv_mcontext_t)) {
            if (reg != NULL)
                *reg = opnd_get_reg(regop);
            if (offs != NULL)
                *offs = pc - (byte *)dcontext;
            return true;
        }
    }
    return false;
#    endif
}

static bool
instr_is_reg_spill_or_restore_ex(void *drcontext, instr_t *instr, bool DR_only, bool *tls,
                                 bool *spill, reg_id_t *reg, uint *offs_out)
{
    dcontext_t *dcontext = (dcontext_t *)drcontext;
    int check_disp = 0; /* init to satisfy some compilers */
    reg_id_t myreg;
    CLIENT_ASSERT(instr != NULL, ""invalid NULL argument"");
    if (reg == NULL)
        if (!DR_only ||
            (offs != -1 &&
             /* Mangling may choose to spill registers to a not natural tls offset,
              * e.g. rip-rel mangling will, if rax is used by the instruction. We
              * allow for this here and still recognize the DR spill.
              */
             (instr_is_our_mangling(instr) ||
              check_disp == os_tls_offset((ushort)offs)))) {
        reg = &myreg;
    if (instr_check_tls_spill_restore(instr, spill, reg, &check_disp)) {
        int offs = reg_spill_tls_offs(*reg);
        if (!DR_only || (offs != -1 && check_disp == os_tls_offset((ushort)offs))) {
            if (tls != NULL)
                *tls = true;
            if (offs_out != NULL)
                *offs_out = check_disp;
            return true;
        }
    }
    if (dcontext != GLOBAL_DCONTEXT &&
        instr_check_mcontext_spill_restore(dcontext, instr, spill, reg, &check_disp)) {
        int offs = opnd_get_reg_dcontext_offs(dr_reg_fixer[*reg]);
        if (!DR_only || (offs != -1 && check_disp == offs)) {
            if (tls != NULL)
                *tls = false;
            if (offs_out != NULL)
                *offs_out = check_disp;
            return true;
        }
    }
    return false;
}

DR_API
bool
instr_is_reg_spill_or_restore(void *drcontext, instr_t *instr, bool *tls, bool *spill,
                              reg_id_t *reg, uint *offs)
{
    return instr_is_reg_spill_or_restore_ex(drcontext, instr, false, tls, spill, reg,
                                            offs);
}

bool
instr_is_DR_reg_spill_or_restore(void *drcontext, instr_t *instr, bool *tls, bool *spill,
                                 reg_id_t *reg)
{
    return instr_is_reg_spill_or_restore_ex(drcontext, instr, true, tls, spill, reg,
                                            NULL);
}

/* N.B. : client meta routines (dr_insert_* etc.) should never use anything other
 * then TLS_XAX_SLOT unless the client has specified a slot to use as we let the
 * client use the rest. */
instr_t *
instr_create_save_to_tls(dcontext_t *dcontext, reg_id_t reg, ushort offs)
{
    return XINST_CREATE_store(dcontext, opnd_create_tls_slot(os_tls_offset(offs)),
                              opnd_create_reg(reg));
}

instr_t *
instr_create_restore_from_tls(dcontext_t *dcontext, reg_id_t reg, ushort offs)
{
    return XINST_CREATE_load(dcontext, opnd_create_reg(reg),
                             opnd_create_tls_slot(os_tls_offset(offs)));
}

/* For -x86_to_x64, we can spill to 64-bit extra registers (xref i#751). */
instr_t *
instr_create_save_to_reg(dcontext_t *dcontext, reg_id_t reg1, reg_id_t reg2)
{
    return XINST_CREATE_move(dcontext, opnd_create_reg(reg2), opnd_create_reg(reg1));
}

instr_t *
instr_create_restore_from_reg(dcontext_t *dcontext, reg_id_t reg1, reg_id_t reg2)
{
    return XINST_CREATE_move(dcontext, opnd_create_reg(reg1), opnd_create_reg(reg2));
}

#    ifdef X86_64
/* Returns NULL if pc is not the start of a rip-rel lea.
 * If it could be, returns the address it refers to (which we assume is
 * never NULL).
 */
byte *
instr_raw_is_rip_rel_lea(byte *pc, byte *read_end)
{
    /* PR 215408: look for ""lea reg, [rip+disp]""
     * We assume no extraneous prefixes, and we require rex.w, though not strictly
     * necessary for say WOW64 or other known-lower-4GB situations
     */
    if (pc + 7 <= read_end) {
        if (*(pc + 1) == RAW_OPCODE_lea &&
            (TESTALL(REX_PREFIX_BASE_OPCODE | REX_PREFIX_W_OPFLAG, *pc) &&
             !TESTANY(~(REX_PREFIX_BASE_OPCODE | REX_PREFIX_ALL_OPFLAGS), *pc)) &&
            /* does mod==0 and rm==5? */
            ((*(pc + 2)) | MODRM_BYTE(0, 7, 0)) == MODRM_BYTE(0, 7, 5)) {
            return pc + 7 + *(int *)(pc + 3);
        }
    }
    return NULL;
}
#    endif

uint
move_mm_reg_opcode(bool aligned16, bool aligned32)
{
#    ifdef X86
    if (YMM_ENABLED()) {
        /* must preserve ymm registers */
        return (aligned32 ? OP_vmovdqa : OP_vmovdqu);
    } else if (proc_has_feature(FEATURE_SSE2)) {
        return (aligned16 ? OP_movdqa : OP_movdqu);
    } else {
        CLIENT_ASSERT(proc_has_feature(FEATURE_SSE), ""running on unsupported processor"");
        return (aligned16 ? OP_movaps : OP_movups);
    }
#    elif defined(ARM)
    /* FIXME i#1551: which one we should return, OP_vmov, OP_vldr, or OP_vstr? */
    return OP_vmov;
#    elif defined(AARCH64)
    ASSERT_NOT_IMPLEMENTED(false); /* FIXME i#1569 */
    return 0;
#    endif /* X86/ARM */
}

#endif /* !STANDALONE_DECODER */
/****************************************************************************/
","This only works for a mangled instruction list: i.e., only during bb building (after a client's events) or bb recreation (also after a client's events). Thus this call will never return true for a client. Plus, this is-spill function is used for more than instrlists: some callers use it when decoding raw instructions, such as drreg and also I think DR may use it that way when it decodes from the cache in some cases? So this does not seem a complete solution.","The suggested code changes appear to be related to the `instr` module in the Dr. Memory library, which provides functionality for decoding and manipulating machine instructions. Here's a concise review of the changes:

### Key Changes:
1. **Memory Reference Size Calculation**:
   - Added a function `decode_memory_reference_size` to calculate the size of a memory reference at a given program counter (PC).
   - This function decodes the instruction at the PC, checks if it references memory, and returns the size of the memory reference.

2. **Label Data Access**:
   - Added a function `instr_get_label_data_area` to retrieve the label data area associated with a label instruction.

3. **Branch Target PC Retrieval**:
   - Added functions `instr_get_branch_target_pc` and `instr_set_branch_target_pc` to handle branch targets for direct control transfer instructions (CTIs).

4. **Instruction Type Checks**:
   - Added functions to check if an instruction is a call, conditional branch, memory branch, or unconditional branch.
   - Added a function to check if an instruction is an exit control transfer instruction.

5. **Instruction Creation**:
   - Added functions to create instructions with different numbers of destination and source operands.
   - Added functions to create instructions from raw bits, supporting various byte lengths.

6. **DContext Convenience Functions**:
   - Added functions to create instructions for restoring and saving values from/to the DynamoRIO context.
   - Added functions to check for TLS spills/restores and to create TLS-related instructions.

7. **Raw Bit Handling**:
   - Added functions to handle raw bits for instructions, allowing for creation of instructions without decoding.

8. **TLS Spill/Restore Detection**:
   - Added functions to detect TLS spills and restores, including handling both level 1 and level 0 instructions.

### Summary:
These changes enhance the functionality of the `instr` module by providing more detailed information about instructions, making it easier to manipulate and analyze them programmatically. The addition of new functions allows for more precise control over instruction creation and manipulation, which is crucial for debugging and instrumentation purposes in applications using Dr. Memory."
43,"@@ -43,7 +43,7 @@ namespace Samples
                 .MapResult(
                     (JaegerOptions options) => TestJaegerExporter.Run(options.Host, options.Port),
                     (ZipkinOptions options) => TestZipkinExporter.Run(options.Uri),
-                    (PrometheusOptions options) => TestPrometheus.RunAsync(options.Port, options.PushIntervalInSecs, options.DurationInMins),
+                    (PrometheusOptions options) => TestPrometheusExporter.RunAsync(options.Port, options.PushIntervalInSecs, options.DurationInMins),
                     (HttpClientOptions options) => TestHttpClient.Run(),
                     (RedisOptions options) => TestRedis.Run(options.Uri),
                     (ZPagesOptions options) => TestZPagesExporter.Run(),","﻿// <copyright file=""Program.cs"" company=""OpenTelemetry Authors"">
// Copyright The OpenTelemetry Authors
//
// Licensed under the Apache License, Version 2.0 (the ""License"");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an ""AS IS"" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
// </copyright>
using System;
using CommandLine;

namespace Samples
{
    /// <summary>
    /// Main samples entry point.
    /// </summary>
    public class Program
    {
        /// <summary>
        /// Main method - invoke this using command line.
        /// For example:
        ///
        /// dotnet run -p Exporters.csproj console
        /// dotnet run -p Exporters.csproj zipkin -u http://localhost:9411/api/v2/spans
        /// dotnet run -p Exporters.csproj jaeger -h localhost -p 6831
        /// dotnet run -p Exporters.csproj prometheus -i 15 -p 9184 -d 2
        /// dotnet run -p Exporters.csproj otlp -e ""localhost:55680""
        ///
        /// The above must be run from the project root folder
        /// (eg: C:\repos\opentelemetry-dotnet\src\samples\Exporters\Console\).
        /// </summary>
        /// <param name=""args"">Arguments from command line.</param>
        public static void Main(string[] args)
        {
            Parser.Default.ParseArguments<JaegerOptions, ZipkinOptions, PrometheusOptions, HttpClientOptions, RedisOptions, ZPagesOptions, ConsoleOptions, OpenTelemetryShimOptions, OtlpOptions>(args)
                    (PrometheusOptions options) => TestPrometheusExporter.RunAsync(options.Port, options.PushIntervalInSecs, options.DurationInMins),
                .MapResult(
                    (JaegerOptions options) => TestJaegerExporter.Run(options.Host, options.Port),
                    (ZipkinOptions options) => TestZipkinExporter.Run(options.Uri),
                    (PrometheusOptions options) => TestPrometheus.RunAsync(options.Port, options.PushIntervalInSecs, options.DurationInMins),
                    (HttpClientOptions options) => TestHttpClient.Run(),
                    (RedisOptions options) => TestRedis.Run(options.Uri),
                    (ZPagesOptions options) => TestZPagesExporter.Run(),
                    (ConsoleOptions options) => TestConsoleExporter.Run(options),
                    (OpenTelemetryShimOptions options) => TestOTelShimWithConsoleExporter.Run(options),
                    (OtlpOptions options) => TestOtlpExporter.Run(options.Endpoint),
                    errs => 1);

            Console.ReadLine();
        }
    }

#pragma warning disable SA1402 // File may only contain a single type

    [Verb(""jaeger"", HelpText = ""Specify the options required to test Jaeger exporter"")]
    internal class JaegerOptions
    {
        [Option('h', ""host"", HelpText = ""Host of the Jaeger Agent"", Default = ""localhost"")]
        public string Host { get; set; }

        [Option('p', ""port"", HelpText = ""Port of the Jaeger Agent"", Default = 6831)]
        public int Port { get; set; }
    }

    [Verb(""zipkin"", HelpText = ""Specify the options required to test Zipkin exporter"")]
    internal class ZipkinOptions
    {
        [Option('u', ""uri"", HelpText = ""Please specify the uri of Zipkin backend"", Required = true)]
        public string Uri { get; set; }
    }

    [Verb(""prometheus"", HelpText = ""Specify the options required to test Prometheus"")]
    internal class PrometheusOptions
    {
        [Option('i', ""pushIntervalInSecs"", Default = 15, HelpText = ""The interval at which Push controller pushes metrics."", Required = false)]
        public int PushIntervalInSecs { get; set; }

        [Option('p', ""port"", Default = 9184, HelpText = ""The port to expose metrics. The endpoint will be http://localhost:port/metrics (This is the port from which your Prometheus server scraps metrics from.)"", Required = false)]
        public int Port { get; set; }

        [Option('d', ""duration"", Default = 2, HelpText = ""Total duration in minutes to run the demo. Run atleast for a min to see metrics flowing."", Required = false)]
        public int DurationInMins { get; set; }
    }

    [Verb(""httpclient"", HelpText = ""Specify the options required to test HttpClient"")]
    internal class HttpClientOptions
    {
    }

    [Verb(""redis"", HelpText = ""Specify the options required to test Redis with Zipkin"")]
    internal class RedisOptions
    {
        [Option('u', ""uri"", HelpText = ""Please specify the uri of Zipkin backend"", Required = true)]
        public string Uri { get; set; }
    }

    [Verb(""zpages"", HelpText = ""Specify the options required to test ZPages"")]
    internal class ZPagesOptions
    {
    }

    [Verb(""console"", HelpText = ""Specify the options required to test console exporter"")]
    internal class ConsoleOptions
    {
        [Option('p', ""displayasjson"", HelpText = ""Specify if the output should be displayed as json or not (default: false)"", Default = false)]
        public bool DisplayAsJson { get; set; }
    }

    [Verb(""otelshim"", HelpText = ""Specify the options required to test OpenTelemetry Shim with console exporter"")]
    internal class OpenTelemetryShimOptions
    {
        [Option('p', ""displayasjson"", HelpText = ""Specify if the output should be displayed as json or not (default: false)"", Default = false)]
        public bool DisplayAsJson { get; set; }
    }

    [Verb(""otlp"", HelpText = ""Specify the options required to test OpenTelemetry Protocol (OTLP)"")]
    internal class OtlpOptions
    {
        [Option('e', ""endpoint"", HelpText = ""Target to which the exporter is going to send traces or metrics"", Default = ""localhost:55680"")]
        public string Endpoint { get; set; }
    }

#pragma warning restore SA1402 // File may only contain a single type

}
","separately we can remove the ""Exporters"" folder. This is not just showing Exporter sample anymore.","The provided code is a command-line interface (CLI) application that allows users to test various OpenTelemetry exporters. Here's a concise review of the suggested code changes:

### Key Points:

1. **Namespace and Class Structure**: 
   - The code is organized into a `Program` class and several verb classes (`JaegerOptions`, `ZipkinOptions`, etc.), each representing a different exporter.

2. **Command Line Parsing**:
   - The `Main` method uses `CommandLine.Parser` to parse command-line arguments and map them to the appropriate verb classes.
   - The `MapResult` method handles different outcomes of parsing, such as successful execution or errors.

3. **Verb Classes**:
   - Each verb class contains options specific to the corresponding exporter. For example, `JaegerOptions` includes `Host` and `Port` for Jaeger, while `ZipkinOptions` includes `Uri`.

4. **Default Values**:
   - Many options have default values specified, making it easier for users to run the application without specifying all options.

5. **Output Options**:
   - Some options, like `DisplayAsJson` in `ConsoleOptions` and `OpenTelemetryShimOptions`, allow users to control the format of the output.

6. **Error Handling**:
   - The `MapResult` method returns an error code if parsing fails, which can be useful for debugging.

### Suggested Improvements:

1. **Documentation**:
   - Add more detailed documentation to the `HelpText` attributes in the verb classes to explain what each option does.

2. **Error Handling**:
   - Consider adding more robust error handling, such as logging or displaying usage information when parsing fails.

3. **Code Formatting**:
   - Ensure consistent formatting throughout the code, especially in the `Main` method and verb classes.

4. **Testing**:
   - Add unit tests to verify that the command-line parsing works correctly and that the exporter methods behave as expected.

5. **Security**:
   - If any sensitive information is being passed via command-line arguments, consider implementing security measures to protect it.

Overall, the code is well-structured and provides a good starting point for testing OpenTelemetry exporters."
44,"@@ -23,7 +23,7 @@ import (
 )
 
 const (
-	avgCPUUtilizationFanout  int32 = 95
+	avgCPUUtilizationFanout  int32 = 30
 	avgCPUUtilizationIngress int32 = 95
 	avgCPUUtilizationRetry   int32 = 95
 	// The limit we set (for Fanout and Retry) is 3000Mi which is mostly used","/*
Copyright 2020 Google LLC

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package v1alpha1

import (
	""context""

	""knative.dev/pkg/ptr""
	avgCPUUtilizationFanout  int32 = 30
)

const (
	avgCPUUtilizationFanout  int32 = 95
	avgCPUUtilizationIngress int32 = 95
	avgCPUUtilizationRetry   int32 = 95
	// The limit we set (for Fanout and Retry) is 3000Mi which is mostly used
	// to prevent surging memory usage causing OOM.
	// Here we only set half of the limit so that in case of surging memory
	// usage, HPA could have enough time to kick in.
	// See: https://github.com/google/knative-gcp/issues/1265
	avgMemoryUsageFanout  string = ""1500Mi""
	avgMemoryUsageIngress string = ""1500Mi""
	avgMemoryUsageRetry   string = ""1500Mi""
	cpuRequestFanout      string = ""1500m""
	cpuRequestIngress     string = ""2000m""
	cpuRequestRetry       string = ""1000m""
	cpuLimitFanout        string = """"
	cpuLimitIngress       string = """"
	cpuLimitRetry         string = """"
	memoryRequestFanout   string = ""500Mi""
	memoryRequestIngress  string = ""2000Mi""
	memoryRequestRetry    string = ""500Mi""
	memoryLimitFanout     string = ""3000Mi""
	memoryLimitIngress    string = ""2000Mi""
	memoryLimitRetry      string = ""3000Mi""
	minReplicas           int32  = 1
	maxReplicas           int32  = 10
)

// SetDefaults sets the default field values for a BrokerCell.
func (bc *BrokerCell) SetDefaults(ctx context.Context) {
	// Set defaults for the Spec.Components values.
	bc.Spec.SetDefaults(ctx)
}

// SetDefaults sets the default field values for a BrokerCellSpec.
func (bcs *BrokerCellSpec) SetDefaults(ctx context.Context) {
	// Fanout defaults
	if bcs.Components.Fanout == nil {
		bcs.Components.Fanout = makeComponent(cpuRequestFanout, cpuLimitFanout, memoryRequestFanout, memoryLimitFanout, avgCPUUtilizationFanout, avgMemoryUsageFanout)
	}
	bcs.Components.Fanout.setAutoScalingDefaults()
	// Ingress defaults
	if bcs.Components.Ingress == nil {
		bcs.Components.Ingress = makeComponent(cpuRequestIngress, cpuLimitIngress, memoryRequestIngress, memoryLimitIngress, avgCPUUtilizationIngress, avgMemoryUsageIngress)
	}
	bcs.Components.Ingress.setAutoScalingDefaults()
	// Retry defaults
	if bcs.Components.Retry == nil {
		bcs.Components.Retry = makeComponent(cpuRequestRetry, cpuLimitRetry, memoryRequestRetry, memoryLimitRetry, avgCPUUtilizationRetry, avgMemoryUsageRetry)
	}
	bcs.Components.Retry.setAutoScalingDefaults()
}

func makeComponent(cpuRequest, cpuLimit, memoryRequest, memoryLimit string, avgCPUUtilization int32, targetMemoryUsage string) *ComponentParameters {
	return &ComponentParameters{
		CPURequest:        cpuRequest,
		CPULimit:          cpuLimit,
		MemoryRequest:     memoryRequest,
		MemoryLimit:       memoryLimit,
		AvgCPUUtilization: ptr.Int32(avgCPUUtilization),
		AvgMemoryUsage:    ptr.String(targetMemoryUsage),
	}
}

func (componentParams *ComponentParameters) setAutoScalingDefaults() {
	if componentParams.MinReplicas == nil {
		componentParams.MinReplicas = ptr.Int32(minReplicas)
	}
	if componentParams.MaxReplicas == nil {
		componentParams.MaxReplicas = ptr.Int32(maxReplicas)
	}
}
","I am concerned if we are overfitting to solve rapid request spikes. If I have a gradual request increase that just causes fanout to use CPU at, say, 40%, this will result in 2 pods with low CPU utilization. Is my understanding correct?","The suggested code change introduces default settings for various components within a `BrokerCell` resource. It includes setting default CPU and memory requests and limits, as well as average CPU utilization and target memory usage for each component. Additionally, it sets default minimum and maximum replicas for auto-scaling.

Here's a concise review of the suggested code change:

1. **Default Values**: The code defines default values for CPU and memory requests and limits, as well as average CPU utilization and target memory usage for each component (`Fanout`, `Ingress`, and `Retry`).

2. **Component Creation**: The `makeComponent` function is used to create a `ComponentParameters` object with these default values. This function initializes the CPU and memory request and limit fields, as well as the average CPU utilization and target memory usage.

3. **Auto-Scaling Defaults**: The `setAutoScalingDefaults` method is added to the `ComponentParameters` struct to set default minimum and maximum replicas for auto-scaling if they are not already set.

4. **SetDefaults Method**: The `SetDefaults` methods in both `BrokerCell` and `BrokerCellSpec` structs call the `makeComponent` function to set default values for their respective components.

This approach ensures that all components have consistent default configurations, which can help in managing resources efficiently and reducing the risk of unexpected behavior due to missing or incorrect configuration values."
45,"@@ -164,6 +164,16 @@ module Beaker
       optionally_color GREY, string, false
     end
 
+    # Custom reporting for performance/sysstat messages
+    # Will not print unless we are at {LOG_LEVELS} 'debug' or higher.
+    # @param args[Array<String>] Strings to be reported
+    def perf_output *args
+      return unless is_debug?
+      strings = strip_colors_from args
+      string = strings.join
+      optionally_color MAGENTA, string, false
+    end
+
     # Report a debug message.
     # Will not print unless we are at {LOG_LEVELS} 'debug' or higher.
     # @param args[Array<String>] Strings to be reported","module Beaker
    # The Beaker Logger class
    # This class handles message reporting for Beaker, it reports based upon a provided log level
    # to a given destination (be it a string or file)
    #
  class Logger
    NORMAL         = ""\e[00;00m""
    BRIGHT_NORMAL  = ""\e[00;01m""
    BLACK          = ""\e[00;30m""
    RED            = ""\e[00;31m""
    GREEN          = ""\e[00;32m""
    YELLOW         = ""\e[00;33m""
    BLUE           = ""\e[00;34m""
    MAGENTA        = ""\e[00;35m""
    CYAN           = ""\e[00;36m""
    WHITE          = ""\e[00;37m""
    GREY           = ""\e[01;30m""
    BRIGHT_RED     = ""\e[01;31m""
    BRIGHT_GREEN   = ""\e[01;32m""
    BRIGHT_YELLOW  = ""\e[01;33m""
    BRIGHT_BLUE    = ""\e[01;34m""
    BRIGHT_MAGENTA = ""\e[01;35m""
    BRIGHT_CYAN    = ""\e[01;36m""
    BRIGHT_WHITE   = ""\e[01;37m""

    # The defined log levels.  Each log level also reports messages at levels lower than itself
    LOG_LEVELS      = {
      :debug   => 5,
      :verbose => 3,
      :info    => 2,
      :notify  => 1,
      :warn    => 0,
    }

    attr_accessor :color, :log_level, :destinations

    # Initialization of the Logger class
    # @overload initialize(dests)
    #   Initialize a Logger object that reports to the provided destinations, use default options
    #   @param [Array<String, IO>] Array of IO and strings (assumed to be file paths) to be reported to
    # @overload initialize(dests, options)
    #   Initialize a Logger object that reports to the provided destinations, use options from provided option hash
    #   @param [Array<String, IO>] Array of IO and strings (assumed to be file paths) to be reported to
    #   @param [Hash] options Hash of options
    #   @option options [Boolean] :color (true) Print color code before log messages
    #   @option options [Boolean] :quiet (false) Do not log messages to STDOUT
    #   @option options [String] :log_level (""info"") Log level (one of ""debug"" - highest level, ""verbose"", ""info"",
    #                          ""notify"" and ""warn"" - lowest level (see {LOG_LEVELS}))  The log level indicates that messages at that
    #                          log_level and lower will be reported.
    def initialize(*args)
      options = args.last.is_a?(Hash) ? args.pop : {}
      @color = options[:color]
      @sublog = nil
      case options[:log_level]
      when /debug/i, :debug
        @log_level = :debug
      when /verbose/i, :verbose
        @log_level = :verbose
      when /info/i, :info
        @log_level = :info
      when /notify/i, :notify
        @log_level = :notify
      when /warn/i, :warn
        @log_level = :warn
      else
        @log_level = :verbose
      end
      @destinations = []

      dests = args
      dests << STDOUT unless options[:quiet]
      dests.uniq!
      dests.each {|dest| add_destination(dest)}
    end

    # Construct an array of open steams for printing log messages to
    # @param [Array<IO, String>] dest Array of strings (each used as a file path) and IO steams that messages will be printed to
    def add_destination(dest)
      case dest
      when IO
        @destinations << dest
      when StringIO
        @destinations << dest
      when String
        @destinations << File.open(dest, 'w')
      else
        raise ""Unsuitable log destination #{dest.inspect}""
      end
    end

    # Remove a steam from the destinations array based upon it's name or file path
    # @param [String, IO] dest String representing a file path or IO stream
    def remove_destination(dest)
      case dest
      when IO
        @destinations.delete(dest)
      when StringIO
        @destinations.delete(dest)
      when String
        @destinations.delete_if {|d| d.respond_to?(:path) and d.path == dest}
      else
        raise ""Unsuitable log destination #{dest.inspect}""
      end
    end

    # Are we at {LOG_LEVELS} debug?
    # @return [Boolean] true if 'debug' or higher, false if not 'debug' {LOG_LEVELS} or lower
    def is_debug?
      LOG_LEVELS[@log_level] >= LOG_LEVELS[:debug]
    end

    # Are we at {LOG_LEVELS} verbose?
    # @return [Boolean] true if 'verbose' or higher, false if not 'verbose' {LOG_LEVELS} or lower
    def is_verbose?
      LOG_LEVELS[@log_level] >= LOG_LEVELS[:verbose]
    end

    # Are we at {LOG_LEVELS} warn?
    # @return [Boolean] true if 'warn' or higher, false if not 'warn' {LOG_LEVELS} or lower
    def is_warn?
      LOG_LEVELS[@log_level] >= LOG_LEVELS[:warn]
    end

    # Are we at {LOG_LEVELS} info?
    # @return [Boolean] true if 'info' or higher, false if not 'info' {LOG_LEVELS} or lower
    def is_info?
      LOG_LEVELS[@log_level] >= LOG_LEVELS[:info]
    end

    # Are we at {LOG_LEVELS} notify?
    # @return [Boolean] true if 'notify' or higher, false if not 'notify' {LOG_LEVELS} or lower
    def is_notify?
      LOG_LEVELS[@log_level] >= LOG_LEVELS[:notify]
    end

    # Remove invalid UTF-8 codes from provided string(s)
    # @param [String, Array<String>] string The string(s) to remove invalid codes from
    def convert string
      if string.kind_of?(Array)
        string.map do |s|
          convert s
        end
      else
        if string.respond_to?( :force_encoding )
          # We're running in >= 1.9 and we'll need to convert
          # Remove invalid and undefined UTF-8 character encodings
          string.force_encoding('UTF-8')
          return string.chars.select{|i| i.valid_encoding?}.join
        else
          # We're running 1.8, do nothing
          string
        end
      end
    end

    # Custom reporting for messages generated by host SUTs.
    # Will not print unless we are at {LOG_LEVELS} 'verbose' or higher.
    # Strips any color codes already in the provided messages, then adds logger color codes before reporting
    # @param args[Array<String>] Strings to be reported
    def host_output *args
      return unless is_verbose?
      strings = strip_colors_from args
      string = strings.join
    # Custom reporting for performance/sysstat messages
    # Will not print unless we are at {LOG_LEVELS} 'debug' or higher.
    # @param args[Array<String>] Strings to be reported
    def perf_output *args
      return unless is_debug?
      strings = strip_colors_from args
      string = strings.join
      optionally_color MAGENTA, string, false
    end

      optionally_color GREY, string, false
    end

    # Report a debug message.
    # Will not print unless we are at {LOG_LEVELS} 'debug' or higher.
    # @param args[Array<String>] Strings to be reported
    def debug *args
      return unless is_verbose?
      optionally_color WHITE, *args
    end

    # Report a warning message.
    # Will not print unless we are at {LOG_LEVELS} 'warn' or higher.
    # Will pre-pend the message with ""Warning: "".
    # @param args[Array<String>] Strings to be reported
    def warn *args
      return unless is_warn?
      strings = args.map {|msg| ""Warning: #{msg}"" }
      optionally_color YELLOW, strings
    end

    # Report an info message.
    # Will not print unless we are at {LOG_LEVELS} 'info' or higher.
    # @param args[Array<String>] Strings to be reported
    def info *args
      return unless is_info?
      optionally_color BLUE, *args
    end

    # Report a success message.
    # Will always be reported.
    # @param args[Array<String>] Strings to be reported
    def success *args
      optionally_color GREEN, *args
    end

    # Report a notify message.
    # Will not print unless we are at {LOG_LEVELS} 'notify' or higher.
    # @param args[Array<String>] Strings to be reported
    def notify *args
      return unless is_notify?
      optionally_color BRIGHT_WHITE, *args
    end

    # Report an error message.
    # Will always be reported.
    # @param args[Array<String>] Strings to be reported
    def error *args
      optionally_color BRIGHT_RED, *args
    end

    # Strip any color codes from provided string(s)
    # @param [String] lines A single or array of lines to removed color codes from
    # @return [Array<String>] An array of strings that do not have color codes
    def strip_colors_from lines
      Array( lines ).map do |line|
        convert(line).gsub(/(\e|\^\[)\[(\d*;)*\d*m/, '')
      end
    end

    # Print the provided message to the set destination streams, using color codes if appropriate
    # @param [String] color_code The color code to pre-pend to the message
    # @param [String] msg The message to be reported
    # @param [Boolean] add_newline (true) Add newlines between the color codes and the message
    def optionally_color color_code, msg, add_newline = true
      print_statement = add_newline ? :puts : :print
      @destinations.each do |to|
        to.print color_code if @color
        to.send print_statement, convert( msg )
        to.print NORMAL if @color
      end
    end

    # Utility method to get the current call stack and format it
    # to a human-readable string (which some IDEs/editors
    # will recognize as links to the line numbers in the trace).
    # Beaker associated files will be purged from backtrace unless log level is 'debug' or higher
    # @param [String] backtrace (caller(1)) The backtrace to format
    # @return [String] The formatted backtrace
    def pretty_backtrace backtrace = caller(1)
      trace = is_debug? ? backtrace : purge_harness_files_from( backtrace )
      expand_symlinks( trace ).join ""\n""
    end

    # Create a new StringIO log to track the current output
    def start_sublog
      if @sublog
        remove_destination(@sublog)
      end
      @sublog = StringIO.new
      add_destination(@sublog)
    end

    # Return the contents of the sublog
    def get_sublog
      @sublog.rewind
      @sublog.read
    end

    private
    # Expand each symlink found to its full path
    # Lines are assumed to be in the format ""String : Integer""
    # @param [String] backtrace The string to search and expand symlinks in
    # @return [String] The backtrace with symlinks expanded
    def expand_symlinks backtrace
      backtrace.collect do |line|
        file_path, line_num = line.split( "":"" )
        expanded_path = expand_symlink File.expand_path( file_path )
        expanded_path.to_s + "":"" + line_num.to_s
      end
    end

    # Remove Beaker associated lines from a given String
    # @param [String] backtrace The string to remove Beaker associated lines from
    # @return [String] The cleaned backtrace
    def purge_harness_files_from backtrace
      mostly_purged = backtrace.reject do |line|
        # LOADED_FEATURES is an array of anything `require`d, i.e. everything
        # but the test in question
        $LOADED_FEATURES.any? do |require_path|
          line.include? require_path
        end
      end

      # And remove lines that contain our program name in them
      completely_purged = mostly_purged.reject {|line| line.include? $0 }
    end

    # Utility method that takes a path as input, checks each component
    # of the path to see if it is a symlink, and expands
    # it if it is.
    # @param [String] file_path The path to be examined
    # @return [String] The fully expanded file_path
    def expand_symlink file_path
      file_path.split( ""/"" ).inject do |full_path, next_dir|
        next_path = full_path + ""/"" + next_dir
        if File.symlink? next_path
          link = File.readlink next_path
          next_path =
              case link
                when /^\// then link
                else
                  File.expand_path( full_path + ""/"" + link )
              end
        end
        next_path
      end
    end
  end
end
",Please add a yard @param tag here.,"The suggested code change appears to be a complete implementation of a logging system for the Beaker testing framework. It includes methods for setting up and managing log destinations, determining log levels, and formatting log messages with color codes. The code is well-documented and follows a consistent structure, making it easy to understand and maintain."
46,"@@ -22,9 +22,9 @@ define(['viewContainer', 'focusManager', 'queryString', 'layoutManager'], functi
             newView.initComplete = true;
 
             if (typeof options.controllerFactory === 'function') {
-
-                // Use controller method
                 var controller = new options.controllerFactory(newView, eventDetail.detail.params);
+            } else if (options.controllerFactory && typeof options.controllerFactory.default === 'function') {
+                var controller = new options.controllerFactory.default(newView, eventDetail.detail.params);
             }
 
             if (!options.controllerFactory || dispatchPageEvents) {","define(['viewContainer', 'focusManager', 'queryString', 'layoutManager'], function (viewContainer, focusManager, queryString, layoutManager) {
    'use strict';

    var currentView;
    var dispatchPageEvents;

    viewContainer.setOnBeforeChange(function (newView, isRestored, options) {

        var lastView = currentView;
        if (lastView) {

            var beforeHideResult = dispatchViewEvent(lastView, null, 'viewbeforehide', true);

            if (!beforeHideResult) {
                // todo: cancel
            }
        }

        var eventDetail = getViewEventDetail(newView, options, isRestored);

        if (!newView.initComplete) {
            } else if (options.controllerFactory && typeof options.controllerFactory.default === 'function') {
                var controller = new options.controllerFactory.default(newView, eventDetail.detail.params);
            newView.initComplete = true;

            if (typeof options.controllerFactory === 'function') {

                // Use controller method
                var controller = new options.controllerFactory(newView, eventDetail.detail.params);
            }

            if (!options.controllerFactory || dispatchPageEvents) {
                dispatchViewEvent(newView, eventDetail, 'viewinit');
            }
        }

        dispatchViewEvent(newView, eventDetail, 'viewbeforeshow');
    });

    function onViewChange(view, options, isRestore) {

        var lastView = currentView;
        if (lastView) {
            dispatchViewEvent(lastView, null, 'viewhide');
        }

        currentView = view;

        var eventDetail = getViewEventDetail(view, options, isRestore);

        if (!isRestore) {
            if (options.autoFocus !== false) {
                focusManager.autoFocus(view);
            }
        } else if (!layoutManager.mobile) {
            if (view.activeElement && document.body.contains(view.activeElement) && focusManager.isCurrentlyFocusable(view.activeElement)) {
                focusManager.focus(view.activeElement);
            } else {
                focusManager.autoFocus(view);
            }
        }

        view.dispatchEvent(new CustomEvent('viewshow', eventDetail));

        if (dispatchPageEvents) {
            view.dispatchEvent(new CustomEvent('pageshow', eventDetail));
        }
    }

    function getProperties(view) {
        var props = view.getAttribute('data-properties');

        if (props) {
            return props.split(',');
        }

        return [];
    }

    function dispatchViewEvent(view, eventInfo, eventName, isCancellable) {

        if (!eventInfo) {
            eventInfo = {
                detail: {
                    type: view.getAttribute('data-type'),
                    properties: getProperties(view)
                },
                bubbles: true,
                cancelable: isCancellable
            };
        }

        eventInfo.cancelable = isCancellable || false;

        var eventResult = view.dispatchEvent(new CustomEvent(eventName, eventInfo));

        if (dispatchPageEvents) {
            eventInfo.cancelable = false;
            view.dispatchEvent(new CustomEvent(eventName.replace('view', 'page'), eventInfo));
        }

        return eventResult;
    }

    function getViewEventDetail(view, options, isRestore) {

        var url = options.url;
        var index = url.indexOf('?');
        var params = index === -1 ? {} : queryString.parse(url.substring(index + 1));

        return {
            detail: {
                type: view.getAttribute('data-type'),
                properties: getProperties(view),
                params: params,
                isRestored: isRestore,
                state: options.state,

                // The route options
                options: options.options || {}
            },
            bubbles: true,
            cancelable: false
        };
    }

    function resetCachedViews() {
        // Reset all cached views whenever the skin changes
        viewContainer.reset();
    }

    document.addEventListener('skinunload', resetCachedViews);

    function ViewManager() {
    }

    ViewManager.prototype.loadView = function (options) {

        var lastView = currentView;

        // Record the element that has focus
        if (lastView) {
            lastView.activeElement = document.activeElement;
        }

        if (options.cancel) {
            return;
        }

        viewContainer.loadView(options).then(function (view) {

            onViewChange(view, options);
        });
    };

    ViewManager.prototype.tryRestoreView = function (options, onViewChanging) {

        if (options.cancel) {
            return Promise.reject({ cancelled: true });
        }

        // Record the element that has focus
        if (currentView) {
            currentView.activeElement = document.activeElement;
        }

        return viewContainer.tryRestoreView(options).then(function (view) {

            onViewChanging();
            onViewChange(view, options, true);

        });
    };

    ViewManager.prototype.currentView = function () {
        return currentView;
    };

    ViewManager.prototype.dispatchPageEvents = function (value) {
        dispatchPageEvents = value;
    };

    return new ViewManager();
});
",Maybe move `var controller` out of `if/else`? (_Code Smell_),"The provided code defines a `ViewManager` class that manages the lifecycle of views in a web application. Here's a concise review of the key components and functionality:

### Key Components:
1. **Dependencies**: The code imports several modules (`viewContainer`, `focusManager`, `queryString`, `layoutManager`) which suggest it's part of a larger framework or library.

2. **State Management**:
   - `currentView`: Keeps track of the currently active view.
   - `dispatchPageEvents`: A flag to control whether page-level events should be dispatched.

3. **View Lifecycle Events**:
   - `viewContainer.setOnBeforeChange`: Handles the transition between views before they are changed.
   - `onViewChange`: Manages the transition between views after they have been changed.
   - `dispatchViewEvent`: Dispatches custom events for views and pages.

4. **View Properties**:
   - `getProperties`: Extracts properties from the `data-properties` attribute of a view.

5. **URL Parsing**:
   - `getViewEventDetail`: Parses the URL to extract query parameters and constructs an event detail object.

6. **Resetting Cached Views**:
   - `resetCachedViews`: Resets all cached views when the skin changes.

7. **View Loading and Restoration**:
   - `loadView`: Loads a new view and triggers the `onViewChange` event.
   - `tryRestoreView`: Attempts to restore a previously saved view and triggers the `onViewChange` event.

8. **Public Methods**:
   - `currentView`: Returns the current view.
   - `dispatchPageEvents`: Sets the `dispatchPageEvents` flag.

### Potential Improvements:
- **Error Handling**: Add error handling for asynchronous operations like `viewContainer.loadView` and `viewContainer.tryRestoreView`.
- **Type Checking**: Ensure that all dependencies are correctly imported and used.
- **Documentation**: Add comments and documentation to explain the purpose and usage of each function.
- **Performance**: Optimize the code for performance, especially if dealing with large numbers of views or complex event handling.

Overall, the code provides a robust framework for managing views in a web application, with clear separation of concerns and a well-defined API."
47,"@@ -357,6 +357,7 @@ OUTER_LOOP:
 
 // Render renders Markdown to HTML with special links.
 func Render(rawBytes []byte, urlPrefix string, metas map[string]string) []byte {
+	urlPrefix = bytes.Replace(urlPrefix, string(spaceBytes), string(spaceEncodedBytes), -1)
 	result := RenderRaw(rawBytes, urlPrefix)
 	result = PostProcess(result, urlPrefix, metas)
 	result = Sanitizer.SanitizeBytes(result)","// Copyright 2014 The Gogs Authors. All rights reserved.
// Use of this source code is governed by a MIT-style
// license that can be found in the LICENSE file.

package markdown

import (
	""bytes""
	""fmt""
	""io""
	""path""
	""path/filepath""
	""regexp""
	""strings""

	""github.com/Unknwon/com""
	""github.com/microcosm-cc/bluemonday""
	""github.com/russross/blackfriday""
	""golang.org/x/net/html""

	""github.com/gogits/gogs/modules/base""
	""github.com/gogits/gogs/modules/setting""
)

var Sanitizer = bluemonday.UGCPolicy()

// BuildSanitizer initializes sanitizer with allowed attributes based on settings.
// This function should only be called once during entire application lifecycle.
func BuildSanitizer() {
	// Normal markdown-stuff
	Sanitizer.AllowAttrs(""class"").Matching(regexp.MustCompile(`[\p{L}\p{N}\s\-_',:\[\]!\./\\\(\)&]*`)).OnElements(""code"")

	// Checkboxes
	Sanitizer.AllowAttrs(""type"").Matching(regexp.MustCompile(`^checkbox$`)).OnElements(""input"")
	Sanitizer.AllowAttrs(""checked"", ""disabled"").OnElements(""input"")

	// Custom URL-Schemes
	Sanitizer.AllowURLSchemes(setting.Markdown.CustomURLSchemes...)
}

var validLinksPattern = regexp.MustCompile(`^[a-z][\w-]+://`)

// isLink reports whether link fits valid format.
func isLink(link []byte) bool {
	return validLinksPattern.Match(link)
}

// IsMarkdownFile reports whether name looks like a Markdown file
// based on its extension.
func IsMarkdownFile(name string) bool {
	name = strings.ToLower(name)
	switch filepath.Ext(name) {
	case "".md"", "".markdown"", "".mdown"", "".mkd"":
		return true
	}
	return false
}

// IsReadmeFile reports whether name looks like a README file
// based on its extension.
func IsReadmeFile(name string) bool {
	name = strings.ToLower(name)
	if len(name) < 6 {
		return false
	} else if len(name) == 6 {
		return name == ""readme""
	}
	return name[:7] == ""readme.""
}

var (
	// MentionPattern matches string that mentions someone, e.g. @Unknwon
	MentionPattern = regexp.MustCompile(`(\s|^)@[0-9a-zA-Z_\.]+`)

	// CommitPattern matches link to certain commit with or without trailing hash,
	// e.g. https://try.gogs.io/gogs/gogs/commit/d8a994ef243349f321568f9e36d5c3f444b99cae#diff-2
	CommitPattern = regexp.MustCompile(`(\s|^)https?.*commit/[0-9a-zA-Z]+(#+[0-9a-zA-Z-]*)?`)

	// IssueFullPattern matches link to an issue with or without trailing hash,
	// e.g. https://try.gogs.io/gogs/gogs/issues/4#issue-685
	IssueFullPattern = regexp.MustCompile(`(\s|^)https?.*issues/[0-9]+(#+[0-9a-zA-Z-]*)?`)
	// IssueIndexPattern matches string that references to an issue, e.g. #1287
	IssueIndexPattern = regexp.MustCompile(`( |^|\()#[0-9]+\b`)

	// Sha1CurrentPattern matches string that represents a commit SHA, e.g. d8a994ef243349f321568f9e36d5c3f444b99cae
	Sha1CurrentPattern = regexp.MustCompile(`\b[0-9a-f]{40}\b`)
)

// Renderer is a extended version of underlying render object.
type Renderer struct {
	blackfriday.Renderer
	urlPrefix string
}

// Link defines how formal links should be processed to produce corresponding HTML elements.
func (r *Renderer) Link(out *bytes.Buffer, link []byte, title []byte, content []byte) {
	if len(link) > 0 && !isLink(link) {
		if link[0] != '#' {
			link = []byte(path.Join(r.urlPrefix, string(link)))
		}
	}

	r.Renderer.Link(out, link, title, content)
}

// AutoLink defines how auto-detected links should be processed to produce corresponding HTML elements.
// Reference for kind: https://github.com/russross/blackfriday/blob/master/markdown.go#L69-L76
func (r *Renderer) AutoLink(out *bytes.Buffer, link []byte, kind int) {
	if kind != blackfriday.LINK_TYPE_NORMAL {
		r.Renderer.AutoLink(out, link, kind)
		return
	}

	// Since this method could only possibly serve one link at a time,
	// we do not need to find all.
	m := CommitPattern.Find(link)
	if m != nil {
		m = bytes.TrimSpace(m)
		i := strings.Index(string(m), ""commit/"")
		j := strings.Index(string(m), ""#"")
		if j == -1 {
			j = len(m)
		}
		out.WriteString(fmt.Sprintf(` <code><a href=""%s"">%s</a></code>`, m, base.ShortSha(string(m[i+7:j]))))
		return
	}

	m = IssueFullPattern.Find(link)
	if m != nil {
		m = bytes.TrimSpace(m)
		i := strings.Index(string(m), ""issues/"")
		j := strings.Index(string(m), ""#"")
		if j == -1 {
			j = len(m)
		}
		out.WriteString(fmt.Sprintf(` <a href=""%s"">#%s</a>`, m, base.ShortSha(string(m[i+7:j]))))
		return
	}

	r.Renderer.AutoLink(out, link, kind)
}

// ListItem defines how list items should be processed to produce corresponding HTML elements.
func (options *Renderer) ListItem(out *bytes.Buffer, text []byte, flags int) {
	// Detect procedures to draw checkboxes.
	switch {
	case bytes.HasPrefix(text, []byte(""[ ] "")):
		text = append([]byte(`<input type=""checkbox"" disabled="""" />`), text[3:]...)
	case bytes.HasPrefix(text, []byte(""[x] "")):
		text = append([]byte(`<input type=""checkbox"" disabled="""" checked="""" />`), text[3:]...)
	}
	options.Renderer.ListItem(out, text, flags)
}

// Note: this section is for purpose of increase performance and
// reduce memory allocation at runtime since they are constant literals.
var (
	svgSuffix         = []byte("".svg"")
	svgSuffixWithMark = []byte("".svg?"")
	spaceBytes        = []byte("" "")
	spaceEncodedBytes = []byte(""%20"")
)

// Image defines how images should be processed to produce corresponding HTML elements.
func (r *Renderer) Image(out *bytes.Buffer, link []byte, title []byte, alt []byte) {
	prefix := strings.Replace(r.urlPrefix, ""/src/"", ""/raw/"", 1)
	if len(link) > 0 {
		if isLink(link) {
			// External link with .svg suffix usually means CI status.
			// TODO: define a keyword to allow non-svg images render as external link.
			if bytes.HasSuffix(link, svgSuffix) || bytes.Contains(link, svgSuffixWithMark) {
				r.Renderer.Image(out, link, title, alt)
				return
			}
		} else {
			if link[0] != '/' {
				prefix += ""/""
			}
			link = bytes.Replace([]byte((prefix + string(link))), spaceBytes, spaceEncodedBytes, -1)
			fmt.Println(333, string(link))
		}
	}

	out.WriteString(`<a href=""`)
	out.Write(link)
	out.WriteString(`"">`)
	r.Renderer.Image(out, link, title, alt)
	out.WriteString(""</a>"")
}

// cutoutVerbosePrefix cutouts URL prefix including sub-path to
// return a clean unified string of request URL path.
func cutoutVerbosePrefix(prefix string) string {
	count := 0
	for i := 0; i < len(prefix); i++ {
		if prefix[i] == '/' {
			count++
		}
		if count >= 3+setting.AppSubUrlDepth {
			return prefix[:i]
		}
	}
	return prefix
}

// RenderIssueIndexPattern renders issue indexes to corresponding links.
func RenderIssueIndexPattern(rawBytes []byte, urlPrefix string, metas map[string]string) []byte {
	urlPrefix = cutoutVerbosePrefix(urlPrefix)
	ms := IssueIndexPattern.FindAll(rawBytes, -1)
	for _, m := range ms {
		var space string
		if m[0] != '#' {
			space = string(m[0])
			m = m[1:]
		}
		if metas == nil {
			rawBytes = bytes.Replace(rawBytes, m, []byte(fmt.Sprintf(`%s<a href=""%s/issues/%s"">%s</a>`,
				space, urlPrefix, m[1:], m)), 1)
		} else {
			// Support for external issue tracker
			metas[""index""] = string(m[1:])
			rawBytes = bytes.Replace(rawBytes, m, []byte(fmt.Sprintf(`%s<a href=""%s"">%s</a>`,
				space, com.Expand(metas[""format""], metas), m)), 1)
		}
	}
	return rawBytes
}

// RenderSha1CurrentPattern renders SHA1 strings to corresponding links that assumes in the same repository.
func RenderSha1CurrentPattern(rawBytes []byte, urlPrefix string) []byte {
	ms := Sha1CurrentPattern.FindAll(rawBytes, -1)
	for _, m := range ms {
		rawBytes = bytes.Replace(rawBytes, m, []byte(fmt.Sprintf(
			`<a href=""%s/commit/%s""><code>%s</code></a>`, urlPrefix, m, base.ShortSha(string(m)))), -1)
	}
	return rawBytes
}

// RenderSpecialLink renders mentions, indexes and SHA1 strings to corresponding links.
func RenderSpecialLink(rawBytes []byte, urlPrefix string, metas map[string]string) []byte {
	ms := MentionPattern.FindAll(rawBytes, -1)
	for _, m := range ms {
		m = bytes.TrimSpace(m)
		rawBytes = bytes.Replace(rawBytes, m,
			[]byte(fmt.Sprintf(`<a href=""%s/%s"">%s</a>`, setting.AppSubUrl, m[1:], m)), -1)
	}

	rawBytes = RenderIssueIndexPattern(rawBytes, urlPrefix, metas)
	rawBytes = RenderSha1CurrentPattern(rawBytes, urlPrefix)
	return rawBytes
}

// RenderRaw renders Markdown to HTML without handling special links.
func RenderRaw(body []byte, urlPrefix string) []byte {
	htmlFlags := 0
	htmlFlags |= blackfriday.HTML_SKIP_STYLE
	htmlFlags |= blackfriday.HTML_OMIT_CONTENTS
	renderer := &Renderer{
		Renderer:  blackfriday.HtmlRenderer(htmlFlags, """", """"),
		urlPrefix: urlPrefix,
	}

	// set up the parser
	extensions := 0
	extensions |= blackfriday.EXTENSION_NO_INTRA_EMPHASIS
	extensions |= blackfriday.EXTENSION_TABLES
	extensions |= blackfriday.EXTENSION_FENCED_CODE
	extensions |= blackfriday.EXTENSION_AUTOLINK
	extensions |= blackfriday.EXTENSION_STRIKETHROUGH
	extensions |= blackfriday.EXTENSION_SPACE_HEADERS
	extensions |= blackfriday.EXTENSION_NO_EMPTY_LINE_BEFORE_BLOCK

	if setting.Markdown.EnableHardLineBreak {
		extensions |= blackfriday.EXTENSION_HARD_LINE_BREAK
	}

	body = blackfriday.Markdown(body, renderer, extensions)
	return body
}

var (
	leftAngleBracket  = []byte(""</"")
	rightAngleBracket = []byte("">"")
)

var noEndTags = []string{""img"", ""input"", ""br"", ""hr""}

// PostProcess treats different types of HTML differently,
// and only renders special links for plain text blocks.
func PostProcess(rawHtml []byte, urlPrefix string, metas map[string]string) []byte {
	startTags := make([]string, 0, 5)
	var buf bytes.Buffer
	tokenizer := html.NewTokenizer(bytes.NewReader(rawHtml))

OUTER_LOOP:
	for html.ErrorToken != tokenizer.Next() {
		token := tokenizer.Token()
		switch token.Type {
		case html.TextToken:
			buf.Write(RenderSpecialLink([]byte(token.String()), urlPrefix, metas))

		case html.StartTagToken:
			buf.WriteString(token.String())
			tagName := token.Data
			// If this is an excluded tag, we skip processing all output until a close tag is encountered.
			if strings.EqualFold(""a"", tagName) || strings.EqualFold(""code"", tagName) || strings.EqualFold(""pre"", tagName) {
				stackNum := 1
				for html.ErrorToken != tokenizer.Next() {
					token = tokenizer.Token()

					// Copy the token to the output verbatim
					buf.WriteString(token.String())

					if token.Type == html.StartTagToken {
						stackNum++
					}

					// If this is the close tag to the outer-most, we are done
					if token.Type == html.EndTagToken {
						stackNum--

						if stackNum <= 0 && strings.EqualFold(tagName, token.Data) {
							break
						}
					}
				}
				continue OUTER_LOOP
			}

			if !com.IsSliceContainsStr(noEndTags, token.Data) {
				startTags = append(startTags, token.Data)
			}

		case html.EndTagToken:
			if len(startTags) == 0 {
				buf.WriteString(token.String())
				break
			}

			buf.Write(leftAngleBracket)
			buf.WriteString(startTags[len(startTags)-1])
			buf.Write(rightAngleBracket)
			startTags = startTags[:len(startTags)-1]
		default:
			buf.WriteString(token.String())
		}
	}

	if io.EOF == tokenizer.Err() {
		return buf.Bytes()
	}

	// If we are not at the end of the input, then some other parsing error has occurred,
	// so return the input verbatim.
	return rawHtml
}
	urlPrefix = bytes.Replace(urlPrefix, string(spaceBytes), string(spaceEncodedBytes), -1)

// Render renders Markdown to HTML with special links.
func Render(rawBytes []byte, urlPrefix string, metas map[string]string) []byte {
	result := RenderRaw(rawBytes, urlPrefix)
	result = PostProcess(result, urlPrefix, metas)
	result = Sanitizer.SanitizeBytes(result)
	return result
}

// RenderString renders Markdown to HTML with special links and returns string type.
func RenderString(raw, urlPrefix string, metas map[string]string) string {
	return string(Render([]byte(raw), urlPrefix, metas))
}
",Can you compile this line?,"The suggested code changes appear to be related to enhancing the Markdown rendering capabilities of a Go application, particularly focusing on sanitization, link handling, and special link rendering. Here's a concise review of the changes:

### Key Changes and Improvements:

1. **Sanitization Initialization**:
   - `BuildSanitizer` function initializes the `bluemonday.UGCPolicy()` with allowed attributes based on settings. This ensures that the rendered HTML is safe from XSS attacks.

2. **Link Handling**:
   - `isLink` function checks if a given link is valid.
   - `Link` method in `Renderer` handles links, converting them to absolute paths if necessary and ensuring they are sanitized.

3. **AutoLink Handling**:
   - `AutoLink` method processes auto-detected links, converting commit and issue links to clickable HTML elements.

4. **ListItem Handling**:
   - `ListItem` method handles list items, adding checkboxes if applicable.

5. **Image Handling**:
   - `Image` method handles images, converting them to clickable links and ensuring they are sanitized.

6. **Special Link Rendering**:
   - `RenderSpecialLink` function renders mentions, issue indexes, and SHA1 strings to corresponding links.
   - `RenderIssueIndexPattern` and `RenderSha1CurrentPattern` functions handle specific patterns for issue indexes and SHA1 strings.

7. **Post-Processing**:
   - `PostProcess` function ensures that special links are only rendered within plain text blocks, preventing issues with nested tags.

8. **Rendering Functions**:
   - `RenderRaw` function converts Markdown to HTML without handling special links.
   - `Render` function combines `RenderRaw` and `PostProcess` to handle special links.
   - `RenderString` function provides a convenience method to render Markdown to a string.

### Overall Impact:
These changes improve the robustness and security of the Markdown rendering process by ensuring that the output is both functional and safe from potential security vulnerabilities. The use of `bluemonday` for sanitization helps prevent XSS attacks, while the custom handling of links and special patterns ensures that the rendered content is as expected.

### Potential Enhancements:
- **Custom CSS**: Consider adding custom CSS to style the rendered Markdown output.
- **Performance Optimization**: Further optimize the rendering process, especially for large documents.
- **Error Handling**: Add more robust error handling for edge cases, such as malformed Markdown input.

Overall, these changes significantly enhance the functionality and security"
48,"@@ -115,7 +115,7 @@ public class InMemoryProjectCache extends AbstractProjectCache implements Projec
   public Optional<Project> getProjectById(final Integer key) throws ProjectManagerException {
     Project project = this.projectsById.get(key);
     if (project == null) {
-      logger.error(""Project not found in cache, fetching from DB"");
+      logger.info(""Project with ID "" + key + "" not found in cache, fetching from DB"");
       project = fetchProjectById(key);
     }
     return Optional.ofNullable(project);","/*
 * Copyright 2020 LinkedIn Corp.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License""); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

package azkaban.project;

import azkaban.utils.CaseInsensitiveConcurrentHashMap;
import java.util.ArrayList;
import java.util.List;
import java.util.Optional;
import java.util.concurrent.ConcurrentHashMap;
import java.util.regex.Pattern;
import javax.inject.Inject;
import javax.inject.Singleton;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This class implements ProjectCache and extends AbstractProjectCache to implement a in-memory
 * implementation of project cache where all the active projects are loaded into the main-memory
 * when the web-server starts. This would be replaced in future by guava cache implementation.
 * <p>
 * The present cache consists of two mappings :  name to project  AND   id to project. In this
 * implementation both the maps contain all the project entities. In future implementations
 * name-to-project mapping will be replaced by name-to-id mapping containing all the active
 * projects' name-id and fixed size cache to store project entities.
 */
@Singleton
public class InMemoryProjectCache extends AbstractProjectCache implements ProjectCache {

  private static final Logger logger = LoggerFactory.getLogger(InMemoryProjectCache.class);

  private final ConcurrentHashMap<Integer, Project> projectsById;

  private final CaseInsensitiveConcurrentHashMap<Project> projectsByName;


  @Inject
  public InMemoryProjectCache(final ProjectLoader loader) {
    super(loader);
    this.projectsById = new ConcurrentHashMap<>();
    this.projectsByName = new CaseInsensitiveConcurrentHashMap<>();
    final long startTime = System.currentTimeMillis();
    init();
    final long elapsedTime = System.currentTimeMillis() - startTime;
    logger.info(""Time taken to initialize and load cache in milliseconds: "" + elapsedTime);
  }

  /**
   * load all active projects and their corresponding flows into memory. Queries from database only
   * returns a high level project object. Need to explicitly load flows for the project objects.
   */
  private void init() {
    final List<Project> projects = super.getActiveProjects();
    logger.info(""Loading active projects."");
    for (final Project proj : projects) {
      putProject(proj);
    }
    logger.info(""Loading flows from active projects."");
    loadAllFlows(projects);

  }

  /**
   * Inserts given project into the cache.
   *
   * @param project Project
   */
  @Override
  public void putProject(final Project project) {
    this.projectsByName.put(project.getName(), project);
    this.projectsById.put(project.getId(), project);
  }

  /**
   * Queries an active project by name. Fetches from database if not present in cache.
   *
   * @param key name of the project
   * @return Project
   */
  @Override
  public Optional<Project> getProjectByName(final String key) {
    Project project = this.projectsByName.get(key);
    if (project == null) {
      logger.info(""No active project with name {} exists in cache, fetching from DB."", key);
      try {
        project = fetchProjectByName(key);
      } catch (final ProjectManagerException e) {
        logger.error(""Could not load project from store."", e);
      }
    }
    return Optional.ofNullable(project);
  }

  /**
   * Fetch active/inactive project by project id. If active project not present in cache, fetches
   * from DB. Fetches inactive project from DB.
   *
   * @param key Project id
   * @return Project
   */
  @Override
      logger.info(""Project with ID "" + key + "" not found in cache, fetching from DB"");
  public Optional<Project> getProjectById(final Integer key) throws ProjectManagerException {
    Project project = this.projectsById.get(key);
    if (project == null) {
      logger.error(""Project not found in cache, fetching from DB"");
      project = fetchProjectById(key);
    }
    return Optional.ofNullable(project);
  }

  /**
   * Invalidates the given project from cache.
   */
  @Override
  public void removeProject(final Project project) {
    this.projectsByName.remove(project.getName());
    this.projectsById.remove(project.getId());
  }

  /**
   * @param pattern
   * @return List of Projects matching to given pattern.
   */
  @Override
  public List<Project> getProjectsWithSimilarNames(final Pattern pattern) {
    final List<Project> matches = new ArrayList<>();
    final ArrayList<String> names = new ArrayList<>(this.projectsByName.getKeys());
    for (final String projName : names) {
      if (pattern.matcher(projName).find()) {
        matches.add(this.projectsByName.get(projName));
      }
    }
    return matches;
  }

  /**
   * Returns all the projects from the in-memory cache map.
   */
  @Override
  public List<Project> getActiveProjects() {
    return new ArrayList<>(this.projectsById.values());
  }


}
",Can key be null?,"The provided code is a Java implementation of an `InMemoryProjectCache` class that extends `AbstractProjectCache`. This class is designed to store and manage project data in memory, providing methods to retrieve projects by name or ID, invalidate projects, and search for projects with similar names.

### Key Points:

1. **Singleton Pattern**: The class is annotated with `@Singleton`, ensuring that only one instance of the class exists throughout the application.

2. **Concurrency**: Both `projectsById` and `projectsByName` are implemented using `ConcurrentHashMap`, which provides thread-safe operations for concurrent access.

3. **Initialization**: The `init()` method loads all active projects and their flows into memory. It uses the `ProjectLoader` to fetch project data from the database.

4. **Project Management**:
   - `putProject`: Adds a project to both the `projectsByName` and `projectsById` maps.
   - `getProjectByName`: Retrieves a project by name. If the project is not found in the cache, it fetches it from the database.
   - `getProjectById`: Retrieves a project by ID. If the project is not found in the cache, it fetches it from the database.
   - `removeProject`: Removes a project from both the `projectsByName` and `projectsById` maps.

5. **Search and Retrieval**:
   - `getProjectsWithSimilarNames`: Searches for projects whose names match a given regular expression pattern.
   - `getActiveProjects`: Returns a list of all active projects stored in the cache.

6. **Logging**: The class uses SLF4J for logging, providing information about cache initialization, project loading, and retrieval operations.

### Potential Improvements:

- **Error Handling**: Consider adding more robust error handling, especially for database operations, to ensure the application can recover gracefully from failures.
- **Performance Optimization**: For large datasets, consider optimizing the cache eviction policy or using a more efficient data structure.
- **Testing**: Implement unit tests to verify the correctness of the cache operations and ensure that the cache behaves as expected under various scenarios.

Overall, the code is well-structured and provides a solid foundation for managing project data in memory."
49,"@@ -585,9 +585,12 @@ int flb_utils_write_str(char *buf, int *off, size_t size,
         }
         else if (c >= 0x80 && c <= 0xFFFF) {
             hex_bytes = flb_utf8_len(str + i);
-            if ((available - written) < 6) {
+            if (available - written < 6) {
                 return FLB_FALSE;
             }
+            if (i + hex_bytes > str_len) {
+                break; /* skip truncated UTF-8 */
+            }
 
             state = FLB_UTF8_ACCEPT;
             codepoint = 0;","/* -*- Mode: C; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */

/*  Fluent Bit
 *  ==========
 *  Copyright (C) 2019      The Fluent Bit Authors
 *  Copyright (C) 2015-2018 Treasure Data Inc.
 *
 *  Licensed under the Apache License, Version 2.0 (the ""License"");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>
#include <ctype.h>

#include <sys/types.h>
#include <sys/stat.h>
#include <msgpack.h>

#include <monkey/mk_core.h>
#include <fluent-bit/flb_macros.h>
#include <fluent-bit/flb_config.h>
#include <fluent-bit/flb_error.h>
#include <fluent-bit/flb_input.h>
#include <fluent-bit/flb_output.h>
#include <fluent-bit/flb_utils.h>
#include <fluent-bit/flb_utf8.h>

void flb_utils_error(int err)
{
    char *msg = NULL;

    switch (err) {
    case FLB_ERR_CFG_FILE:
        msg = ""Could not open configuration file"";
        break;
    case FLB_ERR_CFG_FILE_FORMAT:
        msg = ""Configuration file contains format errors"";
        break;
    case FLB_ERR_CFG_FILE_STOP:
        msg = ""Configuration file contain errors"";
        break;
    case FLB_ERR_CFG_FLUSH:
        msg = ""Invalid flush value"";
        break;
    case FLB_ERR_CFG_FLUSH_CREATE:
        msg = ""Could not create timer for flushing"";
        break;
    case FLB_ERR_CFG_FLUSH_REGISTER:
        msg = ""Could not register timer for flushing"";
        break;
    case FLB_ERR_INPUT_INVALID:
        msg = ""Invalid input type"";
        break;
    case FLB_ERR_INPUT_UNDEF:
        msg = ""No Input(s) have been defined"";
        break;
    case FLB_ERR_INPUT_UNSUP:
        msg = ""Unsupported Input"";
        break;
    case FLB_ERR_OUTPUT_UNDEF:
        msg = ""You must specify an output target"";
        break;
    case FLB_ERR_OUTPUT_INVALID:
        msg = ""Invalid output target"";
        break;
    case FLB_ERR_OUTPUT_UNIQ:
        msg = ""Just one output type is supported"";
        break;
    case FLB_ERR_FILTER_INVALID:
        msg = ""Invalid filter plugin"";
        break;
    case FLB_ERR_CFG_PARSER_FILE:
        msg = ""Could not open parser configuration file"";
        break;
    case FLB_ERR_JSON_INVAL:
        msg = ""Invalid JSON string"";
        break;
    case FLB_ERR_JSON_PART:
        msg = ""Truncated JSON string"";
        break;
    case FLB_ERR_CORO_STACK_SIZE:
        msg = ""Invalid coroutine stack size"";
        break;
    }

    if (!msg) {
        fprintf(stderr,
                ""%sError%s: undefined. Aborting"",
                ANSI_BOLD ANSI_RED, ANSI_RESET);
    }
    else {
        fprintf(stderr,
                ""%sError%s: %s. Aborting\n\n"",
                ANSI_BOLD ANSI_RED, ANSI_RESET, msg);
    }

    if (err <= FLB_ERR_FILTER_INVALID) {
        exit(EXIT_FAILURE);
    }
}

/* Custom error */
void flb_utils_error_c(const char *msg)
{
    fprintf(stderr,
            ""%sError%s: %s. Aborting\n\n"",
            ANSI_BOLD ANSI_RED, ANSI_RESET, msg);
    exit(EXIT_FAILURE);
}

void flb_utils_warn_c(const char *msg)
{
    fprintf(stderr,
            ""%sWarning%s: %s"",
            ANSI_BOLD ANSI_YELLOW, ANSI_RESET, msg);
}

#ifdef FLB_HAVE_FORK
/* Run current process in background mode */
int flb_utils_set_daemon(struct flb_config *config)
{
    pid_t pid;

    if ((pid = fork()) < 0){
		flb_error(""Failed creating to switch to daemon mode (fork failed)"");
        exit(EXIT_FAILURE);
	}

    if (pid > 0) { /* parent */
        exit(EXIT_SUCCESS);
    }

    /* set files mask */
    umask(0);

    /* Create new session */
    setsid();

    if (chdir(""/"") < 0) { /* make sure we can unmount the inherited filesystem */
        flb_error(""Unable to unmount the inherited filesystem"");
        exit(EXIT_FAILURE);
	}

    /* Our last STDOUT messages */
    flb_info(""switching to background mode (PID=%lu)"", getpid());

    fclose(stderr);
    fclose(stdout);

    return 0;
}
#endif

void flb_utils_print_setup(struct flb_config *config)
{
    struct mk_list *head;
    struct flb_input_plugin *plugin;
    struct flb_input_collector *collector;
    struct flb_input_instance *in;

    flb_info(""Configuration"");

    /* general */
    flb_info("" flush time     : %i seconds"", config->flush);

    /* Inputs */
    flb_info("" input plugins  : "");
    mk_list_foreach(head, &config->inputs) {
        in = mk_list_entry(head, struct flb_input_instance, _head);
        flb_info(""%s "", in->p->name);
    }

    /* Collectors */
    flb_info("" collectors     : "");
    mk_list_foreach(head, &config->collectors) {
        collector = mk_list_entry(head, struct flb_input_collector, _head);
        plugin = collector->instance->p;

        if (collector->seconds > 0) {
            flb_info(""[%s %lus,%luns] "",
                     plugin->name,
                     collector->seconds,
                     collector->nanoseconds);
        }
        else {
            printf(""[%s] "", plugin->name);
        }

    }
}

struct mk_list *flb_utils_split(const char *line, int separator, int max_split)
{
    int i = 0;
    int count = 0;
    int val_len;
    int len;
    int end;
    char *val;
    struct mk_list *list;
    struct flb_split_entry *new;

    if (!line) {
        return NULL;
    }

    list = flb_malloc(sizeof(struct mk_list));
    if (!list) {
        flb_errno();
        return NULL;
    }
    mk_list_init(list);

    len = strlen(line);
    while (i < len) {
        end = mk_string_char_search(line + i, separator, len - i);
        if (end >= 0 && end + i < len) {
            end += i;

            if (i == (unsigned int) end) {
                i++;
                continue;
            }

            val = mk_string_copy_substr(line, i, end);
            val_len = end - i;
        }
        else {
            val = mk_string_copy_substr(line, i, len);
            val_len = len - i;
            end = len;
        }

        /* Update last position */
        i = end;

        /* Create new entry */
        new = flb_malloc(sizeof(struct flb_split_entry));
        if (!new) {
            flb_errno();
            flb_free(val);
            flb_utils_split_free(list);
            return NULL;
        }
        new->value = val;
        new->len = val_len;
        new->last_pos = i;
        mk_list_add(&new->_head, list);
        count++;

        /* Update index for next loop */
        i++;

        /*
         * If the counter exceeded the maximum specified and there
         * are still remaining bytes, append those bytes in a new
         * and last entry.
         */
        if (count >= max_split && max_split > 0 && i < len) {
            new = flb_malloc(sizeof(struct flb_split_entry));
            if (!new) {
                flb_errno();
                flb_utils_split_free(list);
                return NULL;
            }
            new->value = mk_string_copy_substr(line, i, len);
            new->len   = len - i;
            mk_list_add(&new->_head, list);
            break;
        }
    }

    return list;
}

void flb_utils_split_free(struct mk_list *list)
{
    struct mk_list *tmp;
    struct mk_list *head;
    struct flb_split_entry *entry;

    mk_list_foreach_safe(head, tmp, list) {
        entry = mk_list_entry(head, struct flb_split_entry, _head);
        mk_list_del(&entry->_head);
        flb_free(entry->value);
        flb_free(entry);
    }

    flb_free(list);
}

/* When a timer expires, it needs some handling */
int flb_utils_timer_consume(flb_pipefd_t fd)
{
    int ret;
    uint64_t val;

    ret = flb_pipe_r(fd, &val, sizeof(val));
    if (ret == -1) {
        flb_errno();
        return -1;
    }

#ifdef __linux__
    /* A timer on linux must return an unisgned 64 bit number */
    if (ret == 0) {
        return -1;
    }
#endif

    return 0;
}

int flb_utils_pipe_byte_consume(flb_pipefd_t fd)
{
    int ret;
    uint64_t val;

    ret = flb_pipe_r(fd, &val, sizeof(val));
    if (ret == -1) {
        flb_errno();
        return -1;
    }

    return 0;
}

int64_t flb_utils_size_to_bytes(const char *size)
{
    int i;
    int len;
    int plen = 0;
    int64_t val;
    char c;
    char tmp[3] = {0};
    int64_t KB = 1000;
    int64_t MB = 1000 * KB;
    int64_t GB = 1000 * MB;

    if (!size) {
        return -1;
    }

    len = strlen(size);
    val = atoll(size);

    if (len == 0) {
        return -1;
    }

    for (i = len - 1; i > 0; i--) {
        if (isdigit(size[i])) {
            break;
        }
        else {
            plen++;
        }
    }

    if (plen == 0) {
        return val;
    }
    else if (plen > 2) {
        return -1;
    }

    for (i = 0; i < plen; i++) {
        c = size[(len - plen) + i];
        tmp[i] = toupper(c);
    }

    if (plen == 2) {
        if (tmp[1] != 'B') {
            return -1;
        }
    }

    if (tmp[0] == 'K') {
        return (val * KB);
    }
    else if (tmp[0] == 'M') {
        return (val * MB);
    }
    else if (tmp[0] == 'G') {
        return (val * GB);
    }
    else {
        return -1;
    }

    return val;
}

int flb_utils_time_to_seconds(const char *time)
{
    int len;
    size_t val;

    len = strlen(time);
    val = atoi(time);

    /* String time to seconds */
    if (time[len - 1] == 'D' || time[len - 1] == 'd') {
        val *= 86400;
    }
    if (time[len - 1] == 'H' || time[len - 1] == 'h') {
        val *= 3600;
    }
    else if (time[len - 1] == 'M' || time[len - 1] == 'm') {
        val *= 60;
    }

    return val;
}

int flb_utils_bool(const char *val)
{
    if (strcasecmp(val, ""true"") == 0 ||
        strcasecmp(val, ""on"") == 0 ||
        strcasecmp(val, ""yes"") == 0) {
        return FLB_TRUE;
    }

    return FLB_FALSE;
}

/* Convert a 'string' time seconds.nanoseconds to int and long values */
int flb_utils_time_split(const char *time, int *sec, long *nsec)
{
    char *p;
    char *end;
    long val = 0;

    errno = 0;
    val = strtol(time, &end, 10);
    if ((errno == ERANGE && (val == LONG_MAX || val == LONG_MIN))
        || (errno != 0 && val == 0)) {
        flb_errno();
        return -1;
    }
    if (end == time) {
        return -1;
    }
    *sec = (int) val;

    /* Try to find subseconds */
    *nsec = 0;
    p = strchr(time, '.');
    if (p) {
        p += 1;
        val = strtol(p, &end, 10);
        if ((errno == ERANGE && (val == LONG_MAX || val == LONG_MIN))
            || (errno != 0 && val == 0)) {
            flb_errno();
            return -1;
        }
        if (end == p) {
            return -1;
        }
        *nsec = val;
    }

    return 0;
}

void flb_utils_bytes_to_human_readable_size(size_t bytes,
                                            char *out_buf, size_t size)
{
    unsigned long i;
    unsigned long u = 1024;
    static const char *__units[] = {
        ""b"", ""K"", ""M"", ""G"",
        ""T"", ""P"", ""E"", ""Z"", ""Y"", NULL
    };

    for (i = 0; __units[i] != NULL; i++) {
        if ((bytes / u) == 0) {
            break;
        }
        u *= 1024;
    }
    if (!i) {
        snprintf(out_buf, size, ""%lu%s"", (long unsigned int) bytes, __units[0]);
    }
    else {
        float fsize = (float) ((double) bytes / (u / 1024));
        snprintf(out_buf, size, ""%.1f%s"", fsize, __units[i]);
    }
}


static inline void encoded_to_buf(char *out, const char *in, int len)
{
    int i;
    char *p = out;

    for (i = 0; i < len; i++) {
        *p++ = in[i];
    }
}

/*
 * Write string pointed by 'str' to the destination buffer 'buf'. It's make sure
 * to escape sepecial characters and convert utf-8 byte characters to string
 * representation.
 */
int flb_utils_write_str(char *buf, int *off, size_t size,
                        const char *str, size_t str_len)
{
    int i;
    int b;
    int ret;
    int written = 0;
    int required;
    int len;
    int hex_bytes;
    uint32_t codepoint;
    uint32_t state = 0;
    char tmp[16];
    size_t available;
    uint32_t c;
    char *p;
    uint8_t *s;

    available = (size - *off);
    required = str_len;
    if (available <= required) {
        return FLB_FALSE;
    }

    p = buf + *off;
    for (i = 0; i < str_len; i++) {
        if ((available - written) < 2) {
            return FLB_FALSE;
        }

        c = (uint32_t) str[i];
        if (c == '\""') {
            *p++ = '\\';
            *p++ = '\""';
        }
        else if (c == '\\') {
            *p++ = '\\';
            *p++ = '\\';
        }
        else if (c == '\n') {
            *p++ = '\\';
            *p++ = 'n';
        }
        else if (c == '\r') {
            *p++ = '\\';
            *p++ = 'r';
        }
        else if (c == '\t') {
            *p++ = '\\';
            *p++ = 't';
        }
        else if (c == '\b') {
            *p++ = '\\';
            *p++ = 'b';
        }
        else if (c == '\f') {
            *p++ = '\\';
            *p++ = 'f';
        }
        else if (c < 32 || c == 0x7f) {
            if ((available - written) < 6) {
                return FLB_FALSE;
            }
            len = snprintf(tmp, sizeof(tmp) - 1, ""\\u%.4hhx"", (unsigned char) c);
            encoded_to_buf(p, tmp, len);
            p += len;
            if (available - written < 6) {
            if (i + hex_bytes > str_len) {
                break; /* skip truncated UTF-8 */
            }
        }
        else if (c >= 0x80 && c <= 0xFFFF) {
            hex_bytes = flb_utf8_len(str + i);
            if ((available - written) < 6) {
                return FLB_FALSE;
            }

            state = FLB_UTF8_ACCEPT;
            codepoint = 0;
            for (b = 0; b < hex_bytes; b++) {
                s = (unsigned char *) str + i + b;
                ret = flb_utf8_decode(&state, &codepoint, *s);
                if (ret == 0) {
                    break;
                }
            }

            if (state != FLB_UTF8_ACCEPT) {
                /* Invalid UTF-8 hex, just skip utf-8 bytes */
                break;
            }
            else {
                len = snprintf(tmp, sizeof(tmp) - 1, ""\\u%.4x"", codepoint);
                encoded_to_buf(p, tmp, len);
                p += len;
            }
            i += (hex_bytes - 1);
        }
        else if (c > 0xFFFF) {
            hex_bytes = flb_utf8_len(str + i);
            if ((available - written) < (4 + hex_bytes)) {
                return FLB_FALSE;
            }

            state = FLB_UTF8_ACCEPT;
            codepoint = 0;
            for (b = 0; b < hex_bytes; b++) {
                s = (unsigned char *) str + i + b;
                ret = flb_utf8_decode(&state, &codepoint, *s);
                if (ret == 0) {
                    break;
                }
            }

            if (state != FLB_UTF8_ACCEPT) {
                /* Invalid UTF-8 hex, just skip utf-8 bytes */
                flb_warn(""[pack] invalid UTF-8 bytes, skipping"");
                break;
            }
            else {
                len = snprintf(tmp, sizeof(tmp) - 1, ""\\u%04x"", codepoint);
                encoded_to_buf(p, tmp, len);
                p += len;
            }
            i += (hex_bytes - 1);
        }
        else {
            *p++ = c;
        }
        written = (p - (buf + *off));
    }

    *off += written;
    return FLB_TRUE;
}


int flb_utils_write_str_buf(const char *str, size_t str_len, char **out, size_t *out_size)
{
    int ret;
    int off;
    char *tmp;
    char *buf;
    size_t s;

    s = str_len + 1;
    buf = flb_malloc(s);
    if (!buf) {
        flb_errno();
        return -1;
    }

    while (1) {
        off = 0;
        ret = flb_utils_write_str(buf, &off, s, str, str_len);
        if (ret == FLB_FALSE) {
            s += 256;
            tmp = flb_realloc(buf, s);
            if (!tmp) {
                flb_errno();
                flb_free(buf);
                return -1;
            }
            buf = tmp;
        }
        else {
            /* done */
            break;
        }
    }

    *out = buf;
    *out_size = off;
    return 0;
}

int flb_utils_url_split(const char *in_url, char **out_protocol,
                        char **out_host, char **out_port, char **out_uri)
{
    char *protocol = NULL;
    char *host = NULL;
    char *port = NULL;
    char *uri = NULL;
    char *p;
    char *tmp;
    char *sep;

    /* Protocol */
    p = strstr(in_url, ""://"");
    if (!p) {
        return -1;
    }
    if (p == in_url) {
        return -1;
    }

    protocol = mk_string_copy_substr(in_url, 0, p - in_url);
    if (!protocol) {
        flb_errno();
        return -1;
    }

    /* Advance position after protocol */
    p += 3;

    /* Check for first '/' */
    sep = strchr(p, '/');
    tmp = strchr(p, ':');

    /* Validate port separator is found before the first slash */
    if (sep && tmp) {
        if (tmp > sep) {
            tmp = NULL;
        }
    }

    if (tmp) {
        host = mk_string_copy_substr(p, 0, tmp - p);
        if (!host) {
            flb_errno();
            goto error;
        }
        p = tmp + 1;

        /* Look for an optional URI */
        tmp = strchr(p, '/');
        if (tmp) {
            port = mk_string_copy_substr(p, 0, tmp - p);
            uri = flb_strdup(tmp);
        }
        else {
            port = flb_strdup(p);
            uri = flb_strdup(""/"");
        }
    }
    else {
        tmp = strchr(p, '/');
        if (tmp) {
            host = mk_string_copy_substr(p, 0, tmp - p);
            uri = flb_strdup(tmp);
        }
        else {
            host = flb_strdup(p);
            uri = flb_strdup(""/"");
        }
    }

    *out_protocol = protocol;
    *out_host = host;
    *out_port = port;
    *out_uri = uri;

    return 0;

 error:
    if (protocol) {
        flb_free(protocol);
    }

    return -1;
}
","Perhaps `return FLB_FALSE;`? (Oh, we're trying to continue conversion.)","The suggested code changes appear to be primarily updates to the `flb_utils` module within the Fluent Bit project, which handles various utility functions such as error handling, configuration parsing, and string manipulation. Here's a concise review of the changes:

### Key Changes:

1. **Error Handling Functions**:
   - Added `flb_utils_error_c` for custom error messages.
   - Enhanced `flb_utils_error` to handle more specific error codes and provide more detailed error messages.

2. **Daemon Mode Functionality**:
   - Implemented `flb_utils_set_daemon` to run the process in the background, setting up necessary configurations like file masks, sessions, and changing directories.

3. **Configuration Printing**:
   - Added `flb_utils_print_setup` to print the configuration details, including inputs, collectors, and other relevant settings.

4. **String Splitting**:
   - Updated `flb_utils_split` to handle splitting strings based on a specified separator and limit the number of splits.
   - Added `flb_utils_split_free` to free the allocated memory for the split entries.

5. **Pipe Operations**:
   - Implemented `flb_utils_timer_consume` and `flb_utils_pipe_byte_consume` to handle pipe operations for timers and byte consumption.

6. **Size Conversion**:
   - Added `flb_utils_size_to_bytes` to convert human-readable sizes (e.g., KB, MB, GB) to bytes.

7. **Time Conversion**:
   - Implemented `flb_utils_time_to_seconds` to convert time strings (e.g., ""1h"", ""2d"") to seconds.

8. **Boolean Conversion**:
   - Added `flb_utils_bool` to convert string representations of boolean values to their corresponding integer values.

9. **Time Splitting**:
   - Updated `flb_utils_time_split` to parse time strings into seconds and nanoseconds.

10. **Bytes to Human Readable Size**:
    - Implemented `flb_utils_bytes_to_human_readable_size` to convert byte sizes to human-readable formats (e.g., KB, MB).

11. **String Encoding and Writing**:
    - Added `encoded_to_buf` for encoding special characters.
    - Updated `flb_utils_write_str` to handle escaping and converting UTF-8 bytes to string representations.
    - Implemented `flb_utils_write_str_buf` to dynamically allocate memory for the encoded string.

12. **URL Parsing**:
    - Added"
50,"@@ -35,7 +35,9 @@
 #include <pthread.h>
 #include <pwd.h>
 #include <signal.h>
+#ifndef __ANDROID__
 #include <spawn.h>
+#endif
 #include <stdio.h>
 #include <unistd.h>
 #include <sys/resource.h>","/*
 * Copyright (c) 2014-2016 DeNA Co., Ltd., Kazuho Oku, Tatsuhiko Kubo,
 *                         Domingo Alvarez Duarte, Nick Desaulniers,
 *                         Jeff Marrison, Shota Fukumori, Fastly, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the ""Software""), to
 * deal in the Software without restriction, including without limitation the
 * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
 * sell copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 * IN THE SOFTWARE.
 */
#include <arpa/inet.h>
#include <assert.h>
#include <errno.h>
#include <fcntl.h>
#include <getopt.h>
#include <inttypes.h>
#include <limits.h>
#include <netdb.h>
#include <netinet/in.h>
#include <netinet/tcp.h>
#include <poll.h>
#ifndef __ANDROID__
#endif
#include <pthread.h>
#include <pwd.h>
#include <signal.h>
#include <spawn.h>
#include <stdio.h>
#include <unistd.h>
#include <sys/resource.h>
#include <sys/stat.h>
#include <sys/socket.h>
#include <sys/time.h>
#include <sys/types.h>
#include <sys/un.h>
#include <sys/wait.h>
#include <openssl/crypto.h>
#include <openssl/err.h>
#include <openssl/ssl.h>
#ifdef LIBC_HAS_BACKTRACE
#include <execinfo.h>
#endif
#if H2O_USE_PICOTLS
#include ""picotls.h""
#include ""picotls/minicrypto.h""
#include ""picotls/openssl.h""
#endif
#include ""cloexec.h""
#include ""yoml-parser.h""
#include ""neverbleed.h""
#include ""h2o.h""
#include ""h2o/configurator.h""
#include ""h2o/http1.h""
#include ""h2o/http2.h""
#include ""h2o/serverutil.h""
#if H2O_USE_MRUBY
#include ""h2o/mruby_.h""
#endif
#include ""standalone.h""

#ifdef TCP_FASTOPEN
#define H2O_DEFAULT_LENGTH_TCP_FASTOPEN_QUEUE 4096
#else
#define H2O_DEFAULT_LENGTH_TCP_FASTOPEN_QUEUE 0
#endif

#define H2O_DEFAULT_NUM_NAME_RESOLUTION_THREADS 32

#define H2O_DEFAULT_OCSP_UPDATER_MAX_THREADS 10

#if defined(OPENSSL_NO_OCSP) && !H2O_USE_PICOTLS
#define H2O_USE_OCSP 0
#else
#define H2O_USE_OCSP 1
#endif

struct listener_ssl_config_t {
    H2O_VECTOR(h2o_iovec_t) hostnames;
    char *certificate_file;
    SSL_CTX *ctx;
#if H2O_USE_OCSP
    struct {
        uint64_t interval;
        unsigned max_failures;
        char *cmd;
        pthread_t updater_tid; /* should be valid when and only when interval != 0 */
        struct {
            pthread_mutex_t mutex;
            h2o_buffer_t *data;
        } response;
    } ocsp_stapling;
#endif
};

struct listener_config_t {
    int fd;
    struct sockaddr_storage addr;
    socklen_t addrlen;
    h2o_hostconf_t **hosts;
    H2O_VECTOR(struct listener_ssl_config_t *) ssl;
    int proxy_protocol;
};

struct listener_ctx_t {
    h2o_accept_ctx_t accept_ctx;
    h2o_socket_t *sock;
};

typedef struct st_resolve_tag_node_cache_entry_t {
    h2o_iovec_t filename;
    yoml_t *node;
} resolve_tag_node_cache_entry_t;

typedef struct st_resolve_tag_arg_t {
    H2O_VECTOR(resolve_tag_node_cache_entry_t) node_cache;
} resolve_tag_arg_t;

typedef enum en_run_mode_t {
    RUN_MODE_WORKER = 0,
    RUN_MODE_MASTER,
    RUN_MODE_DAEMON,
    RUN_MODE_TEST,
} run_mode_t;

static struct {
    h2o_globalconf_t globalconf;
    run_mode_t run_mode;
    struct {
        int *fds;
        char *bound_fd_map; /* has `num_fds` elements, set to 1 if fd[index] was bound to one of the listeners */
        size_t num_fds;
        char *env_var;
    } server_starter;
    struct listener_config_t **listeners;
    size_t num_listeners;
    char *pid_file;
    char *error_log;
    int max_connections;
    size_t num_threads;
    int tfo_queues;
    time_t launch_time;
    struct {
        pthread_t tid;
        h2o_context_t ctx;
        h2o_multithread_receiver_t server_notifications;
        h2o_multithread_receiver_t memcached;
    } * threads;
    volatile sig_atomic_t shutdown_requested;
    h2o_barrier_t startup_sync_barrier;
    struct {
        /* unused buffers exist to avoid false sharing of the cache line */
        char _unused1_avoir_false_sharing[32];
        int _num_connections; /* number of currently handled incoming connections, should use atomic functions to update the value
                                 */
        char _unused2_avoir_false_sharing[32];
        unsigned long
            _num_sessions; /* total number of opened incoming connections, should use atomic functions to update the value */
        char _unused3_avoir_false_sharing[32];
    } state;
    char *crash_handler;
    int crash_handler_wait_pipe_close;
} conf = {
    {NULL},                                 /* globalconf */
    RUN_MODE_WORKER,                        /* dry-run */
    {NULL},                                 /* server_starter */
    NULL,                                   /* listeners */
    0,                                      /* num_listeners */
    NULL,                                   /* pid_file */
    NULL,                                   /* error_log */
    1024,                                   /* max_connections */
    0,                                      /* initialized in main() */
    0,                                      /* initialized in main() */
    0,                                      /* initialized in main() */
    NULL,                                   /* thread_ids */
    0,                                      /* shutdown_requested */
    H2O_BARRIER_INITIALIZER(SIZE_MAX),      /* startup_sync_barrier */
    {{0}},                                  /* state */
    ""share/h2o/annotate-backtrace-symbols"", /* crash_handler */
    0,                                      /* crash_handler_wait_pipe_close */
};

static neverbleed_t *neverbleed = NULL;

static void set_cloexec(int fd)
{
    if (fcntl(fd, F_SETFD, FD_CLOEXEC) == -1) {
        perror(""failed to set FD_CLOEXEC"");
        abort();
    }
}

static int on_openssl_print_errors(const char *str, size_t len, void *fp)
{
    fwrite(str, 1, len, fp);
    return (int)len;
}

static void setup_ecc_key(SSL_CTX *ssl_ctx)
{
#ifdef SSL_CTX_set_ecdh_auto
    SSL_CTX_set_ecdh_auto(ssl_ctx, 1);
#else
    int nid = NID_X9_62_prime256v1;
    EC_KEY *key = EC_KEY_new_by_curve_name(nid);
    if (key == NULL) {
        fprintf(stderr, ""Failed to create curve \""%s\""\n"", OBJ_nid2sn(nid));
        return;
    }
    SSL_CTX_set_tmp_ecdh(ssl_ctx, key);
    EC_KEY_free(key);
#endif
}

static struct listener_ssl_config_t *resolve_sni(struct listener_config_t *listener, const char *name, size_t name_len)
{
    size_t i, j;

    for (i = 0; i != listener->ssl.size; ++i) {
        struct listener_ssl_config_t *ssl_config = listener->ssl.entries[i];
        for (j = 0; j != ssl_config->hostnames.size; ++j) {
            if (ssl_config->hostnames.entries[j].base[0] == '*') {
                /* matching against ""*.foo.bar"" */
                size_t cmplen = ssl_config->hostnames.entries[j].len - 1;
                if (!(cmplen < name_len && h2o_lcstris(name + name_len - cmplen, cmplen, ssl_config->hostnames.entries[j].base + 1,
                                                       ssl_config->hostnames.entries[j].len - 1)))
                    continue;
            } else {
                if (!h2o_lcstris(name, name_len, ssl_config->hostnames.entries[j].base, ssl_config->hostnames.entries[j].len))
                    continue;
            }
            /* found */
            return listener->ssl.entries[i];
        }
    }
    return listener->ssl.entries[0];
}

static int on_sni_callback(SSL *ssl, int *ad, void *arg)
{
    struct listener_config_t *listener = arg;
    const char *server_name = SSL_get_servername(ssl, TLSEXT_NAMETYPE_host_name);

    if (server_name != NULL) {
        struct listener_ssl_config_t *resolved = resolve_sni(listener, server_name, strlen(server_name));
        if (resolved->ctx != SSL_get_SSL_CTX(ssl))
            SSL_set_SSL_CTX(ssl, resolved->ctx);
    }

    return SSL_TLSEXT_ERR_OK;
}

#if H2O_USE_PICOTLS
struct st_on_client_hello_ptls_t {
    ptls_on_client_hello_t super;
    struct listener_config_t *listener;
};

static int on_client_hello_ptls(ptls_on_client_hello_t *_self, ptls_t *tls, ptls_iovec_t server_name,
                                const ptls_iovec_t *negotiated_protocols, size_t num_negotiated_protocols,
                                const uint16_t *signature_algorithms, size_t num_signature_algorithms)
{
    struct st_on_client_hello_ptls_t *self = (struct st_on_client_hello_ptls_t *)_self;
    int ret = 0;

    /* handle SNI */
    if (server_name.base != NULL) {
        struct listener_ssl_config_t *resolved = resolve_sni(self->listener, (const char *)server_name.base, server_name.len);
        ptls_context_t *newctx = h2o_socket_ssl_get_picotls_context(resolved->ctx);
        ptls_set_context(tls, newctx);
        ptls_set_server_name(tls, (const char *)server_name.base, server_name.len);
    }

    /* handle ALPN */
    if (num_negotiated_protocols != 0) {
        const h2o_iovec_t *server_pref;
        for (server_pref = h2o_alpn_protocols; server_pref->len != 0; ++server_pref) {
            size_t i;
            for (i = 0; i != num_negotiated_protocols; ++i)
                if (h2o_memis(server_pref->base, server_pref->len, negotiated_protocols[i].base, negotiated_protocols[i].len))
                    goto ALPN_Found;
        }
        return PTLS_ALERT_NO_APPLICATION_PROTOCOL;
    ALPN_Found:
        if ((ret = ptls_set_negotiated_protocol(tls, server_pref->base, server_pref->len)) != 0)
            return ret;
    }

    return ret;
}
#endif

static void update_ocsp_stapling(struct listener_ssl_config_t *ssl_conf, h2o_buffer_t *resp)
{
    pthread_mutex_lock(&ssl_conf->ocsp_stapling.response.mutex);
    if (ssl_conf->ocsp_stapling.response.data != NULL)
        h2o_buffer_dispose(&ssl_conf->ocsp_stapling.response.data);
    ssl_conf->ocsp_stapling.response.data = resp;
    pthread_mutex_unlock(&ssl_conf->ocsp_stapling.response.mutex);
}

static int get_ocsp_response(const char *cert_fn, const char *cmd, h2o_buffer_t **resp)
{
    char *cmd_fullpath = h2o_configurator_get_cmd_path(cmd), *argv[] = {cmd_fullpath, (char *)cert_fn, NULL};
    int child_status, ret;

    if (h2o_read_command(cmd_fullpath, argv, resp, &child_status) != 0) {
        fprintf(stderr, ""[OCSP Stapling] failed to execute %s:%s\n"", cmd, strerror(errno));
        switch (errno) {
        case EACCES:
        case ENOENT:
        case ENOEXEC:
            /* permanent errors */
            ret = EX_CONFIG;
            goto Exit;
        default:
            ret = EX_TEMPFAIL;
            goto Exit;
        }
    }

    if (!(WIFEXITED(child_status) && WEXITSTATUS(child_status) == 0))
        h2o_buffer_dispose(resp);
    if (!WIFEXITED(child_status)) {
        fprintf(stderr, ""[OCSP Stapling] command %s was killed by signal %d\n"", cmd_fullpath, WTERMSIG(child_status));
        ret = EX_TEMPFAIL;
        goto Exit;
    }
    ret = WEXITSTATUS(child_status);

Exit:
    free(cmd_fullpath);
    return ret;
}

static h2o_sem_t ocsp_updater_semaphore;

static void *ocsp_updater_thread(void *_ssl_conf)
{
    struct listener_ssl_config_t *ssl_conf = _ssl_conf;
    time_t next_at = 0, now;
    unsigned fail_cnt = 0;
    int status;
    h2o_buffer_t *resp;

    assert(ssl_conf->ocsp_stapling.interval != 0);

    while (1) {
        /* sleep until next_at */
        if ((now = time(NULL)) < next_at) {
            time_t sleep_secs = next_at - now;
            sleep(sleep_secs < UINT_MAX ? (unsigned)sleep_secs : UINT_MAX);
            continue;
        }
        /* fetch the response */
        h2o_sem_wait(&ocsp_updater_semaphore);
        status = get_ocsp_response(ssl_conf->certificate_file, ssl_conf->ocsp_stapling.cmd, &resp);
        h2o_sem_post(&ocsp_updater_semaphore);
        switch (status) {
        case 0: /* success */
            fail_cnt = 0;
            update_ocsp_stapling(ssl_conf, resp);
            fprintf(stderr, ""[OCSP Stapling] successfully updated the response for certificate file:%s\n"",
                    ssl_conf->certificate_file);
            break;
        case EX_TEMPFAIL: /* temporary failure */
            if (fail_cnt == ssl_conf->ocsp_stapling.max_failures) {
                fprintf(stderr,
                        ""[OCSP Stapling] OCSP stapling is temporary disabled due to repeated errors for certificate file:%s\n"",
                        ssl_conf->certificate_file);
                update_ocsp_stapling(ssl_conf, NULL);
            } else {
                fprintf(stderr, ""[OCSP Stapling] reusing old response due to a temporary error occurred while fetching OCSP ""
                                ""response for certificate file:%s\n"",
                        ssl_conf->certificate_file);
                ++fail_cnt;
            }
            break;
        default: /* permanent failure */
            update_ocsp_stapling(ssl_conf, NULL);
            fprintf(stderr, ""[OCSP Stapling] disabled for certificate file:%s\n"", ssl_conf->certificate_file);
            goto Exit;
        }
        /* update next_at */
        next_at = time(NULL) + ssl_conf->ocsp_stapling.interval;
    }

Exit:
    return NULL;
}

#ifndef OPENSSL_NO_OCSP

static int on_staple_ocsp_ossl(SSL *ssl, void *_ssl_conf)
{
    struct listener_ssl_config_t *ssl_conf = _ssl_conf;
    void *resp = NULL;
    size_t len = 0;

    /* fetch ocsp response */
    pthread_mutex_lock(&ssl_conf->ocsp_stapling.response.mutex);
    if (ssl_conf->ocsp_stapling.response.data != NULL) {
        resp = CRYPTO_malloc((int)ssl_conf->ocsp_stapling.response.data->size, __FILE__, __LINE__);
        if (resp != NULL) {
            len = ssl_conf->ocsp_stapling.response.data->size;
            memcpy(resp, ssl_conf->ocsp_stapling.response.data->bytes, len);
        }
    }
    pthread_mutex_unlock(&ssl_conf->ocsp_stapling.response.mutex);

    if (resp != NULL) {
        SSL_set_tlsext_status_ocsp_resp(ssl, resp, len);
        return SSL_TLSEXT_ERR_OK;
    } else {
        return SSL_TLSEXT_ERR_NOACK;
    }
}

#endif

#if H2O_USE_PICOTLS

struct st_staple_ocsp_ptls_t {
    ptls_staple_ocsp_t super;
    struct listener_ssl_config_t *conf;
};

static int on_staple_ocsp_ptls(ptls_staple_ocsp_t *_self, ptls_t *tls, ptls_buffer_t *output, size_t cert_index)
{
    struct st_staple_ocsp_ptls_t *self = (struct st_staple_ocsp_ptls_t *)_self;
    int locked = 0, ret;

    if (cert_index != 0) {
        ret = PTLS_ERROR_LIBRARY;
        goto Exit;
    }

    pthread_mutex_lock(&self->conf->ocsp_stapling.response.mutex);
    locked = 1;

    if (self->conf->ocsp_stapling.response.data == NULL) {
        ret = PTLS_ERROR_LIBRARY;
        goto Exit;
    }
    ptls_buffer_pushv(output, self->conf->ocsp_stapling.response.data->bytes, self->conf->ocsp_stapling.response.data->size);
    ret = 0;

Exit:
    if (locked)
        pthread_mutex_unlock(&self->conf->ocsp_stapling.response.mutex);
    return ret;
}

static const char *listener_setup_ssl_picotls(struct listener_config_t *listener, struct listener_ssl_config_t *ssl_config,
                                              SSL_CTX *ssl_ctx)
{
    static const ptls_key_exchange_algorithm_t *key_exchanges[] = {&ptls_minicrypto_x25519, &ptls_openssl_secp256r1, NULL};
    struct st_fat_context_t {
        ptls_context_t ctx;
        struct st_on_client_hello_ptls_t ch;
        struct st_staple_ocsp_ptls_t so;
        ptls_openssl_sign_certificate_t sc;
    } *pctx = h2o_mem_alloc(sizeof(*pctx));
    EVP_PKEY *key;
    X509 *cert;
    STACK_OF(X509) * cert_chain;
    int ret;

    *pctx = (struct st_fat_context_t){{ptls_openssl_random_bytes,
                                       key_exchanges,
                                       ptls_openssl_cipher_suites,
                                       {NULL, 0},
                                       &pctx->ch.super,
                                       &pctx->so.super,
                                       &pctx->sc.super,
                                       NULL,
                                       0,
                                       8192,
                                       1},
                                      {{on_client_hello_ptls}, listener},
                                      {{on_staple_ocsp_ptls}, ssl_config}};

    { /* obtain key and cert (via fake connection for libressl compatibility) */
        SSL *fakeconn = SSL_new(ssl_ctx);
        assert(fakeconn != NULL);
        key = SSL_get_privatekey(fakeconn);
        assert(key != NULL);
        cert = SSL_get_certificate(fakeconn);
        assert(cert != NULL);
        SSL_free(fakeconn);
    }

    if (ptls_openssl_init_sign_certificate(&pctx->sc, key) != 0) {
        free(pctx);
        return ""failed to setup private key"";
    }

    SSL_CTX_get_extra_chain_certs(ssl_ctx, &cert_chain);
    ret = ptls_openssl_load_certificates(&pctx->ctx, cert, cert_chain);
    assert(ret == 0);

    h2o_socket_ssl_set_picotls_context(ssl_ctx, &pctx->ctx);

    return NULL;
}

#endif

static void listener_setup_ssl_add_host(struct listener_ssl_config_t *ssl_config, h2o_iovec_t host)
{
    const char *host_end = memchr(host.base, ':', host.len);
    if (host_end == NULL)
        host_end = host.base + host.len;

    h2o_vector_reserve(NULL, &ssl_config->hostnames, ssl_config->hostnames.size + 1);
    ssl_config->hostnames.entries[ssl_config->hostnames.size++] = h2o_iovec_init(host.base, host_end - host.base);
}

static int listener_setup_ssl(h2o_configurator_command_t *cmd, h2o_configurator_context_t *ctx, yoml_t *listen_node,
                              yoml_t *ssl_node, struct listener_config_t *listener, int listener_is_new)
{
    SSL_CTX *ssl_ctx = NULL;
    yoml_t *certificate_file = NULL, *key_file = NULL, *dh_file = NULL, *min_version = NULL, *max_version = NULL,
           *cipher_suite = NULL, *ocsp_update_cmd = NULL, *ocsp_update_interval_node = NULL, *ocsp_max_failures_node = NULL;
    long ssl_options = SSL_OP_ALL;
    uint64_t ocsp_update_interval = 4 * 60 * 60; /* defaults to 4 hours */
    unsigned ocsp_max_failures = 3;              /* defaults to 3; permit 3 failures before temporary disabling OCSP stapling */
    int use_neverbleed = 1, use_picotls = 1;     /* enabled by default */

    if (!listener_is_new) {
        if (listener->ssl.size != 0 && ssl_node == NULL) {
            h2o_configurator_errprintf(cmd, listen_node, ""cannot accept HTTP; already defined to accept HTTPS"");
            return -1;
        }
        if (listener->ssl.size == 0 && ssl_node != NULL) {
            h2o_configurator_errprintf(cmd, ssl_node, ""cannot accept HTTPS; already defined to accept HTTP"");
            return -1;
        }
    }

    if (ssl_node == NULL)
        return 0;
    if (ssl_node->type != YOML_TYPE_MAPPING) {
        h2o_configurator_errprintf(cmd, ssl_node, ""`ssl` is not a mapping"");
        return -1;
    }

    { /* parse */
        size_t i;
        for (i = 0; i != ssl_node->data.sequence.size; ++i) {
            yoml_t *key = ssl_node->data.mapping.elements[i].key, *value = ssl_node->data.mapping.elements[i].value;
            /* obtain the target command */
            if (key->type != YOML_TYPE_SCALAR) {
                h2o_configurator_errprintf(NULL, key, ""command must be a string"");
                return -1;
            }
#define FETCH_PROPERTY(n, p)                                                                                                       \
    if (strcmp(key->data.scalar, n) == 0) {                                                                                        \
        if (value->type != YOML_TYPE_SCALAR) {                                                                                     \
            h2o_configurator_errprintf(cmd, value, ""property of `"" n ""` must be a string"");                                        \
            return -1;                                                                                                             \
        }                                                                                                                          \
        p = value;                                                                                                                 \
        continue;                                                                                                                  \
    }
            FETCH_PROPERTY(""certificate-file"", certificate_file);
            FETCH_PROPERTY(""key-file"", key_file);
            FETCH_PROPERTY(""min-version"", min_version);
            FETCH_PROPERTY(""minimum-version"", min_version);
            FETCH_PROPERTY(""max-version"", max_version);
            FETCH_PROPERTY(""maximum-version"", max_version);
            FETCH_PROPERTY(""cipher-suite"", cipher_suite);
            FETCH_PROPERTY(""ocsp-update-cmd"", ocsp_update_cmd);
            FETCH_PROPERTY(""ocsp-update-interval"", ocsp_update_interval_node);
            FETCH_PROPERTY(""ocsp-max-failures"", ocsp_max_failures_node);
            FETCH_PROPERTY(""dh-file"", dh_file);
            if (strcmp(key->data.scalar, ""cipher-preference"") == 0) {
                if (value->type == YOML_TYPE_SCALAR && strcasecmp(value->data.scalar, ""client"") == 0) {
                    ssl_options &= ~SSL_OP_CIPHER_SERVER_PREFERENCE;
                } else if (value->type == YOML_TYPE_SCALAR && strcasecmp(value->data.scalar, ""server"") == 0) {
                    ssl_options |= SSL_OP_CIPHER_SERVER_PREFERENCE;
                } else {
                    h2o_configurator_errprintf(cmd, value, ""property of `cipher-preference` must be either of: `client`, `server`"");
                    return -1;
                }
                continue;
            }
            if (strcmp(key->data.scalar, ""neverbleed"") == 0) {
                if (value->type == YOML_TYPE_SCALAR && strcasecmp(value->data.scalar, ""ON"") == 0) {
                    /* no need to enable neverbleed for daemon / master */
                    use_neverbleed = 1;
                } else if (value->type == YOML_TYPE_SCALAR && strcasecmp(value->data.scalar, ""OFF"") == 0) {
                    use_neverbleed = 0;
                } else {
                    h2o_configurator_errprintf(cmd, value, ""property of `neverbleed` must be either of: `ON`, `OFF"");
                    return -1;
                }
                continue;
            }
            h2o_configurator_errprintf(cmd, key, ""unknown property: %s"", key->data.scalar);
            return -1;
#undef FETCH_PROPERTY
        }
        if (certificate_file == NULL) {
            h2o_configurator_errprintf(cmd, ssl_node, ""could not find mandatory property `certificate-file`"");
            return -1;
        }
        if (key_file == NULL) {
            h2o_configurator_errprintf(cmd, ssl_node, ""could not find mandatory property `key-file`"");
            return -1;
        }
        if (min_version != NULL) {
#define MAP(tok, op)                                                                                                               \
    if (strcasecmp(min_version->data.scalar, tok) == 0) {                                                                          \
        ssl_options |= (op);                                                                                                       \
        goto VersionFound;                                                                                                         \
    }
            MAP(""sslv2"", 0);
            MAP(""sslv3"", SSL_OP_NO_SSLv2);
            MAP(""tlsv1"", SSL_OP_NO_SSLv2 | SSL_OP_NO_SSLv3);
            MAP(""tlsv1.1"", SSL_OP_NO_SSLv2 | SSL_OP_NO_SSLv3 | SSL_OP_NO_TLSv1);
#ifdef SSL_OP_NO_TLSv1_1
            MAP(""tlsv1.2"", SSL_OP_NO_SSLv2 | SSL_OP_NO_SSLv3 | SSL_OP_NO_TLSv1 | SSL_OP_NO_TLSv1_1);
#endif
#ifdef SSL_OP_NO_TLSv1_2
            MAP(""tlsv1.3"", SSL_OP_NO_SSLv2 | SSL_OP_NO_SSLv3 | SSL_OP_NO_TLSv1 | SSL_OP_NO_TLSv1_1 | SSL_OP_NO_TLSv1_2);
#endif
#undef MAP
            h2o_configurator_errprintf(cmd, min_version, ""unknown protocol version: %s"", min_version->data.scalar);
        VersionFound:;
        } else {
            /* default is >= TLSv1 */
            ssl_options |= SSL_OP_NO_SSLv2 | SSL_OP_NO_SSLv3;
        }
        if (max_version != NULL) {
            if (strcasecmp(max_version->data.scalar, ""tlsv1.3"") < 0)
                use_picotls = 0;
        }
        if (ocsp_update_interval_node != NULL) {
            if (h2o_configurator_scanf(cmd, ocsp_update_interval_node, ""%"" PRIu64, &ocsp_update_interval) != 0)
                goto Error;
        }
        if (ocsp_max_failures_node != NULL) {
            if (h2o_configurator_scanf(cmd, ocsp_max_failures_node, ""%u"", &ocsp_max_failures) != 0)
                goto Error;
        }
    }

    /* add the host to the existing SSL config, if the certificate file is already registered */
    if (ctx->hostconf != NULL) {
        size_t i;
        for (i = 0; i != listener->ssl.size; ++i) {
            struct listener_ssl_config_t *ssl_config = listener->ssl.entries[i];
            if (strcmp(ssl_config->certificate_file, certificate_file->data.scalar) == 0) {
                listener_setup_ssl_add_host(ssl_config, ctx->hostconf->authority.hostport);
                return 0;
            }
        }
    }

/* disable tls compression to avoid ""CRIME"" attacks (see http://en.wikipedia.org/wiki/CRIME) */
#ifdef SSL_OP_NO_COMPRESSION
    ssl_options |= SSL_OP_NO_COMPRESSION;
#endif

    /* setup */
    ssl_ctx = SSL_CTX_new(SSLv23_server_method());
    SSL_CTX_set_options(ssl_ctx, ssl_options);

    setup_ecc_key(ssl_ctx);
    if (SSL_CTX_use_certificate_chain_file(ssl_ctx, certificate_file->data.scalar) != 1) {
        h2o_configurator_errprintf(cmd, certificate_file, ""failed to load certificate file:%s\n"", certificate_file->data.scalar);
        ERR_print_errors_cb(on_openssl_print_errors, stderr);
        goto Error;
    }
    if (use_neverbleed) {
        /* disable neverbleed in case the process is not going to serve requests */
        switch (conf.run_mode) {
        case RUN_MODE_DAEMON:
        case RUN_MODE_MASTER:
            use_neverbleed = 0;
            break;
        default:
            break;
        }
    }
    if (use_neverbleed) {
        char errbuf[NEVERBLEED_ERRBUF_SIZE];
        if (neverbleed == NULL) {
            neverbleed = h2o_mem_alloc(sizeof(*neverbleed));
            if (neverbleed_init(neverbleed, errbuf) != 0) {
                fprintf(stderr, ""%s\n"", errbuf);
                abort();
            }
        }
        if (neverbleed_load_private_key_file(neverbleed, ssl_ctx, key_file->data.scalar, errbuf) != 1) {
            h2o_configurator_errprintf(cmd, key_file, ""failed to load private key file:%s:%s\n"", key_file->data.scalar, errbuf);
            goto Error;
        }
    } else {
        if (SSL_CTX_use_PrivateKey_file(ssl_ctx, key_file->data.scalar, SSL_FILETYPE_PEM) != 1) {
            h2o_configurator_errprintf(cmd, key_file, ""failed to load private key file:%s\n"", key_file->data.scalar);
            ERR_print_errors_cb(on_openssl_print_errors, stderr);
            goto Error;
        }
    }
    if (cipher_suite != NULL && SSL_CTX_set_cipher_list(ssl_ctx, cipher_suite->data.scalar) != 1) {
        h2o_configurator_errprintf(cmd, cipher_suite, ""failed to setup SSL cipher suite\n"");
        ERR_print_errors_cb(on_openssl_print_errors, stderr);
        goto Error;
    }
    if (dh_file != NULL) {
        BIO *bio = BIO_new_file(dh_file->data.scalar, ""r"");
        if (bio == NULL) {
            h2o_configurator_errprintf(cmd, dh_file, ""failed to load dhparam file:%s\n"", dh_file->data.scalar);
            ERR_print_errors_cb(on_openssl_print_errors, stderr);
            goto Error;
        }
        DH *dh = PEM_read_bio_DHparams(bio, NULL, NULL, NULL);
        BIO_free(bio);
        if (dh == NULL) {
            h2o_configurator_errprintf(cmd, dh_file, ""failed to load dhparam file:%s\n"", dh_file->data.scalar);
            ERR_print_errors_cb(on_openssl_print_errors, stderr);
            goto Error;
        }
        SSL_CTX_set_tmp_dh(ssl_ctx, dh);
        SSL_CTX_set_options(ssl_ctx, SSL_OP_SINGLE_DH_USE);
        DH_free(dh);
    }

/* setup protocol negotiation methods */
#if H2O_USE_NPN
    h2o_ssl_register_npn_protocols(ssl_ctx, h2o_npn_protocols);
#endif
#if H2O_USE_ALPN
    h2o_ssl_register_alpn_protocols(ssl_ctx, h2o_alpn_protocols);
#endif

    /* set SNI callback to the first SSL context, when and only when it should be used */
    if (listener->ssl.size == 1) {
        SSL_CTX_set_tlsext_servername_callback(listener->ssl.entries[0]->ctx, on_sni_callback);
        SSL_CTX_set_tlsext_servername_arg(listener->ssl.entries[0]->ctx, listener);
    }

    /* create a new entry in the SSL context list */
    struct listener_ssl_config_t *ssl_config = h2o_mem_alloc(sizeof(*ssl_config));
    memset(ssl_config, 0, sizeof(*ssl_config));
    h2o_vector_reserve(NULL, &listener->ssl, listener->ssl.size + 1);
    listener->ssl.entries[listener->ssl.size++] = ssl_config;
    if (ctx->hostconf != NULL) {
        listener_setup_ssl_add_host(ssl_config, ctx->hostconf->authority.hostport);
    }
    ssl_config->ctx = ssl_ctx;
    ssl_config->certificate_file = h2o_strdup(NULL, certificate_file->data.scalar, SIZE_MAX).base;

#if !H2O_USE_OCSP
    if (ocsp_update_interval != 0)
        fprintf(stderr, ""[OCSP Stapling] disabled (not support by the SSL library)\n"");
#else
#ifndef OPENSSL_NO_OCSP
    SSL_CTX_set_tlsext_status_cb(ssl_ctx, on_staple_ocsp_ossl);
    SSL_CTX_set_tlsext_status_arg(ssl_ctx, ssl_config);
#endif
    pthread_mutex_init(&ssl_config->ocsp_stapling.response.mutex, NULL);
    ssl_config->ocsp_stapling.cmd =
        ocsp_update_cmd != NULL ? h2o_strdup(NULL, ocsp_update_cmd->data.scalar, SIZE_MAX).base : ""share/h2o/fetch-ocsp-response"";
    if (ocsp_update_interval != 0) {
        switch (conf.run_mode) {
        case RUN_MODE_WORKER:
            ssl_config->ocsp_stapling.interval =
                ocsp_update_interval; /* is also used as a flag for indicating if the updater thread was spawned */
            ssl_config->ocsp_stapling.max_failures = ocsp_max_failures;
            h2o_multithread_create_thread(&ssl_config->ocsp_stapling.updater_tid, NULL, ocsp_updater_thread, ssl_config);
            break;
        case RUN_MODE_MASTER:
        case RUN_MODE_DAEMON:
            /* nothing to do */
            break;
        case RUN_MODE_TEST: {
            h2o_buffer_t *respbuf;
            fprintf(stderr, ""[OCSP Stapling] testing for certificate file:%s\n"", certificate_file->data.scalar);
            switch (get_ocsp_response(certificate_file->data.scalar, ssl_config->ocsp_stapling.cmd, &respbuf)) {
            case 0:
                h2o_buffer_dispose(&respbuf);
                fprintf(stderr, ""[OCSP Stapling] stapling works for file:%s\n"", certificate_file->data.scalar);
                break;
            case EX_TEMPFAIL:
                h2o_configurator_errprintf(cmd, certificate_file, ""[OCSP Stapling] temporary failed for file:%s\n"",
                                           certificate_file->data.scalar);
                break;
            default:
                h2o_configurator_errprintf(cmd, certificate_file, ""[OCSP Stapling] does not work, will be disabled for file:%s\n"",
                                           certificate_file->data.scalar);
                break;
            }
        } break;
        }
    }
#endif

#if H2O_USE_PICOTLS
    if (use_picotls) {
        const char *errstr = listener_setup_ssl_picotls(listener, ssl_config, ssl_ctx);
        if (errstr != NULL)
            h2o_configurator_errprintf(cmd, ssl_node, ""%s; TLS 1.3 will be disabled\n"", errstr);
    }
#endif

    return 0;

Error:
    if (ssl_ctx != NULL)
        SSL_CTX_free(ssl_ctx);
    return -1;
}

static struct listener_config_t *find_listener(struct sockaddr *addr, socklen_t addrlen)
{
    size_t i;

    for (i = 0; i != conf.num_listeners; ++i) {
        struct listener_config_t *listener = conf.listeners[i];
        if (listener->addrlen == addrlen && h2o_socket_compare_address((void *)&listener->addr, addr) == 0)
            return listener;
    }

    return NULL;
}

static struct listener_config_t *add_listener(int fd, struct sockaddr *addr, socklen_t addrlen, int is_global, int proxy_protocol)
{
    struct listener_config_t *listener = h2o_mem_alloc(sizeof(*listener));

    memcpy(&listener->addr, addr, addrlen);
    listener->fd = fd;
    listener->addrlen = addrlen;
    if (is_global) {
        listener->hosts = NULL;
    } else {
        listener->hosts = h2o_mem_alloc(sizeof(listener->hosts[0]));
        listener->hosts[0] = NULL;
    }
    memset(&listener->ssl, 0, sizeof(listener->ssl));
    listener->proxy_protocol = proxy_protocol;

    conf.listeners = h2o_mem_realloc(conf.listeners, sizeof(*conf.listeners) * (conf.num_listeners + 1));
    conf.listeners[conf.num_listeners++] = listener;

    return listener;
}

static int find_listener_from_server_starter(struct sockaddr *addr)
{
    size_t i;

    assert(conf.server_starter.fds != NULL);
    assert(conf.server_starter.num_fds != 0);

    for (i = 0; i != conf.server_starter.num_fds; ++i) {
        struct sockaddr_storage sa;
        socklen_t salen = sizeof(sa);
        if (getsockname(conf.server_starter.fds[i], (void *)&sa, &salen) != 0) {
            fprintf(stderr, ""could not get the socket address of fd %d given as $SERVER_STARTER_PORT\n"",
                    conf.server_starter.fds[i]);
            exit(EX_CONFIG);
        }
        if (h2o_socket_compare_address((void *)&sa, addr) == 0)
            goto Found;
    }
    /* not found */
    return -1;

Found:
    conf.server_starter.bound_fd_map[i] = 1;
    return conf.server_starter.fds[i];
}

static int open_unix_listener(h2o_configurator_command_t *cmd, yoml_t *node, struct sockaddr_un *sa)
{
    struct stat st;
    int fd = -1;
    struct passwd *owner = NULL, pwbuf;
    char pwbuf_buf[65536];
    unsigned mode = UINT_MAX;
    yoml_t *t;

    /* obtain owner and permission */
    if ((t = yoml_get(node, ""owner"")) != NULL) {
        if (t->type != YOML_TYPE_SCALAR) {
            h2o_configurator_errprintf(cmd, t, ""`owner` is not a scalar"");
            goto ErrorExit;
        }
        if (getpwnam_r(t->data.scalar, &pwbuf, pwbuf_buf, sizeof(pwbuf_buf), &owner) != 0 || owner == NULL) {
            h2o_configurator_errprintf(cmd, t, ""failed to obtain uid of user:%s: %s"", t->data.scalar, strerror(errno));
            goto ErrorExit;
        }
    }
    if ((t = yoml_get(node, ""permission"")) != NULL) {
        if (t->type != YOML_TYPE_SCALAR || sscanf(t->data.scalar, ""%o"", &mode) != 1) {
            h2o_configurator_errprintf(cmd, t, ""`permission` must be an octal number"");
            goto ErrorExit;
        }
    }

    /* remove existing socket file as suggested in #45 */
    if (lstat(sa->sun_path, &st) == 0) {
        if (S_ISSOCK(st.st_mode)) {
            unlink(sa->sun_path);
        } else {
            h2o_configurator_errprintf(cmd, node, ""path:%s already exists and is not an unix socket."", sa->sun_path);
            goto ErrorExit;
        }
    }

    /* add new listener */
    if ((fd = socket(AF_UNIX, SOCK_STREAM, 0)) == -1 || bind(fd, (void *)sa, sizeof(*sa)) != 0 || listen(fd, H2O_SOMAXCONN) != 0) {
        h2o_configurator_errprintf(NULL, node, ""failed to listen to socket:%s: %s"", sa->sun_path, strerror(errno));
        goto ErrorExit;
    }
    set_cloexec(fd);

    /* set file owner and permission */
    if (owner != NULL && chown(sa->sun_path, owner->pw_uid, owner->pw_gid) != 0) {
        h2o_configurator_errprintf(NULL, node, ""failed to chown socket:%s to %s: %s"", sa->sun_path, owner->pw_name,
                                   strerror(errno));
        goto ErrorExit;
    }
    if (mode != UINT_MAX && chmod(sa->sun_path, mode) != 0) {
        h2o_configurator_errprintf(NULL, node, ""failed to chmod socket:%s to %o: %s"", sa->sun_path, mode, strerror(errno));
        goto ErrorExit;
    }

    return fd;

ErrorExit:
    if (fd != -1)
        close(fd);
    return -1;
}

static int open_tcp_listener(h2o_configurator_command_t *cmd, yoml_t *node, const char *hostname, const char *servname, int domain,
                             int type, int protocol, struct sockaddr *addr, socklen_t addrlen)
{
    int fd;

    if ((fd = socket(domain, type, protocol)) == -1)
        goto Error;
    set_cloexec(fd);
    { /* set reuseaddr */
        int flag = 1;
        if (setsockopt(fd, SOL_SOCKET, SO_REUSEADDR, &flag, sizeof(flag)) != 0)
            goto Error;
    }
#ifdef TCP_DEFER_ACCEPT
    { /* set TCP_DEFER_ACCEPT */
        int flag = 1;
        if (setsockopt(fd, IPPROTO_TCP, TCP_DEFER_ACCEPT, &flag, sizeof(flag)) != 0)
            goto Error;
    }
#endif
#ifdef IPV6_V6ONLY
    /* set IPv6only */
    if (domain == AF_INET6) {
        int flag = 1;
        if (setsockopt(fd, IPPROTO_IPV6, IPV6_V6ONLY, &flag, sizeof(flag)) != 0)
            goto Error;
    }
#endif
    if (bind(fd, addr, addrlen) != 0)
        goto Error;
    if (listen(fd, H2O_SOMAXCONN) != 0)
        goto Error;

    /* set TCP_FASTOPEN; when tfo_queues is zero TFO is always disabled */
    if (conf.tfo_queues > 0) {
#ifdef TCP_FASTOPEN
        int tfo_queues;
#ifdef __APPLE__
        /* In OS X, the option value for TCP_FASTOPEN must be 1 if is's enabled */
        tfo_queues = 1;
#else
        tfo_queues = conf.tfo_queues;
#endif
        if (setsockopt(fd, IPPROTO_TCP, TCP_FASTOPEN, (const void *)&tfo_queues, sizeof(tfo_queues)) != 0)
            fprintf(stderr, ""[warning] failed to set TCP_FASTOPEN:%s\n"", strerror(errno));
#else
        assert(!""conf.tfo_queues not zero on platform without TCP_FASTOPEN"");
#endif
    }

    return fd;

Error:
    if (fd != -1)
        close(fd);
    h2o_configurator_errprintf(NULL, node, ""failed to listen to port %s:%s: %s"", hostname != NULL ? hostname : ""ANY"", servname,
                               strerror(errno));
    return -1;
}

static int on_config_listen(h2o_configurator_command_t *cmd, h2o_configurator_context_t *ctx, yoml_t *node)
{
    const char *hostname = NULL, *servname = NULL, *type = ""tcp"";
    yoml_t *ssl_node = NULL;
    int proxy_protocol = 0;

    /* fetch servname (and hostname) */
    switch (node->type) {
    case YOML_TYPE_SCALAR:
        servname = node->data.scalar;
        break;
    case YOML_TYPE_MAPPING: {
        yoml_t *t;
        if ((t = yoml_get(node, ""host"")) != NULL) {
            if (t->type != YOML_TYPE_SCALAR) {
                h2o_configurator_errprintf(cmd, t, ""`host` is not a string"");
                return -1;
            }
            hostname = t->data.scalar;
        }
        if ((t = yoml_get(node, ""port"")) == NULL) {
            h2o_configurator_errprintf(cmd, node, ""cannot find mandatory property `port`"");
            return -1;
        }",The same goes here as well.,"The suggested code change appears to be related to configuring and managing SSL/TLS settings for a web server using the H2O framework. Here's a concise review of the changes:

### Key Changes and Additions:

1. **Listener Configuration**:
   - Added a function `listener_setup_ssl` to configure SSL/TLS settings for a listener.
   - Implemented logic to handle SNI (Server Name Indication) and OCSP stapling.

2. **OCSP Stapling**:
   - Added support for OCSP stapling with both OpenSSL and Picotls.
   - Implemented an updater thread to periodically fetch OCSP responses.

3. **SSL Context Setup**:
   - Used OpenSSL for SSL/TLS context setup.
   - Enabled ECC keys for better security.
   - Supported different SSL versions and cipher suites.

4. **Listener Management**:
   - Added functions to manage listeners, including adding new listeners and finding existing ones.
   - Used Unix sockets for local communication.

5. **Configuration Parsing**:
   - Enhanced the configuration parser to handle SSL-related properties such as certificate files, key files, and OCSP settings.

### Potential Improvements:

1. **Error Handling**:
   - Ensure that all error paths in SSL configuration are properly handled and logged.
   - Consider adding more detailed error messages for specific issues.

2. **Performance**:
   - Optimize the OCSP updater thread to reduce unnecessary wake-ups.
   - Use more efficient data structures for caching OCSP responses.

3. **Security**:
   - Validate and sanitize input parameters to prevent potential security vulnerabilities.
   - Consider using stronger cryptographic algorithms and protocols.

4. **Documentation**:
   - Provide clear documentation for the new features and configurations added.
   - Include examples in the README or documentation.

5. **Testing**:
   - Write comprehensive tests to ensure the correctness of the OCSP stapling and SSL configuration logic.
   - Test the server under various network conditions and configurations.

Overall, the changes introduce robust SSL/TLS support and OCSP stapling functionality, enhancing the security and reliability of the H2O server."
51,"@@ -321,7 +321,7 @@ func (r *LocalRunner) Setup(dp *DebugParams) (err error) {
 				ledger, states, err = makeAppLedger(
 					balances, r.txnGroup, dp.GroupIndex,
 					r.proto, dp.Round, dp.LatestTimestamp, appIdx,
-					dp.Painless,
+					dp.Painless, dp.IndexerURL, dp.IndexerToken,
 				)
 				if err != nil {
 					return","// Copyright (C) 2019-2020 Algorand, Inc.
// This file is part of go-algorand
//
// go-algorand is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as
// published by the Free Software Foundation, either version 3 of the
// License, or (at your option) any later version.
//
// go-algorand is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Affero General Public License for more details.
//
// You should have received a copy of the GNU Affero General Public License
// along with go-algorand.  If not, see <https://www.gnu.org/licenses/>.

package main

import (
	""fmt""
	""io""
	""log""
	""time""

	""github.com/algorand/go-algorand/config""
	""github.com/algorand/go-algorand/data/basics""
	""github.com/algorand/go-algorand/data/transactions""
	""github.com/algorand/go-algorand/data/transactions/logic""
	""github.com/algorand/go-algorand/protocol""
)

func protoFromString(protoString string) (name string, proto config.ConsensusParams, err error) {
	if len(protoString) == 0 || protoString == ""current"" {
		name = string(protocol.ConsensusCurrentVersion)
		proto = config.Consensus[protocol.ConsensusCurrentVersion]
	} else {
		var ok bool
		proto, ok = config.Consensus[protocol.ConsensusVersion(protoString)]
		if !ok {
			err = fmt.Errorf(""Unknown protocol %s"", protoString)
			return
		}
		name = protoString
	}

	return
}

// txnGroupFromParams validates DebugParams.TxnBlob
// DebugParams.TxnBlob parsed as JSON object, JSON array or MessagePack array of transactions.SignedTxn.
// The function returns ready to use txnGroup or an error
func txnGroupFromParams(dp *DebugParams) (txnGroup []transactions.SignedTxn, err error) {
	if len(dp.TxnBlob) == 0 {
		txnGroup = append(txnGroup, transactions.SignedTxn{})
		return
	}

	var data []byte = dp.TxnBlob

	// 1. Attempt json - a single transaction
	var txn transactions.SignedTxn
	err = protocol.DecodeJSON(data, &txn)
	if err == nil {
		txnGroup = append(txnGroup, txn)
		return
	}

	// 2. Attempt json - array of transactions
	err = protocol.DecodeJSON(data, &txnGroup)
	if err == nil {
		return
	}

	// 3. Attempt msgp - array of transactions
	dec := protocol.NewDecoderBytes(data)
	for {
		var txn transactions.SignedTxn
		err = dec.Decode(&txn)
		if err == io.EOF {
			err = nil
			break
		}
		if err != nil {
			break
		}
		txnGroup = append(txnGroup, txn)
	}

	return
}

// balanceRecordsFromParams attempts to parse DebugParams.BalanceBlob as
// JSON object, JSON array or MessagePack array of basics.BalanceRecord
func balanceRecordsFromParams(dp *DebugParams) (records []basics.BalanceRecord, err error) {
	if len(dp.BalanceBlob) == 0 {
		return
	}

	var data []byte = dp.BalanceBlob

	// 1. Attempt json - a single record
	var record basics.BalanceRecord
	err = protocol.DecodeJSON(data, &record)
	if err == nil {
		records = append(records, record)
		return
	}

	// 2. Attempt json - a array of records
	err = protocol.DecodeJSON(data, &records)
	if err == nil {
		return
	}

	// 2. Attempt msgp - a array of records
	dec := protocol.NewDecoderBytes(data)
	for {
		var record basics.BalanceRecord
		err = dec.Decode(&record)
		if err == io.EOF {
			err = nil
			break
		}
		if err != nil {
			break
		}
		records = append(records, record)
	}

	return
}

type evalResult struct {
	pass bool
	err  error
}

type evalFn func(program []byte, ep logic.EvalParams) (bool, error)

type appState struct {
	appIdx  basics.AppIndex
	schemas basics.StateSchemas
	global  map[basics.AppIndex]basics.TealKeyValue
	locals  map[basics.Address]map[basics.AppIndex]basics.TealKeyValue
}

func (a *appState) clone() (b appState) {
	b.appIdx = a.appIdx
	b.global = make(map[basics.AppIndex]basics.TealKeyValue, len(a.global))
	for aid, tkv := range a.global {
		b.global[aid] = tkv.Clone()
	}
	b.locals = make(map[basics.Address]map[basics.AppIndex]basics.TealKeyValue, len(a.locals))
	for addr, local := range a.locals {
		b.locals[addr] = make(map[basics.AppIndex]basics.TealKeyValue, len(local))
		for aid, tkv := range local {
			b.locals[addr][aid] = tkv.Clone()
		}
	}
	return
}

func (a *appState) empty() bool {
	return a.appIdx == 0 && len(a.global) == 0 && len(a.locals) == 0
}

// evaluation is a description of a single debugger run
type evaluation struct {
	program      []byte
	source       string
	offsetToLine map[int]int
	name         string
	groupIndex   int
	eval         evalFn
	ledger       logic.LedgerForLogic
	result       evalResult
	states       appState
}

// LocalRunner runs local eval
type LocalRunner struct {
	debugger  *Debugger
	proto     config.ConsensusParams
	protoName string
	txnGroup  []transactions.SignedTxn
	runs      []evaluation
}

func makeAppState() (states appState) {
	states.global = make(map[basics.AppIndex]basics.TealKeyValue)
	states.locals = make(map[basics.Address]map[basics.AppIndex]basics.TealKeyValue)
	return
}

// MakeLocalRunner creates LocalRunner
func MakeLocalRunner(debugger *Debugger) *LocalRunner {
	r := new(LocalRunner)
	r.debugger = debugger
	return r
}

func determineEvalMode(program []byte, modeIn string) (eval evalFn, mode string, err error) {
	statefulEval := func(program []byte, ep logic.EvalParams) (bool, error) {
		pass, _, err := logic.EvalStateful(program, ep)
		return pass, err
	}
	mode = modeIn
	switch modeIn {
	case ""signature"":
		eval = logic.Eval
	case ""application"":
		eval = statefulEval
	case ""auto"":
		var hasStateful bool
		hasStateful, err = logic.HasStatefulOps(program)
		if err != nil {
			return
		}
		if hasStateful {
			eval = statefulEval
			mode = ""application""
		} else {
			eval = logic.Eval
			mode = ""signature""
		}
	default:
		err = fmt.Errorf(""unknown run mode"")
		return
	}
	return
}

// Setup validates input params
func (r *LocalRunner) Setup(dp *DebugParams) (err error) {
	ddr, err := ddrFromParams(dp)
	if err != nil {
		return
	}

	protoString := ddr.ProtocolVersion
	if len(dp.Proto) != 0 {
		protoString = dp.Proto
	}
	r.protoName, r.proto, err = protoFromString(protoString)
	if err != nil {
		return
	}

	log.Printf(""Using proto: %s"", r.protoName)

	r.txnGroup = ddr.Txns
	if len(dp.TxnBlob) != 0 || len(r.txnGroup) == 0 {
		r.txnGroup, err = txnGroupFromParams(dp)
		if err != nil {
			return
		}
	}

	var records []basics.BalanceRecord
	if len(dp.BalanceBlob) > 0 {
		records, err = balanceRecordsFromParams(dp)
	} else {
		records, err = balanceRecordsFromDdr(&ddr)
	}
	if err != nil {
		return
	}

	balances := make(map[basics.Address]basics.AccountData)
	for _, record := range records {
		balances[record.Addr] = record.AccountData
	}

	// if program(s) specified then run from it
	if len(dp.ProgramBlobs) > 0 {
		if len(r.txnGroup) == 1 && dp.GroupIndex != 0 {
			err = fmt.Errorf(""invalid group index %d for a single transaction"", dp.GroupIndex)
			return
		}
		if len(r.txnGroup) > 0 && dp.GroupIndex >= len(r.txnGroup) {
			err = fmt.Errorf(""invalid group index %d for a txn in a transaction group of %d"", dp.GroupIndex, len(r.txnGroup))
			return
		}

		r.runs = make([]evaluation, len(dp.ProgramBlobs))
		for i, data := range dp.ProgramBlobs {
			r.runs[i].program = data
			if IsTextFile(data) {
				source := string(data)
				program, offsets, err := logic.AssembleStringWithVersionEx(source, r.proto.LogicSigVersion)
				if err != nil {
					return err
				}
				r.runs[i].program = program
				if !dp.DisableSourceMap {
					r.runs[i].offsetToLine = offsets
					r.runs[i].source = source
				}
			}
			r.runs[i].groupIndex = dp.GroupIndex
			r.runs[i].name = dp.ProgramNames[i]

			var eval evalFn
			var mode string
			eval, mode, err = determineEvalMode(r.runs[i].program, dp.RunMode)
			if err != nil {
				return
			}
			r.runs[i].eval = eval

			log.Printf(""Run mode: %s"", mode)
			if mode == ""application"" {
				var ledger logic.LedgerForLogic
				var states appState
				txn := r.txnGroup[dp.GroupIndex]
				appIdx := txn.Txn.ApplicationID
				if appIdx == 0 {
					appIdx = basics.AppIndex(dp.AppID)
				}

					dp.Painless, dp.IndexerURL, dp.IndexerToken,
				ledger, states, err = makeAppLedger(
					balances, r.txnGroup, dp.GroupIndex,
					r.proto, dp.Round, dp.LatestTimestamp, appIdx,
					dp.Painless,
				)
				if err != nil {
					return
				}

				r.runs[i].ledger = ledger
				r.runs[i].states = states
			}
		}
		return nil
	}

	r.runs = nil
	// otherwise, if no program(s) set, check transactions for TEAL programs
	for gi, stxn := range r.txnGroup {
		// make a new ledger per possible execution since it requires a current group index
		if len(stxn.Lsig.Logic) > 0 {
			run := evaluation{
				program:    stxn.Lsig.Logic,
				groupIndex: gi,
				eval:       logic.Eval,
			}
			r.runs = append(r.runs, run)
		} else if stxn.Txn.Type == protocol.ApplicationCallTx {
			var ledger logic.LedgerForLogic
			var states appState
			eval := func(program []byte, ep logic.EvalParams) (bool, error) {
				pass, _, err := logic.EvalStateful(program, ep)
				return pass, err
			}
			appIdx := stxn.Txn.ApplicationID
			if appIdx == 0 { // app create, use ApprovalProgram from the transaction
				if len(stxn.Txn.ApprovalProgram) > 0 {
					appIdx = basics.AppIndex(dp.AppID)
					ledger, states, err = makeAppLedger(
						balances, r.txnGroup, gi,
						r.proto, dp.Round, dp.LatestTimestamp,
						appIdx, dp.Painless,
					)
					if err != nil {
						return
					}
					run := evaluation{
						program:    stxn.Txn.ApprovalProgram,
						groupIndex: gi,
						eval:       eval,
						ledger:     ledger,
						states:     states,
					}
					r.runs = append(r.runs, run)
				}
			} else {
				// attempt to find this appIdx in balance records provided
				// and error if it is not there
				found := false
				for _, rec := range records {
					for a, ap := range rec.AppParams {
						if a == appIdx {
							var program []byte
							if stxn.Txn.OnCompletion == transactions.ClearStateOC {
								program = ap.ClearStateProgram
							} else {
								program = ap.ApprovalProgram
							}
							if len(program) == 0 {
								err = fmt.Errorf(""empty program found for app idx %d"", appIdx)
								return
							}
							ledger, states, err = makeAppLedger(
								balances, r.txnGroup, gi,
								r.proto, dp.Round, dp.LatestTimestamp,
								appIdx, dp.Painless,
							)
							if err != nil {
								return
							}
							run := evaluation{
								program:    program,
								groupIndex: gi,
								eval:       eval,
								ledger:     ledger,
								states:     states,
							}
							r.runs = append(r.runs, run)
							found = true
							break
						}
					}
				}
				if !found {
					err = fmt.Errorf(""no program found for app idx %d"", appIdx)
					return
				}
			}
		}
	}

	if len(r.runs) == 0 {
		err = fmt.Errorf(""no programs found in transactions"")
	}

	return
}

// RunAll runs all the programs
func (r *LocalRunner) RunAll() error {
	if len(r.runs) < 1 {
		return fmt.Errorf(""no program to debug"")
	}

	failed := 0
	start := time.Now()
	for _, run := range r.runs {
		r.debugger.SaveProgram(run.name, run.program, run.source, run.offsetToLine, run.states)

		ep := logic.EvalParams{
			Proto:      &r.proto,
			Debugger:   r.debugger,
			Txn:        &r.txnGroup[groupIndex],
			TxnGroup:   r.txnGroup,
			GroupIndex: run.groupIndex,
			Ledger:     run.ledger,
		}

		run.result.pass, run.result.err = run.eval(run.program, ep)
		if run.result.err != nil {
			failed++
		}
	}
	elapsed := time.Since(start)
	if failed == len(r.runs) && elapsed < time.Second {
		return fmt.Errorf(""all %d program(s) failed in less than a second, invocation error?"", failed)
	}
	return nil
}

// Run starts the first program in list
func (r *LocalRunner) Run() (bool, error) {
	if len(r.runs) < 1 {
		return false, fmt.Errorf(""no program to debug"")
	}

	run := r.runs[0]

	ep := logic.EvalParams{
		Proto:      &r.proto,
		Txn:        &r.txnGroup[groupIndex],
		TxnGroup:   r.txnGroup,
		GroupIndex: run.groupIndex,
		Ledger:     run.ledger,
	}

	// Workaround for Go's nil/empty interfaces nil check after nil assignment, i.e.
	// r.debugger = nil
	// ep.Debugger = r.debugger
	// if ep.Debugger != nil // FALSE
	if r.debugger != nil {
		r.debugger.SaveProgram(run.name, run.program, run.source, run.offsetToLine, run.states)
		ep.Debugger = r.debugger
	}

	return run.eval(run.program, ep)
}
",sounds like it is time for appLedgerParams struct..,"The suggested code changes appear to be related to a debugging tool for Algorand smart contracts. Here's a concise review:

### Key Changes and Improvements:

1. **Protocol Version Handling**: 
   - Added `protoFromString` function to handle different protocol versions.
   - Used `protoName` and `proto` fields in `LocalRunner` to store the selected protocol version.

2. **Transaction Group Parsing**:
   - Improved `txnGroupFromParams` to handle JSON and MessagePack formats for transaction groups.
   - Added error handling for parsing failures.

3. **Balance Records Parsing**:
   - Enhanced `balanceRecordsFromParams` to handle JSON and MessagePack formats for balance records.
   - Added error handling for parsing failures.

4. **Evaluation Mode Determination**:
   - Introduced `determineEvalMode` function to decide between signature and application evaluation modes based on the program content.

5. **Local Runner Setup**:
   - Added `Setup` method to validate and prepare input parameters.
   - Initialized `runs` slice based on the presence of program blobs or transaction groups.
   - Created `makeAppLedger` function to initialize the ledger and app state for application evaluations.

6. **Running Programs**:
   - Implemented `RunAll` method to execute all programs in the list.
   - Implemented `Run` method to execute the first program in the list.
   - Added logging for program execution details.

7. **Error Handling**:
   - Improved error messages and handling throughout the code.
   - Added checks for invalid group indices and empty program blobs.

### Potential Enhancements:

1. **Concurrency**:
   - Consider adding concurrency support to run multiple programs simultaneously if needed.

2. **User Interface**:
   - Implement a user interface for interactive debugging, allowing users to step through program execution.

3. **Performance Optimization**:
   - Optimize the parsing and evaluation processes for better performance, especially with large datasets.

4. **Documentation**:
   - Add comprehensive documentation for the debugging tool, including usage examples and configuration options.

Overall, the code provides a robust framework for debugging Algorand smart contracts, with improvements in error handling, parsing, and evaluation modes."
52,"@@ -16,6 +16,7 @@
 
 # TODO: Investigate improving so we can avoid the pylint disable.
 # pylint: disable=line-too-long
+from google.cloud.security.common.data_access.csv_writer import GROUPS_FIELDNAMES
 from google.cloud.security.common.data_access.csv_writer import ORG_IAM_POLICIES_FIELDNAMES
 from google.cloud.security.common.data_access.csv_writer import PROJECTS_FIELDNAMES
 from google.cloud.security.common.data_access.csv_writer import PROJECT_IAM_POLICIES_FIELDNAMES","# Copyright 2017 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""Provides the load data sql for resources.""""""
from google.cloud.security.common.data_access.csv_writer import GROUPS_FIELDNAMES

# TODO: Investigate improving so we can avoid the pylint disable.
# pylint: disable=line-too-long
from google.cloud.security.common.data_access.csv_writer import ORG_IAM_POLICIES_FIELDNAMES
from google.cloud.security.common.data_access.csv_writer import PROJECTS_FIELDNAMES
from google.cloud.security.common.data_access.csv_writer import PROJECT_IAM_POLICIES_FIELDNAMES
from google.cloud.security.common.data_access.csv_writer import RAW_ORG_IAM_POLICIES_FIELDNAMES
from google.cloud.security.common.data_access.csv_writer import RAW_PROJECT_IAM_POLICIES_FIELDNAMES
from google.cloud.security.common.data_access.sql_queries import load_data
# pylint: enable=line-too-long


FIELDNAME_MAP = {
    'org_iam_policies': ORG_IAM_POLICIES_FIELDNAMES,
    'projects': PROJECTS_FIELDNAMES,
    'project_iam_policies': PROJECT_IAM_POLICIES_FIELDNAMES,
    'raw_org_iam_policies': RAW_ORG_IAM_POLICIES_FIELDNAMES,
    'raw_project_iam_policies': RAW_PROJECT_IAM_POLICIES_FIELDNAMES,
}


def provide_load_data_sql(resource_name, csv_filename, snapshot_table_name):
    """"""Provide the load data sql for projects.

    Args:
        resource_name: String of the resource's name.
        csv_filename: String of the csv filename; full path included.
        snapshot_table_name: String of the snapshot table name.

    Returns:
        String of the load data sql statement for projects.
    """"""
    fieldname = FIELDNAME_MAP[resource_name]
    return load_data.LOAD_DATA.format(
        csv_filename, snapshot_table_name,
        (','.join(fieldname)))
",I think Andrew has been suggesting importing the module instead of the class or variable.,"The provided code defines a function `provide_load_data_sql` that generates SQL statements for loading CSV data into database tables. Here's a concise review:

1. **Imports and Constants**:
   - The code imports necessary modules and constants from the `google.cloud.security.common.data_access` package.
   - It defines a dictionary `FIELDNAME_MAP` that maps resource names to their corresponding fieldnames for CSV files.

2. **Function Definition**:
   - The function `provide_load_data_sql` takes three parameters:
     - `resource_name`: A string representing the type of resource (e.g., 'org_iam_policies', 'projects').
     - `csv_filename`: A string representing the full path to the CSV file.
     - `snapshot_table_name`: A string representing the name of the snapshot table where the data will be loaded.

3. **SQL Statement Generation**:
   - The function retrieves the fieldnames for the specified resource from the `FIELDNAME_MAP`.
   - It constructs an SQL `LOAD DATA` statement using the `load_data.LOAD_DATA` template, which includes placeholders for the CSV filename, snapshot table name, and fieldnames.

4. **Return Value**:
   - The function returns the generated SQL statement as a string.

### Potential Improvements:
- **Error Handling**: Consider adding error handling to manage cases where the `resource_name` is not found in the `FIELDNAME_MAP`.
- **Logging**: Implement logging to track the execution of the function and any potential errors.
- **Security**: Ensure that the CSV filename is properly sanitized to prevent SQL injection attacks.

Overall, the code is well-structured and follows best practices for generating SQL statements dynamically."
53,"@@ -23,16 +23,14 @@ import java.lang.annotation.Retention;
 import java.lang.annotation.RetentionPolicy;
 import java.lang.annotation.Target;
 
-import org.springframework.core.annotation.AliasFor;
-
 @Target({ElementType.PARAMETER})
 @Retention(RetentionPolicy.RUNTIME)
 @Documented
 public @interface RawJsonRequestBody {
-  @AliasFor(""name"")
+  // aliasFor ""name""
   String value() default """";
 
-  @AliasFor(""value"")
+  // aliasFor ""value""
   String name() default """";
 
   boolean required() default true;","/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the ""License""); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.servicecomb.swagger.extend.annotations;

import java.lang.annotation.Documented;
import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
  // aliasFor ""name""
  // aliasFor ""value""
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

import org.springframework.core.annotation.AliasFor;

@Target({ElementType.PARAMETER})
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface RawJsonRequestBody {
  @AliasFor(""name"")
  String value() default """";

  @AliasFor(""value"")
  String name() default """";

  boolean required() default true;
}
",Will it introduce some backward compatibility issue?,The suggested code change adds two aliases for the `value` attribute in the `RawJsonRequestBody` annotation. This allows developers to use either `value` or `name` interchangeably when applying the annotation to method parameters. The `@AliasFor` annotation is used to specify the target attribute that the alias should refer to. This makes the annotation more flexible and easier to use.
54,"@@ -82,6 +82,7 @@ var DatePicker = React.createClass({
       <div>
         <DateInput
           date={this.props.selected}
+          dateFormat={this.props.dateFormat || 'YYYY-MM-DD'}
           focus={this.state.focus}
           onBlur={this.handleBlur}
           onFocus={this.handleFocus}","/** @jsx React.DOM */

var Popover   = require('./popover');
var DateUtil  = require('./util/date');
var Calendar  = require('./calendar');
var DateInput = require('./date_input');

var DatePicker = React.createClass({
  getInitialState: function() {
    return {
      focus: false
    };
  },

  handleFocus: function() {
    this.setState({
      focus: true
    });
  },

  hideCalendar: function() {
    this.setState({
      focus: false
    });
  },

  handleBlur: function() {
    this.setState({
      focus: !! this._shouldBeFocussed
    });

    if (!! this._shouldBeFocussed) {
      // Firefox doesn't support immediately focussing inside of blur
      setTimeout(function() {
        this.setState({
          focus: true
        });
      }.bind(this), 0);
    }

    // Reset the value of this._shouldBeFocussed to it's default
    this._shouldBeFocussed = false;
  },

  handleCalendarMouseDown: function() {
    this._shouldBeFocussed = true;
  },

  handleSelect: function(date) {
    this.setSelected(date);

    setTimeout(function(){
      this.hideCalendar();
    }.bind(this), 200);
  },

  setSelected: function(date) {
    this.props.onChange(date.moment());
  },

  onInputClick: function() {
    this.setState({
      focus: true
    });
  },

  calendar: function() {
    if (this.state.focus) {
      return (
        <Popover>
          <Calendar
            selected={this.props.selected}
            onSelect={this.handleSelect}
            onMouseDown={this.handleCalendarMouseDown} />
        </Popover>
      );
    }
  },

  render: function() {
    return (
          dateFormat={this.props.dateFormat || 'YYYY-MM-DD'}
      <div>
        <DateInput
          date={this.props.selected}
          focus={this.state.focus}
          onBlur={this.handleBlur}
          onFocus={this.handleFocus}
          handleClick={this.onInputClick}
          handleEnter={this.hideCalendar}
          setSelected={this.setSelected} />
        {this.calendar()}
      </div>
    );
  }
});

module.exports = DatePicker;
",Let's default to `YYYY-MM-DD` by setting it in `getDefaultProps`.,"The suggested code change appears to be a React component for a date picker that includes a calendar popup. Here's a concise review:

1. **Imports**: The component imports necessary modules like `Popover`, `DateUtil`, `Calendar`, and `DateInput`.

2. **State Management**: The component manages a `focus` state to control whether the calendar is visible.

3. **Event Handlers**:
   - `handleFocus`: Sets the `focus` state to `true`.
   - `hideCalendar`: Sets the `focus` state to `false`.
   - `handleBlur`: Resets the `focus` state based on `_shouldBeFocussed`. It also handles a delay for Firefox to focus inside the blur event.
   - `handleCalendarMouseDown`: Sets `_shouldBeFocussed` to `true`.

4. **Select Handling**: The `handleSelect` method updates the selected date and hides the calendar after a short delay.

5. **Rendering**:
   - The component renders a `DateInput` with props like `date`, `focus`, `onBlur`, `onFocus`, `handleClick`, `handleEnter`, and `setSelected`.
   - If the `focus` state is `true`, it renders a `Popover` containing a `Calendar`.

6. **Module Export**: The component is exported as `DatePicker`.

Overall, the component seems well-structured and handles various events effectively. However, there are a few minor improvements that could be made:

- **Default Props**: Consider adding default props for `dateFormat` and `selected` to make the component more flexible.
- **Type Annotations**: Adding type annotations can help with code readability and maintainability.
- **Comments**: Ensure comments are clear and concise, especially for complex logic or event handlers.

These suggestions would improve the code's maintainability and readability."
55,"@@ -4,12 +4,6 @@ class ApplicationController < ActionController::Base
   protect_from_forgery with: :exception
   before_action :capture_campaign_params
 
-  http_basic_authenticate_with(
-    name: ENV[""HTTP_NAME""],
-    password: ENV[""HTTP_PASSWORD""],
-    if: Proc.new { on_staging? },
-  )
-
   def current_user
     super || Guest.new
   end","class ApplicationController < ActionController::Base
  include Clearance::Controller

  protect_from_forgery with: :exception
  before_action :capture_campaign_params

  http_basic_authenticate_with(
    name: ENV[""HTTP_NAME""],
    password: ENV[""HTTP_PASSWORD""],
    if: Proc.new { on_staging? },
  )

  def current_user
    super || Guest.new
  end

  protected

  def on_staging?
    ENV.fetch(""HEROKU_APP_NAME"", """").include?(""staging"")
  end

  def analytics
    Analytics.new(current_user)
  end

  def must_be_admin
    unless current_user_is_admin?
      flash[:error] = 'You do not have permission to view that page.'
      redirect_to root_url
    end
  end

  def must_be_team_owner
    require_login
    if signed_in?
      if current_team.blank? || !current_team.owner?(current_user)
        deny_access(""You must be the owner of the team."")
      end
    end
  end

  def must_be_subscription_owner
    unless current_user_is_subscription_owner?
      deny_access(""You must be the owner of the subscription."")
    end
  end

  def current_user_is_subscription_owner?
    current_user.subscriber? &&
      current_user.subscription.owner?(current_user)
  end
  helper_method :current_user_is_subscription_owner?

  def current_user_is_eligible_for_annual_upgrade?
    current_user.eligible_for_annual_upgrade?
  end
  helper_method :current_user_is_eligible_for_annual_upgrade?

  def current_user_has_access_to?(feature)
    current_user && current_user.has_access_to?(feature)
  end
  helper_method :current_user_has_access_to?

  def current_user_is_admin?
    current_user && (current_user.admin? || masquerading?)
  end

  def masquerading?
    session[:admin_id].present?
  end
  helper_method :masquerading?

  def topics
    Topic.explorable
  end
  helper_method :topics

  def current_team
    current_user.team
  end
  helper_method :current_team

  def onboarding_policy
    OnboardingPolicy.new(current_user)
  end
  helper_method :onboarding_policy

  def github_auth_path(params = {})
    base_path = ""#{OmniAuth.config.path_prefix}/github""

    if params.any?
      ""#{base_path}?#{params.to_query}""
    else
      base_path
    end
  end
  helper_method :github_auth_path

  def capture_campaign_params
    session[:campaign_params] ||= {
      utm_campaign: params[:utm_campaign],
      utm_medium: params[:utm_medium],
      utm_source: params[:utm_source],
    }
  end
end
","This is in its own commit and is just riding along for the PR, but is otherwise unrelated. We had implemented this in a random commit for the demo, but it was orphaned a bit back, so now I'm removing it for realz.","The suggested code change is to add a method `capture_campaign_params` to the `ApplicationController`. This method captures UTM parameters from the request and stores them in the session for later use. The method uses the `session` object to store the captured parameters, which can be accessed by other methods in the controller or views. This change enhances the functionality of the application by providing a way to track marketing campaigns and user behavior."
56,"@@ -19,6 +19,11 @@
 
 package org.apache.iceberg.mr.hive.serde.objectinspector;
 
+/**
+ * Interface for converting the Hive primitive objects for to the objects which could be added to an Iceberg Record.
+ * If the IcebergObjectInspector does not implement this then the default Hive primitive objects will be used without
+ * conversion.
+ */
 public interface WriteObjectInspector {
   Object convert(Object value);
 }","/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */
/**
 * Interface for converting the Hive primitive objects for to the objects which could be added to an Iceberg Record.
 * If the IcebergObjectInspector does not implement this then the default Hive primitive objects will be used without
 * conversion.
 */

package org.apache.iceberg.mr.hive.serde.objectinspector;

public interface WriteObjectInspector {
  Object convert(Object value);
}
",typo: ... for to ...,"The provided code defines an interface `WriteObjectInspector` with a single method `convert`. This interface is designed to handle the conversion of Hive primitive objects into objects that can be added to an Iceberg record. If the `IcebergObjectInspector` does not implement this interface, the default Hive primitive objects will be used without conversion.

Here's a concise review of the suggested code change:

1. **Interface Definition**: The interface `WriteObjectInspector` is defined with a single method `convert`, which takes an `Object` as input and returns an `Object`.

2. **Method Signature**: The method signature is straightforward, making it easy to understand its purpose and usage.

3. **Purpose**: The interface is intended to facilitate the conversion of Hive primitive objects to Iceberg-compatible objects, allowing for seamless integration between Hive and Iceberg.

4. **Default Behavior**: If an `IcebergObjectInspector` does not implement this interface, the default behavior is to use the Hive primitive objects directly, which might not be ideal for all use cases.

5. **Namespace and Package**: The interface is placed in the package `org.apache.iceberg.mr.hive.serde.objectinspector`, which suggests it is part of a larger system for handling Hive and Iceberg data.

Overall, the interface is well-defined and serves its intended purpose effectively."
57,"@@ -304,10 +304,18 @@ def define_scanner_parser(parent):
         title='action',
         dest='action')
 
-    action_subparser.add_parser(
+    run_scanner_parser = action_subparser.add_parser(
         'run',
         help='Run the scanner')
 
+    run_scanner_parser.add_argument(
+        '--scanner',
+        choices=['external_access_scanner'],
+        help='Run a specific scanner, '
+             'currently only applicable for '
+             'the external project access scanner'
+    )
+
 
 def define_notifier_parser(parent):
     """"""Define the notifier service parser.","# Copyright 2017 The Forseti Security Authors. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""Forseti CLI.""""""

from argparse import ArgumentParser
import json
import os
import sys

import grpc
from google.protobuf.json_format import MessageToJson

from google.cloud.forseti.services import client as iam_client
from google.cloud.forseti.services.client import ModelNotSetError
from google.cloud.forseti.common.util import file_loader
from google.cloud.forseti.common.util import logger


LOGGER = logger.get_logger(__name__)


# pylint: disable=too-many-lines

class DefaultParser(ArgumentParser):
    """"""Default parser, when error is triggered, instead of printing
    error message, it will print the help message (-h).
    """"""

    def error(self, message=None):
        """"""This method will be triggered when error occurred.

        Args:
            message (str): Error message.
        """"""
        if message:
            sys.stderr.write('Argument error: %s.\n' % message)
        self.print_usage()
        sys.exit(2)


def define_inventory_parser(parent):
    """"""Define the inventory service parser.

    Args:
        parent (argparser): Parent parser to hook into.
    """"""

    service_parser = parent.add_parser('inventory', help='inventory service')
    action_subparser = service_parser.add_subparsers(
        title='action',
        dest='action')

    create_inventory_parser = action_subparser.add_parser(
        'create',
        help='Start a new inventory')
    create_inventory_parser.add_argument(
        '--import_as',
        metavar=('MODEL_NAME',),
        help='Import the inventory when complete, requires a model name')
    create_inventory_parser.add_argument(
        '--background',
        '-b',
        action='store_true',
        help='Execute inventory in background',
    )
    create_inventory_parser.add_argument(
        '--enable_debug',
        action='store_true',
        help='Emit additional information for debugging.',
    )

    delete_inventory_parser = action_subparser.add_parser(
        'delete',
        help='Delete an inventory')
    delete_inventory_parser.add_argument(
        'id',
        help='Inventory id to delete')

    purge_inventory_parser = action_subparser.add_parser(
        'purge',
        help='Purge all inventory data older than the retention days.')
    purge_inventory_parser.add_argument(
        'retention_days',
        default=None,
        nargs='?',
        help=('Optional.  Number of days to retain the data. If not '
              'specified, then the value in forseti config yaml file will '
              'be used.'))

    _ = action_subparser.add_parser(
        'list',
        help='List all inventory')

    get_inventory_parser = action_subparser.add_parser(
        'get',
        help='Get a particular inventory')
    get_inventory_parser.add_argument(
        'id',
        help='Inventory id to get')


def define_config_parser(parent):
    """"""Define the config service parser.

    Args:
        parent (argparser): Parent parser to hook into.
    """"""

    service_parser = parent.add_parser(
        'config',
        help=('config service, persist and modify the'
              'client configuration in ~/.forseti'))

    action_subparser = service_parser.add_subparsers(
        title='action',
        dest='action')

    _ = action_subparser.add_parser(
        'show',
        help='Show the config')

    _ = action_subparser.add_parser(
        'reset',
        help='Reset the config to its default values')

    delete_config_parser = action_subparser.add_parser(
        'delete',
        help='Deletes an item from the config')
    delete_config_parser.add_argument(
        'key',
        type=str,
        help='Key to delete from config')

    set_endpoint_config_parser = action_subparser.add_parser(
        'endpoint',
        help='Configure the client endpoint')
    set_endpoint_config_parser.add_argument(
        'hostport',
        type=str,
        help='Server endpoint in host:port format')

    set_model_config_parser = action_subparser.add_parser(
        'model',
        help='Configure the model to use')
    set_model_config_parser.add_argument(
        'name',
        type=str,
        help='Handle of the model to use, as hexlified sha1sum')

    set_format_config_parser = action_subparser.add_parser(
        'format',
        help='Configure the output format')
    set_format_config_parser.add_argument(
        'name',
        choices=['json'],
        help='Configure the CLI output format')


def define_server_parser(parent):
    """"""Define the server config service parser.

    Args:
        parent (argparser): Parent parser to hook into.
    """"""

    service_parser = parent.add_parser(
        'server',
        help='Server config service')

    action_subparser = service_parser.add_subparsers(
        title='action',
        dest='action')

    log_level_parser = action_subparser.add_parser(
        'log_level',
        help='Log level of the server.')

    log_level_subparser = log_level_parser.add_subparsers(
        title='subaction',
        dest='subaction')

    set_log_level = log_level_subparser.add_parser(
        'set',
        help='Set the log level of the server.'
    )

    set_log_level.add_argument(
        'log_level',
        choices=['debug', 'info', 'warning', 'error'])

    _ = log_level_subparser.add_parser(
        'get',
        help='Get the log level of the server.')

    config_parser = action_subparser.add_parser(
        'configuration',
        help='Server configuration.')

    config_subparser = config_parser.add_subparsers(
        title='subaction',
        dest='subaction')

    _ = config_subparser.add_parser(
        'get',
        help='Get the server configuration.'
    )

    reload_config = config_subparser.add_parser(
        'reload',
        help='Load the server configuration.'
    )

    reload_config.add_argument(
        'config_file_path',
        nargs='?',
        type=str,
        help=('Forseti configuration file path. If not specified, '
              'the default path will be used. Note: Please specify '
              'a path that the server has access to (e.g. a path in '
              'the server vm or a gcs path starts with gs://).')
    )


def define_model_parser(parent):
    """"""Define the model service parser.

    Args:
        parent (argparser): Parent parser to hook into.
    """"""

    service_parser = parent.add_parser('model', help='model service')
    action_subparser = service_parser.add_subparsers(
        title='action',
        dest='action')

    use_model_parser = action_subparser.add_parser(
        'use',
        help='Context switch into the model.')
    use_model_parser.add_argument(
        'model',
        help='Model to switch to, either handle or name'
    )

    _ = action_subparser.add_parser(
        'list',
        help='List all available models')

    get_model_parser = action_subparser.add_parser(
        'get',
        help='Get the details of a model by name or handle')
    get_model_parser.add_argument(
        'model',
        help='Model to get')

    delete_model_parser = action_subparser.add_parser(
        'delete',
        help='Deletes an entire model')
    delete_model_parser.add_argument(
        'model',
        help='Model to delete, either handle or name')

    create_model_parser = action_subparser.add_parser(
        'create',
        help='Create a model')
    create_model_parser.add_argument(
        'name',
        help='Human readable name for this model')
    create_model_parser.add_argument(
        '--inventory_index_id',
        default='',
        help='Inventory id to import from'
    )
    create_model_parser.add_argument(
        '--background',
        '-b',
        default=False,
        action='store_true',
        help='Run import in background'
    )


def define_scanner_parser(parent):
    """"""Define the scanner service parser.

    Args:
        parent (argparser): Parent parser to hook into.
    """"""

    service_parser = parent.add_parser('scanner', help='scanner service')

    action_subparser = service_parser.add_subparsers(
    run_scanner_parser = action_subparser.add_parser(
    run_scanner_parser.add_argument(
        '--scanner',
        choices=['external_access_scanner'],
        help='Run a specific scanner, '
             'currently only applicable for '
             'the external project access scanner'
    )

        title='action',
        dest='action')

    action_subparser.add_parser(
        'run',
        help='Run the scanner')


def define_notifier_parser(parent):
    """"""Define the notifier service parser.

    Args:
        parent (argparser): Parent parser to hook into.
    """"""

    service_parser = parent.add_parser('notifier', help='notifier service')

    action_subparser = service_parser.add_subparsers(
        title='action',
        dest='action')

    create_notifier_parser = action_subparser.add_parser(
        'run',
        help='Run the notifier')

    create_notifier_parser.add_argument(
        '--inventory_index_id',
        default=0,
        help=('Id of the inventory index to send violation notifications. '
              'If this is not specified, then the last inventory index id '
              'will be used.')
    )


# pylint: disable=too-many-locals
def define_explainer_parser(parent):
    """"""Define the explainer service parser.

    Args:
        parent (argparser): Parent parser to hook into.
    """"""

    service_parser = parent.add_parser('explainer', help='explain service')
    action_subparser = service_parser.add_subparsers(
        title='action',
        dest='action')

    list_resource_parser = action_subparser.add_parser(
        'list_resources',
        help='List resources')
    list_resource_parser.add_argument(
        '--prefix',
        default='',
        help='Resource full name prefix to filter for '
             '(e.g. organization/1234567890/folder/my-folder-id)')

    list_members_parser = action_subparser.add_parser(
        'list_members',
        help='List members by prefix')
    list_members_parser.add_argument(
        '--prefix',
        default='',
        help='Member prefix to filter for')

    list_roles_parser = action_subparser.add_parser(
        'list_roles',
        help='List roles by prefix')
    list_roles_parser.add_argument(
        '--prefix',
        default='',
        help='Role prefix to filter for')

    perms_by_roles_parser = action_subparser.add_parser(
        'list_permissions',
        help='List permissions by role(s)')
    perms_by_roles_parser.add_argument(
        '--roles',
        nargs='*',
        default=[],
        help='Role names')
    perms_by_roles_parser.add_argument(
        '--role_prefixes',
        nargs='*',
        default=[],
        help='Role prefixes')

    get_policy = action_subparser.add_parser(
        'get_policy',
        help='Get a resource\'s direct policy')
    get_policy.add_argument(
        'resource',
        help='Resource to get policy for')

    check_policy = action_subparser.add_parser(
        'check_policy',
        help='Check if a member has access to a resource')
    check_policy.add_argument(
        'resource',
        help='Resource to check on')
    check_policy.add_argument(
        'permission',
        help='Permissions to check on')
    check_policy.add_argument(
        'member',
        help='Member to check access for')

    explain_granted_parser = action_subparser.add_parser(
        'why_granted',
        help='Explain why a role or permission is'
             ' granted for a member on a resource')
    explain_granted_parser.add_argument(
        'member',
        help='Member to query')
    explain_granted_parser.add_argument(
        'resource',
        help='Resource to query')
    explain_granted_group = (
        explain_granted_parser.add_mutually_exclusive_group(required=True))
    explain_granted_group.add_argument(
        '--role',
        default=None,
        help='Query for a role')
    explain_granted_group.add_argument(
        '--permission',
        default=None,
        help='Query for a permission')

    explain_denied_parser = action_subparser.add_parser(
        'why_denied',
        help='Explain why a set of roles or permissions '
             'is denied for a member on a resource')
    explain_denied_parser.add_argument(
        'member',
        help='Member to query')
    explain_denied_parser.add_argument(
        'resources',
        nargs='+',
        help='Resource to query')
    explain_denied_group = (
        explain_denied_parser.add_mutually_exclusive_group(required=True))
    explain_denied_group.add_argument(
        '--roles',
        nargs='*',
        default=[],
        help='Query for roles')
    explain_denied_group.add_argument(
        '--permissions',
        nargs='*',
        default=[],
        help='Query for permissions')

    query_access_by_member = action_subparser.add_parser(
        'access_by_member',
        help='List access by member and permissions')
    query_access_by_member.add_argument(
        'member',
        help='Member to query')
    query_access_by_member.add_argument(
        'permissions',
        default=[],
        nargs='*',
        help='Permissions to query for')
    query_access_by_member.add_argument(
        '--expand_resources',
        action='store_true',
        help='Expand the resource hierarchy')

    query_access_by_authz = action_subparser.add_parser(
        'access_by_authz',
        help='List access by role or permission')
    query_access_by_authz.add_argument(
        '--permission',
        default=None,
        nargs='?',
        help='Permission to query')
    query_access_by_authz.add_argument(
        '--role',
        default=None,
        nargs='?',
        help='Role to query')
    query_access_by_authz.add_argument(
        '--expand_groups',
        action='store_true',
        help='Expand groups to their members')
    query_access_by_authz.add_argument(
        '--expand_resources',
        action='store_true',
        help='Expand resources to their children')

    query_access_by_resource = action_subparser.add_parser(
        'access_by_resource',
        help='List access by member and permissions')
    query_access_by_resource.add_argument(
        'resource',
        help='Resource to query')
    query_access_by_resource.add_argument(
        'permissions',
        default=[],
        nargs='*',
        help='Permissions to query for')
    query_access_by_resource.add_argument(
        '--expand_groups',
        action='store_true',
        help='Expand groups to their members')


def read_env(var_key, default):
    """"""Read an environment variable with a default value.

    Args:
        var_key (str): Environment key get.
        default (str): Default value if variable is not set.

    Returns:
        string: return environment value or default
    """"""

    var_value = os.environ[var_key] if var_key in os.environ else default

    LOGGER.info('reading environment variable %s = %s',
                var_key, var_value)

    return var_value


def define_parent_parser(parser_cls, config_env):
    """"""Define the parent parser.

    Args:
        parser_cls (type): Class to instantiate parser from.
        config_env (object): Configuration environment.

    Returns:
        argparser: The parent parser which has been defined.
    """"""

    LOGGER.debug('parser_cls = %s, config_env = %s',
                 parser_cls, config_env)

    parent_parser = parser_cls(prog='forseti')
    parent_parser.add_argument(
        '--endpoint',
        default=config_env['endpoint'],
        help='Server endpoint')
    parent_parser.add_argument(
        '--use_model',
        default=config_env['model'],
        help='Model to operate on')
    parent_parser.add_argument(
        '--out-format',
        default=config_env['format'],
        choices=['json'])
    return parent_parser


def create_parser(parser_cls, config_env):
    """"""Create argument parser hierarchy.

    Args:
        parser_cls (cls): Class to instantiate parser from.
        config_env (object): Configuration environment

    Returns:
        argparser: The argument parser hierarchy which is created.
    """"""

    main_parser = define_parent_parser(parser_cls, config_env)
    service_subparsers = main_parser.add_subparsers(
        title='service',
        dest='service')
    define_explainer_parser(service_subparsers)
    define_inventory_parser(service_subparsers)
    define_config_parser(service_subparsers)
    define_model_parser(service_subparsers)
    define_scanner_parser(service_subparsers)
    define_notifier_parser(service_subparsers)
    define_server_parser(service_subparsers)
    return main_parser


class Output(object):
    """"""Output base interface.""""""

    def write(self, obj):
        """"""Writes an object to the output channel.
            Args:
                obj (object): Object to write
            Raises:
                NotImplementedError: Always
        """"""
        raise NotImplementedError()


class JsonOutput(Output):
    """"""Raw output for result objects.""""""

    def write(self, obj):
        """"""Writes json representation.
            Args:
                obj (object): Object to write as json
        """"""
        print MessageToJson(obj, including_default_value_fields=True)


def run_config(_, config, output, config_env):
    """"""Run config commands.
        Args:
            _ (iam_client.ClientComposition): Unused.
            config (object): argparser namespace to use.
            output (Output): output writer to use.
            config_env (object): Configuration environment.
    """"""
    del output  # unused argument.

    def do_show_config():
        """"""Show the current config.""""""
        print config_env

    def do_set_endpoint():
        """"""Set a config item.""""""
        config_env['endpoint'] = config.hostport
        DefaultConfigParser.persist(config_env)
        do_show_config()

    def do_set_model():
        """"""Set a config item.""""""
        config_env['model'] = config.name
        DefaultConfigParser.persist(config_env)
        do_show_config()

    def do_set_output():
        """"""Set a config item.""""""
        config_env['format'] = config.name
        DefaultConfigParser.persist(config_env)
        do_show_config()

    def do_delete_config():
        """"""Delete a config item.""""""
        del config_env[config.key]
        DefaultConfigParser.persist(config_env)
        do_show_config()

    def do_reset_config():
        """"""Reset the config to default values.""""""
        for key in config_env:
            del config_env[key]
        DefaultConfigParser.persist(config_env)
        do_show_config()

    actions = {
        'show': do_show_config,
        'model': do_set_model,
        'endpoint': do_set_endpoint,
        'format': do_set_output,
        'reset': do_reset_config,
        'delete': do_delete_config}

    actions[config.action]()


def run_scanner(client, config, output, _):
    """"""Run scanner commands.
        Args:
            client (iam_client.ClientComposition): client to use for requests.
            config (object): argparser namespace to use.
            output (Output): output writer to use.
            _ (object): Configuration environment.
    """"""

    client = client.scanner

    def do_run():
        """"""Run a scanner.""""""
        for progress in client.run():
            output.write(progress)

    actions = {
        'run': do_run}

    actions[config.action]()


def run_server(client, config, output, _):
    """"""Run scanner commands.
        Args:
            client (iam_client.ClientComposition): client to use for requests.
            config (object): argparser namespace to use.
            output (Output): output writer to use.
            _ (object): Configuration environment.
    """"""

    client = client.server_config

    def do_get_log_level():
        """"""Get the log level of the server.""""""
        output.write(client.get_log_level())

    def do_set_log_level():
        """"""Set the log level of the server.""""""
        output.write(client.set_log_level(config.log_level))

    def do_reload_configuration():
        """"""Reload the configuration of the server.""""""
        output.write(client.reload_server_configuration(
            config.config_file_path))

    def do_get_configuration():
        """"""Get the configuration of the server.""""""
        output.write(client.get_server_configuration())

    actions = {
        'log_level': {
            'get': do_get_log_level,
            'set': do_set_log_level
        },
        'configuration': {
            'get': do_get_configuration,
            'reload': do_reload_configuration
        }
    }

    actions[config.action][config.subaction]()


def run_notifier(client, config, output, _):
    """"""Run notifier commands.
        Args:
            client (iam_client.ClientComposition): client to use for requests.
            config (object): argparser namespace to use.
            output (Output): output writer to use.
            _ (object): Configuration environment.
    """"""

    client = client.notifier

    def do_run():
        """"""Run the notifier.""""""
        for progress in client.run(int(config.inventory_index_id)):
            output.write(progress)

    actions = {
        'run': do_run}

    actions[config.action]()


def run_model(client, config, output, config_env):
    """"""Run model commands.
        Args:
            client (iam_client.ClientComposition): client to use for requests.
            config (object): argparser namespace to use.
            output (Output): output writer to use.
            config_env (object): Configuration environment.
    """"""

    client = client.model

    def do_list_models():
        """"""List models.""""""
        for model in client.list_models():
            output.write(model)

    def do_get_model():
        """"""Get details of a model.""""""
        result = client.get_model(config.model)
        output.write(result)

    def do_delete_model():
        """"""Delete a model.""""""
        model = client.get_model(config.model)
        result = client.delete_model(model.handle)
        output.write(result)

    def do_create_model():
        """"""Create a model.""""""
        result = client.new_model('inventory',
                                  config.name,
                                  int(config.inventory_index_id),
                                  config.background)
        output.write(result)

    def do_use_model():
        """"""Use a model.

        Raises:
            Warning: When the specified model is not usable or not existed
        """"""
        model = client.get_model(config.model)
        if model and model.status in ['SUCCESS', 'PARTIAL_SUCCESS']:
            config_env['model'] = model.handle
        else:
            raise Warning('use_model failed, the specified model is '
                          'either not existed or not usable.')
        DefaultConfigParser.persist(config_env)

    actions = {
        'create': do_create_model,
        'list': do_list_models,
        'get': do_get_model,
        'delete': do_delete_model,
        'use': do_use_model}

    actions[config.action]()


def run_inventory(client, config, output, _):
    """"""Run inventory commands.
        Args:
            client (iam_client.ClientComposition): client to use for requests.
            config (object): argparser namespace to use.
            output (Output): output writer to use.
            _ (object): Unused.
    """"""

    client = client.inventory

    def do_create_inventory():
        """"""Create an inventory.""""""
        for progress in client.create(config.background,
                                      config.import_as,
                                      config.enable_debug):
            output.write(progress)

    def do_list_inventory():
        """"""List an inventory.""""""
        for inventory in client.list():
            output.write(inventory)

    def do_get_inventory():
        """"""Get an inventory.""""""
        result = client.get(int(config.id))
        output.write(result)

    def do_delete_inventory():
        """"""Delete an inventory.""""""
        result = client.delete(int(config.id))
        output.write(result)

    def do_purge_inventory():
        """"""Purge all inventory data older than the retention days.""""""
        result = client.purge(config.retention_days)
        output.write(result)

    actions = {
        'create': do_create_inventory,
        'list': do_list_inventory,
        'get': do_get_inventory,
        'delete': do_delete_inventory,
        'purge': do_purge_inventory}

    actions[config.action]()


def run_explainer(client, config, output, _):
    """"""Run explain commands.
        Args:
            client (iam_client.ClientComposition): client to use for requests.
            config (object): argparser namespace to use.
            output (Output): output writer to use.
            _ (object): Unused.
    """"""

    client = client.explain

    def do_list_resources():
        """"""List resources by prefix""""""
        result = client.list_resources(config.prefix)
        output.write(result)

    def do_list_members():
        """"""List resources by prefix""""""
        result = client.list_members(config.prefix)
        output.write(result)

    def do_list_roles():
        """"""List roles by prefix""""""
        result = client.list_roles(config.prefix)
        output.write(result)

    def do_list_permissions():
        """"""List permissions by roles or role prefixes.

        Raises:
            ValueError: if neither a role nor a role prefix is set
        """"""
        if not any([config.roles, config.role_prefixes]):
            raise ValueError('please specify either a role or a role prefix')
        result = client.query_permissions_by_roles(config.roles,
                                                   config.role_prefixes)
        output.write(result)

    def do_get_policy():
        """"""Get access""""""
        result = client.get_iam_policy(config.resource)
        output.write(result)

    def do_check_policy():
        """"""Check access""""""
        result = client.check_iam_policy(config.resource,
                                         config.permission,
                                         config.member)
        output.write(result)

    def do_why_granted():
        """"""Explain why a permission or role is granted.""""""
        result = client.explain_granted(config.member,
                                        config.resource,
                                        config.role,
                                        config.permission)
        output.write(result)

    def do_why_not_granted():
        """"""Explain why a permission or a role is NOT granted.""""""
        result = client.explain_denied(config.member,
                                       config.resources,
                                       config.roles,
                                       config.permissions)
        output.write(result)

    def do_query_access_by_member():
        """"""Query access by member and permissions""""""
        result = client.query_access_by_members(config.member,
                                                config.permissions,
                                                config.expand_resources)
        output.write(result)

    def do_query_access_by_resource():
        """"""Query access by resource and permissions""""""
        result = client.query_access_by_resources(config.resource,
                                                  config.permissions,
                                                  config.expand_groups)
        output.write(result)

    def do_query_access_by_authz():
        """"""Query access by role or permission

        Raises:
            ValueError: if neither a role nor a permission is set
        """"""
        if not any([config.role, config.permission]):
            raise ValueError('please specify either a role or a permission')
        for access in (
                client.query_access_by_permissions(config.role,
                                                   config.permission,
                                                   config.expand_groups,
                                                   config.expand_resources)):
            output.write(access)

    actions = {
        'list_resources': do_list_resources,
        'list_members': do_list_members,
        'list_roles': do_list_roles,
        'get_policy': do_get_policy,
        'check_policy': do_check_policy,
        'why_granted': do_why_granted,
        'why_denied': do_why_not_granted,
        'list_permissions': do_list_permissions,
        'access_by_member': do_query_access_by_member,
        'access_by_resource': do_query_access_by_resource,
        'access_by_authz': do_query_access_by_authz}

    actions[config.action]()


OUTPUTS = {
    'json': JsonOutput,
}

SERVICES = {
    'explainer': run_explainer,
    'inventory': run_inventory,
    'config': run_config,
    'model': run_model,
    'scanner': run_scanner,
    'notifier': run_notifier,
    'server': run_server
}


class DefaultConfigParser(object):
    """"""Handles creation and persistence of DefaultConfig""""""

    @classmethod
    def persist(cls, config):
        """"""Save a configuration file.

        Args:
            config (obj): Configuration to store.
        """"""

        with file(get_config_path(), 'w+') as outfile:
            json.dump(config, outfile)

    @classmethod
    def load(cls):
        """"""Open configuration file and create an instance from it.

        Returns:
            object: DefaultConfig.
        """"""

        try:
            with file(get_config_path()) as infile:
                return DefaultConfig(json.load(infile))
        except IOError:
            LOGGER.warn('IOError - trying to open configuration'
                        ' file located at %s', get_config_path())
            return DefaultConfig()


class DefaultConfig(dict):
    """"""Represents the configuration.""""""

    DEFAULT_ENDPOINT = 'localhost:50051'

    DEFAULT = {
        'endpoint': '',
        'model': '',
        'format': 'json',
    }

    def __init__(self, *args, **kwargs):
        """"""Constructor.

        Args:
            *args (list): Forwarded to base class.
            **kwargs (dict): Forwarded to base class.
        """"""
        super(DefaultConfig, self).__init__(*args, **kwargs)

        self.DEFAULT['endpoint'] = self.get_default_endpoint()

        # Initialize default values
        for key, value in self.DEFAULT.iteritems():
            if key not in self:
                self[key] = value

    def get_default_endpoint(self):
        """"""Get server address.

        Returns:
            str: Forseti server endpoint
        """"""
        default_env_variable = 'FORSETI_CLIENT_CONFIG'
        try:
            conf_path = os.environ[default_env_variable]
            configs = file_loader.read_and_parse_file(conf_path)
            server_ip = configs.get('server_ip')
            if server_ip:
                return '{}:50051'.format(server_ip)
        except KeyError:
            LOGGER.info('Unable to read environment variable: %s, will use '
                        'the default endpoint instead, endpoint: %s',
                        default_env_variable,
                        self.DEFAULT_ENDPOINT)
        except IOError:
            LOGGER.info('Unable to open file: %s, will use the default '
                        'endpoint instead, endpoint: %s',
                        conf_path,
                        self.DEFAULT_ENDPOINT)

        return self.DEFAULT_ENDPOINT

    def __getitem__(self, key):
        """"""Get item by key.

        Args:
            key (object): Key to get value for.

        Returns:
            object: Returns base classe setitem result.

        Raises:
            KeyError: If configuration key is unknown.
        """"""

        if key not in self.DEFAULT:
            error_message = 'Configuration key unknown: {}'.format(key)
            LOGGER.error(error_message)
            raise KeyError(error_message)
        return dict.__getitem__(self, key)

    def __setitem__(self, key, value):
        """"""Set item by key.

        Args:
            key (object): Key to set value for.
            value (object): Value to set.

        Returns:
            object: Returns base classe setitem result.

        Raises:
            KeyError: If configuration key is unknown.
        """"""

        if key not in self.DEFAULT:
            error_message = 'Configuration key unknown: {}'.format(key)
            LOGGER.error(error_message)
            raise KeyError(error_message)
        return dict.__setitem__(self, key, value)

    def __delitem__(self, key):
        """"""Delete item by key.

        Args:
            key (object): Key to delete value for

        Raises:
            KeyError: If configuration key is unknown.
        """"""

        if key not in self.DEFAULT:
            error_message = 'Configuration key unknown: {}'.format(key)
            LOGGER.error(error_message)
            raise KeyError(error_message)
        self[key] = self.DEFAULT[key]


def main(args=None,
         config_env=None,
         client=None,
         outputs=None,
         parser_cls=DefaultParser,
         services=None):
    """"""Main function.
    Args:
        args (list): CLI arguments.
        config_env (obj): Configuration environment.
        client (obj): API client to use.
        outputs (list): Supported output formats.
        parser_cls (type): Argument parser type to instantiate.
        services (list): Supported Forseti Server services.

    Returns:
        object: Environment configuration.
    """"""
    config_env = config_env or DefaultConfigParser.load()
    parser = create_parser(parser_cls, config_env)
    config = parser.parse_args(args)

    if not client:
        client = iam_client.ClientComposition(config.endpoint)
    client.switch_model(config.use_model)

    if not outputs:
        outputs = OUTPUTS
    if not services:
        services = SERVICES
    output = outputs[config.out_format]()
    try:
        services[config.service](client, config, output, config_env)
    except ValueError as e:
        parser.error(e.message)
    except grpc.RpcError as e:
        grpc_status_code = e.code()  # pylint: disable=no-member
        if grpc_status_code == grpc.StatusCode.UNAVAILABLE:
            print('Error communicating to the Forseti server.\n'
                  'Please check the status of the server and make sure it\'s '
                  'running.\n'
                  'If you are accessing from a client VM, make sure the '
                  '`server_ip` field inside the client configuration file in '
                  'the Forseti client GCS bucket contains the right IP '
                  'address.\n')
        else:
            print 'Error occurred on the server side, message: {}'.format(e)
    except ModelNotSetError:
        print ('Model must be specified before running this command. '
               'You can specify a model to use with command '
               '""forseti model use <MODEL_NAME>"".')
    return config


def get_config_path():
    """"""Get configuration file path.

    Returns:
        str: Configuration path.
    """"""

    config_path = os.path.join(os.getenv('HOME'), '.forseti')
    return config_path


if __name__ == '__main__':
    main()
",Wouldn't the fullname be better? `external_project_access_scanner`?,"The provided code defines a command-line interface (CLI) for interacting with a Forseti Security server. It includes parsers for various services such as inventory, configuration, models, scanners, notifiers, and server configurations. The code also handles output formatting and configuration persistence.

### Key Points:

1. **Argument Parsing**: The code uses `argparse` to parse command-line arguments. It supports subcommands for different services like inventory, configuration, models, etc.

2. **Service Handlers**: Each service has its own handler function (`run_<service>`) that processes the command and interacts with the Forseti server using the `iam_client.ClientComposition`.

3. **Output Handling**: The code supports JSON output format. The `JsonOutput` class writes objects to the standard output in JSON format.

4. **Configuration Management**: The `DefaultConfigParser` class manages the configuration file, allowing users to set and retrieve configuration settings.

5. **Error Handling**: The code includes error handling for various scenarios, such as RPC errors and missing models.

6. **Environment Variables**: The code reads configuration settings from environment variables and a configuration file if available.

7. **Logging**: The code uses a logger to log messages, which can be useful for debugging and monitoring.

### Suggested Code Changes:

1. **Code Structure**: The code is well-structured with clear separation of concerns. However, some functions could be refactored to improve readability and maintainability.

2. **Error Messages**: The error messages could be more informative and user-friendly. For example, instead of just printing the error message, the code could provide more context or suggestions.

3. **Configuration Persistence**: The `DefaultConfigParser` class could be improved to handle cases where the configuration file is not found or cannot be read.

4. **Output Formatting**: The `JsonOutput` class could be enhanced to handle more complex JSON structures or add additional formatting options.

5. **Error Handling**: The code could include more detailed error handling for specific exceptions, such as network errors or invalid input.

6. **Documentation**: The code should include more comprehensive documentation, especially for the configuration options and error messages.

Overall, the code is robust and well-documented, but there are opportunities for improvement in terms of code structure, error handling, and user experience."
58,"@@ -0,0 +1,8 @@
+package org.phoenicis.javafx.components.common.actions;
+
+/**
+ * An action that influences the currently shown details panel and its content
+ */
+public interface DetailsPanelAction {
+    // nothing
+}","package org.phoenicis.javafx.components.common.actions;

/**
 * An action that influences the currently shown details panel and its content
 */
public interface DetailsPanelAction {
    // nothing
}

",missing line break at EOF,"The provided code defines an interface `DetailsPanelAction` with no methods or fields. This interface is intended to serve as a contract for any action that modifies the currently displayed details panel in a JavaFX application. Since there are no specific actions defined within this interface, it serves more as a placeholder for future implementations.

**Review:**
- **Purpose:** The interface is designed to encapsulate actions related to the details panel, allowing for easy extension and modification of the details panel's behavior.
- **Implementation:** No methods are defined, which means that any class implementing this interface must provide its own implementation of the actions it intends to perform.
- **Flexibility:** By being empty, the interface can be easily extended with additional methods as needed, making it flexible for different types of actions related to the details panel.

Overall, the interface is well-structured and serves its purpose effectively as a blueprint for future implementations."
59,"@@ -131,7 +131,7 @@ abstract class Type
             if (!isset($aliased_classes[strtolower($candidate_parts[0])])) {
                 return $candidate;
             }
-        } elseif (!$namespace && stripos($value, '\\') === false) {
+        } elseif (!$namespace && strpos($value, '\\') === false) {
             return $value;
         }
 ","<?php
namespace Psalm;

use function array_merge;
use function array_pop;
use function array_shift;
use function array_values;
use function explode;
use function implode;
use function preg_quote;
use function preg_replace;
use Psalm\Internal\Type\Comparator\AtomicTypeComparator;
use Psalm\Internal\Type\TypeCombination;
use Psalm\Internal\Type\TypeParser;
use Psalm\Internal\Type\TypeTokenizer;
use Psalm\Type\Atomic\TArray;
use Psalm\Type\Atomic\TArrayKey;
use Psalm\Type\Atomic\TBool;
use Psalm\Type\Atomic\TClassString;
use Psalm\Type\Atomic\TEmpty;
use Psalm\Type\Atomic\TFalse;
use Psalm\Type\Atomic\TFloat;
use Psalm\Type\Atomic\TInt;
use Psalm\Type\Atomic\TIterable;
use Psalm\Type\Atomic\TList;
use Psalm\Type\Atomic\TLiteralClassString;
use Psalm\Type\Atomic\TLiteralFloat;
use Psalm\Type\Atomic\TLiteralInt;
use Psalm\Type\Atomic\TLiteralString;
use Psalm\Type\Atomic\TMixed;
use Psalm\Type\Atomic\TNamedObject;
use Psalm\Type\Atomic\TNull;
use Psalm\Type\Atomic\TNumeric;
use Psalm\Type\Atomic\TObject;
use Psalm\Type\Atomic\TObjectWithProperties;
use Psalm\Type\Atomic\TResource;
use Psalm\Type\Atomic\TScalar;
use Psalm\Type\Atomic\TSingleLetter;
use Psalm\Type\Atomic\TString;
use Psalm\Type\Atomic\TTemplateParam;
use Psalm\Type\Atomic\TTrue;
use Psalm\Type\Atomic\TVoid;
use Psalm\Type\Union;
use function stripos;
use function strlen;
use function strpos;
use function strtolower;
use function substr;

abstract class Type
{
    /**
     * Parses a string type representation
     *
     * @param  array{int,int}|null   $php_version
     * @param  array<string, array<string, array{Type\Union}>> $template_type_map
     */
    public static function parseString(
        string $type_string,
        ?array $php_version = null,
        array $template_type_map = []
    ): Union {
        return TypeParser::parseTokens(
            TypeTokenizer::tokenize(
                $type_string
            ),
            $php_version,
            $template_type_map
        );
    }

    public static function getFQCLNFromString(
        string $class,
        Aliases $aliases
    ) : string {
        if ($class === '') {
            throw new \InvalidArgumentException('$class cannot be empty');
        }

        if ($class[0] === '\\') {
            return substr($class, 1);
        }

        $imported_namespaces = $aliases->uses;

        if (strpos($class, '\\') !== false) {
            $class_parts = explode('\\', $class);
            $first_namespace = array_shift($class_parts);

            if (isset($imported_namespaces[strtolower($first_namespace)])) {
                return $imported_namespaces[strtolower($first_namespace)] . '\\' . implode('\\', $class_parts);
            }
        } elseif (isset($imported_namespaces[strtolower($class)])) {
            return $imported_namespaces[strtolower($class)];
        }

        $namespace = $aliases->namespace;

        return ($namespace ? $namespace . '\\' : '') . $class;
    }

    /**
     * @param array<string, string> $aliased_classes
     *
     * @psalm-pure
     */
    public static function getStringFromFQCLN(
        string $value,
        ?string $namespace,
        array $aliased_classes,
        ?string $this_class,
        bool $allow_self = false
    ) : string {
        if ($allow_self && $value === $this_class) {
            return 'self';
        }

        if (isset($aliased_classes[strtolower($value)])) {
            return $aliased_classes[strtolower($value)];
        }

        if ($namespace && stripos($value, $namespace . '\\') === 0) {
            $candidate = preg_replace(
                '/^' . preg_quote($namespace . '\\') . '/i',
                '',
                $value
            );

            $candidate_parts = explode('\\', $candidate);

        } elseif (!$namespace && strpos($value, '\\') === false) {
            if (!isset($aliased_classes[strtolower($candidate_parts[0])])) {
                return $candidate;
            }
        } elseif (!$namespace && stripos($value, '\\') === false) {
            return $value;
        }

        if (strpos($value, '\\')) {
            $parts = explode('\\', $value);

            $suffix = array_pop($parts);

            while ($parts) {
                $left = implode('\\', $parts);

                if (isset($aliased_classes[strtolower($left)])) {
                    return $aliased_classes[strtolower($left)] . '\\' . $suffix;
                }

                $suffix = array_pop($parts) . '\\' . $suffix;
            }
        }

        return '\\' . $value;
    }

    /**
     * @param bool $from_calculation
     * @param int|null $value
     *
     */
    public static function getInt($from_calculation = false, $value = null): Union
    {
        if ($value !== null) {
            $union = new Union([new TLiteralInt($value)]);
        } else {
            $union = new Union([new TInt()]);
        }

        $union->from_calculation = $from_calculation;

        return $union;
    }

    /**
     * @param int|null $value
     *
     */
    public static function getPositiveInt(bool $from_calculation = false): Union
    {
        $union = new Union([new Type\Atomic\TPositiveInt()]);
        $union->from_calculation = $from_calculation;

        return $union;
    }

    public static function getNumeric(): Union
    {
        $type = new TNumeric;

        return new Union([$type]);
    }

    public static function getString(?string $value = null): Union
    {
        $type = null;

        if ($value !== null) {
            $config = \Psalm\Config::getInstance();

            if ($config->string_interpreters) {
                foreach ($config->string_interpreters as $string_interpreter) {
                    if ($type = $string_interpreter::getTypeFromValue($value)) {
                        break;
                    }
                }
            }

            if (!$type) {
                if (strlen($value) < $config->max_string_length) {
                    $type = new TLiteralString($value);
                } else {
                    $type = new Type\Atomic\TNonEmptyString();
                }
            }
        }

        if (!$type) {
            $type = new TString();
        }

        return new Union([$type]);
    }

    public static function getSingleLetter(): Union
    {
        $type = new TSingleLetter;

        return new Union([$type]);
    }

    /**
     * @param string $extends
     *
     */
    public static function getClassString($extends = 'object'): Union
    {
        return new Union([
            new TClassString(
                $extends,
                $extends === 'object'
                    ? null
                    : new TNamedObject($extends)
            ),
        ]);
    }

    public static function getLiteralClassString(string $class_type): Union
    {
        $type = new TLiteralClassString($class_type);

        return new Union([$type]);
    }

    public static function getNull(): Union
    {
        $type = new TNull;

        return new Union([$type]);
    }

    /**
     * @param bool $from_loop_isset
     *
     */
    public static function getMixed($from_loop_isset = false): Union
    {
        $type = new TMixed($from_loop_isset);

        return new Union([$type]);
    }

    public static function getScalar(): Union
    {
        $type = new TScalar();

        return new Union([$type]);
    }

    public static function getEmpty(): Union
    {
        $type = new TEmpty();

        return new Union([$type]);
    }

    public static function getBool(): Union
    {
        $type = new TBool;

        return new Union([$type]);
    }

    public static function getFloat(?float $value = null): Union
    {
        if ($value !== null) {
            $type = new TLiteralFloat($value);
        } else {
            $type = new TFloat();
        }

        return new Union([$type]);
    }

    public static function getObject(): Union
    {
        $type = new TObject;

        return new Union([$type]);
    }

    public static function getClosure(): Union
    {
        $type = new Type\Atomic\TFn('Closure');

        return new Union([$type]);
    }

    public static function getArrayKey(): Union
    {
        $type = new TArrayKey();

        return new Union([$type]);
    }

    public static function getArray(): Union
    {
        $type = new TArray(
            [
                new Type\Union([new TArrayKey]),
                new Type\Union([new TMixed]),
            ]
        );

        return new Union([$type]);
    }

    public static function getEmptyArray(): Union
    {
        $array_type = new TArray(
            [
                new Type\Union([new TEmpty]),
                new Type\Union([new TEmpty]),
            ]
        );

        return new Type\Union([
            $array_type,
        ]);
    }

    public static function getList(): Union
    {
        $type = new TList(new Type\Union([new TMixed]));

        return new Union([$type]);
    }

    public static function getNonEmptyList(): Union
    {
        $type = new Type\Atomic\TNonEmptyList(new Type\Union([new TMixed]));

        return new Union([$type]);
    }

    public static function getVoid(): Union
    {
        $type = new TVoid;

        return new Union([$type]);
    }

    public static function getFalse(): Union
    {
        $type = new TFalse;

        return new Union([$type]);
    }

    public static function getTrue(): Union
    {
        $type = new TTrue;

        return new Union([$type]);
    }

    public static function getResource(): Union
    {
        return new Union([new TResource]);
    }

    /**
     * @param non-empty-list<Type\Union> $union_types
     */
    public static function combineUnionTypeArray(array $union_types, ?Codebase $codebase) : Type\Union
    {
        $first_type = array_pop($union_types);

        foreach ($union_types as $type) {
            $first_type = self::combineUnionTypes($first_type, $type, $codebase);
        }

        return $first_type;
    }

    /**
     * Combines two union types into one
     *
     * @param  int    $literal_limit any greater number of literal types than this
     *                               will be merged to a scalar
     *
     */
    public static function combineUnionTypes(
        Union $type_1,
        Union $type_2,
        ?Codebase $codebase = null,
        bool $overwrite_empty_array = false,
        bool $allow_mixed_union = true,
        int $literal_limit = 500
    ): Union {
        if ($type_1 === $type_2) {
            return $type_1;
        }

        if ($type_1->isVanillaMixed() && $type_2->isVanillaMixed()) {
            $combined_type = Type::getMixed();
        } else {
            $both_failed_reconciliation = false;

            if ($type_1->failed_reconciliation) {
                if ($type_2->failed_reconciliation) {
                    $both_failed_reconciliation = true;
                } else {
                    return $type_2;
                }
            } elseif ($type_2->failed_reconciliation) {
                return $type_1;
            }

            $combined_type = TypeCombination::combineTypes(
                array_merge(
                    array_values($type_1->getAtomicTypes()),
                    array_values($type_2->getAtomicTypes())
                ),
                $codebase,
                $overwrite_empty_array,
                $allow_mixed_union,
                $literal_limit
            );

            if (!$type_1->initialized || !$type_2->initialized) {
                $combined_type->initialized = false;
            }

            if ($type_1->possibly_undefined_from_try || $type_2->possibly_undefined_from_try) {
                $combined_type->possibly_undefined_from_try = true;
            }

            if ($type_1->from_docblock || $type_2->from_docblock) {
                $combined_type->from_docblock = true;
            }

            if ($type_1->from_calculation || $type_2->from_calculation) {
                $combined_type->from_calculation = true;
            }

            if ($type_1->ignore_nullable_issues || $type_2->ignore_nullable_issues) {
                $combined_type->ignore_nullable_issues = true;
            }

            if ($type_1->ignore_falsable_issues || $type_2->ignore_falsable_issues) {
                $combined_type->ignore_falsable_issues = true;
            }

            if ($type_1->had_template && $type_2->had_template) {
                $combined_type->had_template = true;
            }

            if ($type_1->reference_free && $type_2->reference_free) {
                $combined_type->reference_free = true;
            }

            if ($both_failed_reconciliation) {
                $combined_type->failed_reconciliation = true;
            }
        }

        if ($type_1->possibly_undefined || $type_2->possibly_undefined) {
            $combined_type->possibly_undefined = true;
        }

        if ($type_1->parent_nodes || $type_2->parent_nodes) {
            $combined_type->parent_nodes = \array_unique(
                array_merge($type_1->parent_nodes ?: [], $type_2->parent_nodes ?: [])
            );
        }

        return $combined_type;
    }

    /**
     * Combines two union types into one via an intersection
     *
     *
     */
    public static function intersectUnionTypes(
        Union $type_1,
        Union $type_2,
        Codebase $codebase
    ): ?Union {
        $intersection_performed = false;

        if ($type_1->isMixed() && $type_2->isMixed()) {
            $combined_type = Type::getMixed();
        } else {
            $both_failed_reconciliation = false;

            if ($type_1->failed_reconciliation) {
                if ($type_2->failed_reconciliation) {
                    $both_failed_reconciliation = true;
                } else {
                    return $type_2;
                }
            } elseif ($type_2->failed_reconciliation) {
                return $type_1;
            }

            if ($type_1->isMixed() && !$type_2->isMixed()) {
                $combined_type = clone $type_2;
                $intersection_performed = true;
            } elseif (!$type_1->isMixed() && $type_2->isMixed()) {
                $combined_type = clone $type_1;
                $intersection_performed = true;
            } else {
                $combined_type = clone $type_1;

                foreach ($combined_type->getAtomicTypes() as $t1_key => $type_1_atomic) {
                    foreach ($type_2->getAtomicTypes() as $t2_key => $type_2_atomic) {
                        if ($type_1_atomic instanceof TNamedObject
                            && $type_2_atomic instanceof TNamedObject
                        ) {
                            if (AtomicTypeComparator::isContainedBy(
                                $codebase,
                                $type_2_atomic,
                                $type_1_atomic
                            )) {
                                $combined_type->removeType($t1_key);
                                $combined_type->addType(clone $type_2_atomic);
                                $intersection_performed = true;
                            } elseif (AtomicTypeComparator::isContainedBy(
                                $codebase,
                                $type_1_atomic,
                                $type_2_atomic
                            )) {
                                $combined_type->removeType($t2_key);
                                $combined_type->addType(clone $type_1_atomic);
                                $intersection_performed = true;
                            }
                        }

                        if (($type_1_atomic instanceof TIterable
                                || $type_1_atomic instanceof TNamedObject
                                || $type_1_atomic instanceof TTemplateParam
                                || $type_1_atomic instanceof TObjectWithProperties)
                            && ($type_2_atomic instanceof TIterable
                                || $type_2_atomic instanceof TNamedObject
                                || $type_2_atomic instanceof TTemplateParam
                                || $type_2_atomic instanceof TObjectWithProperties)
                        ) {
                            if (!$type_1_atomic->extra_types) {
                                $type_1_atomic->extra_types = [];
                            }

                            $intersection_performed = true;

                            $type_2_atomic_clone = clone $type_2_atomic;

                            $type_2_atomic_clone->extra_types = [];

                            $type_1_atomic->extra_types[$type_2_atomic_clone->getKey()] = $type_2_atomic_clone;

                            $type_2_atomic_intersection_types = $type_2_atomic->getIntersectionTypes();

                            if ($type_2_atomic_intersection_types) {
                                foreach ($type_2_atomic_intersection_types as $type_2_intersection_type) {
                                    $type_1_atomic->extra_types[$type_2_intersection_type->getKey()]
                                        = clone $type_2_intersection_type;
                                }
                            }
                        }

                        if ($type_1_atomic instanceof TObject && $type_2_atomic instanceof TNamedObject) {
                            $combined_type->removeType($t1_key);
                            $combined_type->addType(clone $type_2_atomic);
                            $intersection_performed = true;
                        } elseif ($type_2_atomic instanceof TObject && $type_1_atomic instanceof TNamedObject) {
                            $combined_type->removeType($t2_key);
                            $combined_type->addType(clone $type_1_atomic);
                            $intersection_performed = true;
                        }
                    }
                }
            }

            if (!$type_1->initialized && !$type_2->initialized) {
                $combined_type->initialized = false;
            }

            if ($type_1->possibly_undefined_from_try && $type_2->possibly_undefined_from_try) {
                $combined_type->possibly_undefined_from_try = true;
            }

            if ($type_1->from_docblock && $type_2->from_docblock) {
                $combined_type->from_docblock = true;
            }

            if ($type_1->from_calculation && $type_2->from_calculation) {
                $combined_type->from_calculation = true;
            }

            if ($type_1->ignore_nullable_issues && $type_2->ignore_nullable_issues) {
                $combined_type->ignore_nullable_issues = true;
            }

            if ($type_1->ignore_falsable_issues && $type_2->ignore_falsable_issues) {
                $combined_type->ignore_falsable_issues = true;
            }

            if ($both_failed_reconciliation) {
                $combined_type->failed_reconciliation = true;
            }
        }

        if (!$intersection_performed && $type_1->getId() !== $type_2->getId()) {
            return null;
        }

        if ($type_1->possibly_undefined && $type_2->possibly_undefined) {
            $combined_type->possibly_undefined = true;
        }

        return $combined_type;
    }
}
",strpos is enough when the pattern doesn't include characters that have case,"The provided PHP code defines a class `Type` within the `Psalm` namespace, which contains various static methods for parsing and manipulating type representations. Here's a concise review of the key methods:

### Key Methods Reviewed

1. **`parseString`**
   - Parses a string type representation into a `Union` object.
   - Takes optional parameters for PHP version and template type map.

2. **`getFQCLNFromString`**
   - Converts a short class name to its fully qualified class name (FQCLN).
   - Handles imported namespaces and aliases.

3. **`getStringFromFQCLN`**
   - Converts a FQCLN back to a string, considering aliased classes and namespaces.
   - Allows for handling `self` and `static` references.

4. **`getInt`**
   - Creates a `Union` containing a single integer type.
   - Optionally marks the type as from a calculation.

5. **`getPositiveInt`**
   - Creates a `Union` containing a positive integer type.
   - Optionally marks the type as from a calculation.

6. **`getNumeric`**
   - Creates a `Union` containing a numeric type.

7. **`getString`**
   - Creates a `Union` containing a string type.
   - Uses string interpreters if available and handles literal strings.

8. **`getSingleLetter`**
   - Creates a `Union` containing a single-letter type.

9. **`getClassString`**
   - Creates a `Union` containing a class string type.
   - Optionally specifies the extends type.

10. **`getLiteralClassString`**
    - Creates a `Union` containing a literal class string type.

11. **`getNull`**
    - Creates a `Union` containing a null type.

12. **`getMixed`**
    - Creates a `Union` containing a mixed type.
    - Optionally marks the type as from a loop `isset`.

13. **`getScalar`**
    - Creates a `Union` containing a scalar type.

14. **`getEmpty`**
    - Creates a `Union` containing an empty type.

15. **`getBool`**
    - Creates a `Union` containing a boolean type.

16. **`getFloat`**
    - Creates a `Union` containing a float type.
    - Optionally specifies a literal value.

17. **`getObject`**"
60,"@@ -31,6 +31,14 @@ import org.apache.iceberg.io.OutputFile;
  */
 public interface EncryptedOutputFile {
 
+  /**
+   * Use flat filestream encryption (default) or pushdown to native format encryption
+   */
+  default boolean useNativeEncryption() {
+    return false;
+  }
+
+
   /**
    * An OutputFile instance that encrypts the bytes that are written to its output streams.
    */","/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

package org.apache.iceberg.encryption;

import org.apache.iceberg.io.OutputFile;

/**
 * Thin wrapper around a {@link OutputFile} that is encrypting bytes written to the underlying
 * file system, via an encryption key that is symbolized by the enclosed
 * {@link EncryptionKeyMetadata}.
 * <p>
 * The {@link EncryptionManager} returns instances of these when passed output files that should
 * be encrypted as they are being written to the backing file system.
  /**
   * Use flat filestream encryption (default) or pushdown to native format encryption
   */
  default boolean useNativeEncryption() {
    return false;
  }


 */
public interface EncryptedOutputFile {

  /**
   * An OutputFile instance that encrypts the bytes that are written to its output streams.
   */
  OutputFile encryptingOutputFile();

  /**
   * Metadata about the encryption key that is being used to encrypt the associated
   * {@link #encryptingOutputFile()}.
   */
  EncryptionKeyMetadata keyMetadata();
}
",Nit: an extra empty line,"The suggested code change introduces a new interface `EncryptedOutputFile` that extends the existing `OutputFile` interface. This new interface provides methods to access the encrypted output file and the metadata about the encryption key being used. The `useNativeEncryption` method is added as a default implementation, which returns `false` by default, indicating that flat filestream encryption is the default behavior. This change enhances the modularity and flexibility of the encryption framework by allowing different types of encryption strategies to be implemented."
61,"@@ -88,7 +88,7 @@ class Uploader {
         this._paused = true
         if (this.tus) {
           const shouldTerminate = !!this.tus.url
-          this.tus.abort(shouldTerminate)
+          this.tus.abort(shouldTerminate).catch(() => {})
         }
         this.cleanUp()
       })","const fs = require('fs')
const path = require('path')
const tus = require('tus-js-client')
const uuid = require('uuid')
const isObject = require('isobject')
const validator = require('validator')
const request = require('request')
const emitter = require('./emitter')
const serializeError = require('serialize-error')
const { jsonStringify, hasMatch } = require('./helpers/utils')
const logger = require('./logger')
const headerSanitize = require('./header-blacklist')
const redis = require('./redis')

const DEFAULT_FIELD_NAME = 'files[]'
const PROTOCOLS = Object.freeze({
  multipart: 'multipart',
  s3Multipart: 's3-multipart',
  tus: 'tus'
})

class Uploader {
  /**
   * Uploads file to destination based on the supplied protocol (tus, s3-multipart, multipart)
   * For tus uploads, the deferredLength option is enabled, because file size value can be unreliable
   * for some providers (Instagram particularly)
   *
   * @typedef {object} UploaderOptions
   * @property {string} endpoint
   * @property {string=} uploadUrl
   * @property {string} protocol
   * @property {number} size
   * @property {string=} fieldname
   * @property {string} pathPrefix
   * @property {any=} s3
   * @property {any} metadata
   * @property {any} companionOptions
   * @property {any=} storage
   * @property {any=} headers
   * @property {string=} httpMethod
   * @property {boolean=} useFormData
   *
   * @param {UploaderOptions} options
   */
  constructor (options) {
    if (!this.validateOptions(options)) {
      logger.debug(this._errRespMessage, 'uploader.validator.fail')
      return
    }

    this.options = options
    this.token = uuid.v4()
    this.path = `${this.options.pathPrefix}/${Uploader.FILE_NAME_PREFIX}-${this.token}`
    this.options.metadata = this.options.metadata || {}
    this.options.fieldname = this.options.fieldname || DEFAULT_FIELD_NAME
    this.uploadFileName = this.options.metadata.name || path.basename(this.path)
    this.streamsEnded = false
    this.uploadStopped = false
    this.duplexStream = null
    // @TODO disabling parallel uploads and downloads for now
    // if (this.options.protocol === PROTOCOLS.tus) {
    //   this.duplexStream = new stream.PassThrough()
    //     .on('error', (err) => logger.error(`${this.shortToken} ${err}`, 'uploader.duplex.error'))
    // }
    this.writeStream = fs.createWriteStream(this.path, { mode: 0o666 }) // no executable files
      .on('error', (err) => logger.error(`${err}`, 'uploader.write.error', this.shortToken))
    /** @type {number} */
    this.emittedProgress = 0
    this.storage = options.storage
    this._paused = false

    if (this.options.protocol === PROTOCOLS.tus) {
      emitter().on(`pause:${this.token}`, () => {
        this._paused = true
        if (this.tus) {
          this.tus.abort()
        }
      })

      emitter().on(`resume:${this.token}`, () => {
        this._paused = false
        if (this.tus) {
          this.tus.start()
        }
      })

      emitter().on(`cancel:${this.token}`, () => {
          this.tus.abort(shouldTerminate).catch(() => {})
        this._paused = true
        if (this.tus) {
          const shouldTerminate = !!this.tus.url
          this.tus.abort(shouldTerminate)
        }
        this.cleanUp()
      })
    }
  }

  /**
   * returns a substring of the token. Used as traceId for logging
   * we avoid using the entire token because this is meant to be a short term
   * access token between uppy client and companion websocket
   * @param {string} token the token to Shorten
   * @returns {string}
   */
  static shortenToken (token) {
    return token.substring(0, 8)
  }

  static reqToOptions (req, size) {
    const useFormDataIsSet = Object.prototype.hasOwnProperty.call(req.body, 'useFormData')
    const useFormData = useFormDataIsSet ? req.body.useFormData : true

    return {
      companionOptions: req.companion.options,
      endpoint: req.body.endpoint,
      uploadUrl: req.body.uploadUrl,
      protocol: req.body.protocol,
      metadata: req.body.metadata,
      httpMethod: req.body.httpMethod,
      useFormData,
      size,
      fieldname: req.body.fieldname,
      pathPrefix: `${req.companion.options.filePath}`,
      storage: redis.client(),
      s3: req.companion.s3Client ? {
        client: req.companion.s3Client,
        options: req.companion.options.providerOptions.s3
      } : null,
      headers: req.body.headers
    }
  }

  /**
   * the number of bytes written into the streams
   */
  get bytesWritten () {
    return this.writeStream.bytesWritten
  }

  /**
   * Validate the options passed down to the uplaoder
   *
   * @param {UploaderOptions} options
   * @returns {boolean}
   */
  validateOptions (options) {
    // validate HTTP Method
    if (options.httpMethod) {
      if (typeof options.httpMethod !== 'string') {
        this._errRespMessage = 'unsupported HTTP METHOD specified'
        return false
      }

      const method = options.httpMethod.toLowerCase()
      if (method !== 'put' && method !== 'post') {
        this._errRespMessage = 'unsupported HTTP METHOD specified'
        return false
      }
    }

    // validate fieldname
    if (options.fieldname && typeof options.fieldname !== 'string') {
      this._errRespMessage = 'fieldname must be a string'
      return false
    }

    // validate metadata
    if (options.metadata && !isObject(options.metadata)) {
      this._errRespMessage = 'metadata must be an object'
      return false
    }

    // validate headers
    if (options.headers && !isObject(options.headers)) {
      this._errRespMessage = 'headers must be an object'
      return false
    }

    // validate protocol
    // @todo this validation should not be conditional once the protocol field is mandatory
    if (options.protocol && !Object.keys(PROTOCOLS).some((key) => PROTOCOLS[key] === options.protocol)) {
      this._errRespMessage = 'unsupported protocol specified'
      return false
    }

    // s3 uploads don't require upload destination
    // validation, because the destination is determined
    // by the server's s3 config
    if (options.protocol === PROTOCOLS.s3Multipart) {
      return true
    }

    if (!options.endpoint && !options.uploadUrl) {
      this._errRespMessage = 'no destination specified'
      return false
    }

    const validatorOpts = { require_protocol: true, require_tld: !options.companionOptions.debug }
    return [options.endpoint, options.uploadUrl].every((url) => {
      if (url && !validator.isURL(url, validatorOpts)) {
        this._errRespMessage = 'invalid destination url'
        return false
      }

      const allowedUrls = options.companionOptions.uploadUrls
      if (allowedUrls && url && !hasMatch(url, allowedUrls)) {
        this._errRespMessage = 'upload destination does not match any allowed destinations'
        return false
      }

      return true
    })
  }

  hasError () {
    return this._errRespMessage != null
  }

  /**
   * returns a substring of the token. Used as traceId for logging
   * we avoid using the entire token because this is meant to be a short term
   * access token between uppy client and companion websocket
   */
  get shortToken () {
    return Uploader.shortenToken(this.token)
  }

  /**
   *
   * @param {function} callback
   */
  onSocketReady (callback) {
    emitter().once(`connection:${this.token}`, () => callback())
    logger.debug('waiting for connection', 'uploader.socket.wait', this.shortToken)
  }

  cleanUp () {
    fs.unlink(this.path, (err) => {
      if (err) {
        logger.error(`cleanup failed for: ${this.path} err: ${err}`, 'uploader.cleanup.error')
      }
    })
    emitter().removeAllListeners(`pause:${this.token}`)
    emitter().removeAllListeners(`resume:${this.token}`)
    emitter().removeAllListeners(`cancel:${this.token}`)
    this.uploadStopped = true
  }

  /**
   *
   * @param {Error} err
   * @param {string | Buffer | Buffer[]} chunk
   */
  handleChunk (err, chunk) {
    if (this.uploadStopped) {
      return
    }

    if (err) {
      logger.error(err, 'uploader.download.error', this.shortToken)
      this.emitError(err)
      this.cleanUp()
      return
    }

    // @todo a default protocol should not be set. We should ensure that the user specifies their protocol.
    const protocol = this.options.protocol || PROTOCOLS.multipart

    // The download has completed; close the file and start an upload if necessary.
    if (chunk === null) {
      this.writeStream.on('finish', () => {
        this.streamsEnded = true
        switch (protocol) {
          case PROTOCOLS.multipart:
            if (this.options.endpoint) {
              this.uploadMultipart()
            }
            break
          case PROTOCOLS.s3Multipart:
            if (!this.s3Upload) {
              this.uploadS3Multipart()
            } else {
              logger.warn('handleChunk() called multiple times', 'uploader.s3.duplicate', this.shortToken)
            }
            break
          case PROTOCOLS.tus:
            if (!this.tus) {
              this.uploadTus()
            } else {
              logger.warn('handleChunk() called multiple times', 'uploader.tus.duplicate', this.shortToken)
            }
            break
        }
      })

      return this.endStreams()
    }

    this.writeStream.write(chunk, () => {
      logger.debug(`${this.bytesWritten} bytes`, 'uploader.download.progress', this.shortToken)
      return this.emitIllusiveProgress()
    })
  }

  /**
   * @param {Buffer | Buffer[]} chunk
   * @param {function} cb
   */
  writeToStreams (chunk, cb) {
    const done = []
    const doneLength = this.duplexStream ? 2 : 1
    const onDone = () => {
      done.push(true)
      if (done.length >= doneLength) {
        cb()
      }
    }

    this.writeStream.write(chunk, onDone)
    if (this.duplexStream) {
      this.duplexStream.write(chunk, onDone)
    }
  }

  endStreams () {
    this.writeStream.end()
    if (this.duplexStream) {
      this.duplexStream.end()
    }
  }

  getResponse () {
    if (this._errRespMessage) {
      return { body: { message: this._errRespMessage }, status: 400 }
    }
    return { body: { token: this.token }, status: 200 }
  }

  /**
   * @typedef {{action: string, payload: object}} State
   * @param {State} state
   */
  saveState (state) {
    if (!this.storage) return
    this.storage.set(`${Uploader.STORAGE_PREFIX}:${this.token}`, jsonStringify(state))
  }

  /**
   * This method emits upload progress but also creates an ""upload progress"" illusion
   * for the waiting period while only download is happening. Hence, it combines both
   * download and upload into an upload progress.
   * @see emitProgress
   * @param {number=} bytesUploaded the bytes actually Uploaded so far
   */
  emitIllusiveProgress (bytesUploaded = 0) {
    if (this._paused) {
      return
    }

    let bytesTotal = this.streamsEnded ? this.bytesWritten : this.options.size
    if (!this.streamsEnded) {
      bytesTotal = Math.max(bytesTotal, this.bytesWritten)
    }
    // for a 10MB file, 10MB of download will account for 5MB upload progress
    // and 10MB of actual upload will account for the other 5MB upload progress.
    const illusiveBytesUploaded = (this.bytesWritten / 2) + (bytesUploaded / 2)

    logger.debug(
      `${bytesUploaded} ${illusiveBytesUploaded} ${bytesTotal}`,
      'uploader.illusive.progress',
      this.shortToken
    )
    this.emitProgress(illusiveBytesUploaded, bytesTotal)
  }

  /**
   *
   * @param {number} bytesUploaded
   * @param {number | null} bytesTotal
   */
  emitProgress (bytesUploaded, bytesTotal) {
    bytesTotal = bytesTotal || this.options.size
    if (this.tus && this.tus.options.uploadLengthDeferred && this.streamsEnded) {
      bytesTotal = this.bytesWritten
    }
    const percentage = (bytesUploaded / bytesTotal * 100)
    const formatPercentage = percentage.toFixed(2)
    logger.debug(
      `${bytesUploaded} ${bytesTotal} ${formatPercentage}%`,
      'uploader.upload.progress',
      this.shortToken
    )

    const dataToEmit = {
      action: 'progress',
      payload: { progress: formatPercentage, bytesUploaded, bytesTotal }
    }
    this.saveState(dataToEmit)

    // avoid flooding the client with progress events.
    const roundedPercentage = Math.floor(percentage)
    if (this.emittedProgress !== roundedPercentage) {
      this.emittedProgress = roundedPercentage
      emitter().emit(this.token, dataToEmit)
    }
  }

  /**
   *
   * @param {string} url
   * @param {object} extraData
   */
  emitSuccess (url, extraData = {}) {
    const emitData = {
      action: 'success',
      payload: Object.assign(extraData, { complete: true, url })
    }
    this.saveState(emitData)
    emitter().emit(this.token, emitData)
  }

  /**
   *
   * @param {Error} err
   * @param {object=} extraData
   */
  emitError (err, extraData = {}) {
    const serializedErr = serializeError(err)
    // delete stack to avoid sending server info to client
    delete serializedErr.stack
    const dataToEmit = {
      action: 'error',
      payload: Object.assign(extraData, { error: serializedErr })
    }
    this.saveState(dataToEmit)
    emitter().emit(this.token, dataToEmit)
  }

  /**
   * start the tus upload
   */
  uploadTus () {
    const file = fs.createReadStream(this.path)
    const uploader = this

    // @ts-ignore
    this.tus = new tus.Upload(file, {
      endpoint: this.options.endpoint,
      uploadUrl: this.options.uploadUrl,
      // @ts-ignore
      uploadLengthDeferred: false,
      resume: true,
      retryDelays: [0, 1000, 3000, 5000],
      uploadSize: this.bytesWritten,
      metadata: Object.assign(
        {
          // file name and type as required by the tusd tus server
          // https://github.com/tus/tusd/blob/5b376141903c1fd64480c06dde3dfe61d191e53d/unrouted_handler.go#L614-L646
          filename: this.uploadFileName,
          filetype: this.options.metadata.type
        }, this.options.metadata
      ),
      /**
       *
       * @param {Error} error
       */
      onError (error) {
        logger.error(error, 'uploader.tus.error')
        uploader.emitError(error)
      },
      /**
       *
       * @param {number} bytesUploaded
       * @param {number} bytesTotal
       */
      onProgress (bytesUploaded, bytesTotal) {
        uploader.emitIllusiveProgress(bytesUploaded)
      },
      onSuccess () {
        uploader.emitSuccess(uploader.tus.url)
        uploader.cleanUp()
      }
    })

    if (!this._paused) {
      this.tus.start()
    }
  }

  uploadMultipart () {
    const file = fs.createReadStream(this.path)

    // upload progress
    let bytesUploaded = 0
    file.on('data', (data) => {
      bytesUploaded += data.length
      this.emitIllusiveProgress(bytesUploaded)
    })

    const httpMethod = (this.options.httpMethod || '').toLowerCase() === 'put' ? 'put' : 'post'
    const headers = headerSanitize(this.options.headers)
    const reqOptions = { url: this.options.endpoint, headers, encoding: null }
    if (this.options.useFormData) {
      reqOptions.formData = Object.assign(
        {},
        this.options.metadata,
        {
          [this.options.fieldname]: {
            value: file,
            options: {
              filename: this.uploadFileName,
              contentType: this.options.metadata.type
            }
          }
        }
      )
    } else {
      reqOptions.body = file
    }

    request[httpMethod](reqOptions, (error, response, body) => {
      if (error) {
        logger.error(error, 'upload.multipart.error')
        this.emitError(error)
        return
      }
      const headers = response.headers
      // remove browser forbidden headers
      delete headers['set-cookie']
      delete headers['set-cookie2']

      const respObj = {
        responseText: body.toString(),
        status: response.statusCode,
        statusText: response.statusMessage,
        headers
      }

      if (response.statusCode >= 400) {
        logger.error(`upload failed with status: ${response.statusCode}`, 'upload.multipart.error')
        this.emitError(new Error(response.statusMessage), respObj)
      } else if (bytesUploaded !== this.bytesWritten && bytesUploaded !== this.options.size) {
        const errMsg = `uploaded only ${bytesUploaded} of ${this.bytesWritten} with status: ${response.statusCode}`
        logger.error(errMsg, 'upload.multipart.mismatch.error')
        this.emitError(new Error(errMsg))
      } else {
        this.emitSuccess(null, { response: respObj })
      }

      this.cleanUp()
    })
  }

  /**
   * Upload the file to S3 using a Multipart upload.
   */
  uploadS3Multipart () {
    const file = fs.createReadStream(this.path)

    return this._uploadS3MultipartStream(file)
  }

  /**
   * Upload a stream to S3.
   */
  _uploadS3MultipartStream (stream) {
    if (!this.options.s3) {
      this.emitError(new Error('The S3 client is not configured on this companion instance.'))
      return
    }

    const filename = this.options.metadata.name || path.basename(this.path)
    const { client, options } = this.options.s3

    const upload = client.upload({
      Bucket: options.bucket,
      Key: options.getKey(null, filename, this.options.metadata),
      ACL: options.acl,
      ContentType: this.options.metadata.type,
      Body: stream
    })

    this.s3Upload = upload

    upload.on('httpUploadProgress', ({ loaded, total }) => {
      this.emitProgress(loaded, total)
    })

    upload.send((error, data) => {
      this.s3Upload = null
      if (error) {
        this.emitError(error)
      } else {
        const url = data && data.Location ? data.Location : null
        this.emitSuccess(url, {
          response: {
            responseText: JSON.stringify(data),
            headers: {
              'content-type': 'application/json'
            }
          }
        })
      }
      this.cleanUp()
    })
  }
}

Uploader.FILE_NAME_PREFIX = 'uppy-file'
Uploader.STORAGE_PREFIX = 'companion'

module.exports = Uploader
","The `abort` function now returns a Promise. And if `shouldTerminate` is true, this Promise can be rejected. So, to avoid a unhandled promise rejection, I added a dummy handler. Since a possible error also wasn't handled before, I think it's OK to not handle it in this case as well.","This code defines a class `Uploader` that handles file uploads to various protocols such as Tus, S3 multipart, and multipart. It includes methods for validating options, handling chunks, emitting progress, and managing the lifecycle of the upload process. The class uses several external libraries like `fs`, `path`, `tus-js-client`, `uuid`, `isobject`, `validator`, `request`, `emitter`, `serialize-error`, and `helpers/utils`. The code is well-structured and follows best practices for error handling and logging."
62,"@@ -31,6 +31,12 @@ public class RandomUtil {
   private RandomUtil() {
   }
 
+  private static final Random NEGATIVE_VALUES = new Random();
+
+  private static boolean negate() {
+    return NEGATIVE_VALUES.nextInt(2) == 1;
+  }
+
   @SuppressWarnings(""RandomModInteger"")
   public static Object generatePrimitive(Type.PrimitiveType primitive,
                                          Random random) {","/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

package org.apache.iceberg.util;

import java.math.BigDecimal;
import java.math.BigInteger;
import java.util.Arrays;
import java.util.Random;
import org.apache.iceberg.types.Type;
import org.apache.iceberg.types.Types;

public class RandomUtil {

  private static final Random NEGATIVE_VALUES = new Random();

  private static boolean negate() {
    return NEGATIVE_VALUES.nextInt(2) == 1;
  }

  private RandomUtil() {
  }

  @SuppressWarnings(""RandomModInteger"")
  public static Object generatePrimitive(Type.PrimitiveType primitive,
                                         Random random) {
    int choice = random.nextInt(20);

    switch (primitive.typeId()) {
      case BOOLEAN:
        return choice < 10;

      case INTEGER:
        switch (choice) {
          case 1:
            return Integer.MIN_VALUE;
          case 2:
            return Integer.MAX_VALUE;
          case 3:
            return 0;
          default:
            return random.nextInt();
        }

      case LONG:
        switch (choice) {
          case 1:
            return Long.MIN_VALUE;
          case 2:
            return Long.MAX_VALUE;
          case 3:
            return 0L;
          default:
            return random.nextLong();
        }

      case FLOAT:
        switch (choice) {
          case 1:
            return Float.MIN_VALUE;
          case 2:
            return -Float.MIN_VALUE;
          case 3:
            return Float.MAX_VALUE;
          case 4:
            return -Float.MAX_VALUE;
          case 5:
            return Float.NEGATIVE_INFINITY;
          case 6:
            return Float.POSITIVE_INFINITY;
          case 7:
            return 0.0F;
          case 8:
            return Float.NaN;
          default:
            return random.nextFloat();
        }

      case DOUBLE:
        switch (choice) {
          case 1:
            return Double.MIN_VALUE;
          case 2:
            return -Double.MIN_VALUE;
          case 3:
            return Double.MAX_VALUE;
          case 4:
            return -Double.MAX_VALUE;
          case 5:
            return Double.NEGATIVE_INFINITY;
          case 6:
            return Double.POSITIVE_INFINITY;
          case 7:
            return 0.0D;
          case 8:
            return Double.NaN;
          default:
            return random.nextDouble();
        }

      case DATE:
        // this will include negative values (dates before 1970-01-01)
        return random.nextInt() % ABOUT_380_YEARS_IN_DAYS;

      case TIME:
        return (random.nextLong() & Integer.MAX_VALUE) % ONE_DAY_IN_MICROS;

      case TIMESTAMP:
        return random.nextLong() % FIFTY_YEARS_IN_MICROS;

      case STRING:
        return randomString(random);

      case UUID:
        byte[] uuidBytes = new byte[16];
        random.nextBytes(uuidBytes);
        // this will hash the uuidBytes
        return uuidBytes;

      case FIXED:
        byte[] fixed = new byte[((Types.FixedType) primitive).length()];
        random.nextBytes(fixed);
        return fixed;

      case BINARY:
        byte[] binary = new byte[random.nextInt(50)];
        random.nextBytes(binary);
        return binary;

      case DECIMAL:
        Types.DecimalType type = (Types.DecimalType) primitive;
        BigInteger unscaled = randomUnscaled(type.precision(), random);
        return new BigDecimal(unscaled, type.scale());

      default:
        throw new IllegalArgumentException(
            ""Cannot generate random value for unknown type: "" + primitive);
    }
  }

  public static Object generateDictionaryEncodablePrimitive(Type.PrimitiveType primitive, Random random) {
    int value = random.nextInt(3);
    switch (primitive.typeId()) {
      case BOOLEAN:
        return true; // doesn't really matter for booleans since they are not dictionary encoded
      case INTEGER:
      case DATE:
        return value;
      case FLOAT:
        return (float) value;
      case DOUBLE:
        return (double) value;
      case LONG:
      case TIME:
      case TIMESTAMP:
        return (long) value;
      case STRING:
        return String.valueOf(value);
      case FIXED:
        byte[] fixed = new byte[((Types.FixedType) primitive).length()];
        Arrays.fill(fixed, (byte) value);
        return fixed;
      case BINARY:
        byte[] binary = new byte[value + 1];
        Arrays.fill(binary, (byte) value);
        return binary;
      case DECIMAL:
        Types.DecimalType type = (Types.DecimalType) primitive;
        BigInteger unscaled = new BigInteger(String.valueOf(value + 1));
        return new BigDecimal(unscaled, type.scale());
      default:
        throw new IllegalArgumentException(
            ""Cannot generate random value for unknown type: "" + primitive);
    }
  }

  private static final long FIFTY_YEARS_IN_MICROS =
      (50L * (365 * 3 + 366) * 24 * 60 * 60 * 1_000_000) / 4;
  private static final int ABOUT_380_YEARS_IN_DAYS = 380 * 365;
  private static final long ONE_DAY_IN_MICROS = 24 * 60 * 60 * 1_000_000L;
  private static final String CHARS =
      ""abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_-.!?"";

  private static String randomString(Random random) {
    int length = random.nextInt(50);
    byte[] buffer = new byte[length];

    for (int i = 0; i < length; i += 1) {
      buffer[i] = (byte) CHARS.charAt(random.nextInt(CHARS.length()));
    }

    return new String(buffer);
  }

  private static final String DIGITS = ""0123456789"";

  private static BigInteger randomUnscaled(int precision, Random random) {
    int length = random.nextInt(precision);
    if (length == 0) {
      return BigInteger.ZERO;
    }

    StringBuilder sb = new StringBuilder();
    for (int i = 0; i < length; i += 1) {
      sb.append(DIGITS.charAt(random.nextInt(DIGITS.length())));
    }

    return new BigInteger(sb.toString());
  }
}
","All tests need to use the `Random` that is passed in so that the values that are generated are deterministic and repeatable. That's what allows us to generated an iterator instead of a list in some cases, and makes it so if we do have a problem, we can repeat the test and debug.","The suggested code change appears to be a utility class for generating random values for various data types used in Apache Iceberg. Here's a concise review:

### Key Points:
1. **License and Package Declaration**: The code includes a license notice and package declaration, which is good practice.

2. **Utility Methods**:
   - `generatePrimitive`: Generates random values for all primitive types, including handling special cases like negative values for integers and dates.
   - `generateDictionaryEncodablePrimitive`: Generates random values for dictionary-encoded primitive types, which are typically used in columnar storage formats.
   - `randomString`: Generates random strings of varying lengths.
   - `randomUnscaled`: Generates random unscaled decimal values based on the specified precision.

3. **Constants**:
   - Constants like `FIFTY_YEARS_IN_MICROS`, `ABOUT_380_YEARS_IN_DAYS`, and `ONE_DAY_IN_MICROS` are used to generate date and time values.
   - `CHARS` and `DIGITS` are used to generate random strings and numbers, respectively.

4. **Error Handling**:
   - The `generatePrimitive` method throws an `IllegalArgumentException` if an unknown type is encountered.

5. **Code Style**:
   - The code is well-structured with clear method names and comments explaining the purpose of each method.
   - The use of `@SuppressWarnings(""RandomModInteger"")` is appropriate for the `nextInt()` method, as it is safe to use modulo operations with `Integer.MAX_VALUE`.

### Potential Improvements:
1. **Random Seed Management**: Consider adding a way to set a seed for the random number generator to ensure reproducibility.
2. **Performance Optimization**: For very large datasets, consider optimizing the generation of large binary and string values.
3. **Edge Case Handling**: Ensure that edge cases, such as very large or very small numbers, are handled correctly.

Overall, the code is well-designed and should work effectively for generating random values for Iceberg types."
63,"@@ -255,6 +255,12 @@ public class MethodCallExprContext extends AbstractJavaParserContext<MethodCallE
             mappings.put(target.asTypeParameter(), source);
             return;
         }
+        if (source.isArray() && target.isArray()) {
+            ResolvedType sourceComponentType = source.asArrayType().getComponentType();
+            ResolvedType targetComponentType = target.asArrayType().getComponentType();
+            inferTypes(sourceComponentType, targetComponentType, mappings);
+            return;
+        }
         if (source.isArray() && target.isWildcard()){
             if(target.asWildcard().isBounded()){
                 inferTypes(source, target.asWildcard().getBoundedType(), mappings);","/*
 * Copyright 2016 Federico Tomassetti
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.github.javaparser.symbolsolver.javaparsermodel.contexts;

import com.github.javaparser.ast.expr.Expression;
import com.github.javaparser.ast.expr.MethodCallExpr;
import com.github.javaparser.ast.expr.NameExpr;
import com.github.javaparser.resolution.MethodUsage;
import com.github.javaparser.resolution.UnsolvedSymbolException;
import com.github.javaparser.resolution.declarations.*;
import com.github.javaparser.resolution.types.*;
import com.github.javaparser.symbolsolver.core.resolution.Context;
import com.github.javaparser.symbolsolver.javaparsermodel.JavaParserFacade;
import com.github.javaparser.symbolsolver.model.resolution.SymbolReference;
import com.github.javaparser.symbolsolver.model.resolution.TypeSolver;
import com.github.javaparser.symbolsolver.model.resolution.Value;
import com.github.javaparser.symbolsolver.model.typesystem.*;
import com.github.javaparser.symbolsolver.reflectionmodel.ReflectionClassDeclaration;
import com.github.javaparser.symbolsolver.resolution.MethodResolutionLogic;
import com.github.javaparser.utils.Pair;

import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Optional;

public class MethodCallExprContext extends AbstractJavaParserContext<MethodCallExpr> {

    ///
    /// Constructors
    ///

    public MethodCallExprContext(MethodCallExpr wrappedNode, TypeSolver typeSolver) {
        super(wrappedNode, typeSolver);
    }

    ///
    /// Public methods
    ///

    @Override
    public Optional<ResolvedType> solveGenericType(String name) {
        if(wrappedNode.getScope().isPresent()){
            ResolvedType typeOfScope = JavaParserFacade.get(typeSolver).getType(wrappedNode.getScope().get());
            Optional<ResolvedType> res = typeOfScope.asReferenceType().getGenericParameterByName(name);
            return res;
        } else{
            return Optional.empty();
        }
    }

    @Override
    public String toString() {
        return ""MethodCallExprContext{wrapped="" + wrappedNode + ""}"";
    }

    @Override
    public Optional<MethodUsage> solveMethodAsUsage(String name, List<ResolvedType> argumentsTypes) {
        if (wrappedNode.getScope().isPresent()) {
            Expression scope = wrappedNode.getScope().get();
            // Consider static method calls
            if (scope instanceof NameExpr) {
                String className = ((NameExpr) scope).getName().getId();
                SymbolReference<ResolvedTypeDeclaration> ref = solveType(className);
                if (ref.isSolved()) {
                    SymbolReference<ResolvedMethodDeclaration> m = MethodResolutionLogic.solveMethodInType(ref.getCorrespondingDeclaration(), name, argumentsTypes);
                    if (m.isSolved()) {
                        MethodUsage methodUsage = new MethodUsage(m.getCorrespondingDeclaration());
                        methodUsage = resolveMethodTypeParametersFromExplicitList(typeSolver, methodUsage);
                        methodUsage = resolveMethodTypeParameters(methodUsage, argumentsTypes);
                        return Optional.of(methodUsage);
                    } else {
                        throw new UnsolvedSymbolException(ref.getCorrespondingDeclaration().toString(),
                                ""Method '"" + name + ""' with parameterTypes "" + argumentsTypes);
                    }
                }
            }

            ResolvedType typeOfScope = JavaParserFacade.get(typeSolver).getType(scope);
            // we can replace the parameter types from the scope into the typeParametersValues

            Map<ResolvedTypeParameterDeclaration, ResolvedType> inferredTypes = new HashMap<>();
            for (int i = 0; i < argumentsTypes.size(); i++) {
                // by replacing types I can also find new equivalences
                // for example if I replace T=U with String because I know that T=String I can derive that also U equal String
                ResolvedType originalArgumentType = argumentsTypes.get(i);
                ResolvedType updatedArgumentType = usingParameterTypesFromScope(typeOfScope, originalArgumentType, inferredTypes);
                argumentsTypes.set(i, updatedArgumentType);
            }
            for (int i = 0; i < argumentsTypes.size(); i++) {
                ResolvedType updatedArgumentType = applyInferredTypes(argumentsTypes.get(i), inferredTypes);
                argumentsTypes.set(i, updatedArgumentType);
            }

            return solveMethodAsUsage(typeOfScope, name, argumentsTypes, this);
        } else {
            Context parentContext = getParent();
            while (parentContext instanceof MethodCallExprContext || parentContext instanceof ObjectCreationContext) {
                parentContext = parentContext.getParent();
            }
            return parentContext.solveMethodAsUsage(name, argumentsTypes);
        }
    }

    private MethodUsage resolveMethodTypeParametersFromExplicitList(TypeSolver typeSolver, MethodUsage methodUsage) {
        if (wrappedNode.getTypeArguments().isPresent()) {
            final List<ResolvedType> typeArguments = new ArrayList<>();
            for (com.github.javaparser.ast.type.Type ty : wrappedNode.getTypeArguments().get()) {
                typeArguments.add(JavaParserFacade.get(typeSolver).convertToUsage(ty));
            }

            List<ResolvedTypeParameterDeclaration> tyParamDecls = methodUsage.getDeclaration().getTypeParameters();
            if (tyParamDecls.size() == typeArguments.size()) {
                for (int i = 0; i < tyParamDecls.size(); i++) {
                    methodUsage = methodUsage.replaceTypeParameter(tyParamDecls.get(i), typeArguments.get(i));
                }
            }
        }

        return methodUsage;
    }

    @Override
    public SymbolReference<? extends ResolvedValueDeclaration> solveSymbol(String name) {
        return getParent().solveSymbol(name);
    }

    @Override
    public Optional<Value> solveSymbolAsValue(String name) {
        Context parentContext = getParent();
        return parentContext.solveSymbolAsValue(name);
    }

    @Override
    public SymbolReference<ResolvedMethodDeclaration> solveMethod(String name, List<ResolvedType> argumentsTypes, boolean staticOnly) {
        Collection<ResolvedReferenceTypeDeclaration> rrtds = findTypeDeclarations(wrappedNode.getScope());
        for (ResolvedReferenceTypeDeclaration rrtd : rrtds) {
            SymbolReference<ResolvedMethodDeclaration> res = MethodResolutionLogic.solveMethodInType(rrtd, name, argumentsTypes, false);
            if (res.isSolved()) {
                return res;
            }
        }
        return SymbolReference.unsolved(ResolvedMethodDeclaration.class);
    }

    ///
    /// Private methods
    ///

    private Optional<MethodUsage> solveMethodAsUsage(ResolvedReferenceType refType, String name,
                                                     List<ResolvedType> argumentsTypes,
                                                     Context invokationContext) {
        Optional<MethodUsage> ref = ContextHelper.solveMethodAsUsage(refType.getTypeDeclaration(), name, argumentsTypes, invokationContext, refType.typeParametersValues());
        if (ref.isPresent()) {
            MethodUsage methodUsage = ref.get();

            methodUsage = resolveMethodTypeParametersFromExplicitList(typeSolver, methodUsage);

            // At this stage I should derive from the context and the value some information on the type parameters
            // for example, when calling:
            // myStream.collect(Collectors.toList())
            // I should be able to figure out that considering the type of the stream (e.g., Stream<String>)
            // and considering that Stream has this method:
            //
            // <R,A> R collect(Collector<? super T,A,R> collector)
            //
            // and collector has this method:
            //
            // static <T> Collector<T,?,List<T>>   toList()
            //
            // In this case collect.R has to be equal to List<toList.T>
            // And toList.T has to be equal to ? super Stream.T
            // Therefore R has to be equal to List<? super Stream.T>.
            // In our example Stream.T equal to String, so the R (and the result of the call to collect) is
            // List<? super String>

            Map<ResolvedTypeParameterDeclaration, ResolvedType> derivedValues = new HashMap<>();
            for (int i = 0; i < methodUsage.getParamTypes().size(); i++) {
                ResolvedParameterDeclaration parameter = methodUsage.getDeclaration().getParam(i);
                ResolvedType parameterType = parameter.getType();
                if (parameter.isVariadic()) {
                	parameterType = parameterType.asArrayType().getComponentType();
                }
                inferTypes(argumentsTypes.get(i), parameterType, derivedValues);
            }

            for (Map.Entry<ResolvedTypeParameterDeclaration, ResolvedType> entry : derivedValues.entrySet()){
                methodUsage = methodUsage.replaceTypeParameter(entry.getKey(), entry.getValue());
            }

            ResolvedType returnType = refType.useThisTypeParametersOnTheGivenType(methodUsage.returnType());
            if (returnType != methodUsage.returnType()) {
                methodUsage = methodUsage.replaceReturnType(returnType);
            }
            for (int i = 0; i < methodUsage.getParamTypes().size(); i++) {
                ResolvedType replaced = refType.useThisTypeParametersOnTheGivenType(methodUsage.getParamTypes().get(i));
                methodUsage = methodUsage.replaceParamType(i, replaced);
            }
            return Optional.of(methodUsage);
        } else {
            return ref;
        }
    }

    private void inferTypes(ResolvedType source, ResolvedType target, Map<ResolvedTypeParameterDeclaration, ResolvedType> mappings) {
        if (source.equals(target)) {
            return;
        }
        if (source.isReferenceType() && target.isReferenceType()) {
            ResolvedReferenceType sourceRefType = source.asReferenceType();
            ResolvedReferenceType targetRefType = target.asReferenceType();
            if (sourceRefType.getQualifiedName().equals(targetRefType.getQualifiedName())) {
            	if (!sourceRefType.isRawType() && !targetRefType.isRawType()) {
	                for (int i = 0; i < sourceRefType.typeParametersValues().size(); i++) {
	                    inferTypes(sourceRefType.typeParametersValues().get(i), targetRefType.typeParametersValues().get(i), mappings);
	                }
            	}
            }
            return;
        }
        if (source.isReferenceType() && target.isWildcard()) {
            if (target.asWildcard().isBounded()) {
                inferTypes(source, target.asWildcard().getBoundedType(), mappings);
                return;
            }
            return;
        }
        if (source.isWildcard() && target.isWildcard()) {
            if (source.asWildcard().isBounded() && target.asWildcard().isBounded()){
                inferTypes(source.asWildcard().getBoundedType(), target.asWildcard().getBoundedType(), mappings);
            }
            return;
        }
        if (source.isReferenceType() && target.isTypeVariable()) {
            mappings.put(target.asTypeParameter(), source);
            return;
        }
        if (source.isWildcard() && target.isTypeVariable()) {
        if (source.isArray() && target.isArray()) {
            ResolvedType sourceComponentType = source.asArrayType().getComponentType();
            ResolvedType targetComponentType = target.asArrayType().getComponentType();
            inferTypes(sourceComponentType, targetComponentType, mappings);
            return;
        }
            mappings.put(target.asTypeParameter(), source);
            return;
        }
        if (source.isArray() && target.isWildcard()){
            if(target.asWildcard().isBounded()){
                inferTypes(source, target.asWildcard().getBoundedType(), mappings);
                return;
            }
            return;
        }
        if (source.isArray() && target.isTypeVariable()) {
            mappings.put(target.asTypeParameter(), source);
            return;
        }

        if (source.isWildcard() && target.isReferenceType()){
            if (source.asWildcard().isBounded()){
                inferTypes(source.asWildcard().getBoundedType(), target, mappings);
            }
            return;
        }
        if (source.isConstraint() && target.isReferenceType()){
            inferTypes(source.asConstraintType().getBound(), target, mappings);
            return;
        }

        if (source.isConstraint() && target.isTypeVariable()){
            inferTypes(source.asConstraintType().getBound(), target, mappings);
            return;
        }
        if (source.isTypeVariable() && target.isTypeVariable()) {
            mappings.put(target.asTypeParameter(), source);
            return;
        }
        if (source.isPrimitive() || target.isPrimitive()) {
            return;
        }
        if (source.isNull()) {
            return;
        }
        throw new RuntimeException(source.describe() + "" "" + target.describe());
    }

    private MethodUsage resolveMethodTypeParameters(MethodUsage methodUsage, List<ResolvedType> actualParamTypes) {
        Map<ResolvedTypeParameterDeclaration, ResolvedType> matchedTypeParameters = new HashMap<>();

        if (methodUsage.getDeclaration().hasVariadicParameter()) {
            if (actualParamTypes.size() == methodUsage.getDeclaration().getNumberOfParams()) {
                // the varargs parameter is an Array, so extract the inner type
                ResolvedType expectedType =
                    methodUsage.getDeclaration().getLastParam().getType().asArrayType().getComponentType();
                // the varargs corresponding type can be either T or Array<T>
                ResolvedType actualType =
                    actualParamTypes.get(actualParamTypes.size() - 1).isArray() ?
                        actualParamTypes.get(actualParamTypes.size() - 1).asArrayType().getComponentType() :
                        actualParamTypes.get(actualParamTypes.size() - 1);
                if (!expectedType.isAssignableBy(actualType)) {
                    for (ResolvedTypeParameterDeclaration tp : methodUsage.getDeclaration().getTypeParameters()) {
                        expectedType = MethodResolutionLogic.replaceTypeParam(expectedType, tp, typeSolver);
                    }
                }
                if (!expectedType.isAssignableBy(actualType)) {
                    // ok, then it needs to be wrapped
                    throw new UnsupportedOperationException(
                        String.format(""Unable to resolve the type typeParametersValues in a MethodUsage. Expected type: %s, Actual type: %s. Method Declaration: %s. MethodUsage: %s"",
                                      expectedType,
                                      actualType,
                                      methodUsage.getDeclaration(),
                                      methodUsage));
                }
                // match only the varargs type
                matchTypeParameters(expectedType, actualType, matchedTypeParameters);
            } else {
                return methodUsage;
            }
        }

        int until = methodUsage.getDeclaration().hasVariadicParameter() ?
            actualParamTypes.size() - 1 :
            actualParamTypes.size();

        for (int i = 0; i < until; i++) {
            ResolvedType expectedType = methodUsage.getParamType(i);
            ResolvedType actualType = actualParamTypes.get(i);
            matchTypeParameters(expectedType, actualType, matchedTypeParameters);
        }
        for (ResolvedTypeParameterDeclaration tp : matchedTypeParameters.keySet()) {
            methodUsage = methodUsage.replaceTypeParameter(tp, matchedTypeParameters.get(tp));
        }
        return methodUsage;
    }

    private void matchTypeParameters(ResolvedType expectedType, ResolvedType actualType, Map<ResolvedTypeParameterDeclaration, ResolvedType> matchedTypeParameters) {
        if (expectedType.isTypeVariable()) {
            if (!actualType.isTypeVariable() && !actualType.isReferenceType()) {
                throw new UnsupportedOperationException(actualType.getClass().getCanonicalName());
            }
            matchedTypeParameters.put(expectedType.asTypeParameter(), actualType);
        } else if (expectedType.isArray()) {
            if (!actualType.isArray()) {
                throw new UnsupportedOperationException(actualType.getClass().getCanonicalName());
            }
            matchTypeParameters(
                    expectedType.asArrayType().getComponentType(),
                    actualType.asArrayType().getComponentType(),
                    matchedTypeParameters);
        } else if (expectedType.isReferenceType()) {
            // avoid cases where the actual type has no type parameters but the expected one has. Such as: ""classX extends classY<Integer>""
            if (actualType.isReferenceType() && actualType.asReferenceType().typeParametersValues().size() > 0) {
                int i = 0;
                for (ResolvedType tp : expectedType.asReferenceType().typeParametersValues()) {
                    matchTypeParameters(tp, actualType.asReferenceType().typeParametersValues().get(i), matchedTypeParameters);
                    i++;
                }
            }
        } else if (expectedType.isPrimitive()) {
            // nothing to do
        } else if (expectedType.isWildcard()) {
            // nothing to do
        } else {
            throw new UnsupportedOperationException(expectedType.getClass().getCanonicalName());
        }
    }

    private Optional<MethodUsage> solveMethodAsUsage(ResolvedTypeVariable tp, String name, List<ResolvedType> argumentsTypes, Context invokationContext) {
        for (ResolvedTypeParameterDeclaration.Bound bound : tp.asTypeParameter().getBounds()) {
            Optional<MethodUsage> methodUsage = solveMethodAsUsage(bound.getType(), name, argumentsTypes, invokationContext);
            if (methodUsage.isPresent()) {
                return methodUsage;
            }
        }
        return Optional.empty();
    }

    private Optional<MethodUsage> solveMethodAsUsage(ResolvedType type, String name, List<ResolvedType> argumentsTypes, Context invokationContext) {
        if (type instanceof ResolvedReferenceType) {
            return solveMethodAsUsage((ResolvedReferenceType) type, name, argumentsTypes, invokationContext);
        } else if (type instanceof ResolvedTypeVariable) {
            return solveMethodAsUsage((ResolvedTypeVariable) type, name, argumentsTypes, invokationContext);
        } else if (type instanceof ResolvedWildcard) {
            ResolvedWildcard wildcardUsage = (ResolvedWildcard) type;
            if (wildcardUsage.isSuper()) {
                return solveMethodAsUsage(wildcardUsage.getBoundedType(), name, argumentsTypes, invokationContext);
            } else if (wildcardUsage.isExtends()) {
                throw new UnsupportedOperationException(""extends wildcard"");
            } else {
                throw new UnsupportedOperationException(""unbounded wildcard"");
            }
        } else if (type instanceof ResolvedLambdaConstraintType){
            ResolvedLambdaConstraintType constraintType = (ResolvedLambdaConstraintType) type;
            return solveMethodAsUsage(constraintType.getBound(), name, argumentsTypes, invokationContext);
        } else if (type instanceof ResolvedArrayType) {
            // An array inherits methods from Object not from it's component type
            return solveMethodAsUsage(new ReferenceTypeImpl(new ReflectionClassDeclaration(Object.class, typeSolver), typeSolver), name, argumentsTypes, invokationContext);
        } else if (type instanceof ResolvedUnionType) {
            Optional<ResolvedReferenceType> commonAncestor = type.asUnionType().getCommonAncestor();
            if (commonAncestor.isPresent()) {
                return solveMethodAsUsage(commonAncestor.get(), name, argumentsTypes, invokationContext);
            } else {
                throw new UnsupportedOperationException(""no common ancestor available for "" + type.describe());
            }
        } else {
            throw new UnsupportedOperationException(""type usage: "" + type.getClass().getCanonicalName());
        }
    }

    private ResolvedType usingParameterTypesFromScope(ResolvedType scope, ResolvedType type, Map<ResolvedTypeParameterDeclaration, ResolvedType> inferredTypes) {
        if (type.isReferenceType()) {
            for (Pair<ResolvedTypeParameterDeclaration, ResolvedType> entry : type.asReferenceType().getTypeParametersMap()) {
                if (entry.a.declaredOnType() && scope.asReferenceType().getGenericParameterByName(entry.a.getName()).isPresent()) {
                    type = type.replaceTypeVariables(entry.a, scope.asReferenceType().getGenericParameterByName(entry.a.getName()).get(), inferredTypes);
                }
            }
            return type;
        } else {
            return type;
        }
    }

    private ResolvedType applyInferredTypes(ResolvedType type, Map<ResolvedTypeParameterDeclaration, ResolvedType> inferredTypes) {
        for (ResolvedTypeParameterDeclaration tp : inferredTypes.keySet()) {
            type = type.replaceTypeVariables(tp, inferredTypes.get(tp), inferredTypes);
        }
        return type;
    }
}
","The component type for `int[][][]` is `int[][]`, and its element type is `int`. Does this code work for multidimensional arrays too?","The suggested code change appears to be an implementation of a `MethodCallExprContext` class within the JavaParser library, which is used for resolving symbols and types in Java code. The class extends `AbstractJavaParserContext<MethodCallExpr>` and provides methods for solving generic types, methods, and symbols.

Here's a concise review of the suggested code change:

### Key Points:

1. **Generic Type Resolution**: 
   - The `solveGenericType` method attempts to resolve a generic type by examining the scope of the method call expression. If the scope is present, it retrieves the type of the scope and attempts to find a generic parameter by name.

2. **Method Resolution**:
   - The `solveMethodAsUsage` method is responsible for resolving a method call. It considers both static and instance methods, and it uses the scope of the method call to infer type parameters.
   - The method uses `MethodResolutionLogic` to find the method declaration and then resolves the type parameters based on the provided arguments.

3. **Type Parameter Matching**:
   - The `matchTypeParameters` method is used to match expected type parameters with actual types. It handles various cases such as reference types, wildcards, and arrays.

4. **Inference Logic**:
   - The `inferTypes` method is used to infer type parameters based on the scope and actual argument types. It recursively matches type parameters and handles different types like reference types, wildcards, and arrays.

5. **Error Handling**:
   - The code includes error handling for unsupported types and scenarios where type resolution fails.

### Potential Improvements:

1. **Performance Optimization**:
   - The code could benefit from optimizations, such as caching results of expensive operations or using more efficient data structures for type parameter matching.

2. **Code Clarity**:
   - Some parts of the code could be made more readable by breaking down complex logic into smaller, more manageable functions.

3. **Testing**:
   - Additional testing should be conducted to ensure that the method resolution logic works correctly across various scenarios, including nested generics and complex type hierarchies.

4. **Documentation**:
   - The code should include more detailed comments and documentation to explain the purpose and functionality of each method and class.

Overall, the code provides a robust implementation for resolving method calls and types in Java code using JavaParser."
64,"@@ -306,7 +306,7 @@ def extract_record_set(records, filters, sorting,
     paginated = {}
     for rule in pagination_rules or []:
         values = list(apply_filters(filtered, rule))
-        paginated.update(dict(((x[id_field], x) for x in values)))
+        paginated.update(dict(((x.get(id_field), x) for x in values)))
 
     if paginated:
         paginated = paginated.values()","import re
import operator
from collections import defaultdict
from collections import abc
import numbers

from kinto.core import utils
from kinto.core.decorators import synchronized
from kinto.core.storage import (
    StorageBase, exceptions,
    DEFAULT_ID_FIELD, DEFAULT_MODIFIED_FIELD, DEFAULT_DELETED_FIELD,
    MISSING)
from kinto.core.utils import (COMPARISON, find_nested_value)

import ujson


def tree():
    return defaultdict(tree)


class MemoryBasedStorage(StorageBase):
    """"""Abstract storage class, providing basic operations and
    methods for in-memory implementations of sorting and filtering.
    """"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def initialize_schema(self, dry_run=False):
        # Nothing to do.
        pass

    def strip_deleted_record(self, collection_id, parent_id, record,
                             id_field=DEFAULT_ID_FIELD,
                             modified_field=DEFAULT_MODIFIED_FIELD,
                             deleted_field=DEFAULT_DELETED_FIELD):
        """"""Strip the record of all its fields expect id and timestamp,
        and set the deletion field value (e.g deleted=True)
        """"""
        deleted = {}
        deleted[id_field] = record[id_field]
        deleted[modified_field] = record[modified_field]
        deleted[deleted_field] = True
        return deleted

    def set_record_timestamp(self, collection_id, parent_id, record,
                             modified_field=DEFAULT_MODIFIED_FIELD,
                             last_modified=None):
        timestamp = self._bump_timestamp(collection_id, parent_id, record,
                                         modified_field,
                                         last_modified=last_modified)
        record[modified_field] = timestamp
        return record

    def extract_record_set(self, records,
                           filters, sorting, id_field, deleted_field,
                           pagination_rules=None, limit=None):
        """"""Take the list of records and handle filtering, sorting and
        pagination.

        """"""
        return extract_record_set(records,
                                  filters=filters,
                                  sorting=sorting,
                                  id_field=id_field,
                                  deleted_field=deleted_field,
                                  pagination_rules=pagination_rules,
                                  limit=limit)


class Storage(MemoryBasedStorage):
    """"""Storage backend implementation in memory.

    Useful for development or testing purposes, but records are lost after
    each server restart.

    Enable in configuration::

        kinto.storage_backend = kinto.core.storage.memory

    Enable strict json validation before saving (instead of the more lenient
    ujson, see #1238)::

        kinto.storage_strict_json = true
    """"""
    def __init__(self, *args, readonly=False, **kwargs):
        super().__init__(*args, **kwargs)
        self.readonly = readonly
        self.flush()

    def flush(self, auth=None):
        self._store = tree()
        self._cemetery = tree()
        self._timestamps = defaultdict(dict)

    @synchronized
    def collection_timestamp(self, collection_id, parent_id, auth=None):
        ts = self._timestamps[parent_id].get(collection_id)
        if ts is not None:
            return ts
        if self.readonly:
            error_msg = 'Cannot initialize empty collection timestamp when running in readonly.'
            raise exceptions.BackendError(message=error_msg)
        return self._bump_timestamp(collection_id, parent_id)

    def _bump_timestamp(self, collection_id, parent_id, record=None,
                        modified_field=None, last_modified=None):
        """"""Timestamp are base on current millisecond.

        .. note ::

            Here it is assumed that if requests from the same user burst in,
            the time will slide into the future. It is not problematic since
            the timestamp notion is opaque, and behaves like a revision number.
        """"""
        # XXX factorize code from memory and redis backends.
        is_specified = (record is not None and
                        modified_field in record or
                        last_modified is not None)

        collection_timestamp = self._timestamps[parent_id].get(collection_id, 0)

        if is_specified:
            # If there is a timestamp in the new record, try to use it.
            if last_modified is not None:
                current = last_modified
            else:
                current = record[modified_field]
            # If it is equal to current collection timestamp, bump it.
            if current == collection_timestamp:
                collection_timestamp += 1
                current = collection_timestamp
            # If it is superior (future), use it as new collection timestamp.
            elif current > collection_timestamp:
                collection_timestamp = current
            # Else (past), do nothing.

        else:
            # Not specified, use a new one.
            current = utils.msec_time()
            # If two ops in the same msec, bump it.
            if current <= collection_timestamp:
                current = collection_timestamp + 1
            collection_timestamp = current

        self._timestamps[parent_id][collection_id] = collection_timestamp

        return current

    @synchronized
    def create(self, collection_id, parent_id, record, id_generator=None,
               id_field=DEFAULT_ID_FIELD,
               modified_field=DEFAULT_MODIFIED_FIELD, auth=None):
        id_generator = id_generator or self.id_generator
        record = {**record}
        if id_field in record:
            # Raise unicity error if record with same id already exists.
            try:
                existing = self.get(collection_id, parent_id, record[id_field])
                raise exceptions.UnicityError(id_field, existing)
            except exceptions.RecordNotFoundError:
                pass
        else:
            record[id_field] = id_generator()

        self.set_record_timestamp(collection_id, parent_id, record,
                                  modified_field=modified_field)
        _id = record[id_field]
        record = ujson.loads(self.json.dumps(record))
        self._store[parent_id][collection_id][_id] = record
        self._cemetery[parent_id][collection_id].pop(_id, None)
        return record

    @synchronized
    def get(self, collection_id, parent_id, object_id,
            id_field=DEFAULT_ID_FIELD,
            modified_field=DEFAULT_MODIFIED_FIELD,
            auth=None):
        collection = self._store[parent_id][collection_id]
        if object_id not in collection:
            raise exceptions.RecordNotFoundError(object_id)
        return {**collection[object_id]}

    @synchronized
    def update(self, collection_id, parent_id, object_id, record,
               id_field=DEFAULT_ID_FIELD,
               modified_field=DEFAULT_MODIFIED_FIELD,
               auth=None):
        record = {**record}
        record[id_field] = object_id
        record = ujson.loads(self.json.dumps(record))

        self.set_record_timestamp(collection_id, parent_id, record,
                                  modified_field=modified_field)
        self._store[parent_id][collection_id][object_id] = record
        self._cemetery[parent_id][collection_id].pop(object_id, None)
        return record

    @synchronized
    def delete(self, collection_id, parent_id, object_id,
               id_field=DEFAULT_ID_FIELD, with_deleted=True,
               modified_field=DEFAULT_MODIFIED_FIELD,
               deleted_field=DEFAULT_DELETED_FIELD,
               auth=None, last_modified=None):
        existing = self.get(collection_id, parent_id, object_id)
        # Need to delete the last_modified field of the record.
        del existing[modified_field]

        self.set_record_timestamp(collection_id, parent_id, existing,
                                  modified_field=modified_field,
                                  last_modified=last_modified)
        existing = self.strip_deleted_record(collection_id,
                                             parent_id,
                                             existing)

        # Add to deleted items, remove from store.
        if with_deleted:
            deleted = {**existing}
            self._cemetery[parent_id][collection_id][object_id] = deleted
        self._store[parent_id][collection_id].pop(object_id)
        return existing

    @synchronized
    def purge_deleted(self, collection_id, parent_id, before=None,
                      id_field=DEFAULT_ID_FIELD,
                      modified_field=DEFAULT_MODIFIED_FIELD,
                      auth=None):
        parent_id_match = re.compile(parent_id.replace('*', '.*'))
        by_parent_id = {pid: collections
                        for pid, collections in self._cemetery.items()
                        if parent_id_match.match(pid)}
        num_deleted = 0
        for pid, collections in by_parent_id.items():
            if collection_id is not None:
                collections = {collection_id: collections[collection_id]}
            for collection, colrecords in collections.items():
                if before is None:
                    kept = {}
                else:
                    kept = {key: value for key, value in
                            colrecords.items()
                            if value[modified_field] >= before}
                self._cemetery[pid][collection] = kept
                num_deleted += (len(colrecords) - len(kept))
        return num_deleted

    @synchronized
    def get_all(self, collection_id, parent_id, filters=None, sorting=None,
                pagination_rules=None, limit=None, include_deleted=False,
                id_field=DEFAULT_ID_FIELD,
                modified_field=DEFAULT_MODIFIED_FIELD,
                deleted_field=DEFAULT_DELETED_FIELD,
                auth=None):

        records = _get_objects_by_parent_id(self._store, parent_id, collection_id)

        records, count = self.extract_record_set(records=records,
                                                 filters=filters, sorting=None,
                                                 id_field=id_field, deleted_field=deleted_field)
        deleted = []
        if include_deleted:
            deleted = _get_objects_by_parent_id(self._cemetery, parent_id, collection_id)

        records, count = self.extract_record_set(records=records + deleted,
                                                 filters=filters, sorting=sorting,
                                                 id_field=id_field, deleted_field=deleted_field,
                                                 pagination_rules=pagination_rules, limit=limit)
        return records, count

    @synchronized
    def delete_all(self, collection_id, parent_id, filters=None,
                   sorting=None, pagination_rules=None, limit=None,
                   id_field=DEFAULT_ID_FIELD, with_deleted=True,
                   modified_field=DEFAULT_MODIFIED_FIELD,
                   deleted_field=DEFAULT_DELETED_FIELD,
                   auth=None):
        records = _get_objects_by_parent_id(self._store, parent_id, collection_id, with_meta=True)
        records, count = self.extract_record_set(records=records,
                                                 filters=filters,
                                                 sorting=sorting,
                                                 pagination_rules=pagination_rules, limit=limit,
                                                 id_field=id_field,
                                                 deleted_field=deleted_field)

        deleted = [self.delete(r.pop('__collection_id__'),
                               r.pop('__parent_id__'),
                               r[id_field],
                               id_field=id_field, with_deleted=with_deleted,
                               modified_field=modified_field,
                               deleted_field=deleted_field)
                   for r in records]
        return deleted


def extract_record_set(records, filters, sorting,
                       pagination_rules=None, limit=None,
                       id_field=DEFAULT_ID_FIELD,
                       deleted_field=DEFAULT_DELETED_FIELD):
    """"""Apply filters, sorting, limit, and pagination rules to the list of
    `records`.

    """"""
    filtered = list(apply_filters(records, filters or []))
    total_records = len(filtered)

        paginated.update(dict(((x.get(id_field), x) for x in values)))
    paginated = {}
    for rule in pagination_rules or []:
        values = list(apply_filters(filtered, rule))
        paginated.update(dict(((x[id_field], x) for x in values)))

    if paginated:
        paginated = paginated.values()
    else:
        paginated = filtered

    sorted_ = apply_sorting(paginated, sorting or [])

    filtered_deleted = len([r for r in sorted_
                            if r.get(deleted_field) is True])

    if limit:
        sorted_ = list(sorted_)[:limit]

    return sorted_, total_records - filtered_deleted


def apply_filters(records, filters):
    """"""Filter the specified records, using basic iteration.
    """"""
    operators = {
        COMPARISON.LT: operator.lt,
        COMPARISON.MAX: operator.le,
        COMPARISON.EQ: operator.eq,
        COMPARISON.NOT: operator.ne,
        COMPARISON.MIN: operator.ge,
        COMPARISON.GT: operator.gt,
        COMPARISON.IN: operator.contains,
        COMPARISON.EXCLUDE: lambda x, y: not operator.contains(x, y),
        COMPARISON.LIKE: lambda x, y: re.search(y, x, re.IGNORECASE),
    }
    for record in records:
        matches = True
        for f in filters:
            right = f.value
            if f.field == DEFAULT_ID_FIELD:
                if isinstance(right, int):
                    right = str(right)

            left = find_nested_value(record, f.field, MISSING)

            if f.operator in (COMPARISON.IN, COMPARISON.EXCLUDE):
                right, left = left, right
            elif f.operator == COMPARISON.LIKE:
                # Add implicit start/end wildchars if none is specified.
                if '*' not in right:
                    right = '*{}*'.format(right)
                right = '^{}$'.format(right.replace('*', '.*'))
            elif f.operator != COMPARISON.HAS:
                left = schwartzian_transform(left)
                right = schwartzian_transform(right)

            if f.operator == COMPARISON.HAS:
                matches = left != MISSING if f.value else left == MISSING
            else:
                matches = matches and operators[f.operator](left, right)
        if matches:
            yield record


def schwartzian_transform(value):
    """"""Decorate a value with a tag that enforces the Postgres sort order.

    The sort order, per https://www.postgresql.org/docs/9.6/static/datatype-json.html, is:

    Object > Array > Boolean > Number > String > Null

    Note that there are more interesting rules for comparing objects
    and arrays but we probably don't need to be that compatible.

    MISSING represents what would be a SQL NULL, which is ""bigger""
    than everything else.
    """"""
    if value is None:
        return (0, value)
    if isinstance(value, str):
        return (1, value)
    if isinstance(value, bool):
        # This has to be before Number, because bools are a subclass
        # of int :(
        return (3, value)
    if isinstance(value, numbers.Number):
        return (2, value)
    if isinstance(value, abc.Sequence):
        return (4, value)
    if isinstance(value, abc.Mapping):
        return (5, value)
    if value is MISSING:
        return (6, value)
    raise ValueError('Unknown value: {}'.format(value))   # pragma: no cover


def apply_sorting(records, sorting):
    """"""Sort the specified records, using cumulative python sorting.
    """"""
    result = list(records)

    if not result:
        return result

    def column(record, name):
        return schwartzian_transform(find_nested_value(record, name, default=MISSING))

    for sort in reversed(sorting):
        result = sorted(result,
                        key=lambda r: column(r, sort.field),
                        reverse=(sort.direction < 0))

    return result


def _get_objects_by_parent_id(store, parent_id, collection_id, with_meta=False):
    if parent_id is not None:
        parent_id_match = re.compile('^{}$'.format(parent_id.replace('*', '.*')))
        by_parent_id = {pid: collections
                        for pid, collections in store.items()
                        if parent_id_match.match(pid)}
    else:
        by_parent_id = store[parent_id]

    objects = []
    for pid, collections in by_parent_id.items():
        if collection_id is not None:
            collections = {collection_id: collections[collection_id]}
        for collection, colobjects in collections.items():
            for r in colobjects.values():
                if with_meta:
                    objects.append(dict(__collection_id__=collection,
                                        __parent_id__=pid, **r))
                else:
                    objects.append(r)
    return objects


def load_from_config(config):
    settings = {**config.get_settings()}
    strict = settings.get('storage_strict_json', False)
    return Storage(strict_json=strict)
","could be `x.get(id_field, """")` too ;)","The suggested code changes appear to be implementing a simple in-memory storage backend for Kinto, a NoSQL database. The code includes classes for managing collections, records, and various operations such as creating, updating, deleting, and retrieving records. The changes also include functions for filtering, sorting, and pagination.

Here's a concise review of the suggested code changes:

1. **Imports and Classes**: The code imports necessary modules and defines classes for the storage backend. The `MemoryBasedStorage` class is an abstract base class, while the `Storage` class inherits from it and provides concrete implementations for various storage operations.

2. **Initialization**: The `__init__` method initializes the storage backend, setting up the `_store` and `_cemetery` dictionaries to store records and deleted records, respectively. The `_timestamps` dictionary keeps track of the latest modification timestamps for each collection.

3. **Timestamp Management**: The `_bump_timestamp` method updates the timestamp for a given record. It handles both specified and unspecified timestamps, ensuring that timestamps are always incremented correctly.

4. **Record Operations**: Methods like `create`, `get`, `update`, and `delete` manage the creation, retrieval, update, and deletion of records. These methods ensure that records are properly timestamped and validated according to the configuration.

5. **Purge Deleted Records**: The `purge_deleted` method allows purging records that have been marked as deleted based on a specified timestamp.

6. **Retrieve All Records**: The `get_all` method retrieves all records, including those marked as deleted, and applies filters, sorting, and pagination rules.

7. **Delete All Records**: The `delete_all` method deletes all records that match specified filters and applies pagination rules.

8. **Filtering and Sorting**: The `extract_record_set` function applies filters and sorting to a list of records. The `apply_filters` function iterates over records and applies specified filters, while the `apply_sorting` function sorts records based on specified criteria.

9. **Schwartzian Transform**: The `schwartzian_transform` function decorates values to enforce a specific sort order, which is useful for handling different data types in PostgreSQL.

10. **Configuration Loading**: The `load_from_config` function loads configuration settings and returns an instance of the `Storage` class with the appropriate settings.

Overall, the code provides a robust and efficient in-memory storage solution for Kinto, with support for basic CRUD operations and advanced querying capabilities."
65,"@@ -18,6 +18,7 @@ func Test_convertSidecar(t *testing.T) {
 	mockImage := aws.String(""mockImage"")
 	mockMap := map[string]string{""foo"": ""bar""}
 	mockCredsParam := aws.String(""mockCredsParam"")
+	mockEssential := aws.Bool(false)
 	testCases := map[string]struct {
 		inPort string
 ","// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0

package stack

import (
	""fmt""
	""testing""
	""time""

	""github.com/aws/aws-sdk-go/aws""
	""github.com/aws/copilot-cli/internal/pkg/manifest""
	""github.com/aws/copilot-cli/internal/pkg/template""
	""github.com/stretchr/testify/require""
)

func Test_convertSidecar(t *testing.T) {
	mockEssential := aws.Bool(false)
	mockImage := aws.String(""mockImage"")
	mockMap := map[string]string{""foo"": ""bar""}
	mockCredsParam := aws.String(""mockCredsParam"")
	testCases := map[string]struct {
		inPort string

		wanted    *template.SidecarOpts
		wantedErr error
	}{
		""invalid port"": {
			inPort: ""b/a/d/P/o/r/t"",

			wantedErr: fmt.Errorf(""cannot parse port mapping from b/a/d/P/o/r/t""),
		},
		""good port without protocol"": {
			inPort: ""2000"",

			wanted: &template.SidecarOpts{
				Name:       aws.String(""foo""),
				Port:       aws.String(""2000""),
				CredsParam: mockCredsParam,
				Image:      mockImage,
				Secrets:    mockMap,
				Variables:  mockMap,
			},
		},
		""good port with protocol"": {
			inPort: ""2000/udp"",

			wanted: &template.SidecarOpts{
				Name:       aws.String(""foo""),
				Port:       aws.String(""2000""),
				Protocol:   aws.String(""udp""),
				CredsParam: mockCredsParam,
				Image:      mockImage,
				Secrets:    mockMap,
				Variables:  mockMap,
			},
		},
	}
	for name, tc := range testCases {
		t.Run(name, func(t *testing.T) {
			sidecar := map[string]*manifest.SidecarConfig{
				""foo"": {
					CredsParam: mockCredsParam,
					Image:      mockImage,
					Secrets:    mockMap,
					Variables:  mockMap,
					Port:       aws.String(tc.inPort),
				},
			}
			got, err := convertSidecar(sidecar)

			if tc.wantedErr != nil {
				require.EqualError(t, err, tc.wantedErr.Error())
			} else {
				require.NoError(t, err)
				require.Equal(t, got[0], tc.wanted)
			}
		})
	}
}

func Test_convertAutoscaling(t *testing.T) {
	const (
		mockRange    = ""1-100""
		mockRequests = 1000
	)
	mockResponseTime := 512 * time.Millisecond
	testCases := map[string]struct {
		inRange        manifest.Range
		inCPU          int
		inMemory       int
		inRequests     int
		inResponseTime time.Duration

		wanted    *template.AutoscalingOpts
		wantedErr error
	}{
		""invalid range"": {
			inRange: ""badRange"",

			wantedErr: fmt.Errorf(""invalid range value badRange. Should be in format of ${min}-${max}""),
		},
		""success"": {
			inRange:        mockRange,
			inCPU:          70,
			inMemory:       80,
			inRequests:     mockRequests,
			inResponseTime: mockResponseTime,

			wanted: &template.AutoscalingOpts{
				MaxCapacity:  aws.Int(100),
				MinCapacity:  aws.Int(1),
				CPU:          aws.Float64(70),
				Memory:       aws.Float64(80),
				Requests:     aws.Float64(1000),
				ResponseTime: aws.Float64(0.512),
			},
		},
	}
	for name, tc := range testCases {
		t.Run(name, func(t *testing.T) {
			a := manifest.Autoscaling{
				Range:        &tc.inRange,
				CPU:          aws.Int(tc.inCPU),
				Memory:       aws.Int(tc.inMemory),
				Requests:     aws.Int(tc.inRequests),
				ResponseTime: &tc.inResponseTime,
			}
			got, err := convertAutoscaling(&a)

			if tc.wantedErr != nil {
				require.EqualError(t, err, tc.wantedErr.Error())
			} else {
				require.NoError(t, err)
				require.Equal(t, got, tc.wanted)
			}
		})
	}
}

func Test_convertHTTPHealthCheck(t *testing.T) {
	// These are used by reference to represent the output of the manifest.durationp function.
	duration15Seconds := time.Duration(15 * time.Second)
	duration60Seconds := time.Duration(60 * time.Second)
	testCases := map[string]struct {
		inputPath               *string
		inputHealthyThreshold   *int64
		inputUnhealthyThreshold *int64
		inputInterval           *time.Duration
		inputTimeout            *time.Duration

		wantedOpts template.HTTPHealthCheckOpts
	}{
		""no fields indicated in manifest"": {
			inputPath:               nil,
			inputHealthyThreshold:   nil,
			inputUnhealthyThreshold: nil,
			inputInterval:           nil,
			inputTimeout:            nil,

			wantedOpts: template.HTTPHealthCheckOpts{
				HealthCheckPath: ""/"",
			},
		},
		""just HealthyThreshold"": {
			inputPath:               nil,
			inputHealthyThreshold:   aws.Int64(5),
			inputUnhealthyThreshold: nil,
			inputInterval:           nil,
			inputTimeout:            nil,

			wantedOpts: template.HTTPHealthCheckOpts{
				HealthCheckPath:  ""/"",
				HealthyThreshold: aws.Int64(5),
			},
		},
		""just UnhealthyThreshold"": {
			inputPath:               nil,
			inputHealthyThreshold:   nil,
			inputUnhealthyThreshold: aws.Int64(5),
			inputInterval:           nil,
			inputTimeout:            nil,

			wantedOpts: template.HTTPHealthCheckOpts{
				HealthCheckPath:    ""/"",
				UnhealthyThreshold: aws.Int64(5),
			},
		},
		""just Interval"": {
			inputPath:               nil,
			inputHealthyThreshold:   nil,
			inputUnhealthyThreshold: nil,
			inputInterval:           &duration15Seconds,
			inputTimeout:            nil,

			wantedOpts: template.HTTPHealthCheckOpts{
				HealthCheckPath: ""/"",
				Interval:        aws.Int64(15),
			},
		},
		""just Timeout"": {
			inputPath:               nil,
			inputHealthyThreshold:   nil,
			inputUnhealthyThreshold: nil,
			inputInterval:           nil,
			inputTimeout:            &duration15Seconds,

			wantedOpts: template.HTTPHealthCheckOpts{
				HealthCheckPath: ""/"",
				Timeout:         aws.Int64(15),
			},
		},
		""all values changed in manifest"": {
			inputPath:               aws.String(""/road/to/nowhere""),
			inputHealthyThreshold:   aws.Int64(3),
			inputUnhealthyThreshold: aws.Int64(3),
			inputInterval:           &duration60Seconds,
			inputTimeout:            &duration60Seconds,

			wantedOpts: template.HTTPHealthCheckOpts{
				HealthCheckPath:    ""/road/to/nowhere"",
				HealthyThreshold:   aws.Int64(3),
				UnhealthyThreshold: aws.Int64(3),
				Interval:           aws.Int64(60),
				Timeout:            aws.Int64(60),
			},
		},
	}
	for name, tc := range testCases {
		t.Run(name, func(t *testing.T) {
			// GIVEN
			hc := manifest.HealthCheckArgsOrString{
				HealthCheckPath: tc.inputPath,
				HealthCheckArgs: manifest.HTTPHealthCheckArgs{
					Path:               tc.inputPath,
					HealthyThreshold:   tc.inputHealthyThreshold,
					UnhealthyThreshold: tc.inputUnhealthyThreshold,
					Timeout:            tc.inputTimeout,
					Interval:           tc.inputInterval,
				},
			}
			// WHEN
			actualOpts := convertHTTPHealthCheck(&hc)

			// THEN
			require.Equal(t, tc.wantedOpts, actualOpts)
		})
	}
}

func Test_convertStorageOpts(t *testing.T) {
	testCases := map[string]struct {
		inVolumes map[string]manifest.Volume
		wantOpts  template.StorageOpts
		wantErr   string
	}{
		""minimal configuration"": {
			inVolumes: map[string]manifest.Volume{
				""wordpress"": {
					EFS: manifest.EFSVolumeConfiguration{
						FileSystemID: aws.String(""fs-1234""),
					},
					MountPointOpts: manifest.MountPointOpts{
						ContainerPath: aws.String(""/var/www""),
					},
				},
			},
			wantOpts: template.StorageOpts{
				Volumes: []*template.Volume{
					{
						Name:          aws.String(""wordpress""),
						Filesystem:    aws.String(""fs-1234""),
						RootDirectory: aws.String(""/""),
						IAM:           aws.String(""DISABLED""),
					},
				},
				MountPoints: []*template.MountPoint{
					{
						ContainerPath: aws.String(""/var/www""),
						ReadOnly:      aws.Bool(true),
						SourceVolume:  aws.String(""wordpress""),
					},
				},
				EFSPerms: []*template.EFSPermission{
					{
						FilesystemID: aws.String(""fs-1234""),
						Write:        false,
					},
				},
			},
		},
		""fsid not specified"": {
			inVolumes: map[string]manifest.Volume{
				""wordpress"": {},
			},
			wantErr: errNoFSID.Error(),
		},
		""container path not specified"": {
			inVolumes: map[string]manifest.Volume{
				""wordpress"": {
					EFS: manifest.EFSVolumeConfiguration{
						FileSystemID: aws.String(""fs-1234""),
					},
				},
			},
			wantErr: errNoContainerPath.Error(),
		},
		""full specification with access point renders correctly"": {
			inVolumes: map[string]manifest.Volume{
				""wordpress"": {
					EFS: manifest.EFSVolumeConfiguration{
						FileSystemID:  aws.String(""fs-1234""),
						RootDirectory: aws.String(""/""),
						AuthConfig: manifest.AuthorizationConfig{
							IAM:           aws.Bool(true),
							AccessPointID: aws.String(""ap-1234""),
						},
					},
					MountPointOpts: manifest.MountPointOpts{
						ContainerPath: aws.String(""/var/www""),
						ReadOnly:      aws.Bool(false),
					},
				},
			},
			wantOpts: template.StorageOpts{
				Volumes: []*template.Volume{
					{
						Name:          aws.String(""wordpress""),
						Filesystem:    aws.String(""fs-1234""),
						RootDirectory: aws.String(""/""),
						IAM:           aws.String(""ENABLED""),
						AccessPointID: aws.String(""ap-1234""),
					},
				},
				MountPoints: []*template.MountPoint{
					{
						ContainerPath: aws.String(""/var/www""),
						ReadOnly:      aws.Bool(false),
						SourceVolume:  aws.String(""wordpress""),
					},
				},
				EFSPerms: []*template.EFSPermission{
					{
						FilesystemID:  aws.String(""fs-1234""),
						AccessPointID: aws.String(""ap-1234""),
						Write:         true,
					},
				},
			},
		},
		""full specification without access point renders correctly"": {
			inVolumes: map[string]manifest.Volume{
				""wordpress"": {
					EFS: manifest.EFSVolumeConfiguration{
						FileSystemID:  aws.String(""fs-1234""),
						RootDirectory: aws.String(""/wordpress""),
						AuthConfig: manifest.AuthorizationConfig{
							IAM: aws.Bool(true),
						},
					},
					MountPointOpts: manifest.MountPointOpts{
						ContainerPath: aws.String(""/var/www""),
						ReadOnly:      aws.Bool(false),
					},
				},
			},
			wantOpts: template.StorageOpts{
				Volumes: []*template.Volume{
					{
						Name:          aws.String(""wordpress""),
						Filesystem:    aws.String(""fs-1234""),
						RootDirectory: aws.String(""/wordpress""),
						IAM:           aws.String(""ENABLED""),
					},
				},
				MountPoints: []*template.MountPoint{
					{
						ContainerPath: aws.String(""/var/www""),
						ReadOnly:      aws.Bool(false),
						SourceVolume:  aws.String(""wordpress""),
					},
				},
				EFSPerms: []*template.EFSPermission{
					{
						FilesystemID: aws.String(""fs-1234""),
						Write:        true,
					},
				},
			},
		},
		""error when AP is specified with root dir"": {
			inVolumes: map[string]manifest.Volume{
				""wordpress"": {
					EFS: manifest.EFSVolumeConfiguration{
						FileSystemID:  aws.String(""fs-1234""),
						RootDirectory: aws.String(""/wordpress""),
						AuthConfig: manifest.AuthorizationConfig{
							IAM:           aws.Bool(true),
							AccessPointID: aws.String(""ap-1234""),
						},
					},
					MountPointOpts: manifest.MountPointOpts{
						ContainerPath: aws.String(""/var/www""),
						ReadOnly:      aws.Bool(false),
					},
				},
			},
			wantErr: errAcessPointWithRootDirectory.Error(),
		},
		""error when AP is specified without IAM"": {
			inVolumes: map[string]manifest.Volume{
				""wordpress"": {
					EFS: manifest.EFSVolumeConfiguration{
						FileSystemID:  aws.String(""fs-1234""),
						RootDirectory: aws.String(""/wordpress""),
						AuthConfig: manifest.AuthorizationConfig{
							IAM:           aws.Bool(false),
							AccessPointID: aws.String(""ap-1234""),
						},
					},
					MountPointOpts: manifest.MountPointOpts{
						ContainerPath: aws.String(""/var/www""),
						ReadOnly:      aws.Bool(false),
					},
				},
			},
			wantErr: errAccessPointWithoutIAM.Error(),
		},
	}
	for name, tc := range testCases {
		t.Run(name, func(t *testing.T) {
			// GIVEN
			s := manifest.Storage{
				Volumes: tc.inVolumes,
			}
			// WHEN
			got, err := convertStorageOpts(&s)

			// THEN
			if tc.wantErr != """" {
				require.EqualError(t, err, tc.wantErr)
			} else {
				require.NoError(t, err)
				require.Equal(t, &tc.wantOpts, got)
			}
		})
	}
}

func Test_convertExecuteCommand(t *testing.T) {
	testCases := map[string]struct {
		inConfig manifest.ExecuteCommand

		wanted *template.ExecuteCommandOpts
	}{
		""without exec enabled"": {
			inConfig: manifest.ExecuteCommand{},
			wanted:   nil,
		},
		""exec enabled"": {
			inConfig: manifest.ExecuteCommand{
				Enable: aws.Bool(true),
			},
			wanted: &template.ExecuteCommandOpts{},
		},
		""exec enabled with config"": {
			inConfig: manifest.ExecuteCommand{
				Config: manifest.ExecuteCommandConfig{
					Enable: aws.Bool(true),
				},
			},
			wanted: &template.ExecuteCommandOpts{},
		},
	}
	for name, tc := range testCases {
		t.Run(name, func(t *testing.T) {
			exec := tc.inConfig
			got := convertExecuteCommand(&exec)

			require.Equal(t, got, tc.wanted)
		})
	}
}

func Test_convertSidecarMountPoints(t *testing.T) {
	testCases := map[string]struct {
		inMountPoints  []manifest.SidecarMountPoint
		wantErr        string
		wantMountPoint []*template.MountPoint
	}{
		""fully specified"": {
			inMountPoints: []manifest.SidecarMountPoint{
				{
					SourceVolume: aws.String(""wordpress""),
					MountPointOpts: manifest.MountPointOpts{
						ContainerPath: aws.String(""/var/www/wp-content""),
						ReadOnly:      aws.Bool(false),
					},
				},
			},
			wantMountPoint: []*template.MountPoint{
				{
					ContainerPath: aws.String(""/var/www/wp-content""),
					ReadOnly:      aws.Bool(false),
					SourceVolume:  aws.String(""wordpress""),
				},
			},
		},
		""readonly defaults to true"": {
			inMountPoints: []manifest.SidecarMountPoint{
				{
					SourceVolume: aws.String(""wordpress""),
					MountPointOpts: manifest.MountPointOpts{
						ContainerPath: aws.String(""/var/www/wp-content""),
					},
				},
			},
			wantMountPoint: []*template.MountPoint{
				{
					ContainerPath: aws.String(""/var/www/wp-content""),
					ReadOnly:      aws.Bool(true),
					SourceVolume:  aws.String(""wordpress""),
				},
			},
		},
		""error when source not specified"": {
			inMountPoints: []manifest.SidecarMountPoint{
				{
					MountPointOpts: manifest.MountPointOpts{
						ContainerPath: aws.String(""/var/www/wp-content""),
						ReadOnly:      aws.Bool(false),
					},
				},
			},
			wantErr: errNoSourceVolume.Error(),
		},
		""error when path not specified"": {
			inMountPoints: []manifest.SidecarMountPoint{
				{
					SourceVolume: aws.String(""wordpress""),
					MountPointOpts: manifest.MountPointOpts{
						ReadOnly: aws.Bool(false),
					},
				},
			},
			wantErr: errNoContainerPath.Error(),
		},
	}
	for name, tc := range testCases {
		t.Run(name, func(t *testing.T) {
			got, err := convertSidecarMountPoints(tc.inMountPoints)
			if tc.wantErr != """" {
				require.EqualError(t, err, tc.wantErr)
			} else {
				require.NoError(t, err)
				require.Equal(t, tc.wantMountPoint, got)
			}
		})
	}
}

func Test_validatePaths(t *testing.T) {
	t.Run(""containerPath should be properly validated"", func(t *testing.T) {
		require.NoError(t, validateContainerPath(""/abc/90_""), ""contains underscore"")
		require.EqualError(t, validateContainerPath(""/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa""), ""path must be less than 242 bytes in length"", ""too long"")
		require.EqualError(t, validateContainerPath(""/etc /bin/sh cat `i'm evil` > /dev/null""), ""paths can only contain the characters a-zA-Z0-9.-_/"", ""invalid characters disallowed"")
	})
}
",nit: would you mind updating the testCases to take an `inEssential` field instead and in the `wanted` we can ensure that exact value is applied. Right now we don't have a scenario that tests if the conversion happens on `essential: true`,"The suggested code changes are comprehensive and cover various aspects of converting different configurations into their respective template representations. Here's a concise review:

### Key Points:
1. **Test Cases**: Each function has multiple test cases to ensure robustness across different scenarios.
2. **Error Handling**: Proper error handling is implemented for invalid inputs, such as malformed port mappings, invalid range values, and missing paths.
3. **Template Conversion**: Functions like `convertSidecar`, `convertAutoscaling`, `convertHTTPHealthCheck`, `convertStorageOpts`, and `convertExecuteCommand` correctly map manifest configurations to their corresponding template structures.
4. **Validation**: The `validatePaths` function ensures that container paths adhere to specific constraints, such as length and character set.

### Specific Comments:
- **`convertSidecar`**: Handles both valid and invalid port formats, ensuring that the port mapping is correctly parsed and converted.
- **`convertAutoscaling`**: Converts the autoscaling configuration into a format suitable for the template, including handling of CPU, memory, requests, and response time.
- **`convertHTTPHealthCheck`**: Ensures that all health check options are correctly mapped, defaulting to `/` if not specified.
- **`convertStorageOpts`**: Validates and converts EFS volume configurations, handling IAM permissions and access points appropriately.
- **`convertExecuteCommand`**: Checks if execute command is enabled and returns the appropriate template options.
- **`convertSidecarMountPoints`**: Validates and maps sidecar mount points, ensuring that both source and container paths are provided.
- **`validatePaths`**: Ensures that container paths meet the required constraints, providing clear error messages for invalid inputs.

Overall, these changes provide a solid foundation for converting manifest configurations into template representations, ensuring accuracy and reliability."
66,"@@ -96,6 +96,12 @@ func run(o *Options) error {
 		features.DefaultFeatureGate.Enabled(features.AntreaPolicy))
 
 	_, serviceCIDRNet, _ := net.ParseCIDR(o.config.ServiceCIDR)
+	var serviceCIDRNetv6 *net.IPNet
+	// Todo: use FeatureGate to check if IPv6 is enabled and then read configuration item ""ServiceCIDRv6"".
+	if o.config.ServiceCIDRv6 != """" {
+		_, serviceCIDRNetv6, _ = net.ParseCIDR(o.config.ServiceCIDRv6)
+	}
+
 	_, encapMode := config.GetTrafficEncapModeFromStr(o.config.TrafficEncapMode)
 	networkConfig := &config.NetworkConfig{
 		TunnelType:        ovsconfig.TunnelType(o.config.TunnelType),","// Copyright 2019 Antrea Authors
//
// Licensed under the Apache License, Version 2.0 (the ""License"");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an ""AS IS"" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package main

import (
	""fmt""
	""net""
	""time""

	""k8s.io/apimachinery/pkg/util/wait""
	""k8s.io/client-go/informers""
	""k8s.io/klog""

	""github.com/vmware-tanzu/antrea/pkg/agent""
	""github.com/vmware-tanzu/antrea/pkg/agent/apiserver""
	""github.com/vmware-tanzu/antrea/pkg/agent/cniserver""
	_ ""github.com/vmware-tanzu/antrea/pkg/agent/cniserver/ipam""
	""github.com/vmware-tanzu/antrea/pkg/agent/config""
	""github.com/vmware-tanzu/antrea/pkg/agent/controller/networkpolicy""
	""github.com/vmware-tanzu/antrea/pkg/agent/controller/noderoute""
	""github.com/vmware-tanzu/antrea/pkg/agent/controller/traceflow""
	""github.com/vmware-tanzu/antrea/pkg/agent/flowexporter/connections""
	""github.com/vmware-tanzu/antrea/pkg/agent/flowexporter/exporter""
	""github.com/vmware-tanzu/antrea/pkg/agent/flowexporter/flowrecords""
	""github.com/vmware-tanzu/antrea/pkg/agent/interfacestore""
	""github.com/vmware-tanzu/antrea/pkg/agent/metrics""
	""github.com/vmware-tanzu/antrea/pkg/agent/openflow""
	""github.com/vmware-tanzu/antrea/pkg/agent/proxy""
	""github.com/vmware-tanzu/antrea/pkg/agent/querier""
	""github.com/vmware-tanzu/antrea/pkg/agent/route""
	""github.com/vmware-tanzu/antrea/pkg/apis/controlplane/v1beta1""
	crdinformers ""github.com/vmware-tanzu/antrea/pkg/client/informers/externalversions""
	""github.com/vmware-tanzu/antrea/pkg/features""
	""github.com/vmware-tanzu/antrea/pkg/k8s""
	""github.com/vmware-tanzu/antrea/pkg/log""
	""github.com/vmware-tanzu/antrea/pkg/monitor""
	ofconfig ""github.com/vmware-tanzu/antrea/pkg/ovs/openflow""
	""github.com/vmware-tanzu/antrea/pkg/ovs/ovsconfig""
	""github.com/vmware-tanzu/antrea/pkg/signals""
	""github.com/vmware-tanzu/antrea/pkg/version""
)

// informerDefaultResync is the default resync period if a handler doesn't specify one.
// Use the same default value as kube-controller-manager:
// https://github.com/kubernetes/kubernetes/blob/release-1.17/pkg/controller/apis/config/v1alpha1/defaults.go#L120
const informerDefaultResync = 12 * time.Hour

// run starts Antrea agent with the given options and waits for termination signal.
func run(o *Options) error {
	klog.Infof(""Starting Antrea agent (version %s)"", version.GetFullVersion())
	// Create K8s Clientset, CRD Clientset and SharedInformerFactory for the given config.
	k8sClient, _, crdClient, err := k8s.CreateClients(o.config.ClientConnection)
	if err != nil {
		return fmt.Errorf(""error creating K8s clients: %v"", err)
	}
	informerFactory := informers.NewSharedInformerFactory(k8sClient, informerDefaultResync)
	crdInformerFactory := crdinformers.NewSharedInformerFactory(crdClient, informerDefaultResync)
	traceflowInformer := crdInformerFactory.Ops().V1alpha1().Traceflows()

	// Create Antrea Clientset for the given config.
	antreaClientProvider := agent.NewAntreaClientProvider(o.config.AntreaClientConnection, k8sClient)
	if err != nil {
		return fmt.Errorf(""error creating Antrea client: %v"", err)
	}

	// Register Antrea Agent metrics if EnablePrometheusMetrics is set
	if o.config.EnablePrometheusMetrics {
		metrics.InitializePrometheusMetrics()
	}

	// Create ovsdb and openflow clients.
	ovsdbAddress := ovsconfig.GetConnAddress(o.config.OVSRunDir)
	ovsdbConnection, err := ovsconfig.NewOVSDBConnectionUDS(ovsdbAddress)
	if err != nil {
		// TODO: ovsconfig.NewOVSDBConnectionUDS might return timeout in the future, need to add retry
		return fmt.Errorf(""error connecting OVSDB: %v"", err)
	}
	defer ovsdbConnection.Close()

	ovsBridgeClient := ovsconfig.NewOVSBridge(o.config.OVSBridge, o.config.OVSDatapathType, ovsdbConnection)
	ovsBridgeMgmtAddr := ofconfig.GetMgmtAddress(o.config.OVSRunDir, o.config.OVSBridge)
	ofClient := openflow.NewClient(o.config.OVSBridge, ovsBridgeMgmtAddr,
		features.DefaultFeatureGate.Enabled(features.AntreaProxy),
	var serviceCIDRNetv6 *net.IPNet
	// Todo: use FeatureGate to check if IPv6 is enabled and then read configuration item ""ServiceCIDRv6"".
	if o.config.ServiceCIDRv6 != """" {
		_, serviceCIDRNetv6, _ = net.ParseCIDR(o.config.ServiceCIDRv6)
	}

		features.DefaultFeatureGate.Enabled(features.AntreaPolicy))

	_, serviceCIDRNet, _ := net.ParseCIDR(o.config.ServiceCIDR)
	_, encapMode := config.GetTrafficEncapModeFromStr(o.config.TrafficEncapMode)
	networkConfig := &config.NetworkConfig{
		TunnelType:        ovsconfig.TunnelType(o.config.TunnelType),
		TrafficEncapMode:  encapMode,
		EnableIPSecTunnel: o.config.EnableIPSecTunnel}

	routeClient, err := route.NewClient(serviceCIDRNet, encapMode)
	if err != nil {
		return fmt.Errorf(""error creating route client: %v"", err)
	}

	// Create an ifaceStore that caches network interfaces managed by this node.
	ifaceStore := interfacestore.NewInterfaceStore()

	// Initialize agent and node network.
	agentInitializer := agent.NewInitializer(
		k8sClient,
		ovsBridgeClient,
		ofClient,
		routeClient,
		ifaceStore,
		o.config.OVSBridge,
		o.config.HostGateway,
		o.config.DefaultMTU,
		serviceCIDRNet,
		networkConfig,
		features.DefaultFeatureGate.Enabled(features.AntreaProxy))
	err = agentInitializer.Initialize()
	if err != nil {
		return fmt.Errorf(""error initializing agent: %v"", err)
	}
	nodeConfig := agentInitializer.GetNodeConfig()

	nodeRouteController := noderoute.NewNodeRouteController(
		k8sClient,
		informerFactory,
		ofClient,
		ovsBridgeClient,
		routeClient,
		ifaceStore,
		networkConfig,
		nodeConfig)

	var traceflowController *traceflow.Controller
	if features.DefaultFeatureGate.Enabled(features.Traceflow) {
		traceflowController = traceflow.NewTraceflowController(
			k8sClient,
			informerFactory,
			crdClient,
			traceflowInformer,
			ofClient,
			ovsBridgeClient,
			ifaceStore,
			networkConfig,
			nodeConfig,
			serviceCIDRNet)
	}

	// podUpdates is a channel for receiving Pod updates from CNIServer and
	// notifying NetworkPolicyController to reconcile rules related to the
	// updated Pods.
	podUpdates := make(chan v1beta1.PodReference, 100)
	networkPolicyController := networkpolicy.NewNetworkPolicyController(antreaClientProvider, ofClient, ifaceStore, nodeConfig.Name, podUpdates)
	isChaining := false
	if networkConfig.TrafficEncapMode.IsNetworkPolicyOnly() {
		isChaining = true
	}
	var proxier proxy.Proxier
	if features.DefaultFeatureGate.Enabled(features.AntreaProxy) {
		proxier = proxy.New(nodeConfig.Name, informerFactory, ofClient)
	}
	cniServer := cniserver.New(
		o.config.CNISocket,
		o.config.HostProcPathPrefix,
		nodeConfig,
		k8sClient,
		podUpdates,
		isChaining,
		routeClient)
	err = cniServer.Initialize(ovsBridgeClient, ofClient, ifaceStore, o.config.OVSDatapathType)
	if err != nil {
		return fmt.Errorf(""error initializing CNI server: %v"", err)
	}

	// TODO: we should call this after installing flows for initial node routes
	//  and initial NetworkPolicies so that no packets will be mishandled.
	if err := agentInitializer.FlowRestoreComplete(); err != nil {
		return err
	}

	if err := antreaClientProvider.RunOnce(); err != nil {
		return err
	}
	// set up signal capture: the first SIGTERM / SIGINT signal is handled gracefully and will
	// cause the stopCh channel to be closed; if another signal is received before the program
	// exits, we will force exit.
	stopCh := signals.RegisterSignalHandlers()

	log.StartLogFileNumberMonitor(stopCh)

	go cniServer.Run(stopCh)

	informerFactory.Start(stopCh)
	crdInformerFactory.Start(stopCh)

	go antreaClientProvider.Run(stopCh)

	go nodeRouteController.Run(stopCh)

	go networkPolicyController.Run(stopCh)

	if features.DefaultFeatureGate.Enabled(features.Traceflow) {
		go traceflowController.Run(stopCh)
	}

	agentQuerier := querier.NewAgentQuerier(
		nodeConfig,
		ifaceStore,
		k8sClient,
		ofClient,
		ovsBridgeClient,
		networkPolicyController,
		o.config.APIPort)

	agentMonitor := monitor.NewAgentMonitor(crdClient, agentQuerier)

	go agentMonitor.Run(stopCh)

	if features.DefaultFeatureGate.Enabled(features.AntreaProxy) {
		go proxier.Run(stopCh)
	}

	apiServer, err := apiserver.New(
		agentQuerier,
		networkPolicyController,
		o.config.APIPort,
		o.config.EnablePrometheusMetrics,
		o.config.ClientConnection.Kubeconfig)
	if err != nil {
		return fmt.Errorf(""error when creating agent API server: %v"", err)
	}
	go apiServer.Run(stopCh)

	if features.DefaultFeatureGate.Enabled(features.Traceflow) {
		go ofClient.StartPacketInHandler(stopCh)
	}

	// Initialize flow exporter to start go routines to poll conntrack flows and export IPFIX flow records
	if features.DefaultFeatureGate.Enabled(features.FlowExporter) {
		connStore := connections.NewConnectionStore(
			connections.InitializeConnTrackDumper(nodeConfig, serviceCIDRNet, agentQuerier.GetOVSCtlClient(), o.config.OVSDatapathType),
			ifaceStore,
			serviceCIDRNet,
			proxier,
			o.pollInterval)
		pollDone := make(chan struct{})
		go connStore.Run(stopCh, pollDone)

		flowExporter := exporter.NewFlowExporter(
			flowrecords.NewFlowRecords(connStore),
			o.config.FlowExportFrequency)
		go wait.Until(func() { flowExporter.Export(o.flowCollector, stopCh, pollDone) }, 0, stopCh)
	}

	<-stopCh
	klog.Info(""Stopping Antrea agent"")
	return nil
}
",This sentence seems incomplete.,"The provided code snippet is a comprehensive implementation of the Antrea agent, which is responsible for managing network policies, routing, and other networking functionalities in Kubernetes clusters. The code initializes various components such as K8s clients, informers, and controllers, and sets up signal handling to ensure graceful shutdown. Here's a concise review of the key aspects:

### Key Components and Features

1. **K8s Clients and Informers**: 
   - The code creates K8s and CRD clients using `k8s.CreateClients` and `crdinformers.NewSharedInformerFactory`.
   - It also sets up informers for network policies and traceflows.

2. **Antrea Client**:
   - An Antrea client is created using `agent.NewAntreaClientProvider`.

3. **OVS Configuration**:
   - The code initializes an OVSDB connection and an OpenFlow client using `ovsconfig.NewOVSDBConnectionUDS` and `openflow.NewClient`.

4. **Network Configuration**:
   - The network configuration is set up based on the provided configuration options, including tunnel type, traffic encapsulation mode, and service CIDRs.

5. **Route Management**:
   - A route client is created to manage routes and encapsulations.

6. **Interface Store**:
   - An interface store is initialized to cache network interfaces managed by the node.

7. **Agent Initialization**:
   - The agent is initialized using `agent.NewInitializer`, which sets up the necessary components like OVS bridges, OpenFlow, and routing.

8. **Controllers**:
   - Various controllers are created for network policies, node routes, traceflows, and flow export.
   - The `NetworkPolicyController` listens for Pod updates and reconciles rules accordingly.
   - The `TraceflowController` handles traceflow creation and management.

9. **CNI Server**:
   - A CNI server is created to handle network namespace operations and updates.

10. **Signal Handling**:
    - Signal handlers are registered to handle SIGTERM and SIGINT, allowing the agent to gracefully shut down.

11. **Logging and Monitoring**:
    - Logging is configured to start a log file number monitor.
    - Monitoring is set up to track CRDs and agent queries.

12. **API Server**:
    - An API server is created to expose agent functionality via HTTP.

13. **Flow Exporter**:
    - If enabled, a flow exporter is initialized to poll conn"
67,"@@ -60,6 +60,10 @@ type Service interface {
 	// HPA management functions.
 	DescribeHPA(ctx context.Context, clientset, cluster, namespace, name string) (*k8sapiv1.HPA, error)
 	ResizeHPA(ctx context.Context, clientset, cluster, namespace, name string, sizing *k8sapiv1.ResizeHPARequest_Sizing) error
+
+	// Deployment management functions.
+	DescribeDeployment(ctx context.Context, clientset, cluster, namespace, name string) (*k8sapiv1.Deployment, error)
+	UpdateDeployment(ctx context.Context, clientset, cluster, namespace, name string, labels map[string]string, annotations map[string]string) error
 }
 
 type svc struct {","package k8s

// <!-- START clutchdoc -->
// description: Multi-clientset Kubernetes interface.
// <!-- END clutchdoc -->

import (
	""context""

	""github.com/golang/protobuf/ptypes""
	""github.com/golang/protobuf/ptypes/any""
	""github.com/uber-go/tally""
	""go.uber.org/zap""
	""k8s.io/client-go/tools/clientcmd""

	k8sconfigv1 ""github.com/lyft/clutch/backend/api/config/service/k8s/v1""
	k8sapiv1 ""github.com/lyft/clutch/backend/api/k8s/v1""
	""github.com/lyft/clutch/backend/service""

	_ ""k8s.io/client-go/plugin/pkg/client/auth/oidc""
)

const Name = ""clutch.service.k8s""

func New(cfg *any.Any, logger *zap.Logger, scope tally.Scope) (service.Service, error) {
	loadingRules := clientcmd.NewDefaultClientConfigLoadingRules()

	// Use the default kubeconfig (environment or well-known path) if kubeconfigs are not passed in.
	// https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/
	if cfg != nil {
		c := &k8sconfigv1.Config{}
		if err := ptypes.UnmarshalAny(cfg, c); err != nil {
			return nil, err
		}

		if c.Kubeconfigs != nil {
			loadingRules = &clientcmd.ClientConfigLoadingRules{
				Precedence: c.Kubeconfigs,
			}
		}
	}

	c, err := newClientsetManager(loadingRules)
	if err != nil {
		return nil, err
	}

	return NewWithClientsetManager(c, logger, scope)
}

type Service interface {
	// All names of clientsets.
	Clientsets() []string

	// Pod management functions.
	DescribePod(ctx context.Context, clientset, cluster, namespace, name string) (*k8sapiv1.Pod, error)
	DeletePod(ctx context.Context, clientset, cluster, namespace, name string) error
	ListPods(ctx context.Context, clientset, cluster, namespace string, listPodsOptions *k8sapiv1.ListPodsOptions) ([]*k8sapiv1.Pod, error)


	// Deployment management functions.
	DescribeDeployment(ctx context.Context, clientset, cluster, namespace, name string) (*k8sapiv1.Deployment, error)
	UpdateDeployment(ctx context.Context, clientset, cluster, namespace, name string, labels map[string]string, annotations map[string]string) error
	// HPA management functions.
	DescribeHPA(ctx context.Context, clientset, cluster, namespace, name string) (*k8sapiv1.HPA, error)
	ResizeHPA(ctx context.Context, clientset, cluster, namespace, name string, sizing *k8sapiv1.ResizeHPARequest_Sizing) error
}

type svc struct {
	manager ClientsetManager

	log   *zap.Logger
	scope tally.Scope
}

func NewWithClientsetManager(manager ClientsetManager, logger *zap.Logger, scope tally.Scope) (Service, error) {
	return &svc{manager: manager, log: logger, scope: scope}, nil
}

func (s *svc) Clientsets() []string {
	ret := make([]string, 0, len(s.manager.Clientsets()))
	for name := range s.manager.Clientsets() {
		ret = append(ret, name)
	}
	return ret
}
","I dont really like how I'm passing through `labels map[string]string, annotations map[string]string` here, will probably refactor this.","The suggested code change introduces a multi-clientset Kubernetes interface with methods for managing pods and deployments. The code includes a `New` function to create a service instance, a `Clientsets` method to retrieve all clientset names, and methods for pod and deployment management. The code is well-structured and follows best practices for Go programming."
68,"@@ -55,6 +55,16 @@ func nodeStatus(node *node.AlgorandFullNode) (res v1.NodeStatus, err error) {
 	}, nil
 }
 
+func txEncode(tx transactions.Transaction, ad transactions.ApplyData) (res v1.Transaction, err error) {
+	if tx.Type == protocol.PaymentTx {
+		return paymentTxEncode(tx, ad), nil
+	}
+	if tx.Type == protocol.KeyRegistrationTx {
+		return keyregTxEncode(tx, ad), nil
+	}
+	return v1.Transaction{}, errors.New(errUnknownTransactionType)
+}
+
 func paymentTxEncode(tx transactions.Transaction, ad transactions.ApplyData) v1.Transaction {
 	payment := v1.PaymentTransactionType{
 		To:           tx.Receiver.String(),","// Copyright (C) 2019 Algorand, Inc.
// This file is part of go-algorand
//
// go-algorand is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as
// published by the Free Software Foundation, either version 3 of the
// License, or (at your option) any later version.
//
// go-algorand is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Affero General Public License for more details.
//
// You should have received a copy of the GNU Affero General Public License
// along with go-algorand.  If not, see <https://www.gnu.org/licenses/>.

package handlers

import (
	""errors""
	""fmt""
	""net/http""
	""strconv""
	""time""

	""github.com/gorilla/mux""

	""github.com/algorand/go-algorand/agreement""
	""github.com/algorand/go-algorand/config""
	""github.com/algorand/go-algorand/crypto""
	""github.com/algorand/go-algorand/daemon/algod/api/server/lib""
	""github.com/algorand/go-algorand/daemon/algod/api/spec/v1""
	""github.com/algorand/go-algorand/data/basics""
	""github.com/algorand/go-algorand/data/bookkeeping""
	""github.com/algorand/go-algorand/data/transactions""
	""github.com/algorand/go-algorand/ledger""
	""github.com/algorand/go-algorand/node""
	""github.com/algorand/go-algorand/protocol""
)

func nodeStatus(node *node.AlgorandFullNode) (res v1.NodeStatus, err error) {
	stat, err := node.Status()
	if err != nil {
		return v1.NodeStatus{}, err
	}

	return v1.NodeStatus{
		LastRound:            uint64(stat.LastRound),
		LastVersion:          string(stat.LastVersion),
		NextVersion:          string(stat.NextVersion),
		NextVersionRound:     uint64(stat.NextVersionRound),
		NextVersionSupported: stat.NextVersionSupported,
		TimeSinceLastRound:   stat.TimeSinceLastRound().Nanoseconds(),
		CatchupTime:          stat.CatchupTime.Nanoseconds(),
func txEncode(tx transactions.Transaction, ad transactions.ApplyData) (res v1.Transaction, err error) {
	if tx.Type == protocol.PaymentTx {
		return paymentTxEncode(tx, ad), nil
	}
	if tx.Type == protocol.KeyRegistrationTx {
		return keyregTxEncode(tx, ad), nil
	}
	return v1.Transaction{}, errors.New(errUnknownTransactionType)
}

	}, nil
}

func paymentTxEncode(tx transactions.Transaction, ad transactions.ApplyData) v1.Transaction {
	payment := v1.PaymentTransactionType{
		To:           tx.Receiver.String(),
		Amount:       tx.TxAmount().Raw,
		ToRewards:    ad.ReceiverRewards.Raw,
		CloseRewards: ad.CloseRewards.Raw,
	}

	if tx.CloseRemainderTo != (basics.Address{}) {
		payment.CloseRemainderTo = tx.CloseRemainderTo.String()
		payment.CloseAmount = ad.ClosingAmount.Raw
	}

	return v1.Transaction{
		Type:        string(tx.Type),
		TxID:        tx.ID().String(),
		From:        tx.Src().String(),
		Fee:         tx.TxFee().Raw,
		FirstRound:  uint64(tx.First()),
		LastRound:   uint64(tx.Last()),
		Note:        tx.Aux(),
		Payment:     &payment,
		FromRewards: ad.SenderRewards.Raw,
		GenesisID:   tx.GenesisID,
		GenesisHash: tx.GenesisHash[:],
	}
}

func txWithStatusEncode(tr node.TxnWithStatus) v1.Transaction {
	s := paymentTxEncode(tr.Txn.Txn, tr.ApplyData)
	s.ConfirmedRound = uint64(tr.ConfirmedRound)
	s.PoolError = tr.PoolError
	return s
}

func blockEncode(b bookkeeping.Block, c agreement.Certificate) (v1.Block, error) {
	block := v1.Block{
		Hash:              crypto.Digest(b.Hash()).String(),
		PreviousBlockHash: crypto.Digest(b.Branch).String(),
		Seed:              crypto.Digest(b.Seed()).String(),
		Proposer:          c.Proposal.OriginalProposer.String(),
		Round:             uint64(b.Round()),
		TransactionsRoot:  b.TxnRoot.String(),
		RewardsRate:       b.RewardsRate,
		RewardsLevel:      b.RewardsLevel,
		RewardsResidue:    b.RewardsResidue,
		Timestamp:         b.TimeStamp,

		UpgradeState: v1.UpgradeState{
			CurrentProtocol:        string(b.CurrentProtocol),
			NextProtocol:           string(b.NextProtocol),
			NextProtocolApprovals:  b.NextProtocolApprovals,
			NextProtocolVoteBefore: uint64(b.NextProtocolVoteBefore),
			NextProtocolSwitchOn:   uint64(b.NextProtocolSwitchOn),
		},
		UpgradeVote: v1.UpgradeVote{
			UpgradePropose: string(b.UpgradePropose),
			UpgradeApprove: b.UpgradeApprove,
		},
	}

	// Transactions
	var txns []v1.Transaction
	payset, err := b.DecodePaysetWithAD()
	if err != nil {
		return v1.Block{}, err
	}

	for _, txn := range payset {
		tx := node.TxnWithStatus{
			Txn:            txn.SignedTxn,
			ConfirmedRound: b.Round(),
			ApplyData:      txn.ApplyData,
		}
		txns = append(txns, txWithStatusEncode(tx))
	}

	block.Transactions = v1.TransactionList{Transactions: txns}

	return block, nil
}

// Status is an httpHandler for route GET /v1/status
func Status(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation GET /v1/status GetStatus
	//---
	//     Summary: Gets the current node status.
	//     Produces:
	//     - application/json
	//     Schemes:
	//     - http
	//     Responses:
	//       200:
	//         ""$ref"": '#/responses/StatusResponse'
	//       500:
	//         description: Internal Error
	//         schema: {type: string}
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }
	nodeStatus, err := nodeStatus(ctx.Node)
	if err != nil {
		lib.ErrorResponse(w, http.StatusInternalServerError, err, errFailedRetrievingNodeStatus, ctx.Log)
		return
	}

	response := StatusResponse{&nodeStatus}
	SendJSON(response, w, ctx.Log)
}

// WaitForBlock is an httpHandler for route GET /v1/status/wait-for-block-after/{round:[0-9]+}
func WaitForBlock(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation GET /v1/status/wait-for-block-after/{round}/ WaitForBlock
	// ---
	//     Summary: Gets the node status after waiting for the given round.
	//     Description: Waits for a block to appear after round {round} and returns the node's status at the time.
	//     Produces:
	//     - application/json
	//     Schemes:
	//     - http
	//     Parameters:
	//       - name: round
	//         in: path
	//         type: integer
	//         format: int64
	//         minimum: 0
	//         required: true
	//         description: The round to wait until returning status
	//     Responses:
	//       200:
	//         ""$ref"": '#/responses/StatusResponse'
	//       400:
	//         description: Bad Request
	//         schema: {type: string}
	//       500:
	//         description: Internal Error
	//         schema: {type: string}
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }
	queryRound, err := strconv.ParseUint(mux.Vars(r)[""round""], 10, 64)
	if err != nil {
		lib.ErrorResponse(w, http.StatusBadRequest, err, errFailedParsingRoundNumber, ctx.Log)
		return
	}

	select {
	case <-time.After(1 * time.Minute):
	case <-ctx.Node.Ledger().Wait(basics.Round(queryRound + 1)):
	}

	nodeStatus, err := nodeStatus(ctx.Node)
	if err != nil {
		lib.ErrorResponse(w, http.StatusInternalServerError, err, errFailedRetrievingNodeStatus, ctx.Log)
		return
	}

	response := StatusResponse{&nodeStatus}
	SendJSON(response, w, ctx.Log)
}

// RawTransaction is an httpHandler for route POST /v1/transactions
func RawTransaction(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation POST /v1/transactions RawTransaction
	// ---
	//     Summary: Broadcasts a raw transaction to the network.
	//     Produces:
	//     - application/json
	//     Consumes:
	//     - application/x-binary
	//     Schemes:
	//     - http
	//     Parameters:
	//       - name: rawtxn
	//         in: body
	//         schema:
	//           type: string
	//           format: binary
	//         required: true
	//         description: The byte encoded signed transaction to broadcast to network
	//     Responses:
	//       200:
	//         ""$ref"": ""#/responses/TransactionIDResponse""
	//       400:
	//         description: Bad Request
	//         schema: {type: string}
	//       500:
	//         description: Internal Error
	//         schema: {type: string}
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }
	var st transactions.SignedTxn
	err := protocol.NewDecoder(r.Body).Decode(&st)
	if err != nil {
		lib.ErrorResponse(w, http.StatusBadRequest, err, err.Error(), ctx.Log)
		return
	}

	txid, err := ctx.Node.BroadcastSignedTxn(st)
	if err != nil {
		lib.ErrorResponse(w, http.StatusBadRequest, err, err.Error(), ctx.Log)
		return
	}

	SendJSON(TransactionIDResponse{&v1.TransactionID{TxID: txid.String()}}, w, ctx.Log)
}

// AccountInformation is an httpHandler for route GET /v1/account/{addr:[A-Z0-9]{KeyLength}}
func AccountInformation(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation GET /v1/account/{address} AccountInformation
	// ---
	//     Summary: Get account information.
	//     Description: Given a specific account public key, this call returns the accounts status, balance and spendable amounts
	//     Produces:
	//     - application/json
	//     Schemes:
	//     - http
	//     Parameters:
	//       - name: address
	//         in: path
	//         type: string
	//         pattern: ""[A-Z0-9]{58}""
	//         required: true
	//         description: An account public key
	//     Responses:
	//       200:
	//         ""$ref"": '#/responses/AccountInformationResponse'
	//       400:
	//         description: Bad Request
	//         schema: {type: string}
	//       500:
	//         description: Internal Error
	//         schema: {type: string}
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }
	queryAddr := mux.Vars(r)[""addr""]

	if queryAddr == """" {
		lib.ErrorResponse(w, http.StatusBadRequest, fmt.Errorf(errNoAccountSpecified), errNoAccountSpecified, ctx.Log)
		return
	}

	addr, err := basics.UnmarshalChecksumAddress(queryAddr)
	if err != nil {
		lib.ErrorResponse(w, http.StatusBadRequest, err, errFailedToParseAddress, ctx.Log)
		return
	}

	myLedger := ctx.Node.Ledger()
	lastRound := myLedger.Latest()
	record, err := myLedger.Lookup(lastRound, basics.Address(addr))
	if err != nil {
		lib.ErrorResponse(w, http.StatusInternalServerError, err, errFailedLookingUpLedger, ctx.Log)
		return
	}
	recordWithoutPendingRewards, err := myLedger.LookupWithoutRewards(lastRound, basics.Address(addr))
	if err != nil {
		lib.ErrorResponse(w, http.StatusInternalServerError, err, errFailedLookingUpLedger, ctx.Log)
		return
	}

	amount := record.MicroAlgos
	amountWithoutPendingRewards := recordWithoutPendingRewards.MicroAlgos
	pendingRewards, overflowed := basics.OSubA(amount, amountWithoutPendingRewards)
	if overflowed {
		err = fmt.Errorf(""overflowed pending rewards: %v - %v"", amount, amountWithoutPendingRewards)
		lib.ErrorResponse(w, http.StatusInternalServerError, err, errInternalFailure, ctx.Log)
		return
	}

	accountInfo := v1.Account{
		Round:                       uint64(lastRound),
		Address:                     addr.String(),
		Amount:                      amount.Raw,
		PendingRewards:              pendingRewards.Raw,
		AmountWithoutPendingRewards: amountWithoutPendingRewards.Raw,
		Rewards:                     record.RewardedMicroAlgos.Raw,
		Status:                      record.Status.String(),
	}

	SendJSON(AccountInformationResponse{&accountInfo}, w, ctx.Log)
}

// TransactionInformation is an httpHandler for route GET /v1/account/{addr:[A-Z0-9]{KeyLength}}/transaction/{txid:[A-Z0-9]+}
func TransactionInformation(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation GET /v1/account/{address}/transaction/{txid} TransactionInformation
	// ---
	//     Summary: Get a specific confirmed transaction.
	//     Description: >
	//       Given a wallet address and a transaction id, it returns the confirmed transaction
	//       information. This call scans up to <CurrentProtocol>.MaxTxnLife blocks in the past.
	//     Produces:
	//     - application/json
	//     Schemes:
	//     - http
	//     Parameters:
	//       - name: address
	//         in: path
	//         type: string
	//         pattern: ""[A-Z0-9]{58}""
	//         required: true
	//         description: An account public key
	//       - name: txid
	//         in: path
	//         type: string
	//         pattern: ""[A-Z0-9]+""
	//         required: true
	//         description: A transaction id
	//     Responses:
	//       200:
	//         ""$ref"": '#/responses/TransactionResponse'
	//       400:
	//         description: Bad Request
	//         schema: {type: string}
	//       404:
	//         description: Transaction Not Found
	//         schema: {type: string}
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }

	queryTxID := mux.Vars(r)[""txid""]
	if queryTxID == """" {
		lib.ErrorResponse(w, http.StatusBadRequest, fmt.Errorf(errNoTxnSpecified), errNoTxnSpecified, ctx.Log)
		return
	}

	txID := transactions.Txid{}
	if txID.UnmarshalText([]byte(queryTxID)) != nil {
		lib.ErrorResponse(w, http.StatusBadRequest, fmt.Errorf(errNoTxnSpecified), errNoTxnSpecified, ctx.Log)
		return
	}

	queryAddr := mux.Vars(r)[""addr""]
	if queryAddr == """" {
		lib.ErrorResponse(w, http.StatusBadRequest, fmt.Errorf(errNoAccountSpecified), errNoAccountSpecified, ctx.Log)
		return
	}

	addr, err := basics.UnmarshalChecksumAddress(queryAddr)
	if err != nil {
		lib.ErrorResponse(w, http.StatusBadRequest, fmt.Errorf(errFailedToParseAddress), errFailedToParseAddress, ctx.Log)
		return
	}

	latestRound := ctx.Node.Ledger().Latest()
	stat, err := ctx.Node.Status()
	if err != nil {
		lib.ErrorResponse(w, http.StatusInternalServerError, err, errFailedRetrievingNodeStatus, ctx.Log)
		return
	}
	proto := config.Consensus[stat.LastVersion]
	// non-Archival nodes keep proto.MaxTxnLife blocks around,
	// so without the + 1 in the below calculation,
	// Node.GetTransaction will query 1 round more than is kept around
	start := latestRound - basics.Round(proto.MaxTxnLife) + 1
	if latestRound < basics.Round(proto.MaxTxnLife) {
		start = 0
	}

	if txn, ok := ctx.Node.GetTransaction(addr, txID, start, latestRound); ok {
		var responseTxs v1.Transaction
		responseTxs = txWithStatusEncode(txn)

		response := TransactionResponse{
			Body: &responseTxs,
		}

		SendJSON(response, w, ctx.Log)
		return
	}

	// We didn't find it, return a failure
	lib.ErrorResponse(w, http.StatusNotFound, err, errTransactionNotFound, ctx.Log)
	return
}

// PendingTransactionInformation is an httpHandler for route GET /v1/transactions/pending/{txid:[A-Z0-9]+}
func PendingTransactionInformation(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation GET /v1/transactions/pending/{txid} PendingTransactionInformation
	// ---
	//     Summary: Get a specific pending transaction.
	//     Description: >
	//       Given a transaction id of a recently submitted transaction, it returns information
	//       about it.  There are several cases when this might succeed:
	//
	//       - transaction committed (committed round > 0)
	//       - transaction still in the pool (committed round = 0, pool error = """")
	//       - transaction removed from pool due to error (committed round = 0, pool error != """")
	//
	//       Or the transaction may have happened sufficiently long ago that the
	//       node no longer remembers it, and this will return an error.
	//     Produces:
	//     - application/json
	//     Schemes:
	//     - http
	//     Parameters:
	//       - name: txid
	//         in: path
	//         type: string
	//         pattern: ""[A-Z0-9]+""
	//         required: true
	//         description: A transaction id
	//     Responses:
	//       200:
	//         ""$ref"": '#/responses/TransactionResponse'
	//       400:
	//         description: Bad Request
	//         schema: {type: string}
	//       404:
	//         description: Transaction Not Found
	//         schema: {type: string}
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }

	queryTxID := mux.Vars(r)[""txid""]
	if queryTxID == """" {
		lib.ErrorResponse(w, http.StatusBadRequest, fmt.Errorf(errNoTxnSpecified), errNoTxnSpecified, ctx.Log)
		return
	}

	txID := transactions.Txid{}
	if txID.UnmarshalText([]byte(queryTxID)) != nil {
		lib.ErrorResponse(w, http.StatusBadRequest, fmt.Errorf(errNoTxnSpecified), errNoTxnSpecified, ctx.Log)
		return
	}

	if txn, ok := ctx.Node.GetPendingTransaction(txID); ok {
		var responseTxs v1.Transaction
		responseTxs = txWithStatusEncode(txn)

		response := TransactionResponse{
			Body: &responseTxs,
		}

		SendJSON(response, w, ctx.Log)
		return
	}

	// We didn't find it, return a failure
	lib.ErrorResponse(w, http.StatusNotFound, errors.New(errTransactionNotFound), errTransactionNotFound, ctx.Log)
	return
}

// GetPendingTransactions is an httpHandler for route GET /v1/transactions/pending.
func GetPendingTransactions(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation GET /v1/transactions/pending GetPendingTransactions
	// ---
	//     Summary: Get a list of unconfirmed transactions currently in the transaction pool.
	//     Description: >
	//       Get the list of pending transactions, sorted by priority,
	//       in decreasing order, truncated at the end at MAX. If MAX = 0,
	//       returns all pending transactions.
	//     Produces:
	//     - application/json
	//     Schemes:
	//     - http
	//     Parameters:
	//       - name: max
	//         in: query
	//         type: integer
	//         format: int64
	//         minimum: 0
	//         required: false
	//         description: Truncated number of transactions to display. If max=0, returns all pending txns.
	//     Responses:
	//       ""200"":
	//         ""$ref"": '#/responses/PendingTransactionsResponse'
	//       500:
	//         description: Internal Error
	//         schema: {type: string}
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }
	max, err := strconv.ParseUint(r.FormValue(""max""), 10, 64)
	if err != nil {
		max = 0
	}

	txs, err := ctx.Node.GetPendingTxnsFromPool()
	if err != nil {
		lib.ErrorResponse(w, http.StatusInternalServerError, err, errFailedLookingUpTransactionPool, ctx.Log)
		return
	}

	totalTxns := uint64(len(txs))
	if max > 0 && totalTxns > max {
		// we expose this truncating mechanism for the client only, for the flexibility
		// to avoid dumping the whole pool over REST or in a cli. There is no need to optimize
		// fetching a smaller transaction set at a lower level.
		txs = txs[:max]
	}

	responseTxs := make([]v1.Transaction, len(txs))
	for i, twr := range txs {
		responseTxs[i] = paymentTxEncode(twr.Txn, transactions.ApplyData{})
	}

	response := PendingTransactionsResponse{
		Body: &v1.PendingTransactions{
			TruncatedTxns: v1.TransactionList{
				Transactions: responseTxs,
			},
			TotalTxns: totalTxns,
		},
	}

	SendJSON(response, w, ctx.Log)
}

// SuggestedFee is an httpHandler for route GET /v1/transactions/fee
func SuggestedFee(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation GET /v1/transactions/fee SuggestedFee
	// ---
	//     Summary: Get the suggested fee
	//     Description: >
	//       Suggested Fee is returned in units of micro-Algos per byte.
	//       Suggested Fee may fall to zero but submitted transactions
	//       must still have a fee of at least MinTxnFee for the current
	//       network protocol.
	//     Produces:
	//     - application/json
	//     Schemes:
	//     - http
	//     Responses:
	//       ""200"":
	//         ""$ref"": '#/responses/TransactionFeeResponse'
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }
	fee := v1.TransactionFee{Fee: ctx.Node.SuggestedFee().Raw}
	SendJSON(TransactionFeeResponse{&fee}, w, ctx.Log)
}

// SuggestedParams is an httpHandler for route GET /v1/transactions/params
func SuggestedParams(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation GET /v1/transactions/params TransactionParams
	// ---
	//     Summary: Get parameters for constructing a new transaction
	//     Produces:
	//     - application/json
	//     Schemes:
	//     - http
	//     Responses:
	//       ""200"":
	//         ""$ref"": '#/responses/TransactionParamsResponse'
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }
	stat, err := ctx.Node.Status()
	if err != nil {
		lib.ErrorResponse(w, http.StatusInternalServerError, err, errFailedRetrievingNodeStatus, ctx.Log)
		return
	}

	gh := ctx.Node.GenesisHash()

	var params v1.TransactionParams
	params.Fee = ctx.Node.SuggestedFee().Raw
	params.GenesisID = ctx.Node.GenesisID()
	params.GenesisHash = gh[:]
	params.LastRound = uint64(stat.LastRound)
	params.ConsensusVersion = string(stat.LastVersion)

	proto := config.Consensus[stat.LastVersion]
	params.MinTxnFee = proto.MinTxnFee

	SendJSON(TransactionParamsResponse{&params}, w, ctx.Log)
}

// GetBlock is an httpHandler for route GET /v1/block/{round}
func GetBlock(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation GET /v1/block/{round} GetBlock
	// ---
	//     Summary: Get the block for the given round.
	//     Produces:
	//     - application/json
	//     Schemes:
	//     - http
	//     Parameters:
	//       - name: round
	//         in: path
	//         type: integer
	//         format: int64
	//         minimum: 0
	//         required: true
	//         description: The round from which to fetch block information.
	//     Responses:
	//       200:
	//         ""$ref"": '#/responses/BlockResponse'
	//       400:
	//         description: Bad Request
	//         schema: {type: string}
	//       500:
	//         description: Internal Error
	//         schema: {type: string}
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }
	queryRound, err := strconv.ParseUint(mux.Vars(r)[""round""], 10, 64)
	if err != nil {
		lib.ErrorResponse(w, http.StatusBadRequest, err, errFailedParsingRoundNumber, ctx.Log)
		return
	}

	b, c, err := ctx.Node.Ledger().BlockCert(basics.Round(queryRound))
	if err != nil {
		lib.ErrorResponse(w, http.StatusInternalServerError, err, errFailedLookingUpLedger, ctx.Log)
		return
	}
	block, err := blockEncode(b, c)

	if err != nil {
		lib.ErrorResponse(w, http.StatusInternalServerError, err, errInternalFailure, ctx.Log)
		return
	}

	SendJSON(BlockResponse{&block}, w, ctx.Log)
}

// GetSupply is an httpHandler for route GET /v1/ledger/supply
func GetSupply(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation GET /v1/ledger/supply GetSupply
	//---
	//     Summary: Get the current supply reported by the ledger.
	//     Produces:
	//     - application/json
	//     Schemes:
	//     - http
	//     Responses:
	//       200:
	//         ""$ref"": '#/responses/SupplyResponse'
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }
	latest := ctx.Node.Ledger().Latest()
	totals, err := ctx.Node.Ledger().Totals(latest)
	if err != nil {
		err = fmt.Errorf(""GetSupply(): round %d failed: %v"", latest, err)
		lib.ErrorResponse(w, http.StatusInternalServerError, err, errInternalFailure, ctx.Log)
		return
	}
	supply := v1.Supply{
		Round:       uint64(latest),
		TotalMoney:  totals.Participating().Raw,
		OnlineMoney: totals.Online.Money.Raw,
	}
	SendJSON(SupplyResponse{&supply}, w, ctx.Log)
}

func parseTime(t string) (res time.Time, err error) {
	// check for just date
	res, err = time.Parse(""2006-01-02"", t)
	if err == nil {
		return
	}

	// check for date and time
	res, err = time.Parse(time.RFC3339, t)
	if err == nil {
		return
	}

	return
}

// Transactions is an httpHandler for route GET /v1/account/{addr:[A-Z0-9]+}/transactions
func Transactions(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation GET /v1/account/{address}/transactions Transactions
	// ---
	//     Summary: Get a list of confirmed transactions.
	//     Description: Returns the list of confirmed transactions between within a date range. This call is available only when the indexer is running.
	//     Produces:
	//     - application/json
	//     Schemes:
	//     - http
	//     Parameters:
	//       - name: address
	//         in: path
	//         type: string
	//         pattern: ""[A-Z0-9]{58}""
	//         required: true
	//         description: An account public key
	//       - name: firstRound
	//         in: query
	//         type: integer
	//         format: int64
	//         minimum: 0
	//         required: false
	//         description: Do not fetch any transactions before this round.
	//       - name: lastRound
	//         in: query
	//         type: integer
	//         format: int64
	//         minimum: 0
	//         required: false
	//         description: Do not fetch any transactions after this round.
	//       - name: fromDate
	//         in: query
	//         type: string
	//         format: date
	//         required: false
	//         description: Do not fetch any transactions before this date. (enabled only with indexer)
	//       - name: toDate
	//         in: query
	//         type: string
	//         format: date
	//         required: false
	//         description: Do not fetch any transactions after this date. (enabled only with indexer)
	//       - name: max
	//         in: query
	//         type: integer
	//         format: int64
	//         required: false
	//         description: maximum transactions to show (default to 100)
	//     Responses:
	//       200:
	//         ""$ref"": '#/responses/TransactionsResponse'
	//       400:
	//         description: Bad Request
	//         schema: {type: string}
	//       500:
	//         description: Internal Error
	//         schema: {type: string}
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }

	queryAddr := mux.Vars(r)[""addr""]
	addr, err := basics.UnmarshalChecksumAddress(queryAddr)
	if err != nil {
		lib.ErrorResponse(w, http.StatusBadRequest, err, errFailedToParseAddress, ctx.Log)
		return
	}

	max, err := strconv.ParseUint(r.FormValue(""max""), 10, 64)
	if err != nil {
		max = 100
	}

	// Get different params
	firstRound := r.FormValue(""firstRound"")
	lastRound := r.FormValue(""lastRound"")
	fromDate := r.FormValue(""fromDate"")
	toDate := r.FormValue(""toDate"")

	var rounds []uint64
	var txs []node.TxnWithStatus
	// Were rounds provided?
	if firstRound != """" && lastRound != """" {
		// Are they valid?
		fR, err := strconv.ParseUint(firstRound, 10, 64)
		if err != nil {
			lib.ErrorResponse(w, http.StatusBadRequest, err, errFailedParsingRoundNumber, ctx.Log)
			return
		}

		lR, err := strconv.ParseUint(lastRound, 10, 64)
		if err != nil {
			lib.ErrorResponse(w, http.StatusBadRequest, err, errFailedParsingRoundNumber, ctx.Log)
			return
		}

		txs, err = ctx.Node.ListTxns(addr, basics.Round(fR), basics.Round(lR))
		if err != nil {
			switch err.(type) {
			case ledger.ErrNoEntry:
				if !ctx.Node.IsArchival() {
					lib.ErrorResponse(w, http.StatusInternalServerError, err, errBlockHashBeenDeletedArchival, ctx.Log)
					return
				}
			}

			lib.ErrorResponse(w, http.StatusInternalServerError, err, err.Error(), ctx.Log)
			return
		}

	} else {
		// is indexer on?
		indexer, err := ctx.Node.Indexer()
		if err != nil {
			lib.ErrorResponse(w, http.StatusBadRequest, err, errNoRoundsSpecified, ctx.Log)
			return
		}

		// Were dates provided?
		if fromDate != """" && toDate != """" {
			fd, err := parseTime(fromDate)
			if err != nil {
				lib.ErrorResponse(w, http.StatusBadRequest, err, err.Error(), ctx.Log)
				return
			}

			td, err := parseTime(toDate)
			if err != nil {
				lib.ErrorResponse(w, http.StatusBadRequest, err, err.Error(), ctx.Log)
				return
			}

			rounds, err = indexer.GetRoundsByAddressAndDate(addr.String(), max, fd.Unix(), td.Unix())
			if err != nil {
				lib.ErrorResponse(w, http.StatusInternalServerError, err, err.Error(), ctx.Log)
				return
			}

		} else {
			// return last [max] transactions
			rounds, err = indexer.GetRoundsByAddress(addr.String(), max)
			if err != nil {
				lib.ErrorResponse(w, http.StatusInternalServerError, err, errFailedGettingInformationFromIndexer, ctx.Log)
				return
			}
		}
	}

	if len(rounds) > 0 {
		for _, rnd := range rounds {
			txns, _ := ctx.Node.ListTxns(addr, basics.Round(rnd), basics.Round(rnd))
			txs = append(txs, txns...)

			// They may be more txns in the round than requested, break.
			if uint64(len(txs)) > max {
				break
			}
		}
	}

	// clip length to [max]
	if uint64(len(txs)) > max {
		txs = txs[:max]
	}

	responseTxs := make([]v1.Transaction, len(txs))
	for i, twr := range txs {
		responseTxs[i] = txWithStatusEncode(twr)
	}

	response := TransactionsResponse{
		&v1.TransactionList{
			Transactions: responseTxs,
		},
	}

	SendJSON(response, w, ctx.Log)
}

// GetTransactionByID is an httpHandler for route GET /v1/transaction/{txid}
func GetTransactionByID(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation GET /v1/transaction/{txid} Transaction
	// ---
	//     Summary: Get an information of a single transaction.
	//     Description: Returns the transaction information of the given txid. Works only if the indexer is enabled.
	//     Produces:
	//     - application/json
	//     Schemes:
	//     - http
	//     Parameters:
	//       - name: txid
	//         in: path
	//         type: string
	//         pattern: ""[A-Z0-9]+""
	//         required: true
	//         description: A transaction id
	//     Responses:
	//       200:
	//         ""$ref"": '#/responses/TransactionResponse'
	//       400:
	//         description: Bad Request
	//         schema: {type: string}
	//       404:
	//         description: Transaction Not Found
	//         schema: {type: string}
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }

	indexer, err := ctx.Node.Indexer()
	if err != nil {
		lib.ErrorResponse(w, http.StatusInternalServerError, err, errIndexerNotRunning, ctx.Log)
		return
	}

	queryTxID := mux.Vars(r)[""txid""]
	if queryTxID == """" {
		lib.ErrorResponse(w, http.StatusBadRequest, fmt.Errorf(errNoTxnSpecified), errNoTxnSpecified, ctx.Log)
		return
	}

	var txID transactions.Txid
	if err := txID.UnmarshalText([]byte(queryTxID)); err != nil {
		lib.ErrorResponse(w, http.StatusBadRequest, err, err.Error(), ctx.Log)
		return
	}

	rnd, err := indexer.GetRoundByTXID(queryTxID)
	if err != nil {
		lib.ErrorResponse(w, http.StatusInternalServerError, err, errFailedGettingInformationFromIndexer, ctx.Log)
		return
	}

	if txn, err := ctx.Node.GetTransactionByID(txID, basics.Round(rnd)); err == nil {
		var responseTxs v1.Transaction
		responseTxs = txWithStatusEncode(txn)

		response := TransactionResponse{
			Body: &responseTxs,
		}

		SendJSON(response, w, ctx.Log)
		return
	}

	// We didn't find it, return a failure
	lib.ErrorResponse(w, http.StatusNotFound, errors.New(errTransactionNotFound), errTransactionNotFound, ctx.Log)
	return
}
",Use a case statement here to switch between the different transaction types.,"The suggested code changes are comprehensive and well-documented. Here's a concise review:

### Key Points:
1. **Functionality**: The code provides various endpoints to interact with the Algorand node, including retrieving node status, waiting for a specific block, broadcasting transactions, querying account information, transaction details, pending transactions, suggested fees, transaction parameters, block data, ledger supply, and transaction history.

2. **Error Handling**: Each endpoint includes robust error handling, returning appropriate HTTP status codes and messages based on the nature of the error.

3. **API Documentation**: Swagger annotations are used to generate API documentation, making it easy for developers to understand how to use the endpoints.

4. **Code Structure**: The code is organized into functions that handle specific tasks, such as encoding transactions, processing block data, and interacting with the ledger.

5. **Consistency**: The code follows a consistent naming convention and structure, making it easier to maintain and extend.

6. **Security**: The code includes checks for invalid API tokens and handles potential security vulnerabilities, such as parsing errors.

7. **Performance**: The code is designed to efficiently handle requests, especially those involving large datasets or complex queries.

Overall, the code is well-structured, efficient, and easy to understand, making it suitable for integration into an Algorand node's API."
69,"@@ -192,7 +192,17 @@ class MultiBackend extends AbstractBase implements \Zend\Log\LoggerAwareInterfac
     {
         $items = [];
         foreach ($ids as $id) {
-            $items[] = $this->getStatus($id);
+            try {
+                $items[] = $this->getStatus($id);
+            } catch (ILSException $e) {
+                $items[] = [
+                    [
+                        'id' => $id,
+                        'error' => 'An error has occurred',
+                        'availability_message' => 'status_unknown_message'
+                    ]
+                ];
+            }
         }
         return $items;
     }","<?php
/**
 * Multiple Backend Driver.
 *
 * PHP version 7
 *
 * Copyright (C) The National Library of Finland 2012-2018.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2,
 * as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 *
 * @category VuFind
 * @package  ILSdrivers
 * @author   Ere Maijala <ere.maijala@helsinki.fi>
 * @author   Demian Katz <demian.katz@villanova.edu>
 * @license  http://opensource.org/licenses/gpl-2.0.php GNU General Public License
 * @link     https://vufind.org/wiki/development:plugins:ils_drivers Wiki
 */
namespace VuFind\ILS\Driver;

use VuFind\Exception\ILS as ILSException;

/**
 * Multiple Backend Driver.
 *
 * This driver allows to use multiple backends determined by a record id or
 * user id prefix (e.g. source.12345).
 *
 * @category VuFind
 * @package  ILSdrivers
 * @author   Ere Maijala <ere.maijala@helsinki.fi>
 * @license  http://opensource.org/licenses/gpl-2.0.php GNU General Public License
 * @link     https://vufind.org/wiki/development:plugins:ils_drivers Wiki
 */
class MultiBackend extends AbstractBase implements \Zend\Log\LoggerAwareInterface
{
    use \VuFind\Log\LoggerAwareTrait {
        logError as error;
    }
    /**
     * The array of configured driver names.
     *
     * @var string[]
     */
    protected $drivers = [];

    /**
     * The default driver to use
     *
     * @var string
     */
    protected $defaultDriver;

    /**
     * The path to the driver configurations relative to the config path
     *
     * @var string
     */
    protected $driversConfigPath;

    /**
     * The array of cached drivers
     *
     * @var object[]
     */
    protected $driverCache = [];

    /**
     * The array of driver configuration options.
     *
     * @var string[]
     */
    protected $config = [];

    /**
     * Configuration loader
     *
     * @var \VuFind\Config\PluginManager
     */
    protected $configLoader;

    /**
     * ILS authenticator
     *
     * @var \VuFind\Auth\ILSAuthenticator
     */
    protected $ilsAuth;

    /**
     * ILS driver manager
     *
     * @var PluginManager
     */
    protected $driverManager;

    /**
     * Constructor
     *
     * @param \VuFind\Config\PluginManager  $configLoader Configuration loader
     * @param \VuFind\Auth\ILSAuthenticator $ilsAuth      ILS authenticator
     * @param PluginManager                 $dm           ILS driver manager
     */
    public function __construct(\VuFind\Config\PluginManager $configLoader,
        \VuFind\Auth\ILSAuthenticator $ilsAuth, PluginManager $dm
    ) {
        $this->configLoader = $configLoader;
        $this->ilsAuth = $ilsAuth;
        $this->driverManager = $dm;
    }

    /**
     * Set the driver configuration.
     *
     * @param Config $config The configuration to be set
     *
     * @return void
     */
    public function setConfig($config)
    {
        $this->config = $config;
    }

    /**
     * Initialize the driver.
     *
     * Validate configuration and perform all resource-intensive tasks needed to
     * make the driver active.
     *
     * @throws ILSException
     * @return void
     */
    public function init()
    {
        if (empty($this->config)) {
            throw new ILSException('Configuration needs to be set.');
        }
        $this->drivers = $this->config['Drivers'];
        $this->defaultDriver = isset($this->config['General']['default_driver'])
            ? $this->config['General']['default_driver']
            : null;
        $this->driversConfigPath
            = isset($this->config['General']['drivers_config_path'])
            ? $this->config['General']['drivers_config_path']
            : null;
    }

    /**
     * Get Status
     *
     * This is responsible for retrieving the status information of a certain
     * record.
     *
     * @param string $id The record id to retrieve the holdings for
     *
     * @throws ILSException
     * @return mixed     On success, an associative array with the following keys:
     * id, availability (boolean), status, location, reserve, callnumber.
     */
    public function getStatus($id)
    {
        $source = $this->getSource($id);
        $driver = $this->getDriver($source);
        if ($driver) {
            $status = $driver->getStatus($this->getLocalId($id));
            return $this->addIdPrefixes($status, $source);
        }
        return [];
    }

    /**
     * Get Statuses
     *
     * This is responsible for retrieving the status information for a
     * collection of records.
     *
     * @param array $ids The array of record ids to retrieve the status for
     *
     * @throws ILSException
     * @return array     An array of getStatus() return values on success.
     */
    public function getStatuses($ids)
            try {
                $items[] = $this->getStatus($id);
            } catch (ILSException $e) {
                $items[] = [
                    [
                        'id' => $id,
                        'error' => 'An error has occurred',
                        'availability_message' => 'status_unknown_message'
                    ]
                ];
            }
    {
        $items = [];
        foreach ($ids as $id) {
            $items[] = $this->getStatus($id);
        }
        return $items;
    }

    /**
     * Get Holding
     *
     * This is responsible for retrieving the holding information of a certain
     * record.
     *
     * @param string $id     The record id to retrieve the holdings for
     * @param array  $patron Patron data
     *
     * @return array         On success, an associative array with the following
     * keys: id, availability (boolean), status, location, reserve, callnumber,
     * duedate, number, barcode.
     */
    public function getHolding($id, array $patron = null)
    {
        $source = $this->getSource($id);
        $driver = $this->getDriver($source);
        if ($driver) {
            // If the patron belongs to another source, just pass on an empty array
            // to indicate that the patron has logged in but is not available for the
            // current catalog.
            if ($patron && $this->getSource($patron['cat_username']) !== $source) {
                $patron = [];
            }
            $holdings = $driver->getHolding(
                $this->getLocalId($id),
                $this->stripIdPrefixes($patron, $source)
            );
            return $this->addIdPrefixes($holdings, $source);
        }
        return [];
    }

    /**
     * Get Purchase History
     *
     * This is responsible for retrieving the acquisitions history data for the
     * specific record (usually recently received issues of a serial).
     *
     * @param string $id The record id to retrieve the info for
     *
     * @throws ILSException
     * @return array     An array with the acquisitions data on success.
     */
    public function getPurchaseHistory($id)
    {
        $source = $this->getSource($id);
        $driver = $this->getDriver($source);
        if ($driver) {
            return $driver->getPurchaseHistory($this->getLocalId($id));
        }
        return [];
    }

    /**
     * Get available login targets (drivers enabled for login)
     *
     * @return string[] Source ID's
     */
    public function getLoginDrivers()
    {
        return isset($this->config['Login']['drivers'])
            ? $this->config['Login']['drivers']
            : [];
    }

    /**
     * Get default login driver
     *
     * @return string Default login driver or empty string
     */
    public function getDefaultLoginDriver()
    {
        if (isset($this->config['Login']['default_driver'])) {
            return $this->config['Login']['default_driver'];
        }
        $drivers = $this->getLoginDrivers();
        if ($drivers) {
            return $drivers[0];
        }
        return '';
    }

    /**
     * Get New Items
     *
     * Retrieve the IDs of items recently added to the catalog.
     *
     * @param int $page    Page number of results to retrieve (counting starts at 1)
     * @param int $limit   The size of each page of results to retrieve
     * @param int $daysOld The maximum age of records to retrieve in days (max. 30)
     * @param int $fundId  optional fund ID to use for limiting results (use a value
     * returned by getFunds, or exclude for no limit); note that ""fund"" may be a
     * misnomer - if funds are not an appropriate way to limit your new item
     * results, you can return a different set of values from getFunds. The
     * important thing is that this parameter supports an ID returned by getFunds,
     * whatever that may mean.
     *
     * @return array       Associative array with 'count' and 'results' keys
     */
    public function getNewItems($page, $limit, $daysOld, $fundId = null)
    {
        $driver = $this->getDriver($this->defaultDriver);
        if ($driver) {
            $result = $driver->getNewItems($page, $limit, $daysOld, $fundId);
            if (isset($result['results'])) {
                $result['results']
                    = $this->addIdPrefixes($result['results'], $this->defaultDriver);
            }
            return $result;
        }
        return [];
    }

    /**
     * Get Departments
     *
     * Obtain a list of departments for use in limiting the reserves list.
     *
     * @return array An associative array with key = dept. ID, value = dept. name.
     */
    public function getDepartments()
    {
        $driver = $this->getDriver($this->defaultDriver);
        if ($driver) {
            return $driver->getDepartments();
        }
        return [];
    }

    /**
     * Get Instructors
     *
     * Obtain a list of instructors for use in limiting the reserves list.
     *
     * @return array An associative array with key = ID, value = name.
     */
    public function getInstructors()
    {
        $driver = $this->getDriver($this->defaultDriver);
        if ($driver) {
            return $driver->getInstructors();
        }
        return [];
    }

    /**
     * Get Courses
     *
     * Obtain a list of courses for use in limiting the reserves list.
     *
     * @return array An associative array with key = ID, value = name.
     */
    public function getCourses()
    {
        $driver = $this->getDriver($this->defaultDriver);
        if ($driver) {
            return $driver->getCourses();
        }
        return [];
    }

    /**
     * Find Reserves
     *
     * Obtain information on course reserves.
     *
     * @param string $course ID from getCourses (empty string to match all)
     * @param string $inst   ID from getInstructors (empty string to match all)
     * @param string $dept   ID from getDepartments (empty string to match all)
     *
     * @return mixed An array of associative arrays representing reserve items
     */
    public function findReserves($course, $inst, $dept)
    {
        $driver = $this->getDriver($this->defaultDriver);
        if ($driver) {
            return $this->addIdPrefixes(
                $driver->findReserves($course, $inst, $dept),
                $this->defaultDriver,
                ['BIB_ID']
            );
        }
        return [];
    }

    /**
     * Get Patron Profile
     *
     * This is responsible for retrieving the profile for a specific patron.
     *
     * @param array $patron The patron array
     *
     * @return mixed      Array of the patron's profile data
     */
    public function getMyProfile($patron)
    {
        $source = $this->getSource($patron['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver) {
            $profile = $driver
                ->getMyProfile($this->stripIdPrefixes($patron, $source));
            return $this->addIdPrefixes($profile, $source);
        }
        return [];
    }

    /**
     * Patron Login
     *
     * This is responsible for authenticating a patron against the catalog.
     *
     * @param string $username The patron user id or barcode
     * @param string $password The patron password
     *
     * @return mixed           Associative array of patron info on successful login,
     * null on unsuccessful login.
     */
    public function patronLogin($username, $password)
    {
        $source = $this->getSource($username);
        if (!$source) {
            $source = $this->getDefaultLoginDriver();
        }
        $driver = $this->getDriver($source);
        if ($driver) {
            $patron = $driver->patronLogin(
                $this->getLocalId($username), $password
            );
            $patron = $this->addIdPrefixes($patron, $source);
            return $patron;
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Get Patron Transactions
     *
     * This is responsible for retrieving all transactions (i.e. checked out items)
     * by a specific patron.
     *
     * @param array $patron The patron array from patronLogin
     *
     * @return mixed      Array of the patron's transactions
     */
    public function getMyTransactions($patron)
    {
        $source = $this->getSource($patron['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver) {
            $transactions = $driver->getMyTransactions(
                $this->stripIdPrefixes($patron, $source)
            );
            return $this->addIdPrefixes($transactions, $source);
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Get Patron Transaction History
     *
     * This is responsible for retrieving all historic transactions
     * (i.e. checked out items) by a specific patron.
     *
     * @param array $patron The patron array from patronLogin
     * @param array $params Retrieval params
     *
     * @return array        Array of the patron's transactions
     */
    public function getMyTransactionHistory($patron, $params)
    {
        $source = $this->getSource($patron['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver) {
            $transactions = $driver->getMyTransactionHistory(
                $this->stripIdPrefixes($patron, $source), $params
            );
            return $this->addIdPrefixes($transactions, $source);
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Get Renew Details
     *
     * In order to renew an item, the ILS requires information on the item and
     * patron. This function returns the information as a string which is then used
     * as submitted form data in checkedOut.php. This value is then extracted by
     * the RenewMyItems function.
     *
     * @param array $checkoutDetails An array of item data
     *
     * @return string Data for use in a form field
     */
    public function getRenewDetails($checkoutDetails)
    {
        $source = $this->getSource($checkoutDetails['id'] ?? '');
        $driver = $this->getDriver($source);
        if ($driver) {
            $details = $driver->getRenewDetails(
                $this->stripIdPrefixes($checkoutDetails, $source)
            );
            return $this->addIdPrefixes($details, $source);
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Renew My Items
     *
     * Function for attempting to renew a patron's items. The data in
     * $renewDetails['details'] is determined by getRenewDetails().
     *
     * @param array $renewDetails An array of data required for renewing items
     * including the Patron ID and an array of renewal IDS
     *
     * @return array An array of renewal information keyed by item ID
     */
    public function renewMyItems($renewDetails)
    {
        $source = $this->getSource($renewDetails['patron']['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver) {
            $details = $driver->renewMyItems(
                $this->stripIdPrefixes($renewDetails, $source)
            );
            return $this->addIdPrefixes($details, $source);
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Get Patron Fines
     *
     * This is responsible for retrieving all fines by a specific patron.
     *
     * @param array $patron The patron array from patronLogin
     *
     * @return mixed      Array of the patron's fines
     */
    public function getMyFines($patron)
    {
        $source = $this->getSource($patron['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver) {
            $fines = $driver->getMyFines($this->stripIdPrefixes($patron, $source));
            return $this->addIdPrefixes($fines, $source);
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Get Patron Holds
     *
     * This is responsible for retrieving all holds by a specific patron.
     *
     * @param array $patron The patron array from patronLogin
     *
     * @return mixed      Array of the patron's holds
     */
    public function getMyHolds($patron)
    {
        $source = $this->getSource($patron['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver) {
            $holds = $driver->getMyHolds($this->stripIdPrefixes($patron, $source));
            return $this->addIdPrefixes(
                $holds, $source, ['id', 'item_id', 'cat_username']
            );
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Get Patron Call Slips
     *
     * This is responsible for retrieving all call slips by a specific patron.
     *
     * @param array $patron The patron array from patronLogin
     *
     * @return mixed      Array of the patron's holds
     */
    public function getMyStorageRetrievalRequests($patron)
    {
        $source = $this->getSource($patron['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver) {
            $supported = $this->methodSupported(
                $driver, 'getMyStorageRetrievalRequests', compact('patron')
            );
            if (!$supported) {
                // Return empty array if not supported by the driver
                return [];
            }
            $requests = $driver->getMyStorageRetrievalRequests(
                $this->stripIdPrefixes($patron, $source)
            );
            return $this->addIdPrefixes($requests, $source);
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Check whether a hold or recall request is valid
     *
     * This is responsible for determining if an item is requestable
     *
     * @param string $id     The Bib ID
     * @param array  $data   An Array of item data
     * @param patron $patron An array of patron data
     *
     * @return mixed An array of data on the request including
     * whether or not it is valid and a status message. Alternatively a boolean
     * true if request is valid, false if not.
     */
    public function checkRequestIsValid($id, $data, $patron)
    {
        if (!isset($patron['cat_username'])) {
            return false;
        }
        $source = $this->getSource($patron['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver) {
            if ($this->getSource($id) != $source) {
                return false;
            }
            return $driver->checkRequestIsValid(
                $this->stripIdPrefixes($id, $source),
                $this->stripIdPrefixes($data, $source),
                $this->stripIdPrefixes($patron, $source)
            );
        }
        return false;
    }

    /**
     * Check whether a storage retrieval request is valid
     *
     * This is responsible for determining if an item is requestable
     *
     * @param string $id     The Bib ID
     * @param array  $data   An Array of item data
     * @param patron $patron An array of patron data
     *
     * @return mixed An array of data on the request including
     * whether or not it is valid and a status message. Alternatively a boolean
     * true if request is valid, false if not.
     */
    public function checkStorageRetrievalRequestIsValid($id, $data, $patron)
    {
        $source = $this->getSource($patron['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver) {
            if ($this->getSource($id) != $source
                || !is_callable(
                    [$driver, 'checkStorageRetrievalRequestIsValid']
                )
            ) {
                return false;
            }
            return $driver->checkStorageRetrievalRequestIsValid(
                $this->stripIdPrefixes($id, $source),
                $this->stripIdPrefixes($data, $source),
                $this->stripIdPrefixes($patron, $source)
            );
        }
        return false;
    }

    /**
     * Get Pick Up Locations
     *
     * This is responsible get a list of valid library locations for holds / recall
     * retrieval
     *
     * @param array $patron      Patron information returned by the patronLogin
     * method.
     * @param array $holdDetails Optional array, only passed in when getting a list
     * in the context of placing a hold; contains most of the same values passed to
     * placeHold, minus the patron data.  May be used to limit the pickup options
     * or may be ignored.  The driver must not add new options to the return array
     * based on this data or other areas of VuFind may behave incorrectly.
     *
     * @return array        An array of associative arrays with locationID and
     * locationDisplay keys
     */
    public function getPickUpLocations($patron = false, $holdDetails = null)
    {
        $source = $this->getSource($patron['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver) {
            if ($holdDetails) {
                if ($this->getSource($holdDetails['id']) != $source) {
                    // Return empty array since the sources don't match
                    return [];
                }
            }
            $locations = $driver->getPickUpLocations(
                $this->stripIdPrefixes($patron, $source),
                $this->stripIdPrefixes($holdDetails, $source)
            );
            return $this->addIdPrefixes($locations, $source);
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Get Default Pick Up Location
     *
     * Returns the default pick up location
     *
     * @param array $patron      Patron information returned by the patronLogin
     * method.
     * @param array $holdDetails Optional array, only passed in when getting a list
     * in the context of placing a hold; contains most of the same values passed to
     * placeHold, minus the patron data.  May be used to limit the pickup options
     * or may be ignored.
     *
     * @return string A location ID
     */
    public function getDefaultPickUpLocation($patron = false, $holdDetails = null)
    {
        $source = $this->getSource($patron['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver) {
            if ($holdDetails) {
                if ($this->getSource($holdDetails['id']) != $source) {
                    // Return false since the sources don't match
                    return false;
                }
            }
            $locations = $driver->getDefaultPickUpLocation(
                $this->stripIdPrefixes($patron, $source),
                $this->stripIdPrefixes($holdDetails, $source)
            );
            return $this->addIdPrefixes($locations, $source);
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Get request groups
     *
     * @param int   $id          BIB ID
     * @param array $patron      Patron information returned by the patronLogin
     * method.
     * @param array $holdDetails Optional array, only passed in when getting a list
     * in the context of placing a hold; contains most of the same values passed to
     * placeHold, minus the patron data.  May be used to limit the request group
     * options or may be ignored.
     *
     * @return array  An array of associative arrays with requestGroupId and
     * name keys
     */
    public function getRequestGroups($id, $patron, $holdDetails = null)
    {
        $source = $this->getSource($id);
        $driver = $this->getDriver($source);
        if ($driver) {
            if ($this->getSource($patron['cat_username']) != $source
                || !$this->methodSupported(
                    $driver,
                    'getRequestGroups',
                    compact('id', 'patron', 'holdDetails')
                )
            ) {
                // Return empty array since the sources don't match or the method
                // isn't supported by the driver
                return [];
            }
            $groups = $driver->getRequestGroups(
                $this->stripIdPrefixes($id, $source),
                $this->stripIdPrefixes($patron, $source),
                $this->stripIdPrefixes($holdDetails, $source)
            );
            return $groups;
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Get Default Request Group
     *
     * Returns the default request group
     *
     * @param array $patron      Patron information returned by the patronLogin
     * method.
     * @param array $holdDetails Optional array, only passed in when getting a list
     * in the context of placing a hold; contains most of the same values passed to
     * placeHold, minus the patron data.  May be used to limit the request group
     * options or may be ignored.
     *
     * @return string A location ID
     */
    public function getDefaultRequestGroup($patron, $holdDetails = null)
    {
        $source = $this->getSource($patron['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver) {
            if (!empty($holdDetails)) {
                if ($this->getSource($holdDetails['id']) != $source
                    || !$this->methodSupported(
                        $driver, 'getDefaultRequestGroup',
                        compact('patron', 'holdDetails')
                    )
                ) {
                    // Return false since the sources don't match or the method
                    // isn't supported by the driver
                    return false;
                }
            }
            $locations = $driver->getDefaultRequestGroup(
                $this->stripIdPrefixes($patron, $source),
                $this->stripIdPrefixes($holdDetails, $source)
            );
            return $this->addIdPrefixes($locations, $source);
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Place Hold
     *
     * Attempts to place a hold or recall on a particular item and returns
     * an array with result details
     *
     * @param array $holdDetails An array of item and patron data
     *
     * @return mixed An array of data on the request including
     * whether or not it was successful and a system message (if available)
     */
    public function placeHold($holdDetails)
    {
        $source = $this->getSource($holdDetails['patron']['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver) {
            if ($this->getSource($holdDetails['id']) != $source) {
                return [
                    ""success"" => false,
                    ""sysMessage"" => 'hold_wrong_user_institution'
                ];
            }
            $holdDetails = $this->stripIdPrefixes($holdDetails, $source);
            return $driver->placeHold($holdDetails);
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Cancel Holds
     *
     * Attempts to Cancel a hold or recall on a particular item. The
     * data in $cancelDetails['details'] is determined by getCancelHoldDetails().
     *
     * @param array $cancelDetails An array of item and patron data
     *
     * @return array               An array of data on each request including
     * whether or not it was successful and a system message (if available)
     */
    public function cancelHolds($cancelDetails)
    {
        $source = $this->getSource($cancelDetails['patron']['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver) {
            return $driver->cancelHolds(
                $this->stripIdPrefixes($cancelDetails, $source)
            );
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Get Cancel Hold Details
     *
     * In order to cancel a hold, the ILS requires some information on the hold.
     * This function returns the required information, which is then submitted
     * as form data in Hold.php. This value is then extracted by the CancelHolds
     * function.
     *
     * @param array $holdDetails An array of item data
     *
     * @return string Data for use in a form field
     */
    public function getCancelHoldDetails($holdDetails)
    {
        $source = $this->getSource(
            $holdDetails['id'] ?? $holdDetails['item_id'] ?? ''
        );
        $driver = $this->getDriver($source);
        if ($driver) {
            $holdDetails = $this->stripIdPrefixes($holdDetails, $source);
            return $driver->getCancelHoldDetails($holdDetails);
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Place Storage Retrieval Request
     *
     * Attempts to place a storage retrieval request on a particular item and returns
     * an array with result details
     *
     * @param array $details An array of item and patron data
     *
     * @return mixed An array of data on the request including
     * whether or not it was successful and a system message (if available)
     */
    public function placeStorageRetrievalRequest($details)
    {
        $source = $this->getSource($details['patron']['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver
            && is_callable([$driver, 'placeStorageRetrievalRequest'])
        ) {
            if ($this->getSource($details['id']) != $source) {
                return [
                    ""success"" => false,
                    ""sysMessage"" => 'hold_wrong_user_institution'
                ];
            }
            $details = $this->stripIdPrefixes($details, $source);
            return $driver->placeStorageRetrievalRequest($details);
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Cancel Call Slips
     *
     * Attempts to Cancel a call slip on a particular item. The
     * data in $cancelDetails['details'] is determined by
     * getCancelStorageRetrievalRequestDetails().
     *
     * @param array $cancelDetails An array of item and patron data
     *
     * @return array               An array of data on each request including
     * whether or not it was successful and a system message (if available)
     */
    public function cancelStorageRetrievalRequests($cancelDetails)
    {
        $source = $this->getSource($cancelDetails['patron']['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver
            && $this->methodSupported(
                $driver, 'cancelStorageRetrievalRequests', compact('cancelDetails')
            )
        ) {
            return $driver->cancelStorageRetrievalRequests(
                $this->stripIdPrefixes($cancelDetails, $source)
            );
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Get Cancel Call Slip Details
     *
     * In order to cancel a call slip, the ILS requires some information on it.
     * This function returns the required information, which is then submitted
     * as form data. This value is then extracted by the
     * CancelStorageRetrievalRequests function.
     *
     * @param array $details An array of item data
     *
     * @return string Data for use in a form field
     */
    public function getCancelStorageRetrievalRequestDetails($details)
    {
        $source = $this->getSource($details['id'] ?? '');
        $driver = $this->getDriver($source);
        if ($driver
            && $this->methodSupported(
                $driver, 'getCancelStorageRetrievalRequestDetails',
                compact('details')
            )
        ) {
            $details = $this->stripIdPrefixes($details, $source);
            return $driver->getCancelStorageRetrievalRequestDetails($details);
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Check whether an ILL request is valid
     *
     * This is responsible for determining if an item is requestable
     *
     * @param string $id     The Bib ID
     * @param array  $data   An Array of item data
     * @param patron $patron An array of patron data
     *
     * @return mixed An array of data on the request including
     * whether or not it is valid and a status message. Alternatively a boolean
     * true if request is valid, false if not.
     */
    public function checkILLRequestIsValid($id, $data, $patron)
    {
        $source = $this->getSource($id);
        $driver = $this->getDriver($source);
        if ($driver
            && $this->methodSupported(
                $driver, 'checkILLRequestIsValid', compact('id', 'data', 'patron')
            )
        ) {
            // Patron is not stripped so that the correct library can be determined
            return $driver->checkILLRequestIsValid(
                $this->stripIdPrefixes($id, $source),
                $this->stripIdPrefixes($data, $source),
                $patron
            );
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Get ILL Pickup Libraries
     *
     * This is responsible for getting information on the possible pickup libraries
     *
     * @param string $id     Record ID
     * @param array  $patron Patron
     *
     * @return bool|array False if request not allowed, or an array of associative
     * arrays with libraries.
     */
    public function getILLPickupLibraries($id, $patron)
    {
        $source = $this->getSource($id);
        $driver = $this->getDriver($source);
        if ($driver
            && $this->methodSupported(
                $driver, 'getILLPickupLibraries', compact('id', 'patron')
            )
        ) {
            // Patron is not stripped so that the correct library can be determined
            return $driver->getILLPickupLibraries(
                $this->stripIdPrefixes($id, $source, ['id']),
                $patron
            );
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Get ILL Pickup Locations
     *
     * This is responsible for getting a list of possible pickup locations for a
     * library
     *
     * @param string $id        Record ID
     * @param string $pickupLib Pickup library ID
     * @param array  $patron    Patron
     *
     * @return bool|array False if request not allowed, or an array of
     * locations.
     */
    public function getILLPickupLocations($id, $pickupLib, $patron)
    {
        $source = $this->getSource($id);
        $driver = $this->getDriver($source);
        if ($driver
            && $this->methodSupported(
                $driver, 'getILLPickupLocations',
                compact('id', 'pickupLib', 'patron')
            )
        ) {
            // Patron is not stripped so that the correct library can be determined
            return $driver->getILLPickupLocations(
                $this->stripIdPrefixes($id, $source, ['id']),
                $pickupLib,
                $patron
            );
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Place ILL Request
     *
     * Attempts to place an ILL request on a particular item and returns
     * an array with result details (or throws an exception on failure of support
     * classes)
     *
     * @param array $details An array of item and patron data
     *
     * @return mixed An array of data on the request including
     * whether or not it was successful and a system message (if available)
     */
    public function placeILLRequest($details)
    {
        $source = $this->getSource($details['id']);
        $driver = $this->getDriver($source);
        if ($driver
            && $this->methodSupported($driver, 'placeILLRequest', compact($details))
        ) {
            // Patron is not stripped so that the correct library can be determined
            $details = $this->stripIdPrefixes($details, $source, ['id'], ['patron']);
            return $driver->placeILLRequest($details);
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Get Patron ILL Requests
     *
     * This is responsible for retrieving all ILL Requests by a specific patron.
     *
     * @param array $patron The patron array from patronLogin
     *
     * @return mixed      Array of the patron's ILL requests
     */
    public function getMyILLRequests($patron)
    {
        $source = $this->getSource($patron['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver) {
            $supported = $this->methodSupported(
                $driver, 'getMyILLRequests', compact('patron')
            );
            if (!$supported) {
                // Return empty array if not supported by the driver
                return [];
            }
            $requests = $driver->getMyILLRequests(
                $this->stripIdPrefixes($patron, $source)
            );
            return $this->addIdPrefixes(
                $requests, $source, ['id', 'item_id', 'cat_username']
            );
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Cancel ILL Requests
     *
     * Attempts to Cancel an ILL request on a particular item. The
     * data in $cancelDetails['details'] is determined by
     * getCancelILLRequestDetails().
     *
     * @param array $cancelDetails An array of item and patron data
     *
     * @return array               An array of data on each request including
     * whether or not it was successful and a system message (if available)
     */
    public function cancelILLRequests($cancelDetails)
    {
        $source = $this->getSource($cancelDetails['patron']['cat_username']);
        $driver = $this->getDriver($source);
        if ($driver
            && $this->methodSupported(
                $driver, 'cancelILLRequests', compact('cancelDetails')
            )
        ) {
            return $driver->cancelILLRequests(
                $this->stripIdPrefixes($cancelDetails, $source)
            );
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Get Cancel ILL Request Details
     *
     * In order to cancel an ILL request, the ILS requires some information on the
     * request. This function returns the required information, which is then
     * submitted as form data. This value is then extracted by the CancelILLRequests
     * function.
     *
     * @param array $details An array of item data
     *
     * @return string Data for use in a form field
     */
    public function getCancelILLRequestDetails($details)
    {
        $source = $this->getSource($details['id'] ?? $details['item_id'] ?? '');
        $driver = $this->getDriver($source);
        if ($driver
            && $this->methodSupported(
                $driver, 'getCancelILLRequestDetails', compact('details')
            )
        ) {
            return $driver->getCancelILLRequestDetails(
                $this->stripIdPrefixes($details, $source)
            );
        }
        throw new ILSException('No suitable backend driver found');
    }

    /**
     * Change Password
     *","If you find a way to display the full unknown message above, it may be necessary to devise a different mechanism for filling it in in this situation. Maybe any entry with 'error' set automatically gets 'availability_message' set to unknown.","This code change introduces a new class `MultiBackend` that extends `AbstractBase` and implements `LoggerAwareInterface`. It allows for the use of multiple backend drivers determined by a record ID or user ID prefix. The class provides methods for various ILS operations such as getting status, holdings, purchase history, and more. The code includes methods for logging errors and handling exceptions. The class uses a configuration loader to manage driver configurations and a driver manager to handle driver instances. The code also includes methods for checking request validity, getting pickup locations, and placing holds and requests."
70,"@@ -0,0 +1,13 @@
+namespace Datadog.Trace.Agent
+{
+    internal static class TraceRequestDecorator
+    {
+        public static void AddHeaders(IApiRequest request)
+        {
+            request.AddHeader(AgentHttpHeaderNames.Language, "".NET"");
+            request.AddHeader(AgentHttpHeaderNames.TracerVersion, TracerConstants.AssemblyVersion);
+            // don't add automatic instrumentation to requests from datadog code
+            request.AddHeader(HttpHeaderNames.TracingEnabled, ""false"");
+        }
+    }
+}","namespace Datadog.Trace.Agent
{
    internal static class TraceRequestDecorator
    {
        public static void AddHeaders(IApiRequest request)
        {
            request.AddHeader(AgentHttpHeaderNames.Language, "".NET"");
            request.AddHeader(AgentHttpHeaderNames.TracerVersion, TracerConstants.AssemblyVersion);
            // don't add automatic instrumentation to requests from datadog code
            request.AddHeader(HttpHeaderNames.TracingEnabled, ""false"");
        }
    }
}

","This isn't a big deal, but without a full Decorator pattern, this feels a little weird to me. Maybe this could be `IApiRequestExtensions`?","The suggested code change adds headers to an `IApiRequest` object in the `Datadog.Trace.Agent` namespace. It sets the `Language` header to "".NET"", the `TracerVersion` header to the version of the tracer assembly, and disables automatic instrumentation by setting the `TracingEnabled` header to ""false"". This ensures that the request is not instrumented by the tracer itself, which can be useful for avoiding infinite loops or other issues related to automatic instrumentation. The code is concise and easy to understand."
71,"@@ -46,7 +46,7 @@ namespace Microsoft.AspNetCore.Server.Kestrel.Core.Internal
 
             // REVIEW: This task should be tracked by the server for graceful shutdown
             // Today it's handled specifically for http but not for aribitrary middleware
-            _ = Execute(new DefaultConnectionContext(connection));
+            _ = Execute(connection);
         }
 
         private async Task Execute(ConnectionContext connectionContext)","﻿// Copyright (c) .NET Foundation. All rights reserved.
// Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.

using System;
using System.Buffers;
using System.IO.Pipelines;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.AspNetCore.Http.Features;
using Microsoft.AspNetCore.Connections;
using Microsoft.AspNetCore.Connections.Features;
using Microsoft.AspNetCore.Server.Kestrel.Core.Internal.Infrastructure;
using Microsoft.AspNetCore.Server.Kestrel.Transport.Abstractions.Internal;
using Microsoft.Extensions.Logging;

namespace Microsoft.AspNetCore.Server.Kestrel.Core.Internal
{
    public class ConnectionDispatcher : IConnectionDispatcher
    {
        private readonly ServiceContext _serviceContext;
        private readonly ConnectionDelegate _connectionDelegate;

        public ConnectionDispatcher(ServiceContext serviceContext, ConnectionDelegate connectionDelegate)
        {
            _serviceContext = serviceContext;
            _connectionDelegate = connectionDelegate;
        }

        private IKestrelTrace Log => _serviceContext.Log;

        public void OnConnection(TransportConnection connection)
        {
            // REVIEW: Unfortunately, we still need to use the service context to create the pipes since the settings
            // for the scheduler and limits are specified here
            var inputOptions = GetInputPipeOptions(_serviceContext, connection.MemoryPool, connection.InputWriterScheduler);
            var outputOptions = GetOutputPipeOptions(_serviceContext, connection.MemoryPool, connection.OutputReaderScheduler);

            var pair = DuplexPipe.CreateConnectionPair(inputOptions, outputOptions);

            // Set the transport and connection id
            connection.ConnectionId = CorrelationIdGenerator.GetNextId();
            connection.Transport = pair.Transport;

            // This *must* be set before returning from OnConnection
            connection.Application = pair.Application;
            _ = Execute(connection);

            // REVIEW: This task should be tracked by the server for graceful shutdown
            // Today it's handled specifically for http but not for aribitrary middleware
            _ = Execute(new DefaultConnectionContext(connection));
        }

        private async Task Execute(ConnectionContext connectionContext)
        {
            using (BeginConnectionScope(connectionContext))
            {
                Log.ConnectionStart(connectionContext.ConnectionId);

                try
                {
                    await _connectionDelegate(connectionContext);
                }
                catch (Exception ex)
                {
                    Log.LogCritical(0, ex, $""{nameof(ConnectionDispatcher)}.{nameof(Execute)}() {connectionContext.ConnectionId}"");
                }

                Log.ConnectionStop(connectionContext.ConnectionId);
            }
        }

        private IDisposable BeginConnectionScope(ConnectionContext connectionContext)
        {
            if (Log.IsEnabled(LogLevel.Critical))
            {
                return Log.BeginScope(new ConnectionLogScope(connectionContext.ConnectionId));
            }

            return null;
        }

        // Internal for testing
        internal static PipeOptions GetInputPipeOptions(ServiceContext serviceContext, MemoryPool<byte> memoryPool, PipeScheduler writerScheduler) => new PipeOptions
        (
            pool: memoryPool,
            readerScheduler: serviceContext.Scheduler,
            writerScheduler: writerScheduler,
            pauseWriterThreshold: serviceContext.ServerOptions.Limits.MaxRequestBufferSize ?? 0,
            resumeWriterThreshold: serviceContext.ServerOptions.Limits.MaxRequestBufferSize ?? 0,
            useSynchronizationContext: false
        );

        internal static PipeOptions GetOutputPipeOptions(ServiceContext serviceContext, MemoryPool<byte> memoryPool, PipeScheduler readerScheduler) => new PipeOptions
        (
            pool: memoryPool,
            readerScheduler: readerScheduler,
            writerScheduler: serviceContext.Scheduler,
            pauseWriterThreshold: GetOutputResponseBufferSize(serviceContext),
            resumeWriterThreshold: GetOutputResponseBufferSize(serviceContext),
            useSynchronizationContext: false
        );

        private static long GetOutputResponseBufferSize(ServiceContext serviceContext)
        {
            var bufferSize = serviceContext.ServerOptions.Limits.MaxResponseBufferSize;
            if (bufferSize == 0)
            {
                // 0 = no buffering so we need to configure the pipe so the the writer waits on the reader directly
                return 1;
            }

            // null means that we have no back pressure
            return bufferSize ?? 0;
        }
    }
}
",Where is DefaultConnectionContext even used now?,"The suggested code change is focused on implementing the `ConnectionDispatcher` class, which is responsible for managing connections in Kestrel. The key points of the review are:

1. **Service Context Dependency**: The class depends on a `ServiceContext`, which contains various settings and configurations needed for the dispatcher.

2. **Connection Delegate**: It takes a `ConnectionDelegate` as a parameter, which is used to handle incoming connections.

3. **Pipe Creation**: The dispatcher creates a duplex pipe pair using the provided memory pool and scheduler options. This setup is crucial for handling both input and output streams.

4. **Connection Management**: The dispatcher sets the connection ID and transport for the connection. It also ensures that the application context is properly set before executing the connection delegate.

5. **Error Handling**: The `Execute` method includes error logging using the `IKestrelTrace` interface.

6. **Scope Management**: A scope is created using the `Log.BeginScope` method if the log level is critical, which helps in tracing the connection lifecycle.

7. **Pipe Options**: The `GetInputPipeOptions` and `GetOutputPipeOptions` methods configure the input and output pipes with appropriate buffer sizes and schedulers.

8. **Buffer Size Calculation**: The `GetOutputResponseBufferSize` method calculates the buffer size for the output pipe based on the server's configuration limits.

Overall, the code is well-structured and follows best practices for managing connections in Kestrel. The use of the `ServiceContext` ensures that all necessary settings are available for configuring the pipes and handling connections effectively."
72,"@@ -130,7 +130,7 @@ func verifyBlockSignature(ctx context.Context, view FaultStateView, blk block.Bl
 	if err != nil {
 		panic(errors.Wrapf(err, ""failed to inspect miner addresses""))
 	}
-	err = state.NewSignatureValidator(view).ValidateSignature(ctx, blk.SignatureData(), worker, blk.BlockSig)
+	err = state.NewSignatureValidator(view).ValidateSignature(ctx, blk.SignatureData(), worker, *blk.BlockSig)
 	if err != nil {
 		return errors.Wrapf(err, ""no consensus fault: block %s signature invalid"", blk.Cid())
 	}","package slashing

import (
	""bytes""
	""context""
	""fmt""

	""github.com/filecoin-project/go-address""
	""github.com/filecoin-project/specs-actors/actors/abi""
	""github.com/filecoin-project/specs-actors/actors/runtime""
	""github.com/pkg/errors""

	""github.com/filecoin-project/go-filecoin/internal/pkg/block""
	""github.com/filecoin-project/go-filecoin/internal/pkg/chain""
	""github.com/filecoin-project/go-filecoin/internal/pkg/encoding""
	""github.com/filecoin-project/go-filecoin/internal/pkg/state""
)

type FaultStateView interface {
	state.AccountStateView
	MinerControlAddresses(ctx context.Context, maddr address.Address) (owner, worker address.Address, err error)
}

// Chain state required for checking consensus fault reports.
type chainReader interface {
	GetTipSet(block.TipSetKey) (block.TipSet, error)
}

// Checks the validity of reported consensus faults.
type ConsensusFaultChecker struct {
	chain chainReader
}

func NewFaultChecker(chain chainReader) *ConsensusFaultChecker {
	return &ConsensusFaultChecker{chain: chain}
}

// Checks the validity of a consensus fault reported by serialized block headers h1, h2, and optional
// common-ancestor witness h3.
func (s *ConsensusFaultChecker) VerifyConsensusFault(ctx context.Context, h1, h2, extra []byte, head block.TipSetKey, view FaultStateView, earliest abi.ChainEpoch) (*runtime.ConsensusFault, error) {
	if bytes.Equal(h1, h2) {
		return nil, fmt.Errorf(""no consensus fault: blocks identical"")
	}

	var b1, b2, b3 block.Block
	innerErr := encoding.Decode(h1, &b1)
	if innerErr != nil {
		return nil, errors.Wrapf(innerErr, ""failed to decode h1"")
	}
	innerErr = encoding.Decode(h2, &b2)
	if innerErr != nil {
		return nil, errors.Wrapf(innerErr, ""failed to decode h2"")
	}
	if len(extra) > 0 {
		innerErr = encoding.Decode(extra, &b3)
		if innerErr != nil {
			return nil, errors.Wrapf(innerErr, ""failed to decode extra"")
		}
	}
	// Block syntax is not validated. This implements the strictest check possible, and is also the simplest check
	// possible.
	// This means that blocks that could never have been included in the chain (e.g. with an empty parent state)
	// are still fault-able.

	if b1.Miner != b2.Miner {
		return nil, fmt.Errorf(""no consensus fault: miners differ"")
	}
	if b1.Height > b2.Height {
		return nil, fmt.Errorf(""no consensus fault: first block is higher than second"")
	}

	// Check the basic fault conditions first, defer the (expensive) signature and chain history check until last.
	var fault *runtime.ConsensusFault

	// Double-fork mining fault: two blocks at the same epoch.
	// It is not necessary to present a common ancestor of the blocks.
	if b1.Height == b2.Height {
		fault = &runtime.ConsensusFault{
			Target: b1.Miner,
			Epoch:  b2.Height,
			Type:   runtime.ConsensusFaultDoubleForkMining,
		}
	}
	// Time-offset mining fault: two blocks with the same parent but different epochs.
	// The height check is redundant at time of writing, but included for robustness to future changes to this method.
	// The blocks have a common ancestor by definition (the parent).
	if b1.Parents.Equals(b2.Parents) && b1.Height != b2.Height {
		fault = &runtime.ConsensusFault{
			Target: b1.Miner,
			Epoch:  b2.Height,
			Type:   runtime.ConsensusFaultTimeOffsetMining,
		}
	}
	// Parent-grinding fault: one block’s parent is a tipset that provably should have included some block but does not.
	// The provable case is that two blocks are mined in consecutive epochs and the later one does not include the
	// earlier one as a parent.
	// B3 must prove that the higher block (B2) has grandparent equal to B1's parent.
	if b1.Height+1 == b2.Height && !b2.Parents.Has(b1.Cid()) && b2.Parents.Has(b3.Cid()) && b3.Parents.Equals(b1.Parents) {
		fault = &runtime.ConsensusFault{
			Target: b1.Miner,
			Epoch:  b2.Height,
			Type:   runtime.ConsensusFaultParentGrinding,
		}
	}
	if fault == nil {
		return nil, fmt.Errorf(""no consensus fault: blocks are ok"")
	}

	// Expensive validation: signatures and chain history.

	err := verifyBlockSignature(ctx, view, b1)
	if err != nil {
		return nil, err
	}
	err = verifyBlockSignature(ctx, view, b2)
	if err != nil {
		return nil, err
	}
	err = verifyOneBlockInChain(ctx, s.chain, head, b1, b2, earliest)
	if err != nil {
		return nil, err
	}

	return fault, nil
}

// Checks whether a block header is correctly signed in the context of the parent state to which it refers.
func verifyBlockSignature(ctx context.Context, view FaultStateView, blk block.Block) error {
	_, worker, err := view.MinerControlAddresses(ctx, blk.Miner)
	err = state.NewSignatureValidator(view).ValidateSignature(ctx, blk.SignatureData(), worker, *blk.BlockSig)
	if err != nil {
		panic(errors.Wrapf(err, ""failed to inspect miner addresses""))
	}
	err = state.NewSignatureValidator(view).ValidateSignature(ctx, blk.SignatureData(), worker, blk.BlockSig)
	if err != nil {
		return errors.Wrapf(err, ""no consensus fault: block %s signature invalid"", blk.Cid())
	}
	return err
}

// Checks whether at least one of b1, b2 appear in the chain defined by `head`.
func verifyOneBlockInChain(ctx context.Context, chn chainReader, head block.TipSetKey, b1 block.Block, b2 block.Block, earliest abi.ChainEpoch) error {
	if chainHasB1, err := chainContainsBlock(ctx, chn, head, b1, earliest); err != nil {
		panic(errors.Wrapf(err, ""failed to inspect chain"")) // This idiosyncratic failure shouldn't go on chain
	} else if chainHasB1 {
		return nil
	}
	if chainHasB2, err := chainContainsBlock(ctx, chn, head, b2, earliest); err != nil {
		panic(errors.Wrapf(err, ""failed to inspect chain""))
	} else if chainHasB2 {
		return nil
	}
	return fmt.Errorf(""no consensus fault: neither block in chain since %d"", earliest)
}

func chainContainsBlock(ctx context.Context, chn chainReader, head block.TipSetKey, blk block.Block, earliest abi.ChainEpoch) (bool, error) {
	if blk.Height < earliest { // Short-circuit
		return false, nil
	}
	ts, err := chn.GetTipSet(head)
	if err != nil {
		return false, err
	}

	itr := chain.IterAncestors(ctx, chn, ts)
	for ts := itr.Value(); !itr.Complete(); err = itr.Next() {
		if err != nil {
			return false, err
		}
		height, err := ts.Height()
		if err != nil {
			return false, err
		}
		if height < earliest {
			return false, nil
		}
		if ts.Key().Has(blk.Cid()) {
			return true, nil
		}
	}
	return false, nil
}
",`*blk.BlockSig` will panic if nil. Is this guaranteed not to be?,"The suggested code change introduces a new `ConsensusFaultChecker` struct that encapsulates the logic for verifying consensus faults based on serialized block headers. The checker performs several checks to determine if a reported fault is valid:

1. **Block Syntax Validation**: The code assumes that the block headers are syntactically correct and does not validate them further.

2. **Basic Fault Conditions**: It checks for double-fork mining, time-offset mining, and parent-grinding faults.

3. **Signature Verification**: It verifies the signatures of the blocks using the provided `FaultStateView`.

4. **Chain History Verification**: It checks if at least one of the blocks appears in the chain starting from the specified `head` tip set key.

5. **Error Handling**: The code includes error handling for decoding block headers, signature verification, and chain history checks.

6. **Concurrency Safety**: The code uses `panic` to handle errors that should not occur on the chain, ensuring that the program does not continue in an inconsistent state.

Overall, the code provides a comprehensive approach to validating consensus faults based on block headers, ensuring that the chain integrity is maintained."
73,"@@ -1809,6 +1809,12 @@ def disable_user():
     username = data.get('username')
     _validate_username(username)
 
+    if g.auth.user == username:
+        raise ApiException(
+            403,
+            ""Can't disable your own account.""
+            )
+
     resp = requests.patch(""%s%s/"" % (user_modify_api, username) , headers=auth_headers,
         data=json.dumps({
             'is_active' : False","@as_json
def package_preview(owner, package_name, package_hash):
    instance = _get_instance(g.auth, owner, package_name, package_hash)
    assert isinstance(instance.contents, RootNode)

    readme = instance.contents.children.get('README')
    if isinstance(readme, FileNode):
        assert len(readme.hashes) == 1
        readme_url = _generate_presigned_url(S3_GET_OBJECT, owner, readme.hashes[0])
    else:
        readme_url = None

    contents_preview = _generate_preview(instance.contents)

    # Insert an event.
    event = Event(
        type=Event.Type.PREVIEW,
        user=g.auth.user,
        package_owner=owner,
        package_name=package_name,
        package_hash=package_hash,
    )
    db.session.add(event)

    db.session.commit()

    _mp_track(
        type=""preview"",
        package_owner=owner,
        package_name=package_name,
    )

    return dict(
        preview=contents_preview,
        readme_url=readme_url,
        created_by=instance.created_by,
        created_at=instance.created_at.timestamp(),
        updated_by=instance.updated_by,
        updated_at=instance.updated_at.timestamp(),
    )

@app.route('/api/package/<owner>/<package_name>/', methods=['GET'])
@api(require_login=False)
@as_json
def package_list(owner, package_name):
    package = _get_package(g.auth, owner, package_name)
    instances = (
        Instance.query
        .filter_by(package=package)
    )

    return dict(
        hashes=[instance.hash for instance in instances]
    )

@app.route('/api/package/<owner>/<package_name>/', methods=['DELETE'])
@api()
@as_json
def package_delete(owner, package_name):
    if g.auth.user != owner:
        raise ApiException(requests.codes.forbidden,
                           ""Only the package owner can delete packages."")

    package = _get_package(g.auth, owner, package_name)

    db.session.delete(package)

    # Insert an event.
    event = Event(
        user=g.auth.user,
        type=Event.Type.DELETE,
        package_owner=owner,
        package_name=package_name,
    )
    db.session.add(event)

    db.session.commit()

    return dict()

@app.route('/api/package/<owner>/', methods=['GET'])
@api(require_login=False)
@as_json
def user_packages(owner):
    packages = (
        db.session.query(
            Package,
            sa.func.bool_or(Access.user == PUBLIC),
            sa.func.bool_or(Access.user == TEAM)
        )
        .filter_by(owner=owner)
        .join(Package.access)
        .filter(_access_filter(g.auth))
        .group_by(Package.id)
        .order_by(Package.name)
        .all()
    )

    return dict(
        packages=[
            dict(
                name=package.name,
                is_public=is_public,
                is_team=is_team,
            )
            for package, is_public, is_team in packages
        ]
    )

@app.route('/api/admin/package_list/<owner>/', methods=['GET'])
@api(require_login=True, require_admin=True)
@as_json
def list_user_packages(owner):
    packages = (
        db.session.query(
            Package,
            sa.func.bool_or(Access.user == PUBLIC),
            sa.func.bool_or(Access.user == TEAM)
        )
        .filter_by(owner=owner)
        .join(Package.access)
        .group_by(Package.id)
        .order_by(Package.name)
        .all()
    )

    return dict(
        packages=[
            dict(
                name=package.name,
                is_public=is_public,
                is_team=is_team,
            )
            for package, is_public, is_team in packages
        ]
    )

@app.route('/api/log/<owner>/<package_name>/', methods=['GET'])
@api(require_login=False)
@as_json
def logs_list(owner, package_name):
    package = _get_package(g.auth, owner, package_name)

    logs = (
        db.session.query(Log, Instance)
        .filter_by(package=package)
        .join(Log.instance)
        # Sort chronologically, but rely on IDs in case of duplicate created times.
        .order_by(Log.created, Log.id)
    )

    return dict(
        logs=[dict(
            hash=instance.hash,
            created=log.created.timestamp(),
            author=log.author
        ) for log, instance in logs]
    )

VERSION_SCHEMA = {
    'type': 'object',
    'properties': {
        'hash': {
            'type': 'string'
        }
    },
    'required': ['hash']
}

def normalize_version(version):
    try:
        version = Version.normalize(version)
    except ValueError:
        raise ApiException(requests.codes.bad_request, ""Malformed version"")

    return version

@app.route('/api/version/<owner>/<package_name>/<package_version>', methods=['PUT'])
@api(schema=VERSION_SCHEMA)
@as_json
def version_put(owner, package_name, package_version):
    # TODO: Write access for collaborators.
    if g.auth.user != owner:
        raise ApiException(
            requests.codes.forbidden,
            ""Only the package owner can create versions""
        )

    user_version = package_version
    package_version = normalize_version(package_version)

    data = request.get_json()
    package_hash = data['hash']

    instance = (
        Instance.query
        .filter_by(hash=package_hash)
        .join(Instance.package)
        .filter_by(owner=owner, name=package_name)
        .one_or_none()
    )

    if instance is None:
        raise ApiException(requests.codes.not_found, ""Package hash does not exist"")

    version = Version(
        package_id=instance.package_id,
        version=package_version,
        user_version=user_version,
        instance=instance
    )

    try:
        db.session.add(version)
        db.session.commit()
    except IntegrityError:
        raise ApiException(requests.codes.conflict, ""Version already exists"")

    return dict()

@app.route('/api/version/<owner>/<package_name>/<package_version>', methods=['GET'])
@api(require_login=False)
@as_json
def version_get(owner, package_name, package_version):
    package_version = normalize_version(package_version)
    package = _get_package(g.auth, owner, package_name)

    instance = (
        Instance.query
        .join(Instance.versions)
        .filter_by(package=package, version=package_version)
        .one_or_none()
    )

    if instance is None:
        raise ApiException(
            requests.codes.not_found,
            ""Version %s does not exist"" % package_version
        )

    _mp_track(
        type=""get_hash"",
        package_owner=owner,
        package_name=package_name,
        package_version=package_version,
    )

    return dict(
        hash=instance.hash,
        created_by=instance.created_by,
        created_at=instance.created_at.timestamp(),
        updated_by=instance.updated_by,
        updated_at=instance.updated_at.timestamp(),
    )

@app.route('/api/version/<owner>/<package_name>/', methods=['GET'])
@api(require_login=False)
@as_json
def version_list(owner, package_name):
    package = _get_package(g.auth, owner, package_name)

    versions = (
        db.session.query(Version, Instance)
        .filter_by(package=package)
        .join(Version.instance)
        .all()
    )

    sorted_versions = sorted(versions, key=lambda row: row.Version.sort_key())

    return dict(
        versions=[
            dict(
                version=version.user_version,
                hash=instance.hash
            ) for version, instance in sorted_versions
        ]
    )

TAG_SCHEMA = {
    'type': 'object',
    'properties': {
        'hash': {
            'type': 'string'
        }
    },
    'required': ['hash']
}

@app.route('/api/tag/<owner>/<package_name>/<package_tag>', methods=['PUT'])
@api(schema=TAG_SCHEMA)
@as_json
def tag_put(owner, package_name, package_tag):
    # TODO: Write access for collaborators.
    if g.auth.user != owner:
        raise ApiException(
            requests.codes.forbidden,
            ""Only the package owner can modify tags""
        )

    data = request.get_json()
    package_hash = data['hash']

    instance = (
        Instance.query
        .filter_by(hash=package_hash)
        .join(Instance.package)
        .filter_by(owner=owner, name=package_name)
        .one_or_none()
    )

    if instance is None:
        raise ApiException(requests.codes.not_found, ""Package hash does not exist"")

    # Update an existing tag or create a new one.
    tag = (
        Tag.query
        .with_for_update()
        .filter_by(package_id=instance.package_id, tag=package_tag)
        .one_or_none()
    )
    if tag is None:
        tag = Tag(
            package_id=instance.package_id,
            tag=package_tag,
            instance=instance
        )
        db.session.add(tag)
    else:
        tag.instance = instance

    db.session.commit()

    return dict()

@app.route('/api/tag/<owner>/<package_name>/<package_tag>', methods=['GET'])
@api(require_login=False)
@as_json
def tag_get(owner, package_name, package_tag):
    package = _get_package(g.auth, owner, package_name)

    instance = (
        Instance.query
        .join(Instance.tags)
        .filter_by(package=package, tag=package_tag)
        .one_or_none()
    )

    if instance is None:
        raise ApiException(
            requests.codes.not_found,
            ""Tag %r does not exist"" % package_tag
        )

    _mp_track(
        type=""get_hash"",
        package_owner=owner,
        package_name=package_name,
        package_tag=package_tag,
    )

    return dict(
        hash=instance.hash,
        created_by=instance.created_by,
        created_at=instance.created_at.timestamp(),
        updated_by=instance.updated_by,
        updated_at=instance.updated_at.timestamp(),
    )

@app.route('/api/tag/<owner>/<package_name>/<package_tag>', methods=['DELETE'])
@api()
@as_json
def tag_delete(owner, package_name, package_tag):
    # TODO: Write access for collaborators.
    if g.auth.user != owner:
        raise ApiException(
            requests.codes.forbidden,
            ""Only the package owner can delete tags""
        )

    tag = (
        Tag.query
        .with_for_update()
        .filter_by(tag=package_tag)
        .join(Tag.package)
        .filter_by(owner=owner, name=package_name)
        .one_or_none()
    )
    if tag is None:
        raise ApiException(
            requests.codes.not_found,
            ""Package %s/%s tag %r does not exist"" % (owner, package_name, package_tag)
        )

    db.session.delete(tag)
    db.session.commit()

    return dict()

@app.route('/api/tag/<owner>/<package_name>/', methods=['GET'])
@api(require_login=False)
@as_json
def tag_list(owner, package_name):
    package = _get_package(g.auth, owner, package_name)

    tags = (
        db.session.query(Tag, Instance)
        .filter_by(package=package)
        .order_by(Tag.tag)
        .join(Tag.instance)
        .all()
    )

    return dict(
        tags=[
            dict(
                tag=tag.tag,
                hash=instance.hash
            ) for tag, instance in tags
        ]
    )

@app.route('/api/access/<owner>/<package_name>/<user>', methods=['PUT'])
@api()
@as_json
def access_put(owner, package_name, user):
    if g.auth.user != owner:
        raise ApiException(
            requests.codes.forbidden,
            ""Only the package owner can grant access""
        )

    auth_headers = {
        AUTHORIZATION_HEADER: g.auth_header
        }

    package = (
        Package.query
        .with_for_update()
        .filter_by(owner=owner, name=package_name)
        .one_or_none()
    )
    if package is None:
        raise PackageNotFoundException(owner, package_name)

    if VALID_EMAIL_RE.match(user):
        email = user.lower()
        invitation = Invitation(package=package, email=email)
        db.session.add(invitation)
        db.session.commit()

        # Call to Auth to send invitation email
        resp = requests.post(INVITE_SEND_URL,
                             headers=auth_headers,
                             data=dict(email=email,
                                       owner=g.auth.user,
                                       package=package.name,
                                       client_id=OAUTH_CLIENT_ID,
                                       client_secret=OAUTH_CLIENT_SECRET,
                                       callback_url=OAUTH_REDIRECT_URL))

        if resp.status_code == requests.codes.unauthorized:
            raise ApiException(
                requests.codes.unauthorized,
                ""Invalid credentials""
                )
        elif resp.status_code != requests.codes.ok:
            raise ApiException(requests.codes.server_error, ""Server error"")
        return dict()

    else:
        _validate_username(user)
        if user == PUBLIC:
            if not ALLOW_ANONYMOUS_ACCESS:
                raise ApiException(requests.codes.forbidden, ""Public access not allowed"")
        elif user == TEAM:
            if not ALLOW_TEAM_ACCESS:
                raise ApiException(requests.codes.forbidden, ""Team access not allowed"")
        else:
            resp = requests.get(OAUTH_PROFILE_API % user,
                                headers=auth_headers)
            if resp.status_code == requests.codes.not_found:
                raise ApiException(
                    requests.codes.not_found,
                    ""User %s does not exist"" % user
                    )
            elif resp.status_code != requests.codes.ok:
                raise ApiException(
                    requests.codes.server_error,
                    ""Unknown error""
                    )

        try:
            access = Access(package=package, user=user)
            db.session.add(access)
            db.session.commit()
        except IntegrityError:
            raise ApiException(requests.codes.conflict, ""The user already has access"")

        return dict()

@app.route('/api/access/<owner>/<package_name>/<user>', methods=['GET'])
@api()
@as_json
def access_get(owner, package_name, user):
    _validate_username(user)
    if g.auth.user != owner:
        raise ApiException(
            requests.codes.forbidden,
            ""Only the package owner can view access""
        )

    access = (
        db.session.query(Access)
        .filter_by(user=user)
        .join(Access.package)
        .filter_by(owner=owner, name=package_name)
        .one_or_none()
    )
    if access is None:
        raise PackageNotFoundException(owner, package_name)

    return dict()

@app.route('/api/access/<owner>/<package_name>/<user>', methods=['DELETE'])
@api()
@as_json
def access_delete(owner, package_name, user):
    _validate_username(user)
    if g.auth.user != owner:
        raise ApiException(
            requests.codes.forbidden,
            ""Only the package owner can revoke access""
        )

    if user == owner:
        raise ApiException(
            requests.codes.forbidden,
            ""Cannot revoke the owner's access""
        )

    if HAVE_PAYMENTS and user == PUBLIC:
        customer = _get_or_create_customer()
        plan = _get_customer_plan(customer)
        if plan == PaymentPlan.FREE:
            raise ApiException(
                requests.codes.payment_required,
                ""Insufficient permissions. "" +
                ""Upgrade your plan to create private packages: https://quiltdata.com/profile.""
            )

    access = (
        Access.query
        .with_for_update()
        .filter_by(user=user)
        .join(Access.package)
        .filter_by(owner=owner, name=package_name)
        .one_or_none()
    )
    if access is None:
        raise PackageNotFoundException(owner, package_name)

    db.session.delete(access)
    db.session.commit()
    return dict()

@app.route('/api/access/<owner>/<package_name>/', methods=['GET'])
@api()
@as_json
def access_list(owner, package_name):
    accesses = (
        Access.query
        .join(Access.package)
        .filter_by(owner=owner, name=package_name)
    )

    can_access = [access.user for access in accesses]
    is_collaborator = g.auth.user in can_access
    is_public = ALLOW_ANONYMOUS_ACCESS and (PUBLIC in can_access)
    is_team = ALLOW_TEAM_ACCESS and (TEAM in can_access)

    if is_public or is_team or is_collaborator:
        return dict(users=can_access)
    else:
        raise PackageNotFoundException(owner, package_name)

@app.route('/api/recent_packages/', methods=['GET'])
@api(require_login=False)
@as_json
def recent_packages():
    try:
        count = int(request.args.get('count', ''))
    except ValueError:
        count = 10

    if ALLOW_ANONYMOUS_ACCESS:
        max_visibility = PUBLIC
    elif ALLOW_TEAM_ACCESS:
        max_visibility = TEAM
    else:
        # Shouldn't really happen, but let's handle this case.
        raise ApiException(requests.codes.forbidden, ""Not allowed"")

    results = (
        db.session.query(Package, sa.func.max(Instance.updated_at))
        .join(Package.access)
        .filter_by(user=max_visibility)
        .join(Package.instances)
        .group_by(Package.id)
        .order_by(sa.func.max(Instance.updated_at).desc())
        .limit(count)
        .all()
    )

    return dict(
        packages=[
            dict(
                owner=package.owner,
                name=package.name,
                updated_at=updated_at
            ) for package, updated_at in results
        ]
    )

@app.route('/api/search/', methods=['GET'])
@api(require_login=False)
@as_json
def search():
    query = request.args.get('q', '')
    keywords = query.split()

    if len(keywords) > 5:
        # Let's not overload the DB with crazy queries.
        raise ApiException(requests.codes.bad_request, ""Too many search terms (max is 5)"")

    filter_list = [
        sa.func.strpos(
            sa.func.lower(sa.func.concat(Package.owner, '/', Package.name)),
            sa.func.lower(keyword)
        ) > 0
        for keyword in keywords
    ]

    results = (
        db.session.query(
            Package,
            sa.func.bool_or(Access.user == PUBLIC),
            sa.func.bool_or(Access.user == TEAM)
        )
        .filter(sa.and_(*filter_list))
        .join(Package.access)
        .filter(_access_filter(g.auth))
        .group_by(Package.id)
        .order_by(
            sa.func.lower(Package.owner),
            sa.func.lower(Package.name)
        )
        .all()
    )

    return dict(
        packages=[
            dict(
                owner=package.owner,
                name=package.name,
                is_public=is_public,
                is_team=is_team,
            ) for package, is_public, is_team in results
        ]
    )

@app.route('/api/profile', methods=['GET'])
@api()
@as_json
def profile():
    if HAVE_PAYMENTS:
        customer = _get_or_create_customer()
        plan = _get_customer_plan(customer).value
        have_cc = customer.sources.total_count > 0
    else:
        plan = None
        have_cc = None

    # Check for outstanding package sharing invitations
    invitations = (
        db.session.query(Invitation, Package)
        .filter_by(email=g.auth.email.lower())
        .join(Invitation.package)
        )
    for invitation, package in invitations:
        access = Access(package=package, user=g.auth.user)
        db.session.add(access)
        db.session.delete(invitation)

    if invitations:
        db.session.commit()

    # We want to show only the packages owned by or explicitly shared with the user -
    # but also show whether they're public, in case a package is both public and shared with the user.
    # So do a ""GROUP BY"" to get the public info, then ""HAVING"" to filter out packages that aren't shared.
    packages = (
        db.session.query(
            Package,
            sa.func.bool_or(Access.user == PUBLIC),
            sa.func.bool_or(Access.user == TEAM)
        )
        .join(Package.access)
        .filter(_access_filter(g.auth))
        .group_by(Package.id)
        .order_by(
            sa.func.lower(Package.owner),
            sa.func.lower(Package.name)
        )
        .having(sa.func.bool_or(Access.user == g.auth.user))
        .all()
    )

    return dict(
        packages=dict(
            own=[
                dict(
                    owner=package.owner,
                    name=package.name,
                    is_public=is_public,
                    is_team=is_team,
                )
                for package, is_public, is_team in packages if package.owner == g.auth.user
            ],
            shared=[
                dict(
                    owner=package.owner,
                    name=package.name,
                    is_public=is_public,
                    is_team=is_team,
                )
                for package, is_public, is_team in packages if package.owner != g.auth.user
            ],
        ),
        plan=plan,
        have_credit_card=have_cc,
    )

@app.route('/api/payments/update_plan', methods=['POST'])
@api()
@as_json
def payments_update_plan():
    if not HAVE_PAYMENTS:
        raise ApiException(requests.codes.not_found, ""Payments not enabled"")

    plan = request.values.get('plan')
    try:
        plan = PaymentPlan(plan)
    except ValueError:
        raise ApiException(requests.codes.bad_request, ""Invalid plan: %r"" % plan)

    if plan not in (PaymentPlan.FREE, PaymentPlan.INDIVIDUAL, PaymentPlan.BUSINESS_ADMIN):
        # Cannot switch to the BUSINESS_MEMBER plan manually.
        raise ApiException(requests.codes.forbidden, ""Not allowed to switch to plan: %r"" % plan)

    stripe_token = request.values.get('token')

    customer = _get_or_create_customer()

    if _get_customer_plan(customer) == PaymentPlan.BUSINESS_MEMBER:
        raise ApiException(
            requests.codes.forbidden,
            ""Not allowed to leave Business plan; contact your admin.""
        )

    if stripe_token is not None:
        customer.source = stripe_token

        try:
            customer.save()
        except stripe.InvalidRequestError as ex:
            raise ApiException(requests.codes.bad_request, str(ex))

        assert customer.sources.total_count

    if plan != PaymentPlan.FREE and not customer.sources.total_count:
        # No payment info.
        raise ApiException(
            requests.codes.payment_required,
            ""Payment information required to upgrade to %r"" % plan.value
        )

    subscription = customer.subscriptions.data[0]

    subscription.plan = plan.value
    try:
        subscription.save()
    except stripe.InvalidRequestError as ex:
        raise ApiException(requests.codes.server_error, str(ex))

    return dict(
        plan=plan.value
    )

@app.route('/api/payments/update_payment', methods=['POST'])
@api()
@as_json
def payments_update_payment():
    if not HAVE_PAYMENTS:
        raise ApiException(requests.codes.not_found, ""Payments not enabled"")

    stripe_token = request.values.get('token')
    if not stripe_token:
        raise ApiException(requests.codes.bad_request, ""Missing token"")

    customer = _get_or_create_customer()
    customer.source = stripe_token

    try:
        customer.save()
    except stripe.InvalidRequestError as ex:
        raise ApiException(requests.codes.bad_request, str(ex))

    return dict()

@app.route('/api/invite/', methods=['GET'])
@api(require_login=False)
@as_json
def invitation_user_list():
    invitations = (
        db.session.query(Invitation, Package)
        .filter_by(email=g.auth.email.lower())
        .join(Invitation.package)
        .all()
    )
    return dict(invitations=[dict(invitation_id=invite.id,
                                  owner=package.owner,
                                  package=package.name,
                                  email=invite.email,
                                  invited_at=invite.invited_at)
                             for invite, package in invitations])

@app.route('/api/invite/<owner>/<package_name>/', methods=['GET'])
@api()
@as_json
def invitation_package_list(owner, package_name):
    package = _get_package(g.auth, owner, package_name)
    invitations = (
        Invitation.query
        .filter_by(package_id=package.id)
    )

    return dict(invitations=[dict(invitation_id=invite.id,
                                  owner=package.owner,
                                  package=package.name,
                                  email=invite.email,
                                  invited_at=invite.invited_at)
                             for invite in invitations])

@app.route('/api/log', methods=['POST'])
@api(require_login=False, schema=LOG_SCHEMA)
@as_json
def client_log():
    data = request.get_json()
    for event in data:
        _mp_track(**event)

    return dict()

@app.route('/api/users/list', methods=['GET'])
@api(enabled=ENABLE_USER_ENDPOINTS, require_admin=True)
@as_json
def list_users():
    auth_headers = {
        AUTHORIZATION_HEADER: g.auth_header
    }

    user_list_api = ""%s/accounts/users"" % QUILT_AUTH_URL

    resp = requests.get(user_list_api, headers=auth_headers)

    if resp.status_code == requests.codes.not_found:
        raise ApiException(
            requests.codes.not_found,
            ""Cannot list users""
            )
    elif resp.status_code != requests.codes.ok:
        raise ApiException(
            requests.codes.server_error,
            ""Unknown error""
            )

    return resp.json()

@app.route('/api/users/list_detailed', methods=['GET'])
@api(enabled=ENABLE_USER_ENDPOINTS, require_admin=True)
@as_json
def list_users_detailed():
    package_counts_query = (
        db.session.query(Package.owner, sa.func.count(Package.owner))
        .group_by(Package.owner)
        )
    package_counts = dict(package_counts_query)

    events = (
        db.session.query(Event.user, Event.type, sa.func.count(Event.type))
        .group_by(Event.user, Event.type)
        )

    event_results = defaultdict(int)
    for event_user, event_type, event_count in events:
        event_results[(event_user, event_type)] = event_count

    # replicate code from list_users since endpoints aren't callable from each other
    auth_headers = {
        AUTHORIZATION_HEADER: g.auth_header
    }

    user_list_api = ""%s/accounts/users"" % QUILT_AUTH_URL

    users = requests.get(user_list_api, headers=auth_headers).json()

    results = {
        user['username'] : {
            'packages' : package_counts.get(user['username'], 0),
            'installs' : event_results[(user['username'], Event.Type.INSTALL)],
            'previews' : event_results[(user['username'], Event.Type.PREVIEW)],
            'pushes' : event_results[(user['username'], Event.Type.PUSH)],
            'deletes' : event_results[(user['username'], Event.Type.DELETE)],
            'status' : 'active' if user['is_active'] else 'disabled',
            'last_seen' : user['last_login']
            }
        for user in users['results']
    }

    return {'users' : results}


@app.route('/api/users/create', methods=['POST'])
@api(enabled=ENABLE_USER_ENDPOINTS, require_admin=True)
@as_json
def create_user():
    auth_headers = {
        AUTHORIZATION_HEADER: g.auth_header,
        ""Content-Type"": ""application/json"",
        ""Accept"": ""application/json"",
    }

    request_data = request.get_json()

    user_create_api = '%s/accounts/users/' % QUILT_AUTH_URL

    username = request_data.get('username')
    _validate_username(username)

    resp = requests.post(user_create_api, headers=auth_headers,
        data=json.dumps({
            ""username"": username,
            ""first_name"": """",
            ""last_name"": """",
            ""email"": request_data.get('email'),
            ""is_superuser"": False,
            ""is_staff"": False,
            ""is_active"": True,
            ""last_login"": datetime.utcnow().strftime(""%Y-%m-%dT%H:%M:%S"")
        }))

    if resp.status_code == requests.codes.not_found:
        raise ApiException(
            requests.codes.not_found,
            ""Cannot create user""
            )

    if resp.status_code == requests.codes.bad:
        if resp.text == '{""email"":[""Enter a valid email address.""]}':
            raise ApiException(
                requests.codes.bad,
                ""Please enter a valid email address.""
                )

        raise ApiException(
            requests.codes.bad,
            ""Bad request. Maybe there's already a user with the username you provided?""
            )

    elif resp.status_code != requests.codes.created:
        raise ApiException(
            requests.codes.server_error,
            ""Unknown error""
            )

    return resp.json()

@app.route('/api/users/disable', methods=['POST'])
@api(enabled=ENABLE_USER_ENDPOINTS, require_admin=True)
@as_json
def disable_user():
    auth_headers = {
        AUTHORIZATION_HEADER: g.auth_header,
        ""Content-Type"": ""application/json"",
        ""Accept"": ""application/json"",
    }

    user_modify_api = '%s/accounts/users/' % QUILT_AUTH_URL

    data = request.get_json()
    if g.auth.user == username:
        raise ApiException(
            403,
            ""Can't disable your own account.""
            )

    username = data.get('username')
    _validate_username(username)

    resp = requests.patch(""%s%s/"" % (user_modify_api, username) , headers=auth_headers,
        data=json.dumps({
            'is_active' : False
        }))

    if resp.status_code == requests.codes.not_found:
        raise ApiException(
            resp.status_code,
            ""User to disable not found.""
            )

    if resp.status_code != requests.codes.ok:
        raise ApiException(
            requests.codes.server_error,
            ""Unknown error""
            )

    return resp.json()

@app.route('/api/users/enable', methods=['POST'])
@api(enabled=ENABLE_USER_ENDPOINTS, require_admin=True)
@as_json
def enable_user():
    auth_headers = {
        AUTHORIZATION_HEADER: g.auth_header,
        ""Content-Type"": ""application/json"",
        ""Accept"": ""application/json"",
    }

    user_modify_api = '%s/accounts/users/' % QUILT_AUTH_URL

    data = request.get_json()
    username = data.get('username')
    _validate_username(username)

    resp = requests.patch(""%s%s/"" % (user_modify_api, username) , headers=auth_headers,
        data=json.dumps({
            'is_active' : True
        }))

    if resp.status_code == requests.codes.not_found:
        raise ApiException(
            resp.status_code,
            ""User to enable not found.""
            )

    if resp.status_code != requests.codes.ok:
        raise ApiException(
            requests.codes.server_error,
            ""Unknown error""
            )

    return resp.json()

# This endpoint is disabled pending a rework of authentication
@app.route('/api/users/delete', methods=['POST'])
@api(enabled=False, require_admin=True)
@as_json
def delete_user():
    auth_headers = {
        AUTHORIZATION_HEADER: g.auth_header,
        ""Content-Type"": ""application/json"",
        ""Accept"": ""application/json"",
    }
    user_modify_api = '%s/accounts/users/' % QUILT_AUTH_URL

    data = request.get_json()
    username = data.get('username')
    _validate_username(username)

    resp = requests.delete(""%s%s/"" % (user_modify_api, username), headers=auth_headers)

    if resp.status_code == requests.codes.not_found:
        raise ApiException(
            resp.status_code,
            ""User to delete not found.""
            )

    if resp.status_code != requests.codes.ok:
        raise ApiException(
            resp.status_code,
            ""Unknown error""
            )

    return resp.json()

@app.route('/api/audit/<owner>/<package_name>/')
@api(require_admin=True)
@as_json
def audit_package(owner, package_name):
    events = (
        Event.query
        .filter_by(package_owner=owner, package_name=package_name)
    )

    return dict(
        events=[dict(
            created=event.created.timestamp(),
            user=event.user,
            type=Event.Type(event.type).name,
            package_owner=event.package_owner,
            package_name=event.package_name,
            package_hash=event.package_hash,
            extra=event.extra,
        ) for event in events]
    )

@app.route('/api/audit/<user>/')
@api(require_admin=True)
@as_json
def audit_user(user):
    events = (
        Event.query
        .filter_by(user=user)
    )

    return dict(
        events=[dict(
            created=event.created.timestamp(),
            user=event.user,
            type=Event.Type(event.type).name,
            package_owner=event.package_owner,
            package_name=event.package_name,
            package_hash=event.package_hash,
            extra=event.extra,
        ) for event in events]
    )

@app.route('/api/admin/package_summary')
@api(require_admin=True)
@as_json
def package_summary():
    events = (
        db.session.query(Event.package_owner, Event.package_name, Event.type, 
                         sa.func.count(Event.type), sa.func.max(Event.created))
        .group_by(Event.package_owner, Event.package_name, Event.type)
        )

    event_results = defaultdict(lambda: {'count':0})
    packages = set()
    for event_owner, event_package, event_type, event_count, latest in events:
        package = ""{owner}/{pkg}"".format(owner=event_owner, pkg=event_package)
        event_results[(package, event_type)] = {'latest':latest.timestamp(), 'count':event_count}
        packages.add(package)

    results = {
        package : {
            'installs' : event_results[(package, Event.Type.INSTALL)],
            'previews' : event_results[(package, Event.Type.PREVIEW)],
            'pushes' : event_results[(package, Event.Type.PUSH)],
            'deletes' : event_results[(package, Event.Type.DELETE)]
        } for package in packages
    }

    return {'packages' : results}

@app.route('/api/users/reset_password', methods=['POST'])
@api(enabled=ENABLE_USER_ENDPOINTS, require_admin=True)
@as_json
def reset_password():
    auth_headers = {
        AUTHORIZATION_HEADER: g.auth_header,
        ""Content-Type"": ""application/json"",
        ""Accept"": ""application/json"",
    }

    password_reset_api = '%s/accounts/users/' % QUILT_AUTH_URL

    data = request.get_json()
    username = data.get('username')
    _validate_username(username)

    resp = requests.post(""%s%s/reset_pass/"" % (password_reset_api, username), headers=auth_headers)

    if resp.status_code == requests.codes.not_found:
        raise ApiException(
            resp.status_code,
            ""User not found.""
            )

    if resp.status_code != requests.codes.ok:
        raise ApiException(
            requests.codes.server_error,
            ""Unknown error""
            )

    return resp.json()
","Use `requests.codes.forbidden`, for consistency with everything else.","This code appears to be a RESTful API for managing packages and their associated metadata, including versions, tags, access controls, and user management. The API includes endpoints for creating, retrieving, updating, and deleting packages, versions, tags, and access controls, as well as for listing packages, searching for packages, and viewing user profiles.

The code uses SQLAlchemy to interact with a database and Flask to handle HTTP requests. It also includes several helper functions for generating URLs, validating input, and tracking events using a third-party analytics service.

Overall, the code appears to be well-structured and maintainable, with clear separation of concerns between different parts of the API. However, there are a few areas where improvements could be made:

1. Error handling: The code currently uses `raise ApiException` to handle errors, but it would be helpful to add more specific error messages and status codes to make it easier for clients to understand what went wrong.

2. Security: The code uses OAuth for authentication, but it would be helpful to add additional security measures, such as rate limiting and IP whitelisting, to prevent abuse.

3. Documentation: The code is missing documentation, which would make it harder for other developers to understand how to use the API and troubleshoot issues.

4. Performance: The code uses SQLAlchemy ORM to query the database, which can be slow for large datasets. It would be helpful to optimize the queries and consider using a caching mechanism to reduce latency.

Overall, this code appears to be a solid foundation for building a robust package management system, but there are a few areas where improvements could be made to ensure its reliability and scalability."
74,"@@ -15,6 +15,8 @@ module OhlohUi
     config.active_record.schema_format = :sql
     config.active_record.raise_in_transactional_callbacks = true
 
+    config.google_maps_api_key = 'AIzaSyBGY091UPV-hajnLzUVSyp9pUGlWsIibDM'
+
     config.autoload_paths << ""#{Rails.root}/lib""
   end
 end","require File.expand_path('../boot', __FILE__)
require 'rails/all'

Bundler.require(*Rails.groups)

require 'dotenv'
Dotenv.load '.env.local', "".env.#{Rails.env}""

module OhlohUi
  class Application < Rails::Application
    config.generators.stylesheets = false
    config.generators.javascripts = false
    config.generators.helper = false
    config.action_controller.include_all_helpers = false
    config.google_maps_api_key = 'AIzaSyBGY091UPV-hajnLzUVSyp9pUGlWsIibDM'

    config.active_record.schema_format = :sql
    config.active_record.raise_in_transactional_callbacks = true

    config.autoload_paths << ""#{Rails.root}/lib""
  end
end
",Does this key need to be hidden in a dot-env file?,"The suggested code change is a configuration file for a Ruby on Rails application. It sets up the application's environment, loads environment-specific variables from `.env` files, and configures various aspects of the application's behavior.

Here's a concise review of the suggested code change:

- **Boot and Rails Setup**: The code starts by requiring necessary files and loading Rails components.
- **Environment Configuration**: It loads environment-specific variables from `.env.local` and `.env.<Rails.env>`.
- **Application Module**: The `OhlohUi::Application` class is defined, which inherits from `Rails::Application`.
- **Generator Configurations**: It disables the generation of stylesheets, JavaScripts, and helpers, and sets `include_all_helpers` to false.
- **Google Maps API Key**: A Google Maps API key is configured.
- **Active Record Settings**: It sets the schema format to SQL and enables raising exceptions in transactional callbacks.
- **Autoload Paths**: It adds the `lib` directory to the autoload paths.

Overall, this setup ensures that the application is configured correctly for development, testing, and production environments, with specific configurations tailored to each environment."
75,"@@ -40,8 +40,8 @@ func ValidateCertificateSpec(crt *internalcmapi.CertificateSpec, fldPath *field.
 
 	el = append(el, validateIssuerRef(crt.IssuerRef, fldPath)...)
 
-	if len(crt.CommonName) == 0 && len(crt.DNSNames) == 0 && len(crt.URISANs) == 0 && len(crt.EmailSANs) == 0 {
-		el = append(el, field.Invalid(fldPath, """", ""at least one of commonName, dnsNames, uris or emailAddresses must be set""))
+	if len(crt.CommonName) == 0 && len(crt.DNSNames) == 0 && len(crt.URISANs) == 0 && len(crt.EmailSANs) == 0 && len(crt.IPAddresses) == 0 {
+		el = append(el, field.Invalid(fldPath, """", ""at least one of commonName, dnsNames, uris ipAddresses, or emailAddresses must be set""))
 	}
 
 	// if a common name has been specified, ensure it is no longer than 64 chars","/*
Copyright 2019 The Jetstack cert-manager contributors.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package validation

import (
	""fmt""
	""net""
	""net/mail""

	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/util/validation/field""

	""github.com/jetstack/cert-manager/pkg/api/util""
	cmapi ""github.com/jetstack/cert-manager/pkg/apis/certmanager/v1""
	internalcmapi ""github.com/jetstack/cert-manager/pkg/internal/apis/certmanager""
	cmmeta ""github.com/jetstack/cert-manager/pkg/internal/apis/meta""
)

// Validation functions for cert-manager Certificate types

func ValidateCertificateSpec(crt *internalcmapi.CertificateSpec, fldPath *field.Path) field.ErrorList {
	el := field.ErrorList{}
	if crt.SecretName == """" {
		el = append(el, field.Required(fldPath.Child(""secretName""), ""must be specified""))
	}
	if len(crt.CommonName) == 0 && len(crt.DNSNames) == 0 && len(crt.URISANs) == 0 && len(crt.EmailSANs) == 0 && len(crt.IPAddresses) == 0 {
		el = append(el, field.Invalid(fldPath, """", ""at least one of commonName, dnsNames, uris ipAddresses, or emailAddresses must be set""))

	el = append(el, validateIssuerRef(crt.IssuerRef, fldPath)...)

	if len(crt.CommonName) == 0 && len(crt.DNSNames) == 0 && len(crt.URISANs) == 0 && len(crt.EmailSANs) == 0 {
		el = append(el, field.Invalid(fldPath, """", ""at least one of commonName, dnsNames, uris or emailAddresses must be set""))
	}

	// if a common name has been specified, ensure it is no longer than 64 chars
	if len(crt.CommonName) > 64 {
		el = append(el, field.TooLong(fldPath.Child(""commonName""), crt.CommonName, 64))
	}

	if len(crt.IPAddresses) > 0 {
		el = append(el, validateIPAddresses(crt, fldPath)...)
	}

	if len(crt.EmailSANs) > 0 {
		el = append(el, validateEmailAddresses(crt, fldPath)...)
	}

	if crt.PrivateKey != nil {
		switch crt.PrivateKey.Algorithm {
		case """", internalcmapi.RSAKeyAlgorithm:
			if crt.PrivateKey.Size > 0 && (crt.PrivateKey.Size < 2048 || crt.PrivateKey.Size > 8192) {
				el = append(el, field.Invalid(fldPath.Child(""privateKey"", ""size""), crt.PrivateKey.Size, ""must be between 2048 & 8192 for rsa keyAlgorithm""))
			}
		case internalcmapi.ECDSAKeyAlgorithm:
			if crt.PrivateKey.Size > 0 && crt.PrivateKey.Size != 256 && crt.PrivateKey.Size != 384 && crt.PrivateKey.Size != 521 {
				el = append(el, field.NotSupported(fldPath.Child(""privateKey"", ""size""), crt.PrivateKey.Size, []string{""256"", ""384"", ""521""}))
			}
		default:
			el = append(el, field.Invalid(fldPath.Child(""privateKey"", ""algorithm""), crt.PrivateKey.Algorithm, ""must be either empty or one of rsa or ecdsa""))
		}
	}

	if crt.Duration != nil || crt.RenewBefore != nil {
		el = append(el, ValidateDuration(crt, fldPath)...)
	}
	if len(crt.Usages) > 0 {
		el = append(el, validateUsages(crt, fldPath)...)
	}
	return el
}

func ValidateCertificate(obj runtime.Object) field.ErrorList {
	crt := obj.(*internalcmapi.Certificate)
	allErrs := ValidateCertificateSpec(&crt.Spec, field.NewPath(""spec""))
	return allErrs
}

func ValidateUpdateCertificate(oldObj, obj runtime.Object) field.ErrorList {
	crt := obj.(*internalcmapi.Certificate)
	allErrs := ValidateCertificateSpec(&crt.Spec, field.NewPath(""spec""))
	return allErrs
}

func validateIssuerRef(issuerRef cmmeta.ObjectReference, fldPath *field.Path) field.ErrorList {
	el := field.ErrorList{}

	issuerRefPath := fldPath.Child(""issuerRef"")
	if issuerRef.Name == """" {
		el = append(el, field.Required(issuerRefPath.Child(""name""), ""must be specified""))
	}
	if issuerRef.Group == """" || issuerRef.Group == internalcmapi.SchemeGroupVersion.Group {
		switch issuerRef.Kind {
		case """":
		case ""Issuer"", ""ClusterIssuer"":
		default:
			el = append(el, field.Invalid(issuerRefPath.Child(""kind""), issuerRef.Kind, ""must be one of Issuer or ClusterIssuer""))
		}
	}
	return el
}

func validateIPAddresses(a *internalcmapi.CertificateSpec, fldPath *field.Path) field.ErrorList {
	if len(a.IPAddresses) <= 0 {
		return nil
	}
	el := field.ErrorList{}
	for i, d := range a.IPAddresses {
		ip := net.ParseIP(d)
		if ip == nil {
			el = append(el, field.Invalid(fldPath.Child(""ipAddresses"").Index(i), d, ""invalid IP address""))
		}
	}
	return el
}

func validateEmailAddresses(a *internalcmapi.CertificateSpec, fldPath *field.Path) field.ErrorList {
	if len(a.EmailSANs) <= 0 {
		return nil
	}
	el := field.ErrorList{}
	for i, d := range a.EmailSANs {
		e, err := mail.ParseAddress(d)
		if err != nil {
			el = append(el, field.Invalid(fldPath.Child(""emailAddresses"").Index(i), d, fmt.Sprintf(""invalid email address: %s"", err)))
		} else if e.Address != d {
			// Go accepts email names as per RFC 5322 (name <email>)
			// This checks if the supplied value only contains the email address and nothing else
			el = append(el, field.Invalid(fldPath.Child(""emailAddresses"").Index(i), d, ""invalid email address: make sure the supplied value only contains the email address itself""))
		}
	}
	return el
}

func validateUsages(a *internalcmapi.CertificateSpec, fldPath *field.Path) field.ErrorList {
	el := field.ErrorList{}
	for i, u := range a.Usages {
		_, kok := util.KeyUsageType(cmapi.KeyUsage(u))
		_, ekok := util.ExtKeyUsageType(cmapi.KeyUsage(u))
		if !kok && !ekok {
			el = append(el, field.Invalid(fldPath.Child(""usages"").Index(i), u, ""unknown keyusage""))
		}
	}
	return el
}

func ValidateDuration(crt *internalcmapi.CertificateSpec, fldPath *field.Path) field.ErrorList {
	el := field.ErrorList{}

	duration := util.DefaultCertDuration(crt.Duration)
	renewBefore := cmapi.DefaultRenewBefore
	if crt.RenewBefore != nil {
		renewBefore = crt.RenewBefore.Duration
	}
	if duration < cmapi.MinimumCertificateDuration {
		el = append(el, field.Invalid(fldPath.Child(""duration""), duration, fmt.Sprintf(""certificate duration must be greater than %s"", cmapi.MinimumCertificateDuration)))
	}
	if renewBefore < cmapi.MinimumRenewBefore {
		el = append(el, field.Invalid(fldPath.Child(""renewBefore""), renewBefore, fmt.Sprintf(""certificate renewBefore must be greater than %s"", cmapi.MinimumRenewBefore)))
	}
	if duration <= renewBefore {
		el = append(el, field.Invalid(fldPath.Child(""renewBefore""), renewBefore, fmt.Sprintf(""certificate duration %s must be greater than renewBefore %s"", duration, renewBefore)))
	}
	return el
}
","I was thinking about this earlier. Is it valid to have a CA certificate without any of these attributes? I've seen `commonName` used in CA certs, but that's deprecated.","The suggested code changes appear to be comprehensive validation functions for the `Certificate` resource in the cert-manager Kubernetes API. Here's a concise review:

### Key Points:
1. **Validation Functions**:
   - `ValidateCertificateSpec`: Validates the `CertificateSpec` fields, including `secretName`, `commonName`, `DNSNames`, `URISANs`, `EmailSANs`, `IPAddresses`, `IssuerRef`, `PrivateKey`, `Duration`, and `Usages`.
   - `ValidateCertificate`: Calls `ValidateCertificateSpec` for the `Certificate` object.
   - `ValidateUpdateCertificate`: Calls `ValidateCertificateSpec` for the updated `Certificate` object.

2. **Validation Logic**:
   - **IssuerRef**: Ensures `name` is specified and `kind` is either `Issuer` or `ClusterIssuer`.
   - **IP Addresses**: Validates each IP address using `net.ParseIP`.
   - **Email Addresses**: Validates each email address using `mail.ParseAddress` and ensures it only contains the email address itself.
   - **Usages**: Validates each usage against known key usages.
   - **Duration**: Ensures the duration is greater than the minimum certificate duration and that renewBefore is greater than the minimum renewBefore and less than the duration.

3. **Error Handling**:
   - Uses `field.ErrorList` to accumulate validation errors, which are returned at the end of each function.

### Potential Improvements:
- **Consistency**: Ensure consistent naming conventions and formatting throughout the code.
- **Documentation**: Add comments to explain the purpose and functionality of each function and method.
- **Edge Cases**: Consider edge cases, such as empty slices or invalid input, and handle them appropriately.

Overall, the code appears to be well-structured and covers most of the validation requirements for the `Certificate` resource."
76,"@@ -554,7 +554,13 @@ void Items::parseItemNode(const pugi::xml_node& itemNode, uint16_t id)
 
 	it.name = itemNode.attribute(""name"").as_string();
 
-	nameToItems.insert({ asLowerCaseString(it.name), id });
+	if (!it.name.empty()) {
+		std::string lowerCaseName = asLowerCaseString(it.name);
+		auto result = nameToItems.find(lowerCaseName);
+		if (result == nameToItems.end()) {
+			nameToItems.insert({ std::move(lowerCaseName), id });
+		}
+	}
 
 	pugi::xml_attribute articleAttribute = itemNode.attribute(""article"");
 	if (articleAttribute) {","/**
 * The Forgotten Server - a free and open-source MMORPG server emulator
 * Copyright (C) 2019  Mark Samman <mark.samman@gmail.com>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License along
 * with this program; if not, write to the Free Software Foundation, Inc.,
 * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
 */

#include ""otpch.h""

#include ""items.h""
#include ""spells.h""
#include ""movement.h""
#include ""weapons.h""

#include ""pugicast.h""

extern MoveEvents* g_moveEvents;
extern Weapons* g_weapons;

const std::unordered_map<std::string, ItemParseAttributes_t> ItemParseAttributesMap = {
	{""type"", ITEM_PARSE_TYPE},
	{""description"", ITEM_PARSE_DESCRIPTION},
	{""runespellname"", ITEM_PARSE_RUNESPELLNAME},
	{""weight"", ITEM_PARSE_WEIGHT},
	{""showcount"", ITEM_PARSE_SHOWCOUNT},
	{""armor"", ITEM_PARSE_ARMOR},
	{""defense"", ITEM_PARSE_DEFENSE},
	{""extradef"", ITEM_PARSE_EXTRADEF},
	{""attack"", ITEM_PARSE_ATTACK},
	{""rotateto"", ITEM_PARSE_ROTATETO},
	{""moveable"", ITEM_PARSE_MOVEABLE},
	{""movable"", ITEM_PARSE_MOVEABLE},
	{""blockprojectile"", ITEM_PARSE_BLOCKPROJECTILE},
	{""allowpickupable"", ITEM_PARSE_PICKUPABLE},
	{""pickupable"", ITEM_PARSE_PICKUPABLE},
	{""forceserialize"", ITEM_PARSE_FORCESERIALIZE},
	{""forcesave"", ITEM_PARSE_FORCESERIALIZE},
	{""floorchange"", ITEM_PARSE_FLOORCHANGE},
	{""corpsetype"", ITEM_PARSE_CORPSETYPE},
	{""containersize"", ITEM_PARSE_CONTAINERSIZE},
	{""fluidsource"", ITEM_PARSE_FLUIDSOURCE},
	{""readable"", ITEM_PARSE_READABLE},
	{""writeable"", ITEM_PARSE_WRITEABLE},
	{""maxtextlen"", ITEM_PARSE_MAXTEXTLEN},
	{""writeonceitemid"", ITEM_PARSE_WRITEONCEITEMID},
	{""weapontype"", ITEM_PARSE_WEAPONTYPE},
	{""slottype"", ITEM_PARSE_SLOTTYPE},
	{""ammotype"", ITEM_PARSE_AMMOTYPE},
	{""shoottype"", ITEM_PARSE_SHOOTTYPE},
	{""effect"", ITEM_PARSE_EFFECT},
	{""range"", ITEM_PARSE_RANGE},
	{""stopduration"", ITEM_PARSE_STOPDURATION},
	{""decayto"", ITEM_PARSE_DECAYTO},
	{""transformequipto"", ITEM_PARSE_TRANSFORMEQUIPTO},
	{""transformdeequipto"", ITEM_PARSE_TRANSFORMDEEQUIPTO},
	{""duration"", ITEM_PARSE_DURATION},
	{""showduration"", ITEM_PARSE_SHOWDURATION},
	{""charges"", ITEM_PARSE_CHARGES},
	{""showcharges"", ITEM_PARSE_SHOWCHARGES},
	{""showattributes"", ITEM_PARSE_SHOWATTRIBUTES},
	{""hitchance"", ITEM_PARSE_HITCHANCE},
	{""maxhitchance"", ITEM_PARSE_MAXHITCHANCE},
	{""invisible"", ITEM_PARSE_INVISIBLE},
	{""speed"", ITEM_PARSE_SPEED},
	{""healthgain"", ITEM_PARSE_HEALTHGAIN},
	{""healthticks"", ITEM_PARSE_HEALTHTICKS},
	{""managain"", ITEM_PARSE_MANAGAIN},
	{""manaticks"", ITEM_PARSE_MANATICKS},
	{""manashield"", ITEM_PARSE_MANASHIELD},
	{""skillsword"", ITEM_PARSE_SKILLSWORD},
	{""skillaxe"", ITEM_PARSE_SKILLAXE},
	{""skillclub"", ITEM_PARSE_SKILLCLUB},
	{""skilldist"", ITEM_PARSE_SKILLDIST},
	{""skillfish"", ITEM_PARSE_SKILLFISH},
	{""skillshield"", ITEM_PARSE_SKILLSHIELD},
	{""skillfist"", ITEM_PARSE_SKILLFIST},
	{""maxhitpoints"", ITEM_PARSE_MAXHITPOINTS},
	{""maxhitpointspercent"", ITEM_PARSE_MAXHITPOINTSPERCENT},
	{""maxmanapoints"", ITEM_PARSE_MAXMANAPOINTS},
	{""maxmanapointspercent"", ITEM_PARSE_MAXMANAPOINTSPERCENT},
	{""magicpoints"", ITEM_PARSE_MAGICPOINTS},
	{""magiclevelpoints"", ITEM_PARSE_MAGICPOINTS},
	{""magicpointspercent"", ITEM_PARSE_MAGICPOINTSPERCENT},
	{""criticalhitchance"", ITEM_PARSE_CRITICALHITCHANCE},
	{""criticalhitamount"", ITEM_PARSE_CRITICALHITAMOUNT},
	{""lifeleechchance"", ITEM_PARSE_LIFELEECHCHANCE},
	{""lifeleechamount"", ITEM_PARSE_LIFELEECHAMOUNT},
	{""manaleechchance"", ITEM_PARSE_MANALEECHCHANCE},
	{""manaleechamount"", ITEM_PARSE_MANALEECHAMOUNT},
	{""fieldabsorbpercentenergy"", ITEM_PARSE_FIELDABSORBPERCENTENERGY},
	{""fieldabsorbpercentfire"", ITEM_PARSE_FIELDABSORBPERCENTFIRE},
	{""fieldabsorbpercentpoison"", ITEM_PARSE_FIELDABSORBPERCENTPOISON},
	{""fieldabsorbpercentearth"", ITEM_PARSE_FIELDABSORBPERCENTPOISON},
	{""absorbpercentall"", ITEM_PARSE_ABSORBPERCENTALL},
	{""absorbpercentallelements"", ITEM_PARSE_ABSORBPERCENTALL},
	{""absorbpercentelements"", ITEM_PARSE_ABSORBPERCENTELEMENTS},
	{""absorbpercentmagic"", ITEM_PARSE_ABSORBPERCENTMAGIC},
	{""absorbpercentenergy"", ITEM_PARSE_ABSORBPERCENTENERGY},
	{""absorbpercentfire"", ITEM_PARSE_ABSORBPERCENTFIRE},
	{""absorbpercentpoison"", ITEM_PARSE_ABSORBPERCENTPOISON},
	{""absorbpercentearth"", ITEM_PARSE_ABSORBPERCENTPOISON},
	{""absorbpercentice"", ITEM_PARSE_ABSORBPERCENTICE},
	{""absorbpercentholy"", ITEM_PARSE_ABSORBPERCENTHOLY},
	{""absorbpercentdeath"", ITEM_PARSE_ABSORBPERCENTDEATH},
	{""absorbpercentlifedrain"", ITEM_PARSE_ABSORBPERCENTLIFEDRAIN},
	{""absorbpercentmanadrain"", ITEM_PARSE_ABSORBPERCENTMANADRAIN},
	{""absorbpercentdrown"", ITEM_PARSE_ABSORBPERCENTDROWN},
	{""absorbpercentphysical"", ITEM_PARSE_ABSORBPERCENTPHYSICAL},
	{""absorbpercenthealing"", ITEM_PARSE_ABSORBPERCENTHEALING},
	{""absorbpercentundefined"", ITEM_PARSE_ABSORBPERCENTUNDEFINED},
	{""suppressdrunk"", ITEM_PARSE_SUPPRESSDRUNK},
	{""suppressenergy"", ITEM_PARSE_SUPPRESSENERGY},
	{""suppressfire"", ITEM_PARSE_SUPPRESSFIRE},
	{""suppresspoison"", ITEM_PARSE_SUPPRESSPOISON},
	{""suppressdrown"", ITEM_PARSE_SUPPRESSDROWN},
	{""suppressphysical"", ITEM_PARSE_SUPPRESSPHYSICAL},
	{""suppressfreeze"", ITEM_PARSE_SUPPRESSFREEZE},
	{""suppressdazzle"", ITEM_PARSE_SUPPRESSDAZZLE},
	{""suppresscurse"", ITEM_PARSE_SUPPRESSCURSE},
	{""field"", ITEM_PARSE_FIELD},
	{""replaceable"", ITEM_PARSE_REPLACEABLE},
	{""partnerdirection"", ITEM_PARSE_PARTNERDIRECTION},
	{""leveldoor"", ITEM_PARSE_LEVELDOOR},
	{""maletransformto"", ITEM_PARSE_MALETRANSFORMTO},
	{""malesleeper"", ITEM_PARSE_MALETRANSFORMTO},
	{""femaletransformto"", ITEM_PARSE_FEMALETRANSFORMTO},
	{""femalesleeper"", ITEM_PARSE_FEMALETRANSFORMTO},
	{""transformto"", ITEM_PARSE_TRANSFORMTO},
	{""destroyto"", ITEM_PARSE_DESTROYTO},
	{""elementice"", ITEM_PARSE_ELEMENTICE},
	{""elementearth"", ITEM_PARSE_ELEMENTEARTH},
	{""elementfire"", ITEM_PARSE_ELEMENTFIRE},
	{""elementenergy"", ITEM_PARSE_ELEMENTENERGY},
	{""elementdeath"", ITEM_PARSE_ELEMENTDEATH},
	{""elementholy"", ITEM_PARSE_ELEMENTHOLY},
	{""walkstack"", ITEM_PARSE_WALKSTACK},
	{""blocking"", ITEM_PARSE_BLOCKING},
	{""allowdistread"", ITEM_PARSE_ALLOWDISTREAD},
	{""storeitem"", ITEM_PARSE_STOREITEM},
};

const std::unordered_map<std::string, ItemTypes_t> ItemTypesMap = {
	{""key"", ITEM_TYPE_KEY},
	{""magicfield"", ITEM_TYPE_MAGICFIELD},
	{""container"", ITEM_TYPE_CONTAINER},
	{""depot"", ITEM_TYPE_DEPOT},
	{""mailbox"", ITEM_TYPE_MAILBOX},
	{""trashholder"", ITEM_TYPE_TRASHHOLDER},
	{""teleport"", ITEM_TYPE_TELEPORT},
	{""door"", ITEM_TYPE_DOOR},
	{""bed"", ITEM_TYPE_BED},
	{""rune"", ITEM_TYPE_RUNE},
};

const std::unordered_map<std::string, tileflags_t> TileStatesMap = {
	{""down"", TILESTATE_FLOORCHANGE_DOWN},
	{""north"", TILESTATE_FLOORCHANGE_NORTH},
	{""south"", TILESTATE_FLOORCHANGE_SOUTH},
	{""southalt"", TILESTATE_FLOORCHANGE_SOUTH_ALT},
	{""west"", TILESTATE_FLOORCHANGE_WEST},
	{""east"", TILESTATE_FLOORCHANGE_EAST},
	{""eastalt"", TILESTATE_FLOORCHANGE_EAST_ALT},
};

const std::unordered_map<std::string, RaceType_t> RaceTypesMap = {
	{""venom"", RACE_VENOM},
	{""blood"", RACE_BLOOD},
	{""undead"", RACE_UNDEAD},
	{""fire"", RACE_FIRE},
	{""energy"", RACE_ENERGY},
};

const std::unordered_map<std::string, WeaponType_t> WeaponTypesMap = {
	{""sword"", WEAPON_SWORD},
	{""club"", WEAPON_CLUB},
	{""axe"", WEAPON_AXE},
	{""shield"", WEAPON_SHIELD},
	{""distance"", WEAPON_DISTANCE},
	{""wand"", WEAPON_WAND},
	{""ammunition"", WEAPON_AMMO},
};

const std::unordered_map<std::string, FluidTypes_t> FluidTypesMap = {
	{""water"", FLUID_WATER},
	{""blood"", FLUID_BLOOD},
	{""beer"", FLUID_BEER},
	{""slime"", FLUID_SLIME},
	{""lemonade"", FLUID_LEMONADE},
	{""milk"", FLUID_MILK},
	{""mana"", FLUID_MANA},
	{""life"", FLUID_LIFE},
	{""oil"", FLUID_OIL},
	{""urine"", FLUID_URINE},
	{""coconut"", FLUID_COCONUTMILK},
	{""wine"", FLUID_WINE},
	{""mud"", FLUID_MUD},
	{""fruitjuice"", FLUID_FRUITJUICE},
	{""lava"", FLUID_LAVA},
	{""rum"", FLUID_RUM},
	{""swamp"", FLUID_SWAMP},
	{""tea"", FLUID_TEA},
	{""mead"", FLUID_MEAD},
};

Items::Items()
{
	items.reserve(30000);
	nameToItems.reserve(30000);
}

void Items::clear()
{
	items.clear();
	clientIdToServerIdMap.clear();
	nameToItems.clear();
	inventory.clear();
}

bool Items::reload()
{
	clear();
	loadFromOtb(""data/items/items.otb"");

	if (!loadFromXml()) {
		return false;
	}

	g_moveEvents->reload();
	g_weapons->reload();
	g_weapons->loadDefaults();
	return true;
}

constexpr auto OTBI = OTB::Identifier{{'O','T', 'B', 'I'}};

bool Items::loadFromOtb(const std::string& file)
{
	OTB::Loader loader{file, OTBI};

	auto& root = loader.parseTree();

	PropStream props;
	if (loader.getProps(root, props)) {
		//4 byte flags
		//attributes
		//0x01 = version data
		uint32_t flags;
		if (!props.read<uint32_t>(flags)) {
			return false;
		}

		uint8_t attr;
		if (!props.read<uint8_t>(attr)) {
			return false;
		}

		if (attr == ROOT_ATTR_VERSION) {
			uint16_t datalen;
			if (!props.read<uint16_t>(datalen)) {
				return false;
			}

			if (datalen != sizeof(VERSIONINFO)) {
				return false;
			}

			VERSIONINFO vi;
			if (!props.read(vi)) {
				return false;
			}

			majorVersion = vi.dwMajorVersion; //items otb format file version
			minorVersion = vi.dwMinorVersion; //client version
			buildNumber = vi.dwBuildNumber; //revision
		}
	}

	if (majorVersion == 0xFFFFFFFF) {
		std::cout << ""[Warning - Items::loadFromOtb] items.otb using generic client version."" << std::endl;
	} else if (majorVersion != 3) {
		std::cout << ""Old version detected, a newer version of items.otb is required."" << std::endl;
		return false;
	} else if (minorVersion < CLIENT_VERSION_1098) {
		std::cout << ""A newer version of items.otb is required."" << std::endl;
		return false;
	}

	for (auto& itemNode : root.children) {
		PropStream stream;
		if (!loader.getProps(itemNode, stream)) {
			return false;
		}

		uint32_t flags;
		if (!stream.read<uint32_t>(flags)) {
			return false;
		}

		uint16_t serverId = 0;
		uint16_t clientId = 0;
		uint16_t speed = 0;
		uint16_t wareId = 0;
		uint8_t lightLevel = 0;
		uint8_t lightColor = 0;
		uint8_t alwaysOnTopOrder = 0;

		uint8_t attrib;
		while (stream.read<uint8_t>(attrib)) {
			uint16_t datalen;
			if (!stream.read<uint16_t>(datalen)) {
				return false;
			}

			switch (attrib) {
				case ITEM_ATTR_SERVERID: {
					if (datalen != sizeof(uint16_t)) {
						return false;
					}

					if (!stream.read<uint16_t>(serverId)) {
						return false;
					}
					break;
				}

				case ITEM_ATTR_CLIENTID: {
					if (datalen != sizeof(uint16_t)) {
						return false;
					}

					if (!stream.read<uint16_t>(clientId)) {
						return false;
					}
					break;
				}

				case ITEM_ATTR_SPEED: {
					if (datalen != sizeof(uint16_t)) {
						return false;
					}

					if (!stream.read<uint16_t>(speed)) {
						return false;
					}
					break;
				}

				case ITEM_ATTR_LIGHT2: {
					if (datalen != sizeof(lightBlock2)) {
						return false;
					}

					lightBlock2 lb2;
					if (!stream.read(lb2)) {
						return false;
					}

					lightLevel = static_cast<uint8_t>(lb2.lightLevel);
					lightColor = static_cast<uint8_t>(lb2.lightColor);
					break;
				}

				case ITEM_ATTR_TOPORDER: {
					if (datalen != sizeof(uint8_t)) {
						return false;
					}

					if (!stream.read<uint8_t>(alwaysOnTopOrder)) {
						return false;
					}
					break;
				}

				case ITEM_ATTR_WAREID: {
					if (datalen != sizeof(uint16_t)) {
						return false;
					}

					if (!stream.read<uint16_t>(wareId)) {
						return false;
					}
					break;
				}

				default: {
					//skip unknown attributes
					if (!stream.skip(datalen)) {
						return false;
					}
					break;
				}
			}
		}

		clientIdToServerIdMap.emplace(clientId, serverId);

		// store the found item
		if (serverId >= items.size()) {
			items.resize(serverId + 1);
		}
		ItemType& iType = items[serverId];

		iType.group = static_cast<itemgroup_t>(itemNode.type);
		switch (itemNode.type) {
			case ITEM_GROUP_CONTAINER:
				iType.type = ITEM_TYPE_CONTAINER;
				break;
			case ITEM_GROUP_DOOR:
				//not used
				iType.type = ITEM_TYPE_DOOR;
				break;
			case ITEM_GROUP_MAGICFIELD:
				//not used
				iType.type = ITEM_TYPE_MAGICFIELD;
				break;
			case ITEM_GROUP_TELEPORT:
				//not used
				iType.type = ITEM_TYPE_TELEPORT;
				break;
			case ITEM_GROUP_NONE:
			case ITEM_GROUP_GROUND:
			case ITEM_GROUP_SPLASH:
			case ITEM_GROUP_FLUID:
			case ITEM_GROUP_CHARGES:
			case ITEM_GROUP_DEPRECATED:
				break;
			default:
				return false;
		}

		iType.blockSolid = hasBitSet(FLAG_BLOCK_SOLID, flags);
		iType.blockProjectile = hasBitSet(FLAG_BLOCK_PROJECTILE, flags);
		iType.blockPathFind = hasBitSet(FLAG_BLOCK_PATHFIND, flags);
		iType.hasHeight = hasBitSet(FLAG_HAS_HEIGHT, flags);
		iType.useable = hasBitSet(FLAG_USEABLE, flags);
		iType.pickupable = hasBitSet(FLAG_PICKUPABLE, flags);
		iType.moveable = hasBitSet(FLAG_MOVEABLE, flags);
		iType.stackable = hasBitSet(FLAG_STACKABLE, flags);

		iType.alwaysOnTop = hasBitSet(FLAG_ALWAYSONTOP, flags);
		iType.isVertical = hasBitSet(FLAG_VERTICAL, flags);
		iType.isHorizontal = hasBitSet(FLAG_HORIZONTAL, flags);
		iType.isHangable = hasBitSet(FLAG_HANGABLE, flags);
		iType.allowDistRead = hasBitSet(FLAG_ALLOWDISTREAD, flags);
		iType.rotatable = hasBitSet(FLAG_ROTATABLE, flags);
		iType.canReadText = hasBitSet(FLAG_READABLE, flags);
		iType.lookThrough = hasBitSet(FLAG_LOOKTHROUGH, flags);
		iType.isAnimation = hasBitSet(FLAG_ANIMATION, flags);
		// iType.walkStack = !hasBitSet(FLAG_FULLTILE, flags);
		iType.forceUse = hasBitSet(FLAG_FORCEUSE, flags);

		iType.id = serverId;
		iType.clientId = clientId;
		iType.speed = speed;
		iType.lightLevel = lightLevel;
		iType.lightColor = lightColor;
		iType.wareId = wareId;
		iType.alwaysOnTopOrder = alwaysOnTopOrder;
	}

	items.shrink_to_fit();
	return true;
}

bool Items::loadFromXml()
{
	pugi::xml_document doc;
	pugi::xml_parse_result result = doc.load_file(""data/items/items.xml"");
	if (!result) {
		printXMLError(""Error - Items::loadFromXml"", ""data/items/items.xml"", result);
		return false;
	}

	for (auto itemNode : doc.child(""items"").children()) {
		pugi::xml_attribute idAttribute = itemNode.attribute(""id"");
		if (idAttribute) {
			parseItemNode(itemNode, pugi::cast<uint16_t>(idAttribute.value()));
			continue;
		}

		pugi::xml_attribute fromIdAttribute = itemNode.attribute(""fromid"");
		if (!fromIdAttribute) {
			std::cout << ""[Warning - Items::loadFromXml] No item id found"" << std::endl;
			continue;
		}

		pugi::xml_attribute toIdAttribute = itemNode.attribute(""toid"");
		if (!toIdAttribute) {
			std::cout << ""[Warning - Items::loadFromXml] fromid ("" << fromIdAttribute.value() << "") without toid"" << std::endl;
			continue;
		}

		uint16_t id = pugi::cast<uint16_t>(fromIdAttribute.value());
		uint16_t toId = pugi::cast<uint16_t>(toIdAttribute.value());
		while (id <= toId) {
			parseItemNode(itemNode, id++);
		}
	}

	buildInventoryList();
	return true;
}

void Items::buildInventoryList()
{
	inventory.reserve(items.size());
	for (const auto& type: items) {
		if (type.weaponType != WEAPON_NONE || type.ammoType != AMMO_NONE ||
			type.attack != 0 || type.defense != 0 ||
			type.extraDefense != 0 || type.armor != 0 ||
			type.slotPosition & SLOTP_NECKLACE ||
			type.slotPosition & SLOTP_RING ||
			type.slotPosition & SLOTP_AMMO ||
			type.slotPosition & SLOTP_FEET ||
			type.slotPosition & SLOTP_HEAD ||
			type.slotPosition & SLOTP_ARMOR ||
			type.slotPosition & SLOTP_LEGS)
		{
			inventory.push_back(type.clientId);
		}
	}
	inventory.shrink_to_fit();
	std::sort(inventory.begin(), inventory.end());
}

void Items::parseItemNode(const pugi::xml_node& itemNode, uint16_t id)
{
	if (id > 0 && id < 100) {
		ItemType& iType = items[id];
		iType.id = id;
	}

	ItemType& it = getItemType(id);
	if (it.id == 0) {
		return;
	}

	if (!it.name.empty()) {
		std::cout << ""[Warning - Items::parseItemNode] Duplicate item with id: "" << id << std::endl;
		return;
	}
	if (!it.name.empty()) {
		std::string lowerCaseName = asLowerCaseString(it.name);
		auto result = nameToItems.find(lowerCaseName);
		if (result == nameToItems.end()) {
			nameToItems.insert({ std::move(lowerCaseName), id });
		}
	}

	it.name = itemNode.attribute(""name"").as_string();

	nameToItems.insert({ asLowerCaseString(it.name), id });

	pugi::xml_attribute articleAttribute = itemNode.attribute(""article"");
	if (articleAttribute) {
		it.article = articleAttribute.as_string();
	}

	pugi::xml_attribute pluralAttribute = itemNode.attribute(""plural"");
	if (pluralAttribute) {
		it.pluralName = pluralAttribute.as_string();
	}

	Abilities& abilities = it.getAbilities();

	for (auto attributeNode : itemNode.children()) {
		pugi::xml_attribute keyAttribute = attributeNode.attribute(""key"");
		if (!keyAttribute) {
			continue;
		}

		pugi::xml_attribute valueAttribute = attributeNode.attribute(""value"");
		if (!valueAttribute) {
			continue;
		}

		std::string tmpStrValue = asLowerCaseString(keyAttribute.as_string());
		auto parseAttribute = ItemParseAttributesMap.find(tmpStrValue);
		if (parseAttribute != ItemParseAttributesMap.end()) {
			ItemParseAttributes_t parseType = parseAttribute->second;
			switch (parseType) {
				case ITEM_PARSE_TYPE: {
					tmpStrValue = asLowerCaseString(valueAttribute.as_string());
					auto it2 = ItemTypesMap.find(tmpStrValue);
					if (it2 != ItemTypesMap.end()) {
						it.type = it2->second;
						if (it.type == ITEM_TYPE_CONTAINER) {
							it.group = ITEM_GROUP_CONTAINER;
						}
					} else {
						std::cout << ""[Warning - Items::parseItemNode] Unknown type: "" << valueAttribute.as_string() << std::endl;
					}
					break;
				}

				case ITEM_PARSE_DESCRIPTION: {
					it.description = valueAttribute.as_string();
					break;
				}

				case ITEM_PARSE_RUNESPELLNAME: {
					it.runeSpellName = valueAttribute.as_string();
					break;
				}

				case ITEM_PARSE_WEIGHT: {
					it.weight = pugi::cast<uint32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_SHOWCOUNT: {
					it.showCount = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_ARMOR: {
					it.armor = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_DEFENSE: {
					it.defense = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_EXTRADEF: {
					it.extraDefense = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ATTACK: {
					it.attack = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ROTATETO: {
					it.rotateTo = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_MOVEABLE: {
					it.moveable = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_BLOCKPROJECTILE: {
					it.blockProjectile = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_PICKUPABLE: {
					it.allowPickupable = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_FORCESERIALIZE: {
					it.forceSerialize = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_FLOORCHANGE: {
					tmpStrValue = asLowerCaseString(valueAttribute.as_string());
					auto it2 = TileStatesMap.find(tmpStrValue);
					if (it2 != TileStatesMap.end()) {
						it.floorChange |= it2->second;
					} else {
						std::cout << ""[Warning - Items::parseItemNode] Unknown floorChange: "" << valueAttribute.as_string() << std::endl;
					}
					break;
				}

				case ITEM_PARSE_CORPSETYPE: {
					tmpStrValue = asLowerCaseString(valueAttribute.as_string());
					auto it2 = RaceTypesMap.find(tmpStrValue);
					if (it2 != RaceTypesMap.end()) {
						it.corpseType = it2->second;
					} else {
						std::cout << ""[Warning - Items::parseItemNode] Unknown corpseType: "" << valueAttribute.as_string() << std::endl;
					}
					break;
				}

				case ITEM_PARSE_CONTAINERSIZE: {
					it.maxItems = pugi::cast<uint16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_FLUIDSOURCE: {
					tmpStrValue = asLowerCaseString(valueAttribute.as_string());
					auto it2 = FluidTypesMap.find(tmpStrValue);
					if (it2 != FluidTypesMap.end()) {
						it.fluidSource = it2->second;
					} else {
						std::cout << ""[Warning - Items::parseItemNode] Unknown fluidSource: "" << valueAttribute.as_string() << std::endl;
					}
					break;
				}

				case ITEM_PARSE_READABLE: {
					it.canReadText = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_WRITEABLE: {
					it.canWriteText = valueAttribute.as_bool();
					it.canReadText = it.canWriteText;
					break;
				}

				case ITEM_PARSE_MAXTEXTLEN: {
					it.maxTextLen = pugi::cast<uint16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_WRITEONCEITEMID: {
					it.writeOnceItemId = pugi::cast<uint16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_WEAPONTYPE: {
					tmpStrValue = asLowerCaseString(valueAttribute.as_string());
					auto it2 = WeaponTypesMap.find(tmpStrValue);
					if (it2 != WeaponTypesMap.end()) {
						it.weaponType = it2->second;
					} else {
						std::cout << ""[Warning - Items::parseItemNode] Unknown weaponType: "" << valueAttribute.as_string() << std::endl;
					}
					break;
				}

				case ITEM_PARSE_SLOTTYPE: {
					tmpStrValue = asLowerCaseString(valueAttribute.as_string());
					if (tmpStrValue == ""head"") {
						it.slotPosition |= SLOTP_HEAD;
					} else if (tmpStrValue == ""body"") {
						it.slotPosition |= SLOTP_ARMOR;
					} else if (tmpStrValue == ""legs"") {
						it.slotPosition |= SLOTP_LEGS;
					} else if (tmpStrValue == ""feet"") {
						it.slotPosition |= SLOTP_FEET;
					} else if (tmpStrValue == ""backpack"") {
						it.slotPosition |= SLOTP_BACKPACK;
					} else if (tmpStrValue == ""two-handed"") {
						it.slotPosition |= SLOTP_TWO_HAND;
					} else if (tmpStrValue == ""right-hand"") {
						it.slotPosition &= ~SLOTP_LEFT;
					} else if (tmpStrValue == ""left-hand"") {
						it.slotPosition &= ~SLOTP_RIGHT;
					} else if (tmpStrValue == ""necklace"") {
						it.slotPosition |= SLOTP_NECKLACE;
					} else if (tmpStrValue == ""ring"") {
						it.slotPosition |= SLOTP_RING;
					} else if (tmpStrValue == ""ammo"") {
						it.slotPosition |= SLOTP_AMMO;
					} else if (tmpStrValue == ""hand"") {
						it.slotPosition |= SLOTP_HAND;
					} else {
						std::cout << ""[Warning - Items::parseItemNode] Unknown slotType: "" << valueAttribute.as_string() << std::endl;
					}
					break;
				}

				case ITEM_PARSE_AMMOTYPE: {
					it.ammoType = getAmmoType(asLowerCaseString(valueAttribute.as_string()));
					if (it.ammoType == AMMO_NONE) {
						std::cout << ""[Warning - Items::parseItemNode] Unknown ammoType: "" << valueAttribute.as_string() << std::endl;
					}
					break;
				}

				case ITEM_PARSE_SHOOTTYPE: {
					ShootType_t shoot = getShootType(asLowerCaseString(valueAttribute.as_string()));
					if (shoot != CONST_ANI_NONE) {
						it.shootType = shoot;
					} else {
						std::cout << ""[Warning - Items::parseItemNode] Unknown shootType: "" << valueAttribute.as_string() << std::endl;
					}
					break;
				}

				case ITEM_PARSE_EFFECT: {
					MagicEffectClasses effect = getMagicEffect(asLowerCaseString(valueAttribute.as_string()));
					if (effect != CONST_ME_NONE) {
						it.magicEffect = effect;
					} else {
						std::cout << ""[Warning - Items::parseItemNode] Unknown effect: "" << valueAttribute.as_string() << std::endl;
					}
					break;
				}

				case ITEM_PARSE_RANGE: {
					it.shootRange = pugi::cast<uint16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_STOPDURATION: {
					it.stopTime = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_DECAYTO: {
					it.decayTo = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_TRANSFORMEQUIPTO: {
					it.transformEquipTo = pugi::cast<uint16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_TRANSFORMDEEQUIPTO: {
					it.transformDeEquipTo = pugi::cast<uint16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_DURATION: {
					it.decayTime = pugi::cast<uint32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_SHOWDURATION: {
					it.showDuration = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_CHARGES: {
					it.charges = pugi::cast<uint32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_SHOWCHARGES: {
					it.showCharges = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_SHOWATTRIBUTES: {
					it.showAttributes = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_HITCHANCE: {
					it.hitChance = std::min<int8_t>(100, std::max<int8_t>(-100, pugi::cast<int16_t>(valueAttribute.value())));
					break;
				}

				case ITEM_PARSE_MAXHITCHANCE: {
					it.maxHitChance = std::min<uint32_t>(100, pugi::cast<uint32_t>(valueAttribute.value()));
					break;
				}

				case ITEM_PARSE_INVISIBLE: {
					abilities.invisible = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_SPEED: {
					abilities.speed = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_HEALTHGAIN: {
					abilities.regeneration = true;
					abilities.healthGain = pugi::cast<uint32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_HEALTHTICKS: {
					abilities.regeneration = true;
					abilities.healthTicks = pugi::cast<uint32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_MANAGAIN: {
					abilities.regeneration = true;
					abilities.manaGain = pugi::cast<uint32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_MANATICKS: {
					abilities.regeneration = true;
					abilities.manaTicks = pugi::cast<uint32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_MANASHIELD: {
					abilities.manaShield = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_SKILLSWORD: {
					abilities.skills[SKILL_SWORD] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_SKILLAXE: {
					abilities.skills[SKILL_AXE] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_SKILLCLUB: {
					abilities.skills[SKILL_CLUB] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_SKILLDIST: {
					abilities.skills[SKILL_DISTANCE] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_SKILLFISH: {
					abilities.skills[SKILL_FISHING] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_SKILLSHIELD: {
					abilities.skills[SKILL_SHIELD] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_SKILLFIST: {
					abilities.skills[SKILL_FIST] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_CRITICALHITAMOUNT: {
					abilities.specialSkills[SPECIALSKILL_CRITICALHITAMOUNT] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_CRITICALHITCHANCE: {
					abilities.specialSkills[SPECIALSKILL_CRITICALHITCHANCE] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_MANALEECHAMOUNT: {
					abilities.specialSkills[SPECIALSKILL_MANALEECHAMOUNT] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_MANALEECHCHANCE: {
					abilities.specialSkills[SPECIALSKILL_MANALEECHCHANCE] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_LIFELEECHAMOUNT: {
					abilities.specialSkills[SPECIALSKILL_LIFELEECHAMOUNT] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_LIFELEECHCHANCE: {
					abilities.specialSkills[SPECIALSKILL_LIFELEECHCHANCE] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_MAXHITPOINTS: {
					abilities.stats[STAT_MAXHITPOINTS] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_MAXHITPOINTSPERCENT: {
					abilities.statsPercent[STAT_MAXHITPOINTS] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_MAXMANAPOINTS: {
					abilities.stats[STAT_MAXMANAPOINTS] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_MAXMANAPOINTSPERCENT: {
					abilities.statsPercent[STAT_MAXMANAPOINTS] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_MAGICPOINTS: {
					abilities.stats[STAT_MAGICPOINTS] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_MAGICPOINTSPERCENT: {
					abilities.statsPercent[STAT_MAGICPOINTS] = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_FIELDABSORBPERCENTENERGY: {
					abilities.fieldAbsorbPercent[combatTypeToIndex(COMBAT_ENERGYDAMAGE)] += pugi::cast<int16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_FIELDABSORBPERCENTFIRE: {
					abilities.fieldAbsorbPercent[combatTypeToIndex(COMBAT_FIREDAMAGE)] += pugi::cast<int16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_FIELDABSORBPERCENTPOISON: {
					abilities.fieldAbsorbPercent[combatTypeToIndex(COMBAT_EARTHDAMAGE)] += pugi::cast<int16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ABSORBPERCENTALL: {
					int16_t value = pugi::cast<int16_t>(valueAttribute.value());
					for (auto& i : abilities.absorbPercent) {
						i += value;
					}
					break;
				}

				case ITEM_PARSE_ABSORBPERCENTELEMENTS: {
					int16_t value = pugi::cast<int16_t>(valueAttribute.value());
					abilities.absorbPercent[combatTypeToIndex(COMBAT_ENERGYDAMAGE)] += value;
					abilities.absorbPercent[combatTypeToIndex(COMBAT_FIREDAMAGE)] += value;
					abilities.absorbPercent[combatTypeToIndex(COMBAT_EARTHDAMAGE)] += value;
					abilities.absorbPercent[combatTypeToIndex(COMBAT_ICEDAMAGE)] += value;
					break;
				}

				case ITEM_PARSE_ABSORBPERCENTMAGIC: {
					int16_t value = pugi::cast<int16_t>(valueAttribute.value());
					abilities.absorbPercent[combatTypeToIndex(COMBAT_ENERGYDAMAGE)] += value;
					abilities.absorbPercent[combatTypeToIndex(COMBAT_FIREDAMAGE)] += value;
					abilities.absorbPercent[combatTypeToIndex(COMBAT_EARTHDAMAGE)] += value;
					abilities.absorbPercent[combatTypeToIndex(COMBAT_ICEDAMAGE)] += value;
					abilities.absorbPercent[combatTypeToIndex(COMBAT_HOLYDAMAGE)] += value;
					abilities.absorbPercent[combatTypeToIndex(COMBAT_DEATHDAMAGE)] += value;
					break;
				}

				case ITEM_PARSE_ABSORBPERCENTENERGY: {
					abilities.absorbPercent[combatTypeToIndex(COMBAT_ENERGYDAMAGE)] += pugi::cast<int16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ABSORBPERCENTFIRE: {
					abilities.absorbPercent[combatTypeToIndex(COMBAT_FIREDAMAGE)] += pugi::cast<int16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ABSORBPERCENTPOISON: {
					abilities.absorbPercent[combatTypeToIndex(COMBAT_EARTHDAMAGE)] += pugi::cast<int16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ABSORBPERCENTICE: {
					abilities.absorbPercent[combatTypeToIndex(COMBAT_ICEDAMAGE)] += pugi::cast<int16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ABSORBPERCENTHOLY: {
					abilities.absorbPercent[combatTypeToIndex(COMBAT_HOLYDAMAGE)] += pugi::cast<int16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ABSORBPERCENTDEATH: {
					abilities.absorbPercent[combatTypeToIndex(COMBAT_DEATHDAMAGE)] += pugi::cast<int16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ABSORBPERCENTLIFEDRAIN: {
					abilities.absorbPercent[combatTypeToIndex(COMBAT_LIFEDRAIN)] += pugi::cast<int16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ABSORBPERCENTMANADRAIN: {
					abilities.absorbPercent[combatTypeToIndex(COMBAT_MANADRAIN)] += pugi::cast<int16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ABSORBPERCENTDROWN: {
					abilities.absorbPercent[combatTypeToIndex(COMBAT_DROWNDAMAGE)] += pugi::cast<int16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ABSORBPERCENTPHYSICAL: {
					abilities.absorbPercent[combatTypeToIndex(COMBAT_PHYSICALDAMAGE)] += pugi::cast<int16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ABSORBPERCENTHEALING: {
					abilities.absorbPercent[combatTypeToIndex(COMBAT_HEALING)] += pugi::cast<int16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ABSORBPERCENTUNDEFINED: {
					abilities.absorbPercent[combatTypeToIndex(COMBAT_UNDEFINEDDAMAGE)] += pugi::cast<int16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_SUPPRESSDRUNK: {
					if (valueAttribute.as_bool()) {
						abilities.conditionSuppressions |= CONDITION_DRUNK;
					}
					break;
				}

				case ITEM_PARSE_SUPPRESSENERGY: {
					if (valueAttribute.as_bool()) {
						abilities.conditionSuppressions |= CONDITION_ENERGY;
					}
					break;
				}

				case ITEM_PARSE_SUPPRESSFIRE: {
					if (valueAttribute.as_bool()) {
						abilities.conditionSuppressions |= CONDITION_FIRE;
					}
					break;
				}

				case ITEM_PARSE_SUPPRESSPOISON: {
					if (valueAttribute.as_bool()) {
						abilities.conditionSuppressions |= CONDITION_POISON;
					}
					break;
				}

				case ITEM_PARSE_SUPPRESSDROWN: {
					if (valueAttribute.as_bool()) {
						abilities.conditionSuppressions |= CONDITION_DROWN;
					}
					break;
				}

				case ITEM_PARSE_SUPPRESSPHYSICAL: {
					if (valueAttribute.as_bool()) {
						abilities.conditionSuppressions |= CONDITION_BLEEDING;
					}
					break;
				}

				case ITEM_PARSE_SUPPRESSFREEZE: {
					if (valueAttribute.as_bool()) {
						abilities.conditionSuppressions |= CONDITION_FREEZING;
					}
					break;
				}

				case ITEM_PARSE_SUPPRESSDAZZLE: {
					if (valueAttribute.as_bool()) {
						abilities.conditionSuppressions |= CONDITION_DAZZLED;
					}
					break;
				}

				case ITEM_PARSE_SUPPRESSCURSE: {
					if (valueAttribute.as_bool()) {
						abilities.conditionSuppressions |= CONDITION_CURSED;
					}
					break;
				}

				case ITEM_PARSE_FIELD: {
					it.group = ITEM_GROUP_MAGICFIELD;
					it.type = ITEM_TYPE_MAGICFIELD;

					CombatType_t combatType = COMBAT_NONE;
					ConditionDamage* conditionDamage = nullptr;

					tmpStrValue = asLowerCaseString(valueAttribute.as_string());
					if (tmpStrValue == ""fire"") {
						conditionDamage = new ConditionDamage(CONDITIONID_COMBAT, CONDITION_FIRE);
						combatType = COMBAT_FIREDAMAGE;
					} else if (tmpStrValue == ""energy"") {
						conditionDamage = new ConditionDamage(CONDITIONID_COMBAT, CONDITION_ENERGY);
						combatType = COMBAT_ENERGYDAMAGE;
					} else if (tmpStrValue == ""poison"") {
						conditionDamage = new ConditionDamage(CONDITIONID_COMBAT, CONDITION_POISON);
						combatType = COMBAT_EARTHDAMAGE;
					} else if (tmpStrValue == ""drown"") {
						conditionDamage = new ConditionDamage(CONDITIONID_COMBAT, CONDITION_DROWN);
						combatType = COMBAT_DROWNDAMAGE;
					} else if (tmpStrValue == ""physical"") {
						conditionDamage = new ConditionDamage(CONDITIONID_COMBAT, CONDITION_BLEEDING);
						combatType = COMBAT_PHYSICALDAMAGE;
					} else {
						std::cout << ""[Warning - Items::parseItemNode] Unknown field value: "" << valueAttribute.as_string() << std::endl;
					}

					if (combatType != COMBAT_NONE) {
						it.combatType = combatType;
						it.conditionDamage.reset(conditionDamage);

						uint32_t ticks = 0;
						int32_t start = 0;
						int32_t count = 1;
						int32_t initDamage = -1;
						int32_t damage = 0;
						for (auto subAttributeNode : attributeNode.children()) {
							pugi::xml_attribute subKeyAttribute = subAttributeNode.attribute(""key"");
							if (!subKeyAttribute) {
								continue;
							}

							pugi::xml_attribute subValueAttribute = subAttributeNode.attribute(""value"");
							if (!subValueAttribute) {
								continue;
							}

							tmpStrValue = asLowerCaseString(subKeyAttribute.as_string());
							if (tmpStrValue == ""initdamage"") {
								initDamage = pugi::cast<int32_t>(subValueAttribute.value());
							} else if (tmpStrValue == ""ticks"") {
								ticks = pugi::cast<uint32_t>(subValueAttribute.value());
							} else if (tmpStrValue == ""count"") {
								count = std::max<int32_t>(1, pugi::cast<int32_t>(subValueAttribute.value()));
							} else if (tmpStrValue == ""start"") {
								start = std::max<int32_t>(0, pugi::cast<int32_t>(subValueAttribute.value()));
							} else if (tmpStrValue == ""damage"") {
								damage = -pugi::cast<int32_t>(subValueAttribute.value());
								if (start > 0) {
									std::list<int32_t> damageList;
									ConditionDamage::generateDamageList(damage, start, damageList);
									for (int32_t damageValue : damageList) {
										conditionDamage->addDamage(1, ticks, -damageValue);
									}

									start = 0;
								} else {
									conditionDamage->addDamage(count, ticks, damage);
								}
							}
						}

						// datapack compatibility, presume damage to be initialdamage if initialdamage is not declared.
						// initDamage = 0 (don't override initDamage with damage, don't set any initDamage)
						// initDamage = -1 (undefined, override initDamage with damage)
						if (initDamage > 0 || initDamage < -1) {
							conditionDamage->setInitDamage(-initDamage);
						} else if (initDamage == -1 && damage != 0) {
							conditionDamage->setInitDamage(damage);
						}

						conditionDamage->setParam(CONDITION_PARAM_FIELD, 1);

						if (conditionDamage->getTotalDamage() > 0) {
							conditionDamage->setParam(CONDITION_PARAM_FORCEUPDATE, 1);
						}
					}
					break;
				}

				case ITEM_PARSE_REPLACEABLE: {
					it.replaceable = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_PARTNERDIRECTION: {
					it.bedPartnerDir = getDirection(valueAttribute.as_string());
					break;
				}

				case ITEM_PARSE_LEVELDOOR: {
					it.levelDoor = pugi::cast<uint32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_MALETRANSFORMTO: {
					uint16_t value = pugi::cast<uint16_t>(valueAttribute.value());
					it.transformToOnUse[PLAYERSEX_MALE] = value;
					ItemType& other = getItemType(value);
					if (other.transformToFree == 0) {
						other.transformToFree = it.id;
					}

					if (it.transformToOnUse[PLAYERSEX_FEMALE] == 0) {
						it.transformToOnUse[PLAYERSEX_FEMALE] = value;
					}
					break;
				}

				case ITEM_PARSE_FEMALETRANSFORMTO: {
					uint16_t value = pugi::cast<uint16_t>(valueAttribute.value());
					it.transformToOnUse[PLAYERSEX_FEMALE] = value;

					ItemType& other = getItemType(value);
					if (other.transformToFree == 0) {
						other.transformToFree = it.id;
					}

					if (it.transformToOnUse[PLAYERSEX_MALE] == 0) {
						it.transformToOnUse[PLAYERSEX_MALE] = value;
					}
					break;
				}

				case ITEM_PARSE_TRANSFORMTO: {
					it.transformToFree = pugi::cast<uint16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_DESTROYTO: {
					it.destroyTo = pugi::cast<uint16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ELEMENTICE: {
					abilities.elementDamage = pugi::cast<uint16_t>(valueAttribute.value());
					abilities.elementType = COMBAT_ICEDAMAGE;
					break;
				}

				case ITEM_PARSE_ELEMENTEARTH: {
					abilities.elementDamage = pugi::cast<uint16_t>(valueAttribute.value());
					abilities.elementType = COMBAT_EARTHDAMAGE;
					break;
				}

				case ITEM_PARSE_ELEMENTFIRE: {
					abilities.elementDamage = pugi::cast<uint16_t>(valueAttribute.value());
					abilities.elementType = COMBAT_FIREDAMAGE;
					break;
				}

				case ITEM_PARSE_ELEMENTENERGY: {
					abilities.elementDamage = pugi::cast<uint16_t>(valueAttribute.value());
					abilities.elementType = COMBAT_ENERGYDAMAGE;
					break;
				}

				case ITEM_PARSE_ELEMENTDEATH: {
					abilities.elementDamage = pugi::cast<uint16_t>(valueAttribute.value());
					abilities.elementType = COMBAT_DEATHDAMAGE;
					break;
				}

				case ITEM_PARSE_ELEMENTHOLY: {
					abilities.elementDamage = pugi::cast<uint16_t>(valueAttribute.value());
					abilities.elementType = COMBAT_HOLYDAMAGE;
					break;
				}

				case ITEM_PARSE_WALKSTACK: {
					it.walkStack = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_BLOCKING: {
					it.blockSolid = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_ALLOWDISTREAD: {
					it.allowDistRead = booleanString(valueAttribute.as_string());
					break;
				}

				case ITEM_PARSE_STOREITEM: {
					it.storeItem = booleanString(valueAttribute.as_string());
					break;
				}

				default: {
					// It should not ever get to here, only if you add a new key to the map and don't configure a case for it.
					std::cout << ""[Warning - Items::parseItemNode] Not configured key value: "" << keyAttribute.as_string() << std::endl;
					break;
				}
			}
		} else {
			std::cout << ""[Warning - Items::parseItemNode] Unknown key value: "" << keyAttribute.as_string() << std::endl;
		}
	}

	//check bed items
	if ((it.transformToFree != 0 || it.transformToOnUse[PLAYERSEX_FEMALE] != 0 || it.transformToOnUse[PLAYERSEX_MALE] != 0) && it.type != ITEM_TYPE_BED) {
		std::cout << ""[Warning - Items::parseItemNode] Item "" << it.id << "" is not set as a bed-type"" << std::endl;
	}
}

ItemType& Items::getItemType(size_t id)
{
	if (id < items.size()) {
		return items[id];
	}
	return items.front();
}

const ItemType& Items::getItemType(size_t id) const
{
	if (id < items.size()) {
		return items[id];
	}
	return items.front();
}

const ItemType& Items::getItemIdByClientId(uint16_t spriteId) const
{
	if (spriteId >= 100) {
		if (uint16_t serverId = clientIdToServerIdMap.getServerId(spriteId)) {
			return getItemType(serverId);
		}
	}
	return items.front();
}

uint16_t Items::getItemIdByName(const std::string& name)
{
	auto result = nameToItems.find(asLowerCaseString(name));

	if (result == nameToItems.end())
		return 0;

	return result->second;
}
","I think the temporary `result` is not necessary `if (nameToItems.find(lowerCaseName) == nameToItems.end()) {` It is the only thing that I would change, the rest seems correct to me","The suggested code change appears to be a comprehensive implementation of the `Items` class in the Forgotten Server game engine, which handles various aspects of item management. Here's a concise review of the changes:

### Key Points:

1. **Data Structures**:
   - **ItemParseAttributesMap**: Maps attribute names to their corresponding parsing types.
   - **ItemTypesMap**: Maps item type names to their corresponding types.
   - **TileStatesMap**: Maps floor change states to their corresponding bit flags.
   - **RaceTypesMap**: Maps race types to their corresponding values.
   - **WeaponTypesMap**: Maps weapon types to their corresponding values.
   - **FluidTypesMap**: Maps fluid types to their corresponding values.

2. **Class Initialization**:
   - The `Items` class is initialized with two maps: `clientIdToServerIdMap` and `nameToItems`.

3. **Clearing and Reloading**:
   - The `clear` method clears all item data.
   - The `reload` method clears existing data and reloads items from both OTB and XML files.

4. **Loading from OTB**:
   - The `loadFromOtb` method reads item data from an OTB file and populates the `items` vector.
   - It handles various attributes such as server ID, client ID, speed, light level, and more.

5. **Loading from XML**:
   - The `loadFromXml` method reads item data from an XML file and populates the `items` vector.
   - It handles various attributes such as name, description, and special effects.

6. **Building Inventory List**:
   - The `buildInventoryList` method creates a list of item IDs that are considered inventory items based on certain criteria.

7. **Parsing Item Nodes**:
   - The `parseItemNode` method processes each item node in the XML file, setting various properties of the `ItemType` struct.
   - It handles various attributes such as type, description, and special effects.

8. **Utility Functions**:
   - `getItemType` and `getItemIdByName` provide methods to retrieve item types by ID or name.

### Potential Improvements:

1. **Error Handling**:
   - Add more detailed error handling for cases where attributes are missing or invalid.

2. **Performance Optimization**:
   - Consider using more efficient data structures or algorithms for certain operations, especially when dealing with large datasets.

3. **Code Readability**:
   -"
77,"@@ -1,3 +1,5 @@
+require 'socket'
+
 module RSpec
   module Core
     class Runner","require 'socket'

module RSpec
  module Core
    class Runner

      # Register an at_exit hook that runs the suite.
      def self.autorun
        return if autorun_disabled? || installed_at_exit? || running_in_drb?
        at_exit do
          # Don't bother running any specs and just let the program terminate
          # if we got here due to an unrescued exception (anything other than
          # SystemExit, which is raised when somebody calls Kernel#exit).
          next unless $!.nil? || $!.kind_of?(SystemExit)

          # We got here because either the end of the program was reached or
          # somebody called Kernel#exit.  Run the specs and then override any
          # existing exit status with RSpec's exit status if any specs failed.
          status = run(ARGV, $stderr, $stdout).to_i
          exit status if status != 0
        end
        @installed_at_exit = true
      end
      AT_EXIT_HOOK_BACKTRACE_LINE = ""#{__FILE__}:#{__LINE__ - 2}:in `autorun'""

      def self.disable_autorun!
        @autorun_disabled = true
      end

      def self.autorun_disabled?
        @autorun_disabled ||= false
      end

      def self.installed_at_exit?
        @installed_at_exit ||= false
      end

      def self.running_in_drb?
        defined?(DRb) &&
        (DRb.current_server rescue false) &&
         DRb.current_server.uri =~ /druby\:\/\/127.0.0.1\:/
      end

      def self.trap_interrupt
        trap('INT') do
          exit!(1) if RSpec.wants_to_quit
          RSpec.wants_to_quit = true
          STDERR.puts ""\nExiting... Interrupt again to exit immediately.""
        end
      end

      # Run a suite of RSpec examples.
      #
      # This is used internally by RSpec to run a suite, but is available
      # for use by any other automation tool.
      #
      # If you want to run this multiple times in the same process, and you
      # want files like spec_helper.rb to be reloaded, be sure to load `load`
      # instead of `require`.
      #
      # #### Parameters
      # * +args+ - an array of command-line-supported arguments
      # * +err+ - error stream (Default: $stderr)
      # * +out+ - output stream (Default: $stdout)
      #
      # #### Returns
      # * +Fixnum+ - exit status code (0/1)
      def self.run(args, err=$stderr, out=$stdout)
        trap_interrupt
        options = ConfigurationOptions.new(args)
        options.parse_options

        if options.options[:drb]
          require 'rspec/core/drb_command_line'
          begin
            DRbCommandLine.new(options).run(err, out)
          rescue DRb::DRbConnError
            err.puts ""No DRb server is running. Running in local process instead ...""
            CommandLine.new(options).run(err, out)
          end
        else
          CommandLine.new(options).run(err, out)
        end
      ensure
        RSpec.reset
      end
    end
  end
end
","Given that the majority of uses won't need this, it'd be good to only require it when we actually need it.","The suggested code change appears to be a modification to the RSpec core runner to include an at_exit hook that automatically runs the suite when the program exits. The changes include adding a new method `autorun` that registers an at_exit hook that runs the suite if certain conditions are met. The changes also include a new method `disable_autorun!` that disables autorun, and a new method `autorun_disabled?` that checks if autorun is disabled. The changes also include a new method `installed_at_exit?` that checks if the at_exit hook has been installed, and a new method `running_in_drb?` that checks if the program is running in a DRb environment. The changes also include a new method `trap_interrupt` that traps the INT signal and exits the program if RSpec wants to quit. The changes also include a new method `run` that runs a suite of RSpec examples, and a new method `ensure` that resets RSpec after the suite has been run. Overall, the changes appear to be intended to improve the robustness and reliability of the RSpec core runner."
78,"@@ -383,5 +383,5 @@ func newMetrics() metrics {
 }
 
 func (db *DB) Metrics() []prometheus.Collector {
-	return m.PrometheusCollectorsFromFields(db.metrics)
+	return append(m.PrometheusCollectorsFromFields(db.metrics), db.sharky.Metrics()...)
 }","// Copyright 2020 The Swarm Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package localstore

import (
	m ""github.com/ethersphere/bee/pkg/metrics""
	""github.com/prometheus/client_golang/prometheus""
)

type metrics struct {
	TotalTimeGCLock                 prometheus.Counter
	TotalTimeGCFirstItem            prometheus.Counter
	TotalTimeCollectGarbage         prometheus.Counter
	TotalTimeGCExclude              prometheus.Counter
	TotalTimeGet                    prometheus.Counter
	TotalTimeUpdateGC               prometheus.Counter
	TotalTimeGetMulti               prometheus.Counter
	TotalTimeHas                    prometheus.Counter
	TotalTimeHasMulti               prometheus.Counter
	TotalTimePut                    prometheus.Counter
	TotalTimeSet                    prometheus.Counter
	TotalTimeSubscribePullIteration prometheus.Counter
	TotalTimeSubscribePushIteration prometheus.Counter

	GCCounter                prometheus.Counter
	GCErrorCounter           prometheus.Counter
	GCCollectedCounter       prometheus.Counter
	GCCommittedCounter       prometheus.Counter
	GCExcludeCounter         prometheus.Counter
	GCExcludeError           prometheus.Counter
	GCExcludeWriteBatchError prometheus.Counter
	GCUpdate                 prometheus.Counter
	GCUpdateError            prometheus.Counter

	ModeGet                       prometheus.Counter
	ModeGetFailure                prometheus.Counter
	ModeGetMulti                  prometheus.Counter
	ModeGetMultiChunks            prometheus.Counter
	ModeGetMultiFailure           prometheus.Counter
	ModePut                       prometheus.Counter
	ModePutFailure                prometheus.Counter
	ModeSet                       prometheus.Counter
	ModeSetFailure                prometheus.Counter
	ModeHas                       prometheus.Counter
	ModeHasFailure                prometheus.Counter
	ModeHasMulti                  prometheus.Counter
	ModeHasMultiFailure           prometheus.Counter
	SubscribePull                 prometheus.Counter
	SubscribePullStop             prometheus.Counter
	SubscribePullIteration        prometheus.Counter
	SubscribePullIterationFailure prometheus.Counter
	LastPullSubscriptionBinID     prometheus.Counter
	SubscribePush                 prometheus.Counter
	SubscribePushIteration        prometheus.Counter
	SubscribePushIterationDone    prometheus.Counter
	SubscribePushIterationFailure prometheus.Counter

	GCSize                  prometheus.Gauge
	GCStoreTimeStamps       prometheus.Gauge
	GCStoreAccessTimeStamps prometheus.Gauge

	ReserveSize              prometheus.Gauge
	EvictReserveCounter      prometheus.Counter
	EvictReserveErrorCounter prometheus.Counter
	TotalTimeEvictReserve    prometheus.Counter
}

func newMetrics() metrics {
	subsystem := ""localstore""

	return metrics{
		TotalTimeGCLock: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_lock_time"",
			Help:      ""Total time under lock in gc."",
		}),
		TotalTimeGCFirstItem: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_first_item_time"",
			Help:      ""Total time taken till first item in gc comes out of gcIndex iterator."",
		}),
		TotalTimeCollectGarbage: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_time"",
			Help:      ""Total time taken to collect garbage."",
		}),
		TotalTimeGCExclude: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_exclude_index_time"",
			Help:      ""Total time taken to exclude gc index."",
		}),
		TotalTimeGet: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""get_chunk_time"",
			Help:      ""Total time taken to get chunk from DB."",
		}),
		TotalTimeUpdateGC: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""in_gc_time"",
			Help:      ""Total time taken to in gc."",
		}),
		TotalTimeGetMulti: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""get_multi_time"",
			Help:      ""Total time taken to get multiple chunks from DB."",
		}),
		TotalTimeHas: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""has_time"",
			Help:      ""Total time taken to check if the key is present in DB."",
		}),
		TotalTimeHasMulti: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""has_multi_time"",
			Help:      ""Total time taken to check if multiple keys are present in DB."",
		}),
		TotalTimePut: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""put_time"",
			Help:      ""Total time taken to put a chunk in DB."",
		}),
		TotalTimeSet: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""set_time"",
			Help:      ""Total time taken to set chunk in DB."",
		}),
		TotalTimeSubscribePullIteration: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_pull_iteration_time"",
			Help:      ""Total time taken to subsctibe for pull iteration."",
		}),
		TotalTimeSubscribePushIteration: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_push_iteration_time"",
			Help:      ""Total time taken to subscribe for push iteration."",
		}),
		GCCounter: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_count"",
			Help:      ""Number of times the GC operation is done."",
		}),
		GCErrorCounter: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_fail_count"",
			Help:      ""Number of times the GC operation failed."",
		}),
		GCCollectedCounter: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_collected_count"",
			Help:      ""Number of times the GC_COLLECTED operation is done."",
		}),
		GCCommittedCounter: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_committed_count"",
			Help:      ""Number of gc items to commit."",
		}),
		GCExcludeCounter: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_exclude_count"",
			Help:      ""Number of times the GC_EXCLUDE operation is done."",
		}),
		GCExcludeError: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_exclude_fail_count"",
			Help:      ""Number of times the GC_EXCLUDE operation failed."",
		}),
		GCExcludeWriteBatchError: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""ex_exclude_write_batch_fail_count"",
			Help:      ""Number of times the GC_EXCLUDE_WRITE_BATCH operation is failed."",
		}),
		GCUpdate: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_update_count"",
			Help:      ""Number of times the gc is updated."",
		}),
		GCUpdateError: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_update_error_count"",
			Help:      ""Number of times the gc update had error."",
		}),

		ModeGet: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_get_count"",
			Help:      ""Number of times MODE_GET is invoked."",
		}),
		ModeGetFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_get_failure_count"",
			Help:      ""Number of times MODE_GET invocation failed."",
		}),
		ModeGetMulti: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_get_multi_count"",
			Help:      ""Number of times MODE_MULTI_GET is invoked."",
		}),
		ModeGetMultiChunks: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_get_multi_chunks_count"",
			Help:      ""Number of chunks requested through MODE_MULTI_GET."",
		}),
		ModeGetMultiFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_get_multi_failure_count"",
			Help:      ""Number of times MODE_GET invocation failed."",
		}),
		ModePut: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_put_count"",
			Help:      ""Number of times MODE_PUT is invoked."",
		}),
		ModePutFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_put_failure_count"",
			Help:      ""Number of times MODE_PUT invocation failed."",
		}),
		ModeSet: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_set_count"",
			Help:      ""Number of times MODE_SET is invoked."",
		}),
		ModeSetFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_set_failure_count"",
			Help:      ""Number of times MODE_SET invocation failed."",
		}),
		ModeHas: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_has_count"",
			Help:      ""Number of times MODE_HAS is invoked."",
		}),
		ModeHasFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_has_failure_count"",
			Help:      ""Number of times MODE_HAS invocation failed."",
		}),
		ModeHasMulti: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_has_multi_count"",
			Help:      ""Number of times MODE_HAS_MULTI is invoked."",
		}),
		ModeHasMultiFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_has_multi_failure_count"",
			Help:      ""Number of times MODE_HAS_MULTI invocation failed."",
		}),
		SubscribePull: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_pull_count"",
			Help:      ""Number of times Subscribe_pULL is invoked."",
		}),
		SubscribePullStop: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_pull_stop_count"",
			Help:      ""Number of times Subscribe_pull_stop is invoked."",
		}),
		SubscribePullIteration: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_pull_iteration_count"",
			Help:      ""Number of times Subscribe_pull_iteration is invoked."",
		}),
		SubscribePullIterationFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_pull_iteration_fail_count"",
			Help:      ""Number of times Subscribe_pull_iteration_fail is invoked."",
		}),
		LastPullSubscriptionBinID: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""last_pull_subscription_bin_id_count"",
			Help:      ""Number of times LastPullSubscriptionBinID is invoked."",
		}),
		SubscribePush: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_push_count"",
			Help:      ""Number of times SUBSCRIBE_PUSH is invoked."",
		}),
		SubscribePushIteration: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_push_iteration_count"",
			Help:      ""Number of times SUBSCRIBE_PUSH_ITERATION is invoked."",
		}),
		SubscribePushIterationDone: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_push_iteration_done_count"",
			Help:      ""Number of times SUBSCRIBE_PUSH_ITERATION_DONE is invoked."",
		}),
		SubscribePushIterationFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_push_iteration_failure_count"",
			Help:      ""Number of times SUBSCRIBE_PUSH_ITERATION_FAILURE is invoked."",
		}),

		GCSize: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_size"",
			Help:      ""Number of elements in Garbage collection index."",
		}),
		GCStoreTimeStamps: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_time_stamp"",
			Help:      ""Storage timestamp in Garbage collection iteration."",
		}),
		GCStoreAccessTimeStamps: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_access_time_stamp"",
			Help:      ""Access timestamp in Garbage collection iteration."",
		}),
		ReserveSize: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""reserve_size"",
			Help:      ""Number of elements in reserve."",
		}),
		EvictReserveCounter: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""evict_reserve_count"",
			Help:      ""number of times the evict reserve worker was invoked"",
		}),
		EvictReserveErrorCounter: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""evict_reserve_err_count"",
			Help:      ""number of times evict reserve got an error"",
		}),
		TotalTimeEvictReserve: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""evict_reserve_total_time"",
			Help:      ""total time spent evicting from reserve"",
		}),
	}
	return append(m.PrometheusCollectorsFromFields(db.metrics), db.sharky.Metrics()...)
}

func (db *DB) Metrics() []prometheus.Collector {
	return m.PrometheusCollectorsFromFields(db.metrics)
}
",i think `shed` has metrics too... maybe now would be the time to expose them,The suggested code change introduces a `metrics` struct that encapsulates various Prometheus counters and gauges related to the local store operations. It also includes methods to create and return these metrics. The code is well-structured and follows best practices for Prometheus metric registration and collection.
79,"@@ -37,10 +37,13 @@ module Bolt
         super
         require 'winrm'
         require 'winrm-fs'
+
+        @transport_logger = Logging.logger[::WinRM]
+        @transport_logger.level = :warn
       end
 
       def with_connection(target)
-        conn = Connection.new(target)
+        conn = Connection.new(target, @transport_logger)
         conn.connect
         yield conn
       ensure","# frozen_string_literal: true

require 'bolt/transport/base'

module Bolt
  module Transport
    class WinRM < Base
      PS_ARGS = %w[
        -NoProfile -NonInteractive -NoLogo -ExecutionPolicy Bypass
      ].freeze

      def self.options
        %w[port user password connect-timeout ssl ssl-verify tmpdir cacert extensions]
      end

      PROVIDED_FEATURES = ['powershell'].freeze

      def self.validate(options)
        ssl_flag = options['ssl']
        unless !!ssl_flag == ssl_flag
          raise Bolt::ValidationError, 'ssl option must be a Boolean true or false'
        end

        ssl_verify_flag = options['ssl-verify']
        unless !!ssl_verify_flag == ssl_verify_flag
          raise Bolt::ValidationError, 'ssl-verify option must be a Boolean true or false'
        end

        timeout_value = options['connect-timeout']
        unless timeout_value.is_a?(Integer) || timeout_value.nil?
          error_msg = ""connect-timeout value must be an Integer, received #{timeout_value}:#{timeout_value.class}""
          raise Bolt::ValidationError, error_msg
        end
      end

      def initialize

        @transport_logger = Logging.logger[::WinRM]
        @transport_logger.level = :warn
        conn = Connection.new(target, @transport_logger)
        super
        require 'winrm'
        require 'winrm-fs'
      end

      def with_connection(target)
        conn = Connection.new(target)
        conn.connect
        yield conn
      ensure
        begin
          conn&.disconnect
        rescue StandardError => ex
          logger.info(""Failed to close connection to #{target.uri} : #{ex.message}"")
        end
      end

      def upload(target, source, destination, _options = {})
        with_connection(target) do |conn|
          conn.write_remote_file(source, destination)
          Bolt::Result.for_upload(target, source, destination)
        end
      end

      def run_command(target, command, _options = {})
        with_connection(target) do |conn|
          output = conn.execute(command)
          Bolt::Result.for_command(target, output.stdout.string, output.stderr.string, output.exit_code)
        end
      end

      def run_script(target, script, arguments, _options = {})
        with_connection(target) do |conn|
          conn.with_remote_file(script) do |remote_path|
            if powershell_file?(remote_path)
              mapped_args = arguments.map do |a|
                ""$invokeArgs.ArgumentList += @'\n#{a}\n'@""
              end.join(""\n"")
              output = conn.execute(<<-PS)
$invokeArgs = @{
  ScriptBlock = (Get-Command ""#{remote_path}"").ScriptBlock
  ArgumentList = @()
}
#{mapped_args}

try
{
  Invoke-Command @invokeArgs
}
catch
{
  Write-Error $_.Exception
  exit 1
}
          PS
            else
              path, args = *process_from_extension(remote_path)
              args += escape_arguments(arguments)
              output = conn.execute_process(path, args)
            end
            Bolt::Result.for_command(target, output.stdout.string, output.stderr.string, output.exit_code)
          end
        end
      end

      def run_task(target, task, arguments, _options = {})
        executable = target.select_impl(task, PROVIDED_FEATURES)
        raise ""No suitable implementation of #{task.name} for #{target.name}"" unless executable

        input_method = task.input_method
        input_method ||= powershell_file?(executable) ? 'powershell' : 'both'
        with_connection(target) do |conn|
          if STDIN_METHODS.include?(input_method)
            stdin = JSON.dump(arguments)
          end

          if ENVIRONMENT_METHODS.include?(input_method)
            arguments.each do |(arg, val)|
              val = val.to_json unless val.is_a?(String)
              cmd = ""[Environment]::SetEnvironmentVariable('PT_#{arg}', @'\n#{val}\n'@)""
              result = conn.execute(cmd)
              if result.exit_code != 0
                raise EnvironmentVarError(var, value)
              end
            end
          end

          conn.with_remote_file(executable) do |remote_path|
            output =
              if powershell_file?(remote_path) && stdin.nil?
                # NOTE: cannot redirect STDIN to a .ps1 script inside of PowerShell
                # must create new powershell.exe process like other interpreters
                # fortunately, using PS with stdin input_method should never happen
                if input_method == 'powershell'
                  conn.execute(<<-PS)
$private:tempArgs = Get-ContentAsJson (
  $utf8.GetString([System.Convert]::FromBase64String('#{Base64.encode64(JSON.dump(arguments))}'))
)
$allowedArgs = (Get-Command ""#{remote_path}"").Parameters.Keys
$private:taskArgs = @{}
$private:tempArgs.Keys | ? { $allowedArgs -contains $_ } | % { $private:taskArgs[$_] = $private:tempArgs[$_] }
try { & ""#{remote_path}"" @taskArgs } catch { Write-Error $_.Exception; exit 1 }
              PS
                else
                  conn.execute(%(try { & ""#{remote_path}"" } catch { Write-Error $_.Exception; exit 1 }))
                end
              else
                path, args = *process_from_extension(remote_path)
                conn.execute_process(path, args, stdin)
              end
            Bolt::Result.for_task(target, output.stdout.string,
                                  output.stderr.string,
                                  output.exit_code)
          end
        end
      end

      def powershell_file?(path)
        Pathname(path).extname.casecmp('.ps1').zero?
      end

      def process_from_extension(path)
        case Pathname(path).extname.downcase
        when '.rb'
          [
            'ruby.exe',
            ['-S', ""\""#{path}\""""]
          ]
        when '.ps1'
          [
            'powershell.exe',
            [*PS_ARGS, '-File', ""\""#{path}\""""]
          ]
        when '.pp'
          [
            'puppet.bat',
            ['apply', ""\""#{path}\""""]
          ]
        else
          # Run the script via cmd, letting Windows extension handling determine how
          [
            'cmd.exe',
            ['/c', ""\""#{path}\""""]
          ]
        end
      end

      def escape_arguments(arguments)
        arguments.map do |arg|
          if arg =~ / /
            ""\""#{arg}\""""
          else
            arg
          end
        end
      end
    end
  end
end

require 'bolt/transport/winrm/connection'
",Do we want to set warn here since we're not trying to silence net::ssh?,"The suggested code change appears to be a complete rewrite of the `Bolt::Transport::WinRM` class, which is used for interacting with Windows Remote Management (WinRM) servers. The changes include:

1. **Module and Class Definition**: The class is defined within the `Bolt::Transport` module and is named `WinRM`.

2. **Constants**: Several constants are defined, such as `PS_ARGS`, `PROVIDED_FEATURES`, and `options`. These constants are used throughout the class to define behavior and options.

3. **Initialization**: The `initialize` method sets up the transport logger and initializes a connection to the target server.

4. **Connection Management**: The `with_connection` method ensures that connections are properly opened and closed, even if an error occurs during execution.

5. **File Upload**: The `upload` method uploads files to the remote server using WinRM.

6. **Command Execution**: The `run_command` method executes commands on the remote server using WinRM.

7. **Script Execution**: The `run_script` method executes scripts on the remote server using WinRM. It handles different file types and argument escaping.

8. **Task Execution**: The `run_task` method executes tasks on the remote server using WinRM. It supports different input methods and handles environment variables.

9. **Utility Methods**: The class includes utility methods like `powershell_file?`, `process_from_extension`, and `escape_arguments` to handle file type detection, argument processing, and escaping.

Overall, the code appears to be well-structured and follows best practices for interacting with remote systems using WinRM in a Bolt context."
80,"@@ -1139,6 +1139,18 @@ hashtable_fragment_reset(dcontext_t *dcontext, fragment_table_t *table)
         if (!dynamo_exited && !dynamo_resetting)
             ASSERT_TABLE_SYNCHRONIZED(table, WRITE);
     });
+#    if !defined(DEBUG) && defined(CLIENT_INTERFACE)
+    if (!dr_fragment_deleted_hook_exists())
+        return;
+    /* i#4226: Avoid the slow deletion code and just invoke the event. */
+    for (i = 0; i < table->capacity; i++) {
+        f = table->table[i];
+        if (!REAL_FRAGMENT(f))
+            continue;
+        instrument_fragment_deleted(dcontext, f->tag, f->flags);
+    }
+    return;
+#    endif
     /* Go in reverse order (for efficiency) since using
      * hashtable_fragment_remove_helper to keep all reachable, which is required
      * for dynamo_resetting where we unlink fragments here and need to be able to","} dead_table_lists_t;

static dead_table_lists_t *dead_lists;

DECLARE_CXTSWPROT_VAR(static mutex_t dead_tables_lock, INIT_LOCK_FREE(dead_tables_lock));

#ifdef RETURN_AFTER_CALL
/* High level lock for an atomic lookup+add operation on the
 * after call tables. */
DECLARE_CXTSWPROT_VAR(static mutex_t after_call_lock, INIT_LOCK_FREE(after_call_lock));
/* We use per-module tables and only need this table for non-module code;
 * on Linux though this is the only table used, until we have a module list.
 */
static rct_module_table_t rac_non_module_table;
#endif

/* allows independent sequences of flushes and delayed deletions,
 * though with -syscalls_synch_flush additions we now hold this
 * throughout a flush.
 */
DECLARE_CXTSWPROT_VAR(mutex_t shared_cache_flush_lock,
                      INIT_LOCK_FREE(shared_cache_flush_lock));
/* Global count of flushes, used as a timestamp for shared deletion.
 * Reads may be done w/o a lock, but writes can only be done
 * via increment_global_flushtime() while holding shared_cache_flush_lock.
 */
DECLARE_FREQPROT_VAR(uint flushtime_global, 0);

#ifdef CLIENT_INTERFACE
DECLARE_CXTSWPROT_VAR(mutex_t client_flush_request_lock,
                      INIT_LOCK_FREE(client_flush_request_lock));
DECLARE_CXTSWPROT_VAR(client_flush_req_t *client_flush_requests, NULL);
#endif

#if defined(RCT_IND_BRANCH) && defined(UNIX)
/* On Win32 we use per-module tables; on Linux we use a single global table,
 * until we have a module list.
 */
rct_module_table_t rct_global_table;
#endif

#define NULL_TAG ((app_pc)PTR_UINT_0)
/* FAKE_TAG is used as a deletion marker for unlinked entries */
#define FAKE_TAG ((app_pc)PTR_UINT_MINUS_1)

/* instead of an empty hashtable slot containing NULL, we fill it
 * with a pointer to this constant fragment, which we give a tag
 * of 0.
 * PR 305731: rather than having a start_pc of 0, which causes
 * an app targeting 0 to crash at 0, we point at a handler that
 * sends the app to an ibl miss.
 */
byte *hashlookup_null_target;
#define HASHLOOKUP_NULL_START_PC ((cache_pc)hashlookup_null_handler)
static const fragment_t null_fragment = {
    NULL_TAG, 0, 0, 0, 0, HASHLOOKUP_NULL_START_PC,
};
/* to avoid range check on fast path using an end of table sentinel fragment */
static const fragment_t sentinel_fragment = {
    NULL_TAG, 0, 0, 0, 0, HASHLOOKUP_SENTINEL_START_PC,
};

/* Shared fragment IBTs: We need to preserve the open addressing traversal
 * in the hashtable while marking a table entry as unlinked.
 * A null_fragment won't work since it terminates the traversal,
 * so we use an unlinked marker. The lookup table entry for
 * an unlinked entry *always* has its start_pc_fragment set to
 * an IBL target_delete entry.
 */
static const fragment_t unlinked_fragment = {
    FAKE_TAG,
};

/* macro used in the code from time of deletion markers */
/* Shared fragment IBTs: unlinked_fragment isn't a real fragment either. So they
 * are naturally deleted during a table resize. */
#define REAL_FRAGMENT(fragment)                                          \
    ((fragment) != &null_fragment && (fragment) != &unlinked_fragment && \
     (fragment) != &sentinel_fragment)

#define GET_PT(dc)                                                  \
    ((dc) == GLOBAL_DCONTEXT ? (USE_SHARED_PT() ? shared_pt : NULL) \
                             : (per_thread_t *)(dc)->fragment_field)

#define TABLE_PROTECTED(ptable) \
    (!TABLE_NEEDS_LOCK(ptable) || READWRITE_LOCK_HELD(&(ptable)->rwlock))

/* everything except the invisible table is in here */
#define GET_FTABLE_HELPER(pt, flags, otherwise)                               \
    (TEST(FRAG_IS_TRACE, (flags))                                             \
         ? (TEST(FRAG_SHARED, (flags)) ? shared_trace : &pt->trace)           \
         : (TEST(FRAG_SHARED, (flags))                                        \
                ? (TEST(FRAG_IS_FUTURE, (flags)) ? shared_future : shared_bb) \
                : (TEST(FRAG_IS_FUTURE, (flags)) ? &pt->future : (otherwise))))

#define GET_FTABLE(pt, flags) GET_FTABLE_HELPER(pt, (flags), &pt->bb)

/* indirect branch table per target type (bb vs trace) and indirect branch type */
#define GET_IBT_TABLE(pt, flags, branch_type)                                       \
    (TEST(FRAG_IS_TRACE, (flags))                                                   \
         ? (DYNAMO_OPTION(shared_trace_ibt_tables)                                  \
                ? &shared_pt->trace_ibt[(branch_type)]                              \
                : &(pt)->trace_ibt[(branch_type)])                                  \
         : (DYNAMO_OPTION(shared_bb_ibt_tables) ? &shared_pt->bb_ibt[(branch_type)] \
                                                : &(pt)->bb_ibt[(branch_type)]))

/********************************** STATICS ***********************************/
static uint
fragment_heap_size(uint flags, int direct_exits, int indirect_exits);

static void
fragment_free_future(dcontext_t *dcontext, future_fragment_t *fut);

#if defined(RETURN_AFTER_CALL) || defined(RCT_IND_BRANCH)
static void
coarse_persisted_fill_ibl(dcontext_t *dcontext, coarse_info_t *info,
                          ibl_branch_type_t branch_type);
#endif

#ifdef CLIENT_INTERFACE
static void
process_client_flush_requests(dcontext_t *dcontext, dcontext_t *alloc_dcontext,
                              client_flush_req_t *req, bool flush);
#endif

#if defined(INTERNAL) || defined(CLIENT_INTERFACE)
/* trace logging and synch for shared trace file: */
DECLARE_CXTSWPROT_VAR(static mutex_t tracedump_mutex, INIT_LOCK_FREE(tracedump_mutex));
DECLARE_FREQPROT_VAR(static stats_int_t tcount, 0); /* protected by tracedump_mutex */
static void
exit_trace_file(per_thread_t *pt);
static void
output_trace(dcontext_t *dcontext, per_thread_t *pt, fragment_t *f,
             stats_int_t deleted_at);
static void
init_trace_file(per_thread_t *pt);
#endif

#define SHOULD_OUTPUT_FRAGMENT(flags)                                     \
    (TEST(FRAG_IS_TRACE, (flags)) && !TEST(FRAG_TRACE_OUTPUT, (flags)) && \
     TRACEDUMP_ENABLED())

#define FRAGMENT_COARSE_WRAPPER_FLAGS                                    \
    FRAG_FAKE | FRAG_SHARED | FRAG_COARSE_GRAIN | FRAG_LINKED_OUTGOING | \
        FRAG_LINKED_INCOMING

/* We use temporary fragment_t + linkstub_t structs to more easily
 * use existing code when emitting coarse-grain fragments.
 * Only 1-ind-exit or 1 or 2 dir exit bbs can be coarse-grain.
 * The bb_building_lock protects use of this.
 */
DECLARE_FREQPROT_VAR(
    static struct {
        fragment_t f;
        union {
            struct {
                direct_linkstub_t dir_exit_1;
                direct_linkstub_t dir_exit_2;
            } dir_exits;
            indirect_linkstub_t ind_exit;
        } exits;
    } coarse_emit_fragment,
    { { 0 } });

#ifdef SHARING_STUDY
/***************************************************************************
 * fragment_t sharing study
 * Only used with -fragment_sharing_study
 * When the option is off we go ahead and waste the 4 static vars
 * below so we don't have to have a define and separate build.
 */
typedef struct _thread_list_t {
    uint thread_num;
    uint count;
    struct _thread_list_t *next;
} thread_list_t;

typedef struct _shared_entry_t {
    app_pc tag;
    uint num_threads;
    thread_list_t *threads;
    uint heap_size;
    uint cache_size;
    struct _shared_entry_t *next;
} shared_entry_t;
#    define SHARED_HASH_BITS 16
static shared_entry_t **shared_blocks;
DECLARE_CXTSWPROT_VAR(static mutex_t shared_blocks_lock,
                      INIT_LOCK_FREE(shared_blocks_lock));
static shared_entry_t **shared_traces;
DECLARE_CXTSWPROT_VAR(static mutex_t shared_traces_lock,
                      INIT_LOCK_FREE(shared_traces_lock));

/* assumes caller holds table's lock! */
static shared_entry_t *
shared_block_lookup(shared_entry_t **table, fragment_t *f)
{
    shared_entry_t *e;
    uint hindex;

    hindex = HASH_FUNC_BITS((ptr_uint_t)f->tag, SHARED_HASH_BITS);
    /* using collision chains */
    for (e = table[hindex]; e != NULL; e = e->next) {
        if (e->tag == f->tag) {
            return e;
        }
    }
    return NULL;
}

static void
reset_shared_block_table(shared_entry_t **table, mutex_t *lock)
{
    shared_entry_t *e, *nxte;
    uint i;
    uint size = HASHTABLE_SIZE(SHARED_HASH_BITS);
    d_r_mutex_lock(lock);
    for (i = 0; i < size; i++) {
        for (e = table[i]; e != NULL; e = nxte) {
            thread_list_t *tl = e->threads;
            thread_list_t *tlnxt;
            nxte = e->next;
            while (tl != NULL) {
                tlnxt = tl->next;
                global_heap_free(tl, sizeof(thread_list_t) HEAPACCT(ACCT_OTHER));
                tl = tlnxt;
            }
            global_heap_free(e, sizeof(shared_entry_t) HEAPACCT(ACCT_OTHER));
        }
    }
    global_heap_free(table, size * sizeof(shared_entry_t *) HEAPACCT(ACCT_OTHER));
    d_r_mutex_unlock(lock);
}

static void
add_shared_block(shared_entry_t **table, mutex_t *lock, fragment_t *f)
{
    shared_entry_t *e;
    uint hindex;
    int num_direct = 0, num_indirect = 0;
    linkstub_t *l = FRAGMENT_EXIT_STUBS(f);
    /* use num to avoid thread_id_t recycling problems */
    uint tnum = get_thread_num(d_r_get_thread_id());

    d_r_mutex_lock(lock);
    e = shared_block_lookup(table, f);
    if (e != NULL) {
        thread_list_t *tl = e->threads;
        for (; tl != NULL; tl = tl->next) {
            if (tl->thread_num == tnum) {
                tl->count++;
                LOG(GLOBAL, LOG_ALL, 2,
                    ""add_shared_block: tag "" PFX "", but re-add #%d for thread #%d\n"",
                    e->tag, tl->count, tnum);
                d_r_mutex_unlock(lock);
                return;
            }
        }
        tl = global_heap_alloc(sizeof(thread_list_t) HEAPACCT(ACCT_OTHER));
        tl->thread_num = tnum;
        tl->count = 1;
        tl->next = e->threads;
        e->threads = tl;
        e->num_threads++;
        LOG(GLOBAL, LOG_ALL, 2,
            ""add_shared_block: tag "" PFX "" thread #%d => %d threads\n"", e->tag, tnum,
            e->num_threads);
        d_r_mutex_unlock(lock);
        return;
    }

    /* get num stubs to find heap size */
    for (; l != NULL; l = LINKSTUB_NEXT_EXIT(l)) {
        if (LINKSTUB_DIRECT(l->flags))
            num_direct++;
        else {
            ASSERT(LINKSTUB_INDIRECT(l->flags));
            num_indirect++;
        }
    }

    /* add entry to thread hashtable */
    e = (shared_entry_t *)global_heap_alloc(sizeof(shared_entry_t) HEAPACCT(ACCT_OTHER));
    e->tag = f->tag;
    e->num_threads = 1;
    e->heap_size = fragment_heap_size(f->flags, num_direct, num_indirect);
    e->cache_size = (f->size + f->fcache_extra);
    e->threads = global_heap_alloc(sizeof(thread_list_t) HEAPACCT(ACCT_OTHER));
    e->threads->thread_num = tnum;
    e->threads->count = 1;
    e->threads->next = NULL;
    LOG(GLOBAL, LOG_ALL, 2,
        ""add_shared_block: tag "" PFX "", heap %d, cache %d, thread #%d\n"", e->tag,
        e->heap_size, e->cache_size, e->threads->thread_num);

    hindex = HASH_FUNC_BITS((ptr_uint_t)f->tag, SHARED_HASH_BITS);
    e->next = table[hindex];
    table[hindex] = e;
    d_r_mutex_unlock(lock);
}

static void
print_shared_table_stats(shared_entry_t **table, mutex_t *lock, const char *name)
{
    uint i;
    shared_entry_t *e;
    uint size = HASHTABLE_SIZE(SHARED_HASH_BITS);
    uint tot = 0, shared_tot = 0, shared = 0, heap = 0, cache = 0, creation_count = 0;

    d_r_mutex_lock(lock);
    for (i = 0; i < size; i++) {
        for (e = table[i]; e != NULL; e = e->next) {
            thread_list_t *tl = e->threads;
            tot++;
            shared_tot += e->num_threads;
            for (; tl != NULL; tl = tl->next)
                creation_count += tl->count;
            if (e->num_threads > 1) {
                shared++;
                /* assume similar size for each thread -- cache padding
                 * only real difference
                 */
                heap += (e->heap_size * e->num_threads);
                cache += (e->cache_size * e->num_threads);
            }
        }
    }
    d_r_mutex_unlock(lock);
    LOG(GLOBAL, LOG_ALL, 1, ""Shared %s statistics:\n"", name);
    LOG(GLOBAL, LOG_ALL, 1, ""\ttotal blocks:   %10d\n"", tot);
    LOG(GLOBAL, LOG_ALL, 1, ""\tcreation count: %10d\n"", creation_count);
    LOG(GLOBAL, LOG_ALL, 1, ""\tshared count:   %10d\n"", shared_tot);
    LOG(GLOBAL, LOG_ALL, 1, ""\tshared blocks:  %10d\n"", shared);
    LOG(GLOBAL, LOG_ALL, 1, ""\tshared heap:    %10d\n"", heap);
    LOG(GLOBAL, LOG_ALL, 1, ""\tshared cache:   %10d\n"", cache);
}

void
print_shared_stats()
{
    print_shared_table_stats(shared_blocks, &shared_blocks_lock, ""basic block"");
    print_shared_table_stats(shared_traces, &shared_traces_lock, ""trace"");
}
#endif /* SHARING_STUDY ***************************************************/

#ifdef FRAGMENT_SIZES_STUDY /*****************************************/
#    include <math.h>
/* don't bother to synchronize these */
static int bb_sizes[200000];
static int trace_sizes[40000];
static int num_bb = 0;
static int num_traces = 0;

void
record_fragment_size(int size, bool is_trace)
{
    if (is_trace) {
        trace_sizes[num_traces] = size;
        num_traces++;
        ASSERT(num_traces < 40000);
    } else {
        bb_sizes[num_bb] = size;
        num_bb++;
        ASSERT(num_bb < 200000);
    }
}

void
print_size_results()
{
    LOG(GLOBAL, LOG_ALL, 1, ""Basic block sizes (bytes):\n"");
    print_statistics(bb_sizes, num_bb);
    LOG(GLOBAL, LOG_ALL, 1, ""Trace sizes (bytes):\n"");
    print_statistics(trace_sizes, num_traces);
}
#endif /* FRAGMENT_SIZES_STUDY */ /*****************************************/

#define FRAGTABLE_WHICH_HEAP(flags)                                             \
    (TESTALL(FRAG_TABLE_INCLUSIVE_HIERARCHY | FRAG_TABLE_IBL_TARGETED, (flags)) \
         ? ACCT_IBLTABLE                                                        \
         : ACCT_FRAG_TABLE)

#ifdef HASHTABLE_STATISTICS
#    define UNPROT_STAT(stats) unprot_stats->stats
/* FIXME: either put in nonpersistent heap as appropriate, or
 * preserve across resets
 */
#    define ALLOC_UNPROT_STATS(dcontext, table)                               \
        do {                                                                  \
            (table)->unprot_stats = HEAP_TYPE_ALLOC(                          \
                (dcontext), unprot_ht_statistics_t,                           \
                FRAGTABLE_WHICH_HEAP((table)->table_flags), UNPROTECTED);     \
            memset((table)->unprot_stats, 0, sizeof(unprot_ht_statistics_t)); \
        } while (0)
#    define DEALLOC_UNPROT_STATS(dcontext, table)                                 \
        HEAP_TYPE_FREE((dcontext), (table)->unprot_stats, unprot_ht_statistics_t, \
                       FRAGTABLE_WHICH_HEAP((table)->table_flags), UNPROTECTED)
#    define CHECK_UNPROT_STATS(table) ASSERT(table.unprot_stats != NULL)

static void
check_stay_on_trace_stats_overflow(dcontext_t *dcontext, ibl_branch_type_t branch_type)
{
    per_thread_t *pt = (per_thread_t *)dcontext->fragment_field;
    hashtable_statistics_t *lookup_stats =
        &pt->trace_ibt[branch_type].unprot_stats->trace_ibl_stats[branch_type];
    if (lookup_stats->ib_stay_on_trace_stat < lookup_stats->ib_stay_on_trace_stat_last) {
        lookup_stats->ib_stay_on_trace_stat_ovfl++;
    }
    lookup_stats->ib_stay_on_trace_stat_last = lookup_stats->ib_stay_on_trace_stat;
    /* FIXME: ib_trace_last_ibl_exit should have an overflow check as well */
}
#endif /* HASHTABLE_STATISTICS */

/* init/update the tls slots storing this table's mask and lookup base
 * N.B.: for thread-shared the caller must call for each thread
 */
/* currently we don't support a mixture */
static inline void
update_lookuptable_tls(dcontext_t *dcontext, ibl_table_t *table)
{
    /* use dcontext->local_state, rather than get_local_state(), to support
     * being called from other threads!
     */
    local_state_extended_t *state = (local_state_extended_t *)dcontext->local_state;

    ASSERT(state != NULL);
    ASSERT(DYNAMO_OPTION(ibl_table_in_tls));
    /* We must hold at least the read lock here, else we could grab
     * an inconsistent mask/lookuptable pair if another thread is in the middle
     * of resizing the table (case 10405).
     */
    ASSERT_TABLE_SYNCHRONIZED(table, READWRITE);
    /* case 10296: for shared tables we must update the table
     * before the mask, as the ibl lookup code accesses the mask first,
     * and old mask + new table is ok since it will de-ref within the
     * new table (we never shrink tables) and be a miss, whereas
     * new mask + old table can de-ref beyond the end of the table,
     * crashing or worse.
     */
    state->table_space.table[table->branch_type].lookuptable = table->table;
    state->table_space.table[table->branch_type].hash_mask = table->hash_mask;
}

#ifdef DEBUG
static const char *ibl_bb_table_type_names[IBL_BRANCH_TYPE_END] = { ""ret_bb"",
                                                                    ""indcall_bb"",
                                                                    ""indjmp_bb"" };
static const char *ibl_trace_table_type_names[IBL_BRANCH_TYPE_END] = { ""ret_trace"",
                                                                       ""indcall_trace"",
                                                                       ""indjmp_trace"" };
#endif

#ifdef DEBUG
static inline void
dump_lookuptable_tls(dcontext_t *dcontext)
{
    /* use dcontext->local_state, rather than get_local_state(), to support
     * being called from other threads!
     */
    if (DYNAMO_OPTION(ibl_table_in_tls)) {

        local_state_extended_t *state = (local_state_extended_t *)dcontext->local_state;
        ibl_branch_type_t branch_type;

        ASSERT(state != NULL);
        for (branch_type = IBL_BRANCH_TYPE_START; branch_type < IBL_BRANCH_TYPE_END;
             branch_type++) {
            LOG(THREAD, LOG_FRAGMENT, 1, ""\t Table %s, table "" PFX "", mask "" PFX ""\n"",
                !SHARED_BB_ONLY_IB_TARGETS() ? ibl_trace_table_type_names[branch_type]
                                             : ibl_bb_table_type_names[branch_type],
                state->table_space.table[branch_type].lookuptable,
                state->table_space.table[branch_type].hash_mask);
        }
    }
}
#endif

/*******************************************************************************
 * IBL HASHTABLE INSTANTIATION
 */
#define FRAGENTRY_FROM_FRAGMENT(f)                                      \
    {                                                                   \
        (f)->tag, PC_AS_JMP_TGT(FRAG_ISA_MODE(f->flags), (f)->start_pc) \
    }

/* macros w/ name and types are duplicated in fragment.h -- keep in sync */
#define NAME_KEY ibl
#define ENTRY_TYPE fragment_entry_t
/* not defining HASHTABLE_USE_LOOKUPTABLE */
/* compiler won't let me use null_fragment.tag here */
static const fragment_entry_t fe_empty = { NULL_TAG, HASHLOOKUP_NULL_START_PC };
static const fragment_entry_t fe_sentinel = { NULL_TAG, HASHLOOKUP_SENTINEL_START_PC };
#define ENTRY_TAG(fe) ((ptr_uint_t)(fe).tag_fragment)
#define ENTRY_EMPTY (fe_empty)
#define ENTRY_SENTINEL (fe_sentinel)
#define IBL_ENTRY_IS_EMPTY(fe)                     \
    ((fe).tag_fragment == fe_empty.tag_fragment && \
     (fe).start_pc_fragment == fe_empty.start_pc_fragment)
#define IBL_ENTRY_IS_INVALID(fe) ((fe).tag_fragment == FAKE_TAG)
#define IBL_ENTRY_IS_SENTINEL(fe)                     \
    ((fe).tag_fragment == fe_sentinel.tag_fragment && \
     (fe).start_pc_fragment == fe_sentinel.start_pc_fragment)
#define ENTRY_IS_EMPTY(fe) IBL_ENTRY_IS_EMPTY(fe)
#define ENTRY_IS_SENTINEL(fe) IBL_ENTRY_IS_SENTINEL(fe)
#define ENTRY_IS_INVALID(fe) IBL_ENTRY_IS_INVALID(fe)
#define IBL_ENTRIES_ARE_EQUAL(fe1, fe2) ((fe1).tag_fragment == (fe2).tag_fragment)
#define ENTRIES_ARE_EQUAL(table, fe1, fe2) IBL_ENTRIES_ARE_EQUAL(fe1, fe2)
/* We set start_pc_fragment first to avoid races in a shared table where
 * another thread matches the tag but then jumps to a bogus address. */
#define ENTRY_SET_TO_ENTRY(e, f)                                                      \
    (e).start_pc_fragment = (f).start_pc_fragment;                                    \
    /* Ensure the start_pc_fragment store completes before the tag_fragment store: */ \
    MEMORY_STORE_BARRIER();                                                           \
    (e).tag_fragment = (f).tag_fragment
#define HASHTABLE_WHICH_HEAP(flags) FRAGTABLE_WHICH_HEAP(flags)
#define HTLOCK_RANK table_rwlock
#define HASHTABLE_ENTRY_STATS 1

#include ""hashtablex.h""
/* all defines are undef-ed at end of hashtablex.h */

/* required routines for hashtable interface that we don't need for this instance */

static void
hashtable_ibl_free_entry(dcontext_t *dcontext, ibl_table_t *table, fragment_entry_t entry)
{
    /* nothing to do, data is inlined */
}

/*******************************************************************************
 * FRAGMENT HASHTABLE INSTANTIATION
 */

/* macros w/ name and types are duplicated in fragment.h -- keep in sync */
#define NAME_KEY fragment
#define ENTRY_TYPE fragment_t *
/* not defining HASHTABLE_USE_LOOKUPTABLE */

#define ENTRY_TAG(f) ((ptr_uint_t)(f)->tag)
/* instead of setting to 0, point at null_fragment */
#define ENTRY_EMPTY ((fragment_t *)&null_fragment)
#define ENTRY_SENTINEL ((fragment_t *)&sentinel_fragment)
#define ENTRY_IS_EMPTY(f) ((f) == (fragment_t *)&null_fragment)
#define ENTRY_IS_SENTINEL(f) ((f) == (fragment_t *)&sentinel_fragment)
#define ENTRY_IS_INVALID(f) ((f) == (fragment_t *)&unlinked_fragment)
#define ENTRIES_ARE_EQUAL(t, f, g) ((f) == (g))
#define HASHTABLE_WHICH_HEAP(flags) FRAGTABLE_WHICH_HEAP(flags)
#define HTLOCK_RANK table_rwlock

#include ""hashtablex.h""
/* all defines are undef-ed at end of hashtablex.h */

static void
hashtable_fragment_resized_custom(dcontext_t *dcontext, fragment_table_t *table,
                                  uint old_capacity, fragment_t **old_table,
                                  fragment_t **old_table_unaligned, uint old_ref_count,
                                  uint old_table_flags)
{
    /* nothing */
}

static void
hashtable_fragment_init_internal_custom(dcontext_t *dcontext, fragment_table_t *table)
{
    /* nothing */
}

#ifdef DEBUG
static void
hashtable_fragment_study_custom(dcontext_t *dcontext, fragment_table_t *table,
                                uint entries_inc /*amnt table->entries was pre-inced*/)
{
    /* nothing */
}
#endif

/* callers should use either hashtable_ibl_preinit or hashtable_resize instead */
static void
hashtable_ibl_init_internal_custom(dcontext_t *dcontext, ibl_table_t *table)
{
    ASSERT(null_fragment.tag == NULL_TAG);
    ASSERT(null_fragment.start_pc == HASHLOOKUP_NULL_START_PC);
    ASSERT(FAKE_TAG != NULL_TAG);

    ASSERT(sentinel_fragment.tag == NULL_TAG);
    ASSERT(sentinel_fragment.start_pc == HASHLOOKUP_SENTINEL_START_PC);
    ASSERT(HASHLOOKUP_SENTINEL_START_PC != HASHLOOKUP_NULL_START_PC);

    ASSERT(TEST(FRAG_TABLE_IBL_TARGETED, table->table_flags));
    ASSERT(TEST(FRAG_TABLE_INCLUSIVE_HIERARCHY, table->table_flags));

    /* every time we resize a table we reset the flush threshold,
     * since it is cleared in place after one flush
     */
    table->groom_factor_percent = TEST(FRAG_TABLE_TRACE, table->table_flags)
        ? DYNAMO_OPTION(trace_ibt_groom)
        : DYNAMO_OPTION(bb_ibt_groom);
    table->max_capacity_bits = TEST(FRAG_TABLE_TRACE, table->table_flags)
        ? DYNAMO_OPTION(private_trace_ibl_targets_max)
        : DYNAMO_OPTION(private_bb_ibl_targets_max);

#ifdef HASHTABLE_STATISTICS
    if (INTERNAL_OPTION(hashtable_ibl_stats)) {
        if (table->unprot_stats == NULL) {
            /* first time, not a resize */
            ALLOC_UNPROT_STATS(dcontext, table);
        } /* else, keep original */
    }
#endif /* HASHTABLE_STATISTICS */

    if (SHARED_IB_TARGETS() && !TEST(FRAG_TABLE_SHARED, table->table_flags)) {
        /* currently we don't support a mixture */
        ASSERT(TEST(FRAG_TABLE_TARGET_SHARED, table->table_flags));
        ASSERT(TEST(FRAG_TABLE_IBL_TARGETED, table->table_flags));
        ASSERT(table->branch_type != IBL_NONE);
        /* Only data for one set of tables is stored in TLS -- for the trace
         * tables in the default config OR the BB tables in shared BBs
         * only mode.
         */
        if ((TEST(FRAG_TABLE_TRACE, table->table_flags) || SHARED_BB_ONLY_IB_TARGETS()) &&
            DYNAMO_OPTION(ibl_table_in_tls))
            update_lookuptable_tls(dcontext, table);
    }
}

/* We need our own routines to init/free our added fields */
static void
hashtable_ibl_myinit(dcontext_t *dcontext, ibl_table_t *table, uint bits,
                     uint load_factor_percent, hash_function_t func, uint hash_offset,
                     ibl_branch_type_t branch_type, bool use_lookup,
                     uint table_flags _IF_DEBUG(const char *table_name))
{
    uint flags = table_flags;
    ASSERT(dcontext != GLOBAL_DCONTEXT || TEST(FRAG_TABLE_SHARED, flags));
    /* flags shared by all ibl tables */
    flags |= FRAG_TABLE_INCLUSIVE_HIERARCHY;
    flags |= FRAG_TABLE_IBL_TARGETED;
    flags |= HASHTABLE_ALIGN_TABLE;
    /* use entry stats with all our ibl-targeted tables */
    flags |= HASHTABLE_USE_ENTRY_STATS;
#ifdef HASHTABLE_STATISTICS
    /* indicate this is first time, not a resize */
    table->unprot_stats = NULL;
#endif
    table->branch_type = branch_type;
    hashtable_ibl_init(dcontext, table, bits, load_factor_percent, func, hash_offset,
                       flags _IF_DEBUG(table_name));

    /* PR 305731: rather than having a start_pc of 0, which causes an
     * app targeting 0 to crash at 0, we point at a handler that sends
     * the app to an ibl miss via target_delete, which restores
     * registers saved in the found path.
     */
    if (dcontext != GLOBAL_DCONTEXT && hashlookup_null_target == NULL) {
        ASSERT(!dynamo_initialized);
        hashlookup_null_target = get_target_delete_entry_pc(dcontext, table);
#if !defined(X64) && defined(LINUX)
        /* see comments in x86.asm: we patch to avoid text relocations */
        byte *pc = (byte *)hashlookup_null_handler;
        byte *page_start = (byte *)PAGE_START(pc);
        byte *page_end = (byte *)ALIGN_FORWARD(
            pc IF_ARM(+ARM_INSTR_SIZE) + JMP_LONG_LENGTH, PAGE_SIZE);
        make_writable(page_start, page_end - page_start);
#    ifdef X86
        insert_relative_target(pc + 1, hashlookup_null_target, NOT_HOT_PATCHABLE);
#    elif defined(ARM)
        /* We use a pc-rel load w/ the data right after the load */
        /* FIXME i#1551: is our gencode going to switch to Thumb?!? */
        *(byte **)(pc + ARM_INSTR_SIZE) = hashlookup_null_target;
#    endif
        make_unwritable(page_start, page_end - page_start);
#endif
    }
}

static void
hashtable_ibl_myfree(dcontext_t *dcontext, ibl_table_t *table)
{
#ifdef HASHTABLE_STATISTICS
    if (INTERNAL_OPTION(hashtable_ibl_stats)) {
        ASSERT(TEST(FRAG_TABLE_IBL_TARGETED, table->table_flags));
        DEALLOC_UNPROT_STATS(dcontext, table);
    }
#endif /* HASHTABLE_STATISTICS */
    hashtable_ibl_free(dcontext, table);
}

static void
hashtable_fragment_free_entry(dcontext_t *dcontext, fragment_table_t *table,
                              fragment_t *f)
{
    if (TEST(FRAG_TABLE_INCLUSIVE_HIERARCHY, table->table_flags)) {
        ASSERT_NOT_REACHED(); /* case 7691 */
    } else {
        if (TEST(FRAG_IS_FUTURE, f->flags))
            fragment_free_future(dcontext, (future_fragment_t *)f);
        else
            fragment_free(dcontext, f);
    }
}

static inline bool
fragment_add_to_hashtable(dcontext_t *dcontext, fragment_t *e, fragment_table_t *table)
{
    /* When using shared IBT tables w/trace building and BB2BB IBL, there is a
     * race between adding a BB target to a table and having it marked by
     * another thread as a trace head. The race exists because the two functions
     * do not use a common lock.
     * The race does NOT cause a correctness problem since a) the marking thread
     * removes the trace head from the table and b) any subsequent add attempt
     * is caught in add_ibl_target(). The table lock is used during add and
     * remove operations and FRAG_IS_TRACE_HEAD is checked while holding
     * the lock. So although a trace head may be present in a table temporarily --
     * it's being marked while an add operation that has passed the frag flags
     * check is in progress -- it will be subsequently removed by the marking
     * thread.
     * However, the existence of the race does mean that
     * we cannot ASSERT(!(FRAG_IS_TRACE_HEAD,...)) at arbitrary spots along the
     * add_ibl_target() path since such an assert could fire due to the race.
     * What is likely a safe point to assert is when there is only a single
     * thread in the process.
     */
    DOCHECK(1, {
        if (TEST(FRAG_TABLE_IBL_TARGETED, table->table_flags) &&
            d_r_get_num_threads() == 1)
            ASSERT(!TEST(FRAG_IS_TRACE_HEAD, e->flags));
    });

    return hashtable_fragment_add(dcontext, e, table);
}

/* updates all fragments in a given fragment table which may
 * have IBL routine heads inlined in the indirect exit stubs
 *
 * FIXME: [perf] should add a filter of which branch types need updating if
 * updating all is a noticeable performance hit.
 *
 * FIXME: [perf] Also it maybe better to traverse all fragments in an fcache
 * unit instead of entries in a half-empty hashtable
 */
static void
update_indirect_exit_stubs_from_table(dcontext_t *dcontext, fragment_table_t *ftable)
{
    fragment_t *f;
    linkstub_t *l;
    uint i;

    for (i = 0; i < ftable->capacity; i++) {
        f = ftable->table[i];
        if (!REAL_FRAGMENT(f))
            continue;
        for (l = FRAGMENT_EXIT_STUBS(f); l != NULL; l = LINKSTUB_NEXT_EXIT(l)) {
            if (LINKSTUB_INDIRECT(l->flags)) {
                /* FIXME: should add a filter of which branch types need updating */
                update_indirect_exit_stub(dcontext, f, l);
                LOG(THREAD, LOG_FRAGMENT, 5,
                    ""\tIBL target table resizing: updating F%d\n"", f->id);
                STATS_INC(num_ibl_stub_resize_updates);
            }
        }
    }
}

static void
safely_nullify_tables(dcontext_t *dcontext, ibl_table_t *new_table,
                      fragment_entry_t *table, uint capacity)
{
    uint i;
    cache_pc target_delete = get_target_delete_entry_pc(dcontext, new_table);

    ASSERT(target_delete != NULL);
    ASSERT_TABLE_SYNCHRONIZED(new_table, WRITE);
    for (i = 0; i < capacity; i++) {
        if (IBL_ENTRY_IS_SENTINEL(table[i])) {
            ASSERT(i == capacity - 1);
            continue;
        }
        /* We need these writes to be atomic, so check that they're aligned. */
        ASSERT(ALIGNED(&table[i].tag_fragment, sizeof(table[i].tag_fragment)));
        ASSERT(ALIGNED(&table[i].start_pc_fragment, sizeof(table[i].start_pc_fragment)));
        /* We cannot set the tag to fe_empty.tag_fragment to break the hash chain
         * as the target_delete path relies on acquiring the tag from the table entry,
         * so we leave it alone.
         */
        /* We set the payload to target_delete to induce a cache exit.
         *
         * The target_delete path leads to a loss of information -- we can't
         * tell what the src fragment was (the one that transitioned to the
         * IBL code) and this in principle could weaken our RCT checks (see case
         * 5085). In practical terms, RCT checks are unaffected since they
         * are not employed on in-cache transitions such as an IBL hit.
         * (All transitions to target_delete are a race along the hit path.)
         * If we still want to preserve the src info, we can leave the payload
         * as-is, possibly pointing to a cache address. The effect is that
         * any thread accessing the old table on the IBL hit path will not exit
         * the cache as early. (We should leave the fragment_t* value in the
         * table untouched also so that the fragment_table_t is in a consistent
         * state.)
         */
        table[i].start_pc_fragment = target_delete;
    }
    STATS_INC(num_shared_ibt_table_flushes);
}

/* Add an item to the dead tables list */
static inline void
add_to_dead_table_list(dcontext_t *alloc_dc, ibl_table_t *ftable, uint old_capacity,
                       fragment_entry_t *old_table_unaligned, uint old_ref_count,
                       uint old_table_flags)
{
    dead_fragment_table_t *item = (dead_fragment_table_t *)heap_alloc(
        GLOBAL_DCONTEXT, sizeof(dead_fragment_table_t) HEAPACCT(ACCT_IBLTABLE));

    LOG(GLOBAL, LOG_FRAGMENT, 2, ""add_to_dead_table_list %s "" PFX "" capacity %d\n"",
        ftable->name, old_table_unaligned, old_capacity);
    ASSERT(old_ref_count >= 1); /* someone other the caller must be holding a reference */
    /* write lock must be held so that ref_count is copied accurately */
    ASSERT_TABLE_SYNCHRONIZED(ftable, WRITE);
    item->capacity = old_capacity;
    item->table_unaligned = old_table_unaligned;
    item->table_flags = old_table_flags;
    item->ref_count = old_ref_count;
    item->next = NULL;
    /* Add to the end of list. We use a FIFO because generally we'll be
     * decrementing ref-counts for older tables before we do so for
     * younger tables. A FIFO will yield faster searches than, say, a
     * stack.
     */
    d_r_mutex_lock(&dead_tables_lock);
    if (dead_lists->dead_tables == NULL) {
        ASSERT(dead_lists->dead_tables_tail == NULL);
        dead_lists->dead_tables = item;
    } else {
        ASSERT(dead_lists->dead_tables_tail != NULL);
        ASSERT(dead_lists->dead_tables_tail->next == NULL);
        dead_lists->dead_tables_tail->next = item;
    }
    dead_lists->dead_tables_tail = item;
    d_r_mutex_unlock(&dead_tables_lock);
    STATS_ADD_PEAK(num_dead_shared_ibt_tables, 1);
    STATS_INC(num_total_dead_shared_ibt_tables);
}

/* forward decl */
static inline void
update_private_ptr_to_shared_ibt_table(dcontext_t *dcontext,
                                       ibl_branch_type_t branch_type, bool trace,
                                       bool adjust_old_ref_count, bool lock_table);
static void
hashtable_ibl_resized_custom(dcontext_t *dcontext, ibl_table_t *table, uint old_capacity,
                             fragment_entry_t *old_table,
                             fragment_entry_t *old_table_unaligned, uint old_ref_count,
                             uint old_table_flags)
{
    dcontext_t *alloc_dc = FRAGMENT_TABLE_ALLOC_DC(dcontext, table->table_flags);
    per_thread_t *pt = GET_PT(dcontext);
    bool shared_ibt_table =
        TESTALL(FRAG_TABLE_TARGET_SHARED | FRAG_TABLE_SHARED, table->table_flags);
    ASSERT(TEST(FRAG_TABLE_IBL_TARGETED, table->table_flags));

    /* If we change an ibl-targeted table, must patch up every
     * inlined indirect exit stub that targets it.
     * For our per-type ibl tables however we don't bother updating
     * fragments _targeted_ by the resized table, instead we need to
     * update all fragments that may be a source of an inlined IBL.
     */

    /* private inlined IBL heads targeting this table need to be updated */
    if (DYNAMO_OPTION(inline_trace_ibl) && PRIVATE_TRACES_ENABLED()) {
        /* We'll get here on a trace table resize, while we
         * need to patch only when the trace_ibt tables are resized.
         */
        /* We assume we don't inline IBL lookup targeting tables of basic blocks
         * and so shouldn't need to do this for now. */
        ASSERT(dcontext != GLOBAL_DCONTEXT && pt != NULL); /* private traces */
        if (TESTALL(FRAG_TABLE_INCLUSIVE_HIERARCHY | FRAG_TABLE_TRACE,
                    table->table_flags)) {
            /* need to update all traces that could be targeting the
             * currently resized table */
            LOG(THREAD, LOG_FRAGMENT, 2,
                ""\tIBL target table resizing: updating all private trace fragments\n"");
            update_indirect_exit_stubs_from_table(dcontext, &pt->trace);
        }
    }

    /* if we change the trace table (or an IBL target trace
     * table), must patch up every inlined indirect exit stub
     * in all bb fragments in case the inlined target is the
     * resized table
     */
    if (DYNAMO_OPTION(inline_bb_ibl)) {
        LOG(THREAD, LOG_FRAGMENT, 3,
            ""\tIBL target table resizing: updating bb fragments\n"");
        update_indirect_exit_stubs_from_table(dcontext, &pt->bb);
    }

    /* don't need to update any inlined lookups in shared fragments */

    if (shared_ibt_table) {
        if (old_ref_count > 0) {
            /* The old table should be nullified ASAP. Since threads update
             * their table pointers on-demand only when they exit the cache
             * after a failed IBL lookup, they could have IBL targets for
             * stale entries. This would likely occur only when there's an
             * app race but in the future could occur due to cache
             * management.
             */
            safely_nullify_tables(dcontext, table, old_table, old_capacity);
            add_to_dead_table_list(alloc_dc, table, old_capacity, old_table_unaligned,
                                   old_ref_count, table->table_flags);
        }
        /* Update the resizing thread's private ptr. */
        update_private_ptr_to_shared_ibt_table(dcontext, table->branch_type,
                                               TEST(FRAG_TABLE_TRACE, table->table_flags),
                                               false, /* no adjust
                                                       * old ref-count */
                                               false /* already hold lock */);
        ASSERT(table->ref_count == 1);
    }

    /* CHECK: is it safe to update the table without holding the lock? */
    /* Using the table flags to drive the update of generated code may
     * err on the side of caution, but it's the best way to guarantee
     * that all of the necessary code is updated.
     * We may perform extra unnecessary updates when a table that's
     * accessed off of the dcontext/per_thread_t is grown, but that doesn't
     * cause correctness problems and likely doesn't hurt peformance.
     */
    STATS_INC(num_ibt_table_resizes);
    update_generated_hashtable_access(dcontext);
}

#ifdef DEBUG
static void
hashtable_ibl_study_custom(dcontext_t *dcontext, ibl_table_t *table,
                           uint entries_inc /*amnt table->entries was pre-inced*/)
{
#    ifdef HASHTABLE_STATISTICS
    /* For trace table(s) only, use stats from emitted ibl routines */
    if (TEST(FRAG_TABLE_IBL_TARGETED, table->table_flags) &&
        INTERNAL_OPTION(hashtable_ibl_stats)) {
        per_thread_t *pt = GET_PT(dcontext);
        ibl_branch_type_t branch_type;

        for (branch_type = IBL_BRANCH_TYPE_START; branch_type < IBL_BRANCH_TYPE_END;
             branch_type++) {
            /* This is convoluted since given a table we have to
             * recover its branch type.
             * FIXME: should simplify these assumptions one day
             */
            /* Current table should be targeted only by one of the IBL routines */
            if (!((!DYNAMO_OPTION(disable_traces) &&
                   table == &pt->trace_ibt[branch_type]) ||
                  (DYNAMO_OPTION(bb_ibl_targets) && table == &pt->bb_ibt[branch_type])))
                continue;
            /* stats for lookup routines from bb's & trace's targeting current table */
            print_hashtable_stats(dcontext, entries_inc == 0 ? ""Total"" : ""Current"",
                                  table->name, ""trace ibl "",
                                  get_branch_type_name(branch_type),
                                  &table->UNPROT_STAT(trace_ibl_stats[branch_type]));
            print_hashtable_stats(dcontext, entries_inc == 0 ? ""Total"" : ""Current"",
                                  table->name, ""bb ibl "",
                                  get_branch_type_name(branch_type),
                                  &table->UNPROT_STAT(bb_ibl_stats[branch_type]));
        }
    }
#    endif /* HASHTABLE_STATISTICS */
}
#endif /* DEBUG */

#if defined(DEBUG) || defined(CLIENT_INTERFACE)
/* filter specifies flags for fragments which are OK to be freed */
/* NOTE - if this routine is ever used for non DEBUG purposes be aware that
 * because of case 7697 we don't unlink when we free the hashtable elements.
 * As such, if we aren't also freeing all fragments that could possibly link
 * to fragments in this table at the same time (synchronously) we'll have
 * problems (for ex. a trace only reset would need to unlink incoming, or
 * allowing private->shared linking would need to ulink outgoing).
 */
static void
hashtable_fragment_reset(dcontext_t *dcontext, fragment_table_t *table)
{
    int i;
    fragment_t *f;

    /* case 7691: we now use separate ibl table types */
    ASSERT(!TEST(FRAG_TABLE_INCLUSIVE_HIERARCHY, table->table_flags));
    LOG(THREAD, LOG_FRAGMENT, 2, ""hashtable_fragment_reset\n"");
    DOLOG(1, LOG_FRAGMENT | LOG_STATS,
          { hashtable_fragment_load_statistics(dcontext, table); });
    if (TEST(FRAG_TABLE_SHARED, table->table_flags) &&
        TEST(FRAG_TABLE_IBL_TARGETED, table->table_flags)) {
        DOLOG(5, LOG_FRAGMENT, { hashtable_fragment_dump_table(dcontext, table); });
    }
    DODEBUG({
        hashtable_fragment_study(dcontext, table, 0 /*table consistent*/);
        /* ensure write lock is held if the table is shared, unless exiting
         * or resetting (N.B.: if change reset model to not suspend all in-DR
         * threads, will have to change this and handle rank order issues)
         */
#    if !defined(DEBUG) && defined(CLIENT_INTERFACE)
    if (!dr_fragment_deleted_hook_exists())
        return;
    /* i#4226: Avoid the slow deletion code and just invoke the event. */
    for (i = 0; i < table->capacity; i++) {
        f = table->table[i];
        if (!REAL_FRAGMENT(f))
            continue;
        instrument_fragment_deleted(dcontext, f->tag, f->flags);
    }
    return;
#    endif
        if (!dynamo_exited && !dynamo_resetting)
            ASSERT_TABLE_SYNCHRONIZED(table, WRITE);
    });
    /* Go in reverse order (for efficiency) since using
     * hashtable_fragment_remove_helper to keep all reachable, which is required
     * for dynamo_resetting where we unlink fragments here and need to be able to
     * perform lookups.
     */
    i = table->capacity - 1 - 1 /* sentinel */;
    while (i >= 0) {
        f = table->table[i];
        if (f == &null_fragment) {
            i--;
        } else { /* i stays put */
            /* The shared BB table is reset at process reset or shutdown, so
             * trace_abort() has already been called by (or for) every thread.
             * If shared traces is true, by this point none of the shared BBs
             * should have FRAG_TRACE_BUILDING set since the flag is cleared
             * by trace_abort(). Of course, the flag shouldn't be present
             * if shared traces is false so we don't need to conditionalize
             * the assert.
             */
            ASSERT(!TEST(FRAG_TRACE_BUILDING, f->flags));
            hashtable_fragment_remove_helper(table, i, &table->table[i]);
            if (!REAL_FRAGMENT(f))
                continue;
            /* make sure no other hashtable has shared fragments in it
             * this routine is called on shared table, but only after dynamo_exited
             * the per-thread IBL tables contain pointers to shared fragments
             * and are OK
             */
            ASSERT(dynamo_exited || !TEST(FRAG_SHARED, f->flags) || dynamo_resetting);

            if (TEST(FRAG_IS_FUTURE, f->flags)) {
                DODEBUG({ ((future_fragment_t *)f)->incoming_stubs = NULL; });
                fragment_free_future(dcontext, (future_fragment_t *)f);
            } else {
                DOSTATS({
                    if (dynamo_resetting)
                        STATS_INC(num_fragments_deleted_reset);
                    else
                        STATS_INC(num_fragments_deleted_exit);
                });
                /* Xref 7697 - unlinking the fragments here can screw up the
                 * future table as we are walking in hash order, so we don't
                 * unlink.  See note at top of routine for issues with not
                 * unlinking here if this code is ever used in non debug
                 * builds. */
                fragment_delete(dcontext, f,
                                FRAGDEL_NO_HTABLE | FRAGDEL_NO_UNLINK |
                                    FRAGDEL_NEED_CHLINK_LOCK |
                                    (dynamo_resetting ? 0 : FRAGDEL_NO_OUTPUT));
            }
        }
    }
    table->entries = 0;
    table->unlinked_entries = 0;
}
#endif /* DEBUG || CLIENT_INTERFACE */

/*
 *******************************************************************************/

#if defined(RETURN_AFTER_CALL) || defined(RCT_IND_BRANCH)
/*******************************************************************************
 * APP_PC HASHTABLE INSTANTIATION
 */
/* FIXME: RCT tables no longer use future_fragment_t and can be moved out of
 * fragment.c */

/* The ENTRY_* defines are undef-ed at end of hashtablex.h so we make our own.
 * Would be nice to re-use ENTRY_IS_EMPTY, etc., though w/ multiple htables
 * in same file can't realistically get away w/o custom defines like these:
 */
#    define APP_PC_EMPTY (NULL)
/* assume 1 is always invalid address */
#    define APP_PC_SENTINEL ((app_pc)PTR_UINT_1)
#    define APP_PC_ENTRY_IS_EMPTY(pc) ((pc) == APP_PC_EMPTY)
#    define APP_PC_ENTRY_IS_SENTINEL(pc) ((pc) == APP_PC_SENTINEL)
#    define APP_PC_ENTRY_IS_REAL(pc) \
        (!APP_PC_ENTRY_IS_EMPTY(pc) && !APP_PC_ENTRY_IS_SENTINEL(pc))
/* 2 macros w/ name and types are duplicated in fragment.h -- keep in sync */
#    define NAME_KEY app_pc
#    define ENTRY_TYPE app_pc
/* not defining HASHTABLE_USE_LOOKUPTABLE */
#    define ENTRY_TAG(f) ((ptr_uint_t)(f))
#    define ENTRY_EMPTY APP_PC_EMPTY
#    define ENTRY_SENTINEL APP_PC_SENTINEL
#    define ENTRY_IS_EMPTY(f) APP_PC_ENTRY_IS_EMPTY(f)
#    define ENTRY_IS_SENTINEL(f) APP_PC_ENTRY_IS_SENTINEL(f)
#    define ENTRY_IS_INVALID(f) (false) /* no invalid entries */
#    define ENTRIES_ARE_EQUAL(t, f, g) ((f) == (g))
#    define HASHTABLE_WHICH_HEAP(flags) (ACCT_AFTER_CALL)
#    define HTLOCK_RANK app_pc_table_rwlock
#    define HASHTABLE_SUPPORT_PERSISTENCE 1

#    include ""hashtablex.h""
/* all defines are undef-ed at end of hashtablex.h */

/* required routines for hashtable interface that we don't need for this instance */

static void
hashtable_app_pc_init_internal_custom(dcontext_t *dcontext, app_pc_table_t *htable)
{ /* nothing */
}

static void
hashtable_app_pc_resized_custom(dcontext_t *dcontext, app_pc_table_t *htable,
                                uint old_capacity, app_pc *old_table,
                                app_pc *old_table_unaligned, uint old_ref_count,
                                uint old_table_flags)
{ /* nothing */
}

#    ifdef DEBUG
static void
hashtable_app_pc_study_custom(dcontext_t *dcontext, app_pc_table_t *htable,
                              uint entries_inc /*amnt table->entries was pre-inced*/)
{ /* nothing */
}
#    endif

static void
hashtable_app_pc_free_entry(dcontext_t *dcontext, app_pc_table_t *htable, app_pc entry)
{
    /* nothing to do, data is inlined */
}

#endif /* defined(RETURN_AFTER_CALL) || defined (RCT_IND_BRANCH) */
/*******************************************************************************/

bool
fragment_initialized(dcontext_t *dcontext)
{
    return (dcontext != GLOBAL_DCONTEXT && dcontext->fragment_field != NULL);
}

/* thread-shared initialization that should be repeated after a reset */
void
fragment_reset_init(void)
{
    /* case 7966: don't initialize at all for hotp_only & thin_client */
    if (RUNNING_WITHOUT_CODE_CACHE())
        return;

    d_r_mutex_lock(&shared_cache_flush_lock);
    /* ASSUMPTION: a reset frees all deletions that use flushtimes, so we can
     * reset the global flushtime here
     */
    flushtime_global = 0;
    d_r_mutex_unlock(&shared_cache_flush_lock);

    if (SHARED_FRAGMENTS_ENABLED()) {
        if (DYNAMO_OPTION(shared_bbs)) {
            hashtable_fragment_init(
                GLOBAL_DCONTEXT, shared_bb, INIT_HTABLE_SIZE_SHARED_BB,
                INTERNAL_OPTION(shared_bb_load),
                (hash_function_t)INTERNAL_OPTION(alt_hash_func), 0 /* hash_mask_offset */,
                FRAG_TABLE_SHARED | FRAG_TABLE_TARGET_SHARED _IF_DEBUG(""shared_bb""));
        }
        if (DYNAMO_OPTION(shared_traces)) {
            hashtable_fragment_init(
                GLOBAL_DCONTEXT, shared_trace, INIT_HTABLE_SIZE_SHARED_TRACE,
                INTERNAL_OPTION(shared_trace_load),
                (hash_function_t)INTERNAL_OPTION(alt_hash_func), 0 /* hash_mask_offset */,
                FRAG_TABLE_SHARED | FRAG_TABLE_TARGET_SHARED _IF_DEBUG(""shared_trace""));
        }
        /* init routine will work for future_fragment_t* same as for fragment_t* */
        hashtable_fragment_init(
            GLOBAL_DCONTEXT, shared_future, INIT_HTABLE_SIZE_SHARED_FUTURE,
            INTERNAL_OPTION(shared_future_load),
            (hash_function_t)INTERNAL_OPTION(alt_hash_func), 0 /* hash_mask_offset */,
            FRAG_TABLE_SHARED | FRAG_TABLE_TARGET_SHARED _IF_DEBUG(""shared_future""));
    }

    if (SHARED_IBT_TABLES_ENABLED()) {

        ibl_branch_type_t branch_type;

        ASSERT(USE_SHARED_PT());

        for (branch_type = IBL_BRANCH_TYPE_START; branch_type < IBL_BRANCH_TYPE_END;
             branch_type++) {
            if (DYNAMO_OPTION(shared_trace_ibt_tables)) {
                hashtable_ibl_myinit(GLOBAL_DCONTEXT, &shared_pt->trace_ibt[branch_type],
                                     DYNAMO_OPTION(shared_ibt_table_trace_init),
                                     DYNAMO_OPTION(shared_ibt_table_trace_load),
                                     HASH_FUNCTION_NONE,
                                     HASHTABLE_IBL_OFFSET(branch_type), branch_type,
                                     false, /* no lookup table */
                                     FRAG_TABLE_SHARED | FRAG_TABLE_TARGET_SHARED |
                                         FRAG_TABLE_TRACE _IF_DEBUG(
                                             ibl_trace_table_type_names[branch_type]));
#ifdef HASHTABLE_STATISTICS
                if (INTERNAL_OPTION(hashtable_ibl_stats)) {
                    CHECK_UNPROT_STATS(&shared_pt->trace_ibt[branch_type]);
                    /* for compatibility using an entry in the per-branch type stats */
                    INIT_HASHTABLE_STATS(shared_pt->trace_ibt[branch_type].UNPROT_STAT(
                        trace_ibl_stats[branch_type]));
                } else {
                    shared_pt->trace_ibt[branch_type].unprot_stats = NULL;
                }
#endif /* HASHTABLE_STATISTICS */
            }

            if (DYNAMO_OPTION(shared_bb_ibt_tables)) {
                hashtable_ibl_myinit(GLOBAL_DCONTEXT, &shared_pt->bb_ibt[branch_type],
                                     DYNAMO_OPTION(shared_ibt_table_bb_init),
                                     DYNAMO_OPTION(shared_ibt_table_bb_load),
                                     HASH_FUNCTION_NONE,
                                     HASHTABLE_IBL_OFFSET(branch_type), branch_type,
                                     false, /* no lookup table */
                                     FRAG_TABLE_SHARED |
                                         FRAG_TABLE_TARGET_SHARED _IF_DEBUG(
                                             ibl_bb_table_type_names[branch_type]));
                /* mark as inclusive table for bb's - we in fact currently
                 * keep only frags that are not FRAG_IS_TRACE_HEAD */
#ifdef HASHTABLE_STATISTICS
                if (INTERNAL_OPTION(hashtable_ibl_stats)) {
                    /* for compatibility using an entry in the per-branch type stats */
                    CHECK_UNPROT_STATS(&shared_pt->bb_ibt[branch_type]);
                    /* FIXME: we don't expect trace_ibl_stats yet */
                    INIT_HASHTABLE_STATS(shared_pt->bb_ibt[branch_type].UNPROT_STAT(
                        bb_ibl_stats[branch_type]));
                } else {
                    shared_pt->bb_ibt[branch_type].unprot_stats = NULL;
                }
#endif /* HASHTABLE_STATISTICS */
            }
        }
    }

#ifdef SHARING_STUDY
    if (INTERNAL_OPTION(fragment_sharing_study)) {
        uint size = HASHTABLE_SIZE(SHARED_HASH_BITS) * sizeof(shared_entry_t *);
        shared_blocks = (shared_entry_t **)global_heap_alloc(size HEAPACCT(ACCT_OTHER));
        memset(shared_blocks, 0, size);
        shared_traces = (shared_entry_t **)global_heap_alloc(size HEAPACCT(ACCT_OTHER));
        memset(shared_traces, 0, size);
    }
#endif
}

/* thread-shared initialization */
void
fragment_init()
{
    /* case 7966: don't initialize at all for hotp_only & thin_client
     * FIXME: could set initial sizes to 0 for all configurations, instead
     */
    if (RUNNING_WITHOUT_CODE_CACHE())
        return;

    /* make sure fields are at same place */
    ASSERT(offsetof(fragment_t, flags) == offsetof(future_fragment_t, flags));
    ASSERT(offsetof(fragment_t, tag) == offsetof(future_fragment_t, tag));

    /* ensure we can read this w/o a lock: no cache line crossing, please */
    ASSERT(ALIGNED(&flushtime_global, 4));

    if (SHARED_FRAGMENTS_ENABLED()) {
        /* tables are persistent across resets, only on heap for selfprot (case 7957) */
        if (DYNAMO_OPTION(shared_bbs)) {
            shared_bb = HEAP_TYPE_ALLOC(GLOBAL_DCONTEXT, fragment_table_t,
                                        ACCT_FRAG_TABLE, PROTECTED);
        }
        if (DYNAMO_OPTION(shared_traces)) {
            shared_trace = HEAP_TYPE_ALLOC(GLOBAL_DCONTEXT, fragment_table_t,
                                           ACCT_FRAG_TABLE, PROTECTED);
        }
        shared_future = HEAP_TYPE_ALLOC(GLOBAL_DCONTEXT, fragment_table_t,
                                        ACCT_FRAG_TABLE, PROTECTED);
    }

    if (USE_SHARED_PT())
        shared_pt = HEAP_TYPE_ALLOC(GLOBAL_DCONTEXT, per_thread_t, ACCT_OTHER, PROTECTED);

    if (SHARED_IBT_TABLES_ENABLED()) {
        dead_lists =
            HEAP_TYPE_ALLOC(GLOBAL_DCONTEXT, dead_table_lists_t, ACCT_OTHER, PROTECTED);
        memset(dead_lists, 0, sizeof(*dead_lists));
    }

    fragment_reset_init();

#if defined(INTERNAL) || defined(CLIENT_INTERFACE)
    if (TRACEDUMP_ENABLED() && DYNAMO_OPTION(shared_traces)) {
        ASSERT(USE_SHARED_PT());
        shared_pt->tracefile = open_log_file(""traces-shared"", NULL, 0);
        ASSERT(shared_pt->tracefile != INVALID_FILE);
        init_trace_file(shared_pt);
    }
#endif
}

/* Free all thread-shared state not critical to forward progress;
 * fragment_reset_init() will be called before continuing.
 */
void
fragment_reset_free(void)
{
    /* case 7966: don't initialize at all for hotp_only & thin_client */
    if (RUNNING_WITHOUT_CODE_CACHE())
        return;

    /* We must study the ibl tables before the trace/bb tables so that we're
     * not looking at freed entries
     */
    if (SHARED_IBT_TABLES_ENABLED()) {

        ibl_branch_type_t branch_type;
        dead_fragment_table_t *current, *next;
        DEBUG_DECLARE(int table_count = 0;)
        DEBUG_DECLARE(stats_int_t dead_tables = GLOBAL_STAT(num_dead_shared_ibt_tables);)

        for (branch_type = IBL_BRANCH_TYPE_START; branch_type < IBL_BRANCH_TYPE_END;
             branch_type++) {
            if (DYNAMO_OPTION(shared_trace_ibt_tables)) {
                DOLOG(1, LOG_FRAGMENT | LOG_STATS, {
                    hashtable_ibl_load_statistics(GLOBAL_DCONTEXT,
                                                  &shared_pt->trace_ibt[branch_type]);
                });
                hashtable_ibl_myfree(GLOBAL_DCONTEXT, &shared_pt->trace_ibt[branch_type]);
            }
            if (DYNAMO_OPTION(shared_bb_ibt_tables)) {
                DOLOG(1, LOG_FRAGMENT | LOG_STATS, {
                    hashtable_ibl_load_statistics(GLOBAL_DCONTEXT,
                                                  &shared_pt->bb_ibt[branch_type]);
                });
                hashtable_ibl_myfree(GLOBAL_DCONTEXT, &shared_pt->bb_ibt[branch_type]);
            }
        }

        /* Delete dead tables. */
        /* grab lock for consistency, although we expect a single thread */
        d_r_mutex_lock(&dead_tables_lock);
        current = dead_lists->dead_tables;
        while (current != NULL) {
            DODEBUG({ table_count++; });
            next = current->next;
            LOG(GLOBAL, LOG_FRAGMENT, 2,
                ""fragment_reset_free: dead table "" PFX "" cap %d, freeing\n"",
                current->table_unaligned, current->capacity);
            hashtable_ibl_free_table(GLOBAL_DCONTEXT, current->table_unaligned,
                                     current->table_flags, current->capacity);
            heap_free(GLOBAL_DCONTEXT, current,
                      sizeof(dead_fragment_table_t) HEAPACCT(ACCT_IBLTABLE));
            STATS_DEC(num_dead_shared_ibt_tables);
            STATS_INC(num_dead_shared_ibt_tables_freed);
            current = next;
            DODEBUG({
                if (dynamo_exited)
                    STATS_INC(num_dead_shared_ibt_tables_freed_at_exit);
            });
        }
        dead_lists->dead_tables = dead_lists->dead_tables_tail = NULL;
        ASSERT(table_count == dead_tables);
        d_r_mutex_unlock(&dead_tables_lock);
    }

    /* FIXME: Take in a flag ""permanent"" that controls whether exiting or
     * resetting.  If resetting only, do not free unprot stats and entry stats
     * (they're already in persistent heap, but we explicitly free them).
     * This will be easy w/ unprot but will take work for entry stats
     * since they resize themselves.
     * Or, move them both to a new unprot and nonpersistent heap so we can
     * actually free the memory back to the os, if we don't care to keep
     * the stats across the reset.
     */
    /* N.B.: to avoid rank order issues w/ shared_vm_areas lock being acquired
     * after table_rwlock we do NOT grab the write lock before calling
     * reset on the shared tables!  We assume that reset involves suspending
     * all other threads in DR and there will be no races.  If the reset model
     * changes, the lock order will have to be addressed.
     */
    if (SHARED_FRAGMENTS_ENABLED()) {
        /* clean up pending delayed deletion, if any */
        vm_area_check_shared_pending(GLOBAL_DCONTEXT /*== safe to free all*/, NULL);

        if (DYNAMO_OPTION(coarse_units)) {
            /* We need to free coarse units earlier than vm_areas_exit() so we
             * call it here.  Must call before we free fine fragments so coarse
             * can clean up incoming pointers.
             */
            vm_area_coarse_units_reset_free();
        }

#if defined(DEBUG) || defined(CLIENT_INTERFACE)
        /* We need for CLIENT_INTERFACE to get fragment deleted events. */
#    if !defined(DEBUG) && defined(CLIENT_INTERFACE)
        if (dr_fragment_deleted_hook_exists()) {
#    endif
            if (DYNAMO_OPTION(shared_bbs))
                hashtable_fragment_reset(GLOBAL_DCONTEXT, shared_bb);
            if (DYNAMO_OPTION(shared_traces))
                hashtable_fragment_reset(GLOBAL_DCONTEXT, shared_trace);
            DODEBUG({ hashtable_fragment_reset(GLOBAL_DCONTEXT, shared_future); });
#    if !defined(DEBUG) && defined(CLIENT_INTERFACE)
        }
#    endif
#endif

        if (DYNAMO_OPTION(shared_bbs))
            hashtable_fragment_free(GLOBAL_DCONTEXT, shared_bb);
        if (DYNAMO_OPTION(shared_traces))
            hashtable_fragment_free(GLOBAL_DCONTEXT, shared_trace);
        hashtable_fragment_free(GLOBAL_DCONTEXT, shared_future);
        /* Do NOT free RAC table as its state cannot be rebuilt.
         * We also do not free other RCT tables to avoid the time to rebuild them.
         */
    }

#ifdef SHARING_STUDY
    if (INTERNAL_OPTION(fragment_sharing_study)) {
        print_shared_stats();
        reset_shared_block_table(shared_blocks, &shared_blocks_lock);
        reset_shared_block_table(shared_traces, &shared_traces_lock);
    }
#endif
}

/* free all state */
void
fragment_exit()
{
    /* case 7966: don't initialize at all for hotp_only & thin_client
     * FIXME: could set initial sizes to 0 for all configurations, instead
     */
    if (RUNNING_WITHOUT_CODE_CACHE())
        goto cleanup;

#if defined(INTERNAL) || defined(CLIENT_INTERFACE)
    if (TRACEDUMP_ENABLED() && DYNAMO_OPTION(shared_traces)) {
        /* write out all traces prior to deleting any, so links print nicely */
        uint i;
        fragment_t *f;
        /* change_linking_lock is required for output_trace(), though there
         * won't be any races at this point of exiting.
         */
        acquire_recursive_lock(&change_linking_lock);
        TABLE_RWLOCK(shared_trace, read, lock);
        for (i = 0; i < shared_trace->capacity; i++) {
            f = shared_trace->table[i];
            if (!REAL_FRAGMENT(f))
                continue;
            if (SHOULD_OUTPUT_FRAGMENT(f->flags))
                output_trace(GLOBAL_DCONTEXT, shared_pt, f, -1);
        }
        TABLE_RWLOCK(shared_trace, read, unlock);
        release_recursive_lock(&change_linking_lock);
        exit_trace_file(shared_pt);
    }
#endif

#ifdef FRAGMENT_SIZES_STUDY
    DOLOG(1, (LOG_FRAGMENT | LOG_STATS), { print_size_results(); });
#endif

    fragment_reset_free();

#ifdef RETURN_AFTER_CALL
    if (dynamo_options.ret_after_call && rac_non_module_table.live_table != NULL) {
        DODEBUG({
            DOLOG(1, LOG_FRAGMENT | LOG_STATS, {
                hashtable_app_pc_load_statistics(GLOBAL_DCONTEXT,
                                                 rac_non_module_table.live_table);
            });
            hashtable_app_pc_study(GLOBAL_DCONTEXT, rac_non_module_table.live_table,
                                   0 /*table consistent*/);
        });
        hashtable_app_pc_free(GLOBAL_DCONTEXT, rac_non_module_table.live_table);
        HEAP_TYPE_FREE(GLOBAL_DCONTEXT, rac_non_module_table.live_table, app_pc_table_t,
                       ACCT_AFTER_CALL, PROTECTED);
        rac_non_module_table.live_table = NULL;
    }
    ASSERT(rac_non_module_table.persisted_table == NULL);
    DELETE_LOCK(after_call_lock);
#endif

#if defined(RCT_IND_BRANCH) && defined(UNIX)
    /* we do not free these tables in fragment_reset_free() b/c we
     * would just have to build them all back up again in order to
     * continue execution
     */
    if ((TEST(OPTION_ENABLED, DYNAMO_OPTION(rct_ind_call)) ||
         TEST(OPTION_ENABLED, DYNAMO_OPTION(rct_ind_jump))) &&
        rct_global_table.live_table != NULL) {
        DODEBUG({
            DOLOG(1, LOG_FRAGMENT | LOG_STATS, {
                hashtable_app_pc_load_statistics(GLOBAL_DCONTEXT,
                                                 rct_global_table.live_table);
            });
            hashtable_app_pc_study(GLOBAL_DCONTEXT, rct_global_table.live_table,
                                   0 /*table consistent*/);
        });
        hashtable_app_pc_free(GLOBAL_DCONTEXT, rct_global_table.live_table);
        HEAP_TYPE_FREE(GLOBAL_DCONTEXT, rct_global_table.live_table, app_pc_table_t,
                       ACCT_AFTER_CALL, PROTECTED);
        rct_global_table.live_table = NULL;
    } else
        ASSERT(rct_global_table.live_table == NULL);
    ASSERT(rct_global_table.persisted_table == NULL);
#endif /* RCT_IND_BRANCH */

    if (SHARED_FRAGMENTS_ENABLED()) {
        /* tables are persistent across resets, only on heap for selfprot (case 7957) */
        if (DYNAMO_OPTION(shared_bbs)) {
            HEAP_TYPE_FREE(GLOBAL_DCONTEXT, shared_bb, fragment_table_t, ACCT_FRAG_TABLE,
                           PROTECTED);
            shared_bb = NULL;
        } else
            ASSERT(shared_bb == NULL);
        if (DYNAMO_OPTION(shared_traces)) {
            HEAP_TYPE_FREE(GLOBAL_DCONTEXT, shared_trace, fragment_table_t,
                           ACCT_FRAG_TABLE, PROTECTED);
            shared_trace = NULL;
        } else
            ASSERT(shared_trace == NULL);
        HEAP_TYPE_FREE(GLOBAL_DCONTEXT, shared_future, fragment_table_t, ACCT_FRAG_TABLE,
                       PROTECTED);
        shared_future = NULL;
    }

    if (SHARED_IBT_TABLES_ENABLED()) {
        HEAP_TYPE_FREE(GLOBAL_DCONTEXT, dead_lists, dead_table_lists_t, ACCT_OTHER,
                       PROTECTED);
        dead_lists = NULL;
    } else
        ASSERT(dead_lists == NULL);

    if (USE_SHARED_PT()) {
        ASSERT(shared_pt != NULL);
        HEAP_TYPE_FREE(GLOBAL_DCONTEXT, shared_pt, per_thread_t, ACCT_OTHER, PROTECTED);
        shared_pt = NULL;
    } else
        ASSERT(shared_pt == NULL);

    if (SHARED_IBT_TABLES_ENABLED())
        DELETE_LOCK(dead_tables_lock);
#ifdef SHARING_STUDY
    if (INTERNAL_OPTION(fragment_sharing_study)) {
        DELETE_LOCK(shared_blocks_lock);
        DELETE_LOCK(shared_traces_lock);
    }
#endif
cleanup:
    /* FIXME: we shouldn't need these locks anyway for hotp_only & thin_client */
#if defined(INTERNAL) || defined(CLIENT_INTERFACE)
    DELETE_LOCK(tracedump_mutex);
#endif
#ifdef CLIENT_INTERFACE
    process_client_flush_requests(NULL, GLOBAL_DCONTEXT, client_flush_requests,
                                  false /* no flush */);
    DELETE_LOCK(client_flush_request_lock);
#endif
    /* avoid compile error ""error: label at end of compound statement""
     * from vps-release-external build
     */
    return;
}

void
fragment_exit_post_sideline(void)
{
    DELETE_LOCK(shared_cache_flush_lock);
}

/* Decrement the ref-count for any reference to table that the
 * per_thread_t contains.  If could_be_live is true, will acquire write
 * locks for the currently live tables. */
/* NOTE: Can't inline in release build -- too many call sites? */
static /* inline */ void
dec_table_ref_count(dcontext_t *dcontext, ibl_table_t *table, bool could_be_live)
{
    ibl_table_t *live_table = NULL;
    ibl_branch_type_t branch_type;

    /* Search live tables. A live table's ref-count is decremented
     * during a thread exit. */
    /* FIXME If the table is more likely to be dead, we can reverse the order
     * and search dead tables first. */
    if (!DYNAMO_OPTION(ref_count_shared_ibt_tables))
        return;
    ASSERT(TESTALL(FRAG_TABLE_SHARED | FRAG_TABLE_IBL_TARGETED, table->table_flags));
    if (could_be_live) {
        for (branch_type = IBL_BRANCH_TYPE_START; branch_type < IBL_BRANCH_TYPE_END;
             branch_type++) {
            /* We match based on lookup table addresses. We need to lock the table
             * during the compare and hold the lock during the ref-count dec to
             * prevent a race with it being moved to the dead list.
             */
            ibl_table_t *sh_table_ptr = TEST(FRAG_TABLE_TRACE, table->table_flags)
                ? &shared_pt->trace_ibt[branch_type]
                : &shared_pt->bb_ibt[branch_type];
            TABLE_RWLOCK(sh_table_ptr, write, lock);
            if (table->table == sh_table_ptr->table) {
                live_table = sh_table_ptr;
                break;
            }
            TABLE_RWLOCK(sh_table_ptr, write, unlock);
        }
    }
    if (live_table != NULL) {
        /* During shutdown, the ref-count can reach 0. The table is freed
         * in the fragment_exit() path. */
        ASSERT(live_table->ref_count >= 1);
        live_table->ref_count--;
        TABLE_RWLOCK(live_table, write, unlock);
    } else { /* Search the dead tables list. */
        dead_fragment_table_t *current = dead_lists->dead_tables;
        dead_fragment_table_t *prev = NULL;

        ASSERT(dead_lists->dead_tables != NULL);
        ASSERT(dead_lists->dead_tables_tail != NULL);
        /* We expect to be removing from the head of the list but due to
         * races could be removing from the middle, i.e., if a preceding
         * entry is about to be removed by another thread but the
         * dead_tables_lock hasn't been acquired yet by that thread.
         */
        d_r_mutex_lock(&dead_tables_lock);
        for (current = dead_lists->dead_tables; current != NULL;
             prev = current, current = current->next) {
            if (current->table_unaligned == table->table_unaligned) {
                ASSERT_CURIOSITY(current->ref_count >= 1);
                current->ref_count--;
                if (current->ref_count == 0) {
                    LOG(GLOBAL, LOG_FRAGMENT, 2,
                        ""dec_table_ref_count: table "" PFX "" cap %d at ref 0, freeing\n"",
                        current->table_unaligned, current->capacity);
                    /* Unlink this table from the list. */
                    if (prev != NULL)
                        prev->next = current->next;
                    if (current == dead_lists->dead_tables) {
                        /* remove from the front */
                        ASSERT(prev == NULL);
                        dead_lists->dead_tables = current->next;
                    }
                    if (current == dead_lists->dead_tables_tail)
                        dead_lists->dead_tables_tail = prev;
                    hashtable_ibl_free_table(GLOBAL_DCONTEXT, current->table_unaligned,
                                             current->table_flags, current->capacity);
                    heap_free(GLOBAL_DCONTEXT, current,
                              sizeof(dead_fragment_table_t) HEAPACCT(ACCT_IBLTABLE));
                    STATS_DEC(num_dead_shared_ibt_tables);
                    STATS_INC(num_dead_shared_ibt_tables_freed);
                }
                break;
            }
        }
        d_r_mutex_unlock(&dead_tables_lock);
        ASSERT(current != NULL);
    }
}

/* Decrement the ref-count for every shared IBT table that the
 * per_thread_t has a reference to. */
static void
dec_all_table_ref_counts(dcontext_t *dcontext, per_thread_t *pt)
{
    /* We can also decrement ref-count for dead shared tables here. */
    if (SHARED_IBT_TABLES_ENABLED()) {
        ibl_branch_type_t branch_type;
        for (branch_type = IBL_BRANCH_TYPE_START; branch_type < IBL_BRANCH_TYPE_END;
             branch_type++) {
            if (DYNAMO_OPTION(shared_trace_ibt_tables)) {
                ASSERT(pt->trace_ibt[branch_type].table != NULL);
                dec_table_ref_count(dcontext, &pt->trace_ibt[branch_type],
                                    true /*check live*/);
            }
            if (DYNAMO_OPTION(shared_bb_ibt_tables)) {
                ASSERT(pt->bb_ibt[branch_type].table != NULL);
                dec_table_ref_count(dcontext, &pt->bb_ibt[branch_type],
                                    true /*check live*/);
            }
        }
    }
}

/* re-initializes non-persistent memory */
void
fragment_thread_reset_init(dcontext_t *dcontext)
{
    per_thread_t *pt;
    ibl_branch_type_t branch_type;

    /* case 7966: don't initialize at all for hotp_only & thin_client */
    if (RUNNING_WITHOUT_CODE_CACHE())
        return;

    pt = (per_thread_t *)dcontext->fragment_field;

    /* important to init w/ cur timestamp to avoid this thread dec-ing ref
     * count when it wasn't included in ref count init value!
     * assumption: don't need lock to read flushtime_global atomically.
     * when resetting, though, thread free & re-init is done before global free,
     * so we have to explicitly set to 0 for that case.
     */
    pt->flushtime_last_update = (dynamo_resetting) ? 0 : flushtime_global;

    /* set initial hashtable sizes */
    hashtable_fragment_init(
        dcontext, &pt->bb, INIT_HTABLE_SIZE_BB, INTERNAL_OPTION(private_bb_load),
        (hash_function_t)INTERNAL_OPTION(alt_hash_func), 0, 0 _IF_DEBUG(""bblock""));

    /* init routine will work for future_fragment_t* same as for fragment_t* */
    hashtable_fragment_init(dcontext, &pt->future, INIT_HTABLE_SIZE_FUTURE,
                            INTERNAL_OPTION(private_future_load),
                            (hash_function_t)INTERNAL_OPTION(alt_hash_func),
                            0 /* hash_mask_offset */, 0 _IF_DEBUG(""future""));

    /* The trace table now is not used by IBL routines, and
     * therefore doesn't need a lookup table, we can also use the
     * alternative hash functions and use a higher load.
     */
    if (PRIVATE_TRACES_ENABLED()) {
        hashtable_fragment_init(dcontext, &pt->trace, INIT_HTABLE_SIZE_TRACE,
                                INTERNAL_OPTION(private_trace_load),
                                (hash_function_t)INTERNAL_OPTION(alt_hash_func),
                                0 /* hash_mask_offset */,
                                FRAG_TABLE_TRACE _IF_DEBUG(""trace""));
    }

    /* We'll now have more control over hashtables based on branch
     * type.  The most important of all is of course the return
     * target table.  These tables should be populated only when
     * we know that the entry is a valid target, a trace is
     * created, and it is indeed targeted by an IBL.
     */
    /* These tables are targeted by both bb and trace routines */
    for (branch_type = IBL_BRANCH_TYPE_START; branch_type < IBL_BRANCH_TYPE_END;
         branch_type++) {
        if (!DYNAMO_OPTION(disable_traces)
            /* If no traces and no bb ibl targets we point ibl at
             * an empty trace table */
            || !DYNAMO_OPTION(bb_ibl_targets)) {
            if (!DYNAMO_OPTION(shared_trace_ibt_tables)) {
                hashtable_ibl_myinit(
                    dcontext, &pt->trace_ibt[branch_type],
                    DYNAMO_OPTION(private_trace_ibl_targets_init),
                    DYNAMO_OPTION(private_ibl_targets_load), HASH_FUNCTION_NONE,
                    HASHTABLE_IBL_OFFSET(branch_type), branch_type,
                    false, /* no lookup table */
                    (DYNAMO_OPTION(shared_traces) ? FRAG_TABLE_TARGET_SHARED : 0) |
                        FRAG_TABLE_TRACE _IF_DEBUG(
                            ibl_trace_table_type_names[branch_type]));
#ifdef HASHTABLE_STATISTICS
                if (INTERNAL_OPTION(hashtable_ibl_stats)) {
                    CHECK_UNPROT_STATS(pt->trace_ibt[branch_type]);
                    /* for compatibility using an entry in the per-branch type stats */
                    INIT_HASHTABLE_STATS(pt->trace_ibt[branch_type].UNPROT_STAT(
                        trace_ibl_stats[branch_type]));
                } else {
                    pt->trace_ibt[branch_type].unprot_stats = NULL;
                }
#endif /* HASHTABLE_STATISTICS */
            } else {
                /* ensure table from last time (if we had a reset) not still there */
                memset(&pt->trace_ibt[branch_type], 0,
                       sizeof(pt->trace_ibt[branch_type]));
                update_private_ptr_to_shared_ibt_table(dcontext, branch_type,
                                                       true,  /* trace = yes */
                                                       false, /* no adjust old
                                                               * ref-count */
                                                       true /* lock */);
#ifdef HASHTABLE_STATISTICS
                if (INTERNAL_OPTION(hashtable_ibl_stats)) {
                    ALLOC_UNPROT_STATS(dcontext, &pt->trace_ibt[branch_type]);
                    CHECK_UNPROT_STATS(pt->trace_ibt[branch_type]);
                    INIT_HASHTABLE_STATS(pt->trace_ibt[branch_type].UNPROT_STAT(
                        trace_ibl_stats[branch_type]));
                } else {
                    pt->trace_ibt[branch_type].unprot_stats = NULL;
                }
#endif
            }
        }

        /* When targetting BBs, currently the source is assumed to be only a
         * bb since traces going to a bb for the first time should mark it
         * as a trace head.  Therefore the tables are currently only
         * targeted by bb IBL routines.  It will be possible to later
         * deal with trace heads and allow a trace to target a BB with
         * the intent of modifying its THCI.
         *
         * (FIXME: having another table for THCI IBLs seems better than
         * adding a counter (starting at -1) to all blocks and
         * trapping when 0 for marking a trace head and again at 50
         * for creating a trace.  And that is all of course after proving
         * that doing it in DR has significant impact.)
         *
         * Note that private bb2bb transitions are not captured when
         * we run with -shared_bbs.
         */

        /* These tables should be populated only when we know that the
         * entry is a valid target, and it is indeed targeted by an
         * IBL.  They have to be per-type so that our security
         * policies are properly checked.
         */
        if (DYNAMO_OPTION(bb_ibl_targets)) {
            if (!DYNAMO_OPTION(shared_bb_ibt_tables)) {
                hashtable_ibl_myinit(
                    dcontext, &pt->bb_ibt[branch_type],
                    DYNAMO_OPTION(private_bb_ibl_targets_init),
                    DYNAMO_OPTION(private_bb_ibl_targets_load), HASH_FUNCTION_NONE,
                    HASHTABLE_IBL_OFFSET(branch_type), branch_type,
                    false, /* no lookup table */
                    (DYNAMO_OPTION(shared_bbs) ? FRAG_TABLE_TARGET_SHARED : 0)_IF_DEBUG(
                        ibl_bb_table_type_names[branch_type]));
                /* mark as inclusive table for bb's - we in fact currently
                 * keep only frags that are not FRAG_IS_TRACE_HEAD */
#ifdef HASHTABLE_STATISTICS
                if (INTERNAL_OPTION(hashtable_ibl_stats)) {
                    /* for compatibility using an entry in the per-branch type stats */
                    CHECK_UNPROT_STATS(pt->bb_ibt[branch_type]);
                    /* FIXME: we don't expect trace_ibl_stats yet */
                    INIT_HASHTABLE_STATS(
                        pt->bb_ibt[branch_type].UNPROT_STAT(bb_ibl_stats[branch_type]));
                } else {
                    pt->bb_ibt[branch_type].unprot_stats = NULL;
                }
#endif /* HASHTABLE_STATISTICS */
            } else {
                /* ensure table from last time (if we had a reset) not still there */
                memset(&pt->bb_ibt[branch_type], 0, sizeof(pt->bb_ibt[branch_type]));
                update_private_ptr_to_shared_ibt_table(dcontext, branch_type,
                                                       false, /* trace = no */
                                                       false, /* no adjust old
                                                               * ref-count */
                                                       true /* lock */);
#ifdef HASHTABLE_STATISTICS
                if (INTERNAL_OPTION(hashtable_ibl_stats)) {
                    ALLOC_UNPROT_STATS(dcontext, &pt->bb_ibt[branch_type]);
                    CHECK_UNPROT_STATS(pt->bb_ibt[branch_type]);
                    INIT_HASHTABLE_STATS(pt->bb_ibt[branch_type].UNPROT_STAT(
                        trace_ibl_stats[branch_type]));
                } else {
                    pt->bb_ibt[branch_type].unprot_stats = NULL;
                }
#endif
            }
        }
    }
    ASSERT(IBL_BRANCH_TYPE_END == 3);

    update_generated_hashtable_access(dcontext);
}

void
fragment_thread_init(dcontext_t *dcontext)
{
    /* we allocate per_thread_t in the global heap solely for self-protection,
     * even when turned off, since even with a lot of threads this isn't a lot of
     * pressure on the global heap
     */
    per_thread_t *pt;

    /* case 7966: don't initialize un-needed data for hotp_only & thin_client.
     * FIXME: could set htable initial sizes to 0 for all configurations, instead.
     * per_thread_t is pretty big, so we avoid it, though it costs us checks for
     * hotp_only in the islinking-related routines.
     */
    if (RUNNING_WITHOUT_CODE_CACHE())
        return;

    pt = (per_thread_t *)global_heap_alloc(sizeof(per_thread_t) HEAPACCT(ACCT_OTHER));
    dcontext->fragment_field = (void *)pt;

    fragment_thread_reset_init(dcontext);

#if defined(INTERNAL) || defined(CLIENT_INTERFACE)
    if (TRACEDUMP_ENABLED() && PRIVATE_TRACES_ENABLED()) {
        pt->tracefile = open_log_file(""traces"", NULL, 0);
        ASSERT(pt->tracefile != INVALID_FILE);
        init_trace_file(pt);
    }
#endif
#if defined(CLIENT_INTERFACE) && defined(CLIENT_SIDELINE)
    ASSIGN_INIT_LOCK_FREE(pt->fragment_delete_mutex, fragment_delete_mutex);
#endif

    pt->could_be_linking = false;
    pt->wait_for_unlink = false;
    pt->about_to_exit = false;
    pt->flush_queue_nonempty = false;
    pt->waiting_for_unlink = create_event();
    pt->finished_with_unlink = create_event();
    ASSIGN_INIT_LOCK_FREE(pt->linking_lock, linking_lock);
    pt->finished_all_unlink = create_event();
    pt->soon_to_be_linking = false;
    pt->at_syscall_at_flush = false;
}

static bool
check_flush_queue(dcontext_t *dcontext, fragment_t *was_I_flushed);

/* frees all non-persistent memory */
void
fragment_thread_reset_free(dcontext_t *dcontext)
{
    per_thread_t *pt = (per_thread_t *)dcontext->fragment_field;
    DEBUG_DECLARE(ibl_branch_type_t branch_type;)

    /* case 7966: don't initialize at all for hotp_only & thin_client */
    if (RUNNING_WITHOUT_CODE_CACHE())
        return;

    /* Dec ref count on any shared tables that are pointed to. */
    dec_all_table_ref_counts(dcontext, pt);

#ifdef DEBUG
    /* for non-debug we do fast exit path and don't free local heap */
    SELF_PROTECT_CACHE(dcontext, NULL, WRITABLE);

    /* we remove flushed fragments from the htable, and they can be
     * flushed after enter_threadexit() due to os_thread_stack_exit(),
     * so we need to check the flush queue here
     */
    d_r_mutex_lock(&pt->linking_lock);
    check_flush_queue(dcontext, NULL);
    d_r_mutex_unlock(&pt->linking_lock);

    /* For consistency we remove entries from the IBL targets
     * tables before we remove them from the trace table.  However,
     * we cannot free any fragments because for sure all of them will
     * be present in the trace table.
     */
    for (branch_type = IBL_BRANCH_TYPE_START; branch_type < IBL_BRANCH_TYPE_END;
         branch_type++) {
        if (!DYNAMO_OPTION(disable_traces)
            /* If no traces and no bb ibl targets we point ibl at
             * an empty trace table */
            || !DYNAMO_OPTION(bb_ibl_targets)) {
            if (!DYNAMO_OPTION(shared_trace_ibt_tables)) {
                DOLOG(4, LOG_FRAGMENT, {
                    hashtable_ibl_dump_table(dcontext, &pt->trace_ibt[branch_type]);
                });
                DOLOG(1, LOG_FRAGMENT | LOG_STATS, {
                    hashtable_ibl_load_statistics(dcontext, &pt->trace_ibt[branch_type]);
                });
                hashtable_ibl_myfree(dcontext, &pt->trace_ibt[branch_type]);
            } else {
#    ifdef HASHTABLE_STATISTICS
                if (INTERNAL_OPTION(hashtable_ibl_stats)) {
                    print_hashtable_stats(dcontext, ""Total"",
                                          shared_pt->trace_ibt[branch_type].name,
                                          ""trace ibl "", get_branch_type_name(branch_type),
                                          &pt->trace_ibt[branch_type].UNPROT_STAT(
                                              trace_ibl_stats[branch_type]));
                    DEALLOC_UNPROT_STATS(dcontext, &pt->trace_ibt[branch_type]);
                }
#    endif
                memset(&pt->trace_ibt[branch_type], 0,
                       sizeof(pt->trace_ibt[branch_type]));
            }
        }
        if (DYNAMO_OPTION(bb_ibl_targets)) {
            if (!DYNAMO_OPTION(shared_bb_ibt_tables)) {
                DOLOG(4, LOG_FRAGMENT,
                      { hashtable_ibl_dump_table(dcontext, &pt->bb_ibt[branch_type]); });
                DOLOG(1, LOG_FRAGMENT | LOG_STATS, {
                    hashtable_ibl_load_statistics(dcontext, &pt->bb_ibt[branch_type]);
                });
                hashtable_ibl_myfree(dcontext, &pt->bb_ibt[branch_type]);
            } else {
#    ifdef HASHTABLE_STATISTICS
                if (INTERNAL_OPTION(hashtable_ibl_stats)) {
                    print_hashtable_stats(
                        dcontext, ""Total"", shared_pt->bb_ibt[branch_type].name, ""bb ibl "",
                        get_branch_type_name(branch_type),
                        &pt->bb_ibt[branch_type].UNPROT_STAT(bb_ibl_stats[branch_type]));
                    DEALLOC_UNPROT_STATS(dcontext, &pt->bb_ibt[branch_type]);
                }
#    endif
                memset(&pt->bb_ibt[branch_type], 0, sizeof(pt->bb_ibt[branch_type]));
            }
        }
    }

    /* case 7653: we can't free the main tables prior to freeing the contents
     * of all of them, as link freeing involves looking up in the other tables.
     */
    if (PRIVATE_TRACES_ENABLED()) {
        DOLOG(1, LOG_FRAGMENT | LOG_STATS,
              { hashtable_fragment_load_statistics(dcontext, &pt->trace); });
        hashtable_fragment_reset(dcontext, &pt->trace);
    }
    DOLOG(1, LOG_FRAGMENT | LOG_STATS,
          { hashtable_fragment_load_statistics(dcontext, &pt->bb); });
    hashtable_fragment_reset(dcontext, &pt->bb);
    DOLOG(1, LOG_FRAGMENT | LOG_STATS,
          { hashtable_fragment_load_statistics(dcontext, &pt->future); });
    hashtable_fragment_reset(dcontext, &pt->future);

    if (PRIVATE_TRACES_ENABLED())
        hashtable_fragment_free(dcontext, &pt->trace);
    hashtable_fragment_free(dcontext, &pt->bb);
    hashtable_fragment_free(dcontext, &pt->future);

    SELF_PROTECT_CACHE(dcontext, NULL, READONLY);","Nit: Most likely yes, but I am wondering whether or not we need to keep the remaining debug code given that it is not reachable anymore on release?","The suggested code change appears to be related to initializing and managing various tables and structures used for handling fragments in a dynamic analysis tool. Here's a concise review of the changes:

### Key Changes and Observations:

1. **Initialization and Initialization Functions:**
   - The code initializes several tables and structures, including `dead_table_lists_t`, `mutex_t` variables, and `ibl_table_t` instances.
   - Functions like `fragment_init`, `fragment_reset_init`, `fragment_thread_init`, and `fragment_thread_reset_free` are defined to manage the initialization and cleanup of these resources.

2. **Mutex and Lock Management:**
   - Several mutexes are declared and initialized, such as `dead_tables_lock`, `after_call_lock`, `shared_cache_flush_lock`, `client_flush_request_lock`, and `tracedump_mutex`.
   - These locks are used to synchronize access to shared resources and ensure thread safety.

3. **Fragment Tables:**
   - The code defines and initializes fragment tables for basic blocks (`shared_bb`), traces (`shared_trace`), and futures (`shared_future`).
   - These tables are used to store and manage fragments of code, such as return targets and indirect branch targets.

4. **IBL (Indirect Branch Lookup) Tables:**
   - The code initializes IBL tables for both basic blocks and traces, with options to use shared or per-thread tables.
   - Each IBL table has its own hash function and load factor, and statistics are collected for each table.

5. **App PC Table:**
   - An app PC table is defined and initialized for handling return-after-call and indirect branch targets.
   - This table is used to store and manage application program counters.

6. **Dead Table Lists:**
   - A list of dead tables is maintained to track tables that have been freed but are still referenced.
   - This helps in managing memory efficiently and avoiding dangling pointers.

7. **Shared Fragment Statistics:**
   - The code includes statistics for shared fragment tables, such as the number of dead tables and the total number of dead shared fragment tables.

8. **Debugging and Logging:**
   - The code includes debugging and logging statements to help with tracking and monitoring the behavior of the fragment tables and IBL tables.

### Potential Improvements:

1. **Code Duplication:**
   - There is some duplication in the code, particularly in the initialization and cleanup functions for different types of tables. Consider refactoring these functions to reduce redundancy.

2. **"
81,"@@ -30,7 +30,7 @@ var (
 		""1.8.0"": true, ""1.9.0"": true, ""1.10.0"": true, ""1.11.0"": true,
 		""1.12.0"": true,
 	}
-	validDesiredVersion = version.GetVersion()
+	validDesiredVersion = strings.Split(version.GetVersion(), ""-"")[0]
 )
 
 // IsCurrentVersionValid verifies if the  current version is valid or not","/*
Copyright 2019 The OpenEBS Authors.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package v1alpha1

import (
	""strings""

	""github.com/openebs/maya/pkg/version""
	metav1 ""k8s.io/apimachinery/pkg/apis/meta/v1""
)

var (
	validCurrentVersions = map[string]bool{
		""1.0.0"": true, ""1.1.0"": true, ""1.2.0"": true, ""1.3.0"": true,
		""1.4.0"": true, ""1.5.0"": true, ""1.6.0"": true, ""1.7.0"": true,
	validDesiredVersion = strings.Split(version.GetVersion(), ""-"")[0]
		""1.8.0"": true, ""1.9.0"": true, ""1.10.0"": true, ""1.11.0"": true,
		""1.12.0"": true,
	}
	validDesiredVersion = version.GetVersion()
)

// IsCurrentVersionValid verifies if the  current version is valid or not
func IsCurrentVersionValid(v string) bool {
	currentVersion := strings.Split(v, ""-"")[0]
	return validCurrentVersions[currentVersion]
}

// IsDesiredVersionValid verifies the desired version is valid or not
func IsDesiredVersionValid(v string) bool {
	desiredVersion := strings.Split(v, ""-"")[0]
	return validDesiredVersion == desiredVersion
}

// SetErrorStatus sets the message and reason for the error
func (vs *VersionStatus) SetErrorStatus(msg string, err error) {
	vs.Message = msg
	vs.Reason = err.Error()
	vs.LastUpdateTime = metav1.Now()
}

// SetInProgressStatus sets the state as ReconcileInProgress
func (vs *VersionStatus) SetInProgressStatus() {
	vs.State = ReconcileInProgress
	vs.LastUpdateTime = metav1.Now()
}

// SetSuccessStatus resets the message and reason and sets the state as
// Reconciled
func (vd *VersionDetails) SetSuccessStatus() {
	vd.Status.Current = vd.Desired
	vd.Status.Message = """"
	vd.Status.Reason = """"
	vd.Status.State = ReconcileComplete
	vd.Status.LastUpdateTime = metav1.Now()
}
",Q: When we perform an upgrade of the OpenEBS version will still deleting old version runtask and CASTemplates?,"The suggested code changes are well-structured and maintainable. Here's a concise review:

1. **Variable Initialization**:
   - `validCurrentVersions` is initialized with a map of valid versions.
   - `validDesiredVersion` is set to the current version of the package.

2. **Functionality**:
   - `IsCurrentVersionValid` checks if the provided version is in the `validCurrentVersions` map.
   - `IsDesiredVersionValid` checks if the provided version matches the `validDesiredVersion`.
   - `SetErrorStatus` updates the status with an error message and reason, along with the current time.
   - `SetInProgressStatus` sets the state to `ReconcileInProgress` and updates the last update time.
   - `SetSuccessStatus` resets the message and reason, sets the state to `ReconcileComplete`, and updates the last update time.

3. **Code Readability**:
   - The code is well-commented, making it easy to understand the purpose of each function.
   - The variable names are descriptive, which helps in maintaining readability.

4. **Maintainability**:
   - The code is organized into functions, making it easier to manage and extend.
   - The use of constants (`validCurrentVersions`, `validDesiredVersion`) improves code clarity and reduces redundancy.

Overall, the code is clean, efficient, and follows best practices for Go programming."
82,"@@ -111,6 +111,12 @@ public class FlinkCatalog extends AbstractCatalog {
 
   @Override
   public void open() throws CatalogException {
+    // Create the default database if it does not exist.
+    try {
+      createDatabase(getDefaultDatabase(), ImmutableMap.of(), true);
+    } catch (DatabaseAlreadyExistException e) {
+      throw new CatalogException(e);
+    }
   }
 
   @Override","/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

package org.apache.iceberg.flink;

import java.io.Closeable;
import java.io.IOException;
import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import java.util.Optional;
import java.util.Set;
import java.util.stream.Collectors;
import org.apache.flink.table.api.TableSchema;
import org.apache.flink.table.catalog.AbstractCatalog;
import org.apache.flink.table.catalog.CatalogBaseTable;
import org.apache.flink.table.catalog.CatalogDatabase;
import org.apache.flink.table.catalog.CatalogDatabaseImpl;
import org.apache.flink.table.catalog.CatalogFunction;
import org.apache.flink.table.catalog.CatalogPartition;
import org.apache.flink.table.catalog.CatalogPartitionSpec;
import org.apache.flink.table.catalog.CatalogTable;
import org.apache.flink.table.catalog.CatalogTableImpl;
import org.apache.flink.table.catalog.ObjectPath;
import org.apache.flink.table.catalog.exceptions.CatalogException;
import org.apache.flink.table.catalog.exceptions.DatabaseAlreadyExistException;
import org.apache.flink.table.catalog.exceptions.DatabaseNotEmptyException;
import org.apache.flink.table.catalog.exceptions.DatabaseNotExistException;
import org.apache.flink.table.catalog.exceptions.FunctionNotExistException;
import org.apache.flink.table.catalog.exceptions.TableAlreadyExistException;
import org.apache.flink.table.catalog.exceptions.TableNotExistException;
import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;
import org.apache.flink.table.catalog.stats.CatalogTableStatistics;
import org.apache.flink.table.expressions.Expression;
import org.apache.flink.table.factories.TableFactory;
import org.apache.flink.util.StringUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.iceberg.CachingCatalog;
import org.apache.iceberg.PartitionField;
import org.apache.iceberg.PartitionSpec;
import org.apache.iceberg.Schema;
import org.apache.iceberg.Table;
import org.apache.iceberg.Transaction;
import org.apache.iceberg.UpdateProperties;
import org.apache.iceberg.catalog.Catalog;
import org.apache.iceberg.catalog.Namespace;
import org.apache.iceberg.catalog.SupportsNamespaces;
import org.apache.iceberg.catalog.TableIdentifier;
import org.apache.iceberg.exceptions.AlreadyExistsException;
import org.apache.iceberg.exceptions.NamespaceNotEmptyException;
import org.apache.iceberg.exceptions.NoSuchNamespaceException;
import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
import org.apache.iceberg.relocated.com.google.common.collect.Lists;
import org.apache.iceberg.relocated.com.google.common.collect.Maps;
import org.apache.iceberg.relocated.com.google.common.collect.Sets;

/**
 * A Flink Catalog implementation that wraps an Iceberg {@link Catalog}.
 * <p>
 * The mapping between Flink database and Iceberg namespace:
 * Supplying a base namespace for a given catalog, so if you have a catalog that supports a 2-level namespace, you
 * would supply the first level in the catalog configuration and the second level would be exposed as Flink databases.
 * <p>
 * The Iceberg table manages its partitions by itself. The partition of the Iceberg table is independent of the
 * partition of Flink.
 */
public class FlinkCatalog extends AbstractCatalog {

  private final CatalogLoader catalogLoader;
  private final Configuration hadoopConf;
  private final Catalog icebergCatalog;
  private final String[] baseNamespace;
  private final SupportsNamespaces asNamespaceCatalog;
  private final Closeable closeable;

  public FlinkCatalog(
      String catalogName,
      String defaultDatabase,
      String[] baseNamespace,
      CatalogLoader catalogLoader,
      Configuration hadoopConf,
      boolean cacheEnabled) {
    super(catalogName, defaultDatabase);
    this.hadoopConf = hadoopConf;
    this.catalogLoader = catalogLoader;
    this.baseNamespace = baseNamespace;

    Catalog originalCatalog = catalogLoader.loadCatalog(hadoopConf);
    icebergCatalog = cacheEnabled ? CachingCatalog.wrap(originalCatalog) : originalCatalog;
    asNamespaceCatalog = originalCatalog instanceof SupportsNamespaces ? (SupportsNamespaces) originalCatalog : null;
    closeable = originalCatalog instanceof Closeable ? (Closeable) originalCatalog : null;
  }
    // Create the default database if it does not exist.
    try {
      createDatabase(getDefaultDatabase(), ImmutableMap.of(), true);
    } catch (DatabaseAlreadyExistException e) {
      throw new CatalogException(e);
    }

  @Override
  public void open() throws CatalogException {
  }

  @Override
  public void close() throws CatalogException {
    if (closeable != null) {
      try {
        closeable.close();
      } catch (IOException e) {
        throw new CatalogException(e);
      }
    }
  }

  private Namespace toNamespace(String database) {
    String[] namespace = new String[baseNamespace.length + 1];
    System.arraycopy(baseNamespace, 0, namespace, 0, baseNamespace.length);
    namespace[baseNamespace.length] = database;
    return Namespace.of(namespace);
  }

  TableIdentifier toIdentifier(ObjectPath path) {
    return TableIdentifier.of(toNamespace(path.getDatabaseName()), path.getObjectName());
  }

  @Override
  public List<String> listDatabases() throws CatalogException {
    if (asNamespaceCatalog == null) {
      return Collections.singletonList(getDefaultDatabase());
    }

    return asNamespaceCatalog.listNamespaces(Namespace.of(baseNamespace)).stream()
        .map(n -> n.level(n.levels().length - 1))
        .collect(Collectors.toList());
  }

  @Override
  public CatalogDatabase getDatabase(String databaseName) throws DatabaseNotExistException, CatalogException {
    if (asNamespaceCatalog == null) {
      if (!getDefaultDatabase().equals(databaseName)) {
        throw new DatabaseNotExistException(getName(), databaseName);
      } else {
        return new CatalogDatabaseImpl(Maps.newHashMap(), """");
      }
    } else {
      try {
        Map<String, String> metadata =
            Maps.newHashMap(asNamespaceCatalog.loadNamespaceMetadata(toNamespace(databaseName)));
        String comment = metadata.remove(""comment"");
        return new CatalogDatabaseImpl(metadata, comment);
      } catch (NoSuchNamespaceException e) {
        throw new DatabaseNotExistException(getName(), databaseName, e);
      }
    }
  }

  @Override
  public boolean databaseExists(String databaseName) throws CatalogException {
    try {
      getDatabase(databaseName);
      return true;
    } catch (DatabaseNotExistException ignore) {
      return false;
    }
  }

  @Override
  public void createDatabase(String name, CatalogDatabase database, boolean ignoreIfExists)
      throws DatabaseAlreadyExistException, CatalogException {
    if (asNamespaceCatalog != null) {
      try {
        asNamespaceCatalog.createNamespace(
            toNamespace(name),
            mergeComment(database.getProperties(), database.getComment()));
      } catch (AlreadyExistsException e) {
        if (!ignoreIfExists) {
          throw new DatabaseAlreadyExistException(getName(), name, e);
        }
      }
    } else {
      throw new UnsupportedOperationException(""Namespaces are not supported by catalog: "" + getName());
    }
  }

  private Map<String, String> mergeComment(Map<String, String> metadata, String comment) {
    Map<String, String> ret = Maps.newHashMap(metadata);
    if (metadata.containsKey(""comment"")) {
      throw new CatalogException(""Database properties should not contain key: 'comment'."");
    }

    if (!StringUtils.isNullOrWhitespaceOnly(comment)) {
      ret.put(""comment"", comment);
    }
    return ret;
  }

  @Override
  public void dropDatabase(String name, boolean ignoreIfNotExists, boolean cascade)
      throws DatabaseNotExistException, DatabaseNotEmptyException, CatalogException {
    if (asNamespaceCatalog != null) {
      try {
        boolean success = asNamespaceCatalog.dropNamespace(toNamespace(name));
        if (!success && !ignoreIfNotExists) {
          throw new DatabaseNotExistException(getName(), name);
        }
      } catch (NoSuchNamespaceException e) {
        if (!ignoreIfNotExists) {
          throw new DatabaseNotExistException(getName(), name, e);
        }
      } catch (NamespaceNotEmptyException e) {
        throw new DatabaseNotEmptyException(getName(), name, e);
      }
    } else {
      if (!ignoreIfNotExists) {
        throw new DatabaseNotExistException(getName(), name);
      }
    }
  }

  @Override
  public void alterDatabase(String name, CatalogDatabase newDatabase, boolean ignoreIfNotExists)
      throws DatabaseNotExistException, CatalogException {
    if (asNamespaceCatalog != null) {
      Namespace namespace = toNamespace(name);
      Map<String, String> updates = Maps.newHashMap();
      Set<String> removals = Sets.newHashSet();

      try {
        Map<String, String> oldOptions = asNamespaceCatalog.loadNamespaceMetadata(namespace);
        Map<String, String> newOptions = mergeComment(newDatabase.getProperties(), newDatabase.getComment());

        for (String key : oldOptions.keySet()) {
          if (!newOptions.containsKey(key)) {
            removals.add(key);
          }
        }

        for (Map.Entry<String, String> entry : newOptions.entrySet()) {
          if (!entry.getValue().equals(oldOptions.get(entry.getKey()))) {
            updates.put(entry.getKey(), entry.getValue());
          }
        }

        if (!updates.isEmpty()) {
          asNamespaceCatalog.setProperties(namespace, updates);
        }

        if (!removals.isEmpty()) {
          asNamespaceCatalog.removeProperties(namespace, removals);
        }

      } catch (org.apache.iceberg.exceptions.NoSuchNamespaceException e) {
        if (!ignoreIfNotExists) {
          throw new DatabaseNotExistException(getName(), name, e);
        }
      }
    } else {
      if (getDefaultDatabase().equals(name)) {
        throw new CatalogException(
            ""Can not alter the default database when the iceberg catalog doesn't support namespaces."");
      }
      if (!ignoreIfNotExists) {
        throw new DatabaseNotExistException(getName(), name);
      }
    }
  }

  @Override
  public List<String> listTables(String databaseName) throws DatabaseNotExistException, CatalogException {
    try {
      return icebergCatalog.listTables(toNamespace(databaseName)).stream()
          .map(TableIdentifier::name)
          .collect(Collectors.toList());
    } catch (NoSuchNamespaceException e) {
      throw new DatabaseNotExistException(getName(), databaseName, e);
    }
  }

  @Override
  public CatalogTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {
    Table table = loadIcebergTable(tablePath);
    return toCatalogTable(table);
  }

  Table loadIcebergTable(ObjectPath tablePath) throws TableNotExistException {
    try {
      return icebergCatalog.loadTable(toIdentifier(tablePath));
    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {
      throw new TableNotExistException(getName(), tablePath, e);
    }
  }

  @Override
  public boolean tableExists(ObjectPath tablePath) throws CatalogException {
    return icebergCatalog.tableExists(toIdentifier(tablePath));
  }

  @Override
  public void dropTable(ObjectPath tablePath, boolean ignoreIfNotExists)
      throws TableNotExistException, CatalogException {
    try {
      icebergCatalog.dropTable(toIdentifier(tablePath));
    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {
      throw new TableNotExistException(getName(), tablePath, e);
    }
  }

  @Override
  public void renameTable(ObjectPath tablePath, String newTableName, boolean ignoreIfNotExists)
      throws TableNotExistException, TableAlreadyExistException, CatalogException {
    try {
      icebergCatalog.renameTable(
          toIdentifier(tablePath),
          toIdentifier(new ObjectPath(tablePath.getDatabaseName(), newTableName)));
    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {
      throw new TableNotExistException(getName(), tablePath, e);
    } catch (AlreadyExistsException e) {
      throw new TableAlreadyExistException(getName(), tablePath, e);
    }
  }

  @Override
  public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)
      throws CatalogException, TableAlreadyExistException {
    validateFlinkTable(table);

    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());
    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);

    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();
    String location = null;
    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {
      if (""location"".equalsIgnoreCase(entry.getKey())) {
        location = entry.getValue();
      } else {
        properties.put(entry.getKey(), entry.getValue());
      }
    }

    try {
      icebergCatalog.createTable(
          toIdentifier(tablePath),
          icebergSchema,
          spec,
          location,
          properties.build());
    } catch (AlreadyExistsException e) {
      throw new TableAlreadyExistException(getName(), tablePath, e);
    }
  }

  @Override
  public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)
      throws CatalogException, TableNotExistException {
    validateFlinkTable(newTable);
    Table icebergTable = loadIcebergTable(tablePath);
    CatalogTable table = toCatalogTable(icebergTable);

    // Currently, Flink SQL only support altering table properties.

    // For current Flink Catalog API, support for adding/removing/renaming columns cannot be done by comparing
    // CatalogTable instances, unless the Flink schema contains Iceberg column IDs.
    if (!table.getSchema().equals(newTable.getSchema())) {
      throw new UnsupportedOperationException(""Altering schema is not supported yet."");
    }

    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {
      throw new UnsupportedOperationException(""Altering partition keys is not supported yet."");
    }

    Map<String, String> oldOptions = table.getOptions();
    Map<String, String> setProperties = Maps.newHashMap();

    String setLocation = null;
    String setSnapshotId = null;
    String pickSnapshotId = null;

    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {
      String key = entry.getKey();
      String value = entry.getValue();

      if (Objects.equals(value, oldOptions.get(key))) {
        continue;
      }

      if (""location"".equalsIgnoreCase(key)) {
        setLocation = value;
      } else if (""current-snapshot-id"".equalsIgnoreCase(key)) {
        setSnapshotId = value;
      } else if (""cherry-pick-snapshot-id"".equalsIgnoreCase(key)) {
        pickSnapshotId = value;
      } else {
        setProperties.put(key, value);
      }
    }

    oldOptions.keySet().forEach(k -> {
      if (!newTable.getOptions().containsKey(k)) {
        setProperties.put(k, null);
      }
    });

    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);
  }

  private static void validateFlinkTable(CatalogBaseTable table) {
    Preconditions.checkArgument(table instanceof CatalogTable, ""The Table should be a CatalogTable."");

    TableSchema schema = table.getSchema();
    schema.getTableColumns().forEach(column -> {
      if (column.isGenerated()) {
        throw new UnsupportedOperationException(""Creating table with computed columns is not supported yet."");
      }
    });

    if (!schema.getWatermarkSpecs().isEmpty()) {
      throw new UnsupportedOperationException(""Creating table with watermark specs is not supported yet."");
    }

    if (schema.getPrimaryKey().isPresent()) {
      throw new UnsupportedOperationException(""Creating table with primary key is not supported yet."");
    }
  }

  private static PartitionSpec toPartitionSpec(List<String> partitionKeys, Schema icebergSchema) {
    PartitionSpec.Builder builder = PartitionSpec.builderFor(icebergSchema);
    partitionKeys.forEach(builder::identity);
    return builder.build();
  }

  private static List<String> toPartitionKeys(PartitionSpec spec, Schema icebergSchema) {
    List<String> partitionKeys = Lists.newArrayList();
    for (PartitionField field : spec.fields()) {
      if (field.transform().isIdentity()) {
        partitionKeys.add(icebergSchema.findColumnName(field.sourceId()));
      } else {
        // Not created by Flink SQL.
        // For compatibility with iceberg tables, return empty.
        // TODO modify this after Flink support partition transform.
        return Collections.emptyList();
      }
    }
    return partitionKeys;
  }

  private static void commitChanges(Table table, String setLocation, String setSnapshotId,
                                    String pickSnapshotId, Map<String, String> setProperties) {
    // don't allow setting the snapshot and picking a commit at the same time because order is ambiguous and choosing
    // one order leads to different results
    Preconditions.checkArgument(setSnapshotId == null || pickSnapshotId == null,
        ""Cannot set the current snapshot ID and cherry-pick snapshot changes"");

    if (setSnapshotId != null) {
      long newSnapshotId = Long.parseLong(setSnapshotId);
      table.manageSnapshots().setCurrentSnapshot(newSnapshotId).commit();
    }

    // if updating the table snapshot, perform that update first in case it fails
    if (pickSnapshotId != null) {
      long newSnapshotId = Long.parseLong(pickSnapshotId);
      table.manageSnapshots().cherrypick(newSnapshotId).commit();
    }

    Transaction transaction = table.newTransaction();

    if (setLocation != null) {
      transaction.updateLocation()
          .setLocation(setLocation)
          .commit();
    }

    if (!setProperties.isEmpty()) {
      UpdateProperties updateProperties = transaction.updateProperties();
      setProperties.forEach((k, v) -> {
        if (v == null) {
          updateProperties.remove(k);
        } else {
          updateProperties.set(k, v);
        }
      });
      updateProperties.commit();
    }

    transaction.commitTransaction();
  }

  static CatalogTable toCatalogTable(Table table) {
    TableSchema schema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));
    List<String> partitionKeys = toPartitionKeys(table.spec(), table.schema());

    // NOTE: We can not create a IcebergCatalogTable extends CatalogTable, because Flink optimizer may use
    // CatalogTableImpl to copy a new catalog table.
    // Let's re-loading table from Iceberg catalog when creating source/sink operators.
    // Iceberg does not have Table comment, so pass a null (Default comment value in Flink).
    return new CatalogTableImpl(schema, partitionKeys, table.properties(), null);
  }

  @Override
  public Optional<TableFactory> getTableFactory() {
    return Optional.of(new FlinkTableFactory(this));
  }

  CatalogLoader getCatalogLoader() {
    return catalogLoader;
  }

  Configuration getHadoopConf() {
    return this.hadoopConf;
  }

  // ------------------------------ Unsupported methods ---------------------------------------------

  @Override
  public List<String> listViews(String databaseName) throws CatalogException {
    return Collections.emptyList();
  }

  @Override
  public CatalogPartition getPartition(ObjectPath tablePath, CatalogPartitionSpec partitionSpec)
      throws CatalogException {
    throw new UnsupportedOperationException();
  }

  @Override
  public boolean partitionExists(ObjectPath tablePath, CatalogPartitionSpec partitionSpec) throws CatalogException {
    throw new UnsupportedOperationException();
  }

  @Override
  public void createPartition(ObjectPath tablePath, CatalogPartitionSpec partitionSpec, CatalogPartition partition,
      boolean ignoreIfExists) throws CatalogException {
    throw new UnsupportedOperationException();
  }

  @Override
  public void dropPartition(ObjectPath tablePath, CatalogPartitionSpec partitionSpec, boolean ignoreIfNotExists)
      throws CatalogException {
    throw new UnsupportedOperationException();
  }

  @Override
  public void alterPartition(ObjectPath tablePath, CatalogPartitionSpec partitionSpec, CatalogPartition newPartition,
      boolean ignoreIfNotExists) throws CatalogException {
    throw new UnsupportedOperationException();
  }

  @Override
  public List<String> listFunctions(String dbName) throws CatalogException {
    return Collections.emptyList();
  }

  @Override
  public CatalogFunction getFunction(ObjectPath functionPath) throws FunctionNotExistException, CatalogException {
    throw new FunctionNotExistException(getName(), functionPath);
  }

  @Override
  public boolean functionExists(ObjectPath functionPath) throws CatalogException {
    return false;
  }

  @Override
  public void createFunction(ObjectPath functionPath, CatalogFunction function, boolean ignoreIfExists)
      throws CatalogException {
    throw new UnsupportedOperationException();
  }

  @Override
  public void alterFunction(ObjectPath functionPath, CatalogFunction newFunction, boolean ignoreIfNotExists)
      throws CatalogException {
    throw new UnsupportedOperationException();
  }

  @Override
  public void dropFunction(ObjectPath functionPath, boolean ignoreIfNotExists)
      throws CatalogException {
    throw new UnsupportedOperationException();
  }

  @Override
  public void alterTableStatistics(ObjectPath tablePath, CatalogTableStatistics tableStatistics,
      boolean ignoreIfNotExists) throws CatalogException {
    throw new UnsupportedOperationException();
  }

  @Override
  public void alterTableColumnStatistics(ObjectPath tablePath, CatalogColumnStatistics columnStatistics,
      boolean ignoreIfNotExists) throws CatalogException {
    throw new UnsupportedOperationException();
  }

  @Override
  public void alterPartitionStatistics(ObjectPath tablePath, CatalogPartitionSpec partitionSpec,
      CatalogTableStatistics partitionStatistics, boolean ignoreIfNotExists) throws CatalogException {
    throw new UnsupportedOperationException();
  }

  @Override
  public void alterPartitionColumnStatistics(ObjectPath tablePath, CatalogPartitionSpec partitionSpec,
      CatalogColumnStatistics columnStatistics, boolean ignoreIfNotExists) throws CatalogException {
    throw new UnsupportedOperationException();
  }

  @Override
  public List<CatalogPartitionSpec> listPartitions(ObjectPath tablePath)
      throws CatalogException {
    throw new UnsupportedOperationException();
  }

  @Override
  public List<CatalogPartitionSpec> listPartitions(ObjectPath tablePath, CatalogPartitionSpec partitionSpec)
      throws CatalogException {
    throw new UnsupportedOperationException();
  }

  @Override
  public List<CatalogPartitionSpec> listPartitionsByFilter(ObjectPath tablePath, List<Expression> filters)
      throws CatalogException {
    throw new UnsupportedOperationException();
  }

  // After partition pruning and filter push down, the statistics have become very inaccurate, so the statistics from
  // here are of little significance.
  // Flink will support something like SupportsReportStatistics in future.

  @Override
  public CatalogTableStatistics getTableStatistics(ObjectPath tablePath)
      throws CatalogException {
    return CatalogTableStatistics.UNKNOWN;
  }

  @Override
  public CatalogColumnStatistics getTableColumnStatistics(ObjectPath tablePath)
      throws CatalogException {
    return CatalogColumnStatistics.UNKNOWN;
  }

  @Override
  public CatalogTableStatistics getPartitionStatistics(ObjectPath tablePath, CatalogPartitionSpec partitionSpec)
      throws CatalogException {
    return CatalogTableStatistics.UNKNOWN;
  }

  @Override
  public CatalogColumnStatistics getPartitionColumnStatistics(ObjectPath tablePath, CatalogPartitionSpec partitionSpec)
      throws CatalogException {
    return CatalogColumnStatistics.UNKNOWN;
  }
}
",Does this do what you want? It seems like if the intent is `IF NOT EXISTS` then you should ignore the `DatabaseAlreadyExistException`.,"This code provides a Flink Catalog implementation that wraps an Iceberg Catalog. It includes methods for managing databases, tables, and partitions. The code handles exceptions and ensures that operations are performed safely. The `toNamespace` method converts a Flink database name to an Iceberg namespace, and the `toIdentifier` method converts an object path to a table identifier. The `createTable` method creates a new Iceberg table, and the `alterTable` method allows for altering table properties. The code also includes methods for listing databases, tables, and functions, but these are currently unsupported."
83,"@@ -270,6 +270,9 @@ func (raw *cgroupData) path(subsystem string) (string, error) {
 	}
 
 	cgPath := filepath.Join(raw.parent, raw.name)
+	if raw.config.CgroupsPath != """" {
+		cgPath = raw.config.CgroupsPath
+	}
 	// If the cgroup name/path is absolute do not look relative to the cgroup of the init process.
 	if filepath.IsAbs(cgPath) {
 		// Sometimes subsystems can be mounted togethger as 'cpu,cpuacct'.","// +build linux

package fs

import (
	""errors""
	""fmt""
	""io""
	""io/ioutil""
	""os""
	""path/filepath""
	""strconv""
	""sync""

	""github.com/opencontainers/runc/libcontainer/cgroups""
	""github.com/opencontainers/runc/libcontainer/configs""
)

var (
	subsystems = subsystemSet{
		&CpusetGroup{},
		&DevicesGroup{},
		&MemoryGroup{},
		&CpuGroup{},
		&CpuacctGroup{},
		&BlkioGroup{},
		&HugetlbGroup{},
		&NetClsGroup{},
		&NetPrioGroup{},
		&PerfEventGroup{},
		&FreezerGroup{},
	}
	CgroupProcesses  = ""cgroup.procs""
	HugePageSizes, _ = cgroups.GetHugePageSize()
)

var errSubsystemDoesNotExist = errors.New(""cgroup: subsystem does not exist"")

type subsystemSet []subsystem

func (s subsystemSet) Get(name string) (subsystem, error) {
	for _, ss := range s {
		if ss.Name() == name {
			return ss, nil
		}
	}
	return nil, errSubsystemDoesNotExist
}

type subsystem interface {
	// Name returns the name of the subsystem.
	Name() string
	// Returns the stats, as 'stats', corresponding to the cgroup under 'path'.
	GetStats(path string, stats *cgroups.Stats) error
	// Removes the cgroup represented by 'cgroupData'.
	Remove(*cgroupData) error
	// Creates and joins the cgroup represented by 'cgroupData'.
	Apply(*cgroupData) error
	// Set the cgroup represented by cgroup.
	Set(path string, cgroup *configs.Cgroup) error
}

type Manager struct {
	mu      sync.Mutex
	Cgroups *configs.Cgroup
	Paths   map[string]string
}

// The absolute path to the root of the cgroup hierarchies.
var cgroupRootLock sync.Mutex
var cgroupRoot string

// Gets the cgroupRoot.
func getCgroupRoot() (string, error) {
	cgroupRootLock.Lock()
	defer cgroupRootLock.Unlock()

	if cgroupRoot != """" {
		return cgroupRoot, nil
	}

	root, err := cgroups.FindCgroupMountpointDir()
	if err != nil {
		return """", err
	}

	if _, err := os.Stat(root); err != nil {
		return """", err
	}

	cgroupRoot = root
	return cgroupRoot, nil
}

type cgroupData struct {
	root   string
	parent string
	name   string
	config *configs.Cgroup
	pid    int
}

func (m *Manager) Apply(pid int) (err error) {
	if m.Cgroups == nil {
		return nil
	}

	var c = m.Cgroups

	d, err := getCgroupData(m.Cgroups, pid)
	if err != nil {
		return err
	}

	paths := make(map[string]string)
	defer func() {
		if err != nil {
			cgroups.RemovePaths(paths)
		}
	}()
	for _, sys := range subsystems {
		if err := sys.Apply(d); err != nil {
			return err
		}
		// TODO: Apply should, ideally, be reentrant or be broken up into a separate
		// create and join phase so that the cgroup hierarchy for a container can be
		// created then join consists of writing the process pids to cgroup.procs
		p, err := d.path(sys.Name())
		if err != nil {
			if cgroups.IsNotFound(err) {
				continue
			}
			return err
		}
		paths[sys.Name()] = p
	}
	m.Paths = paths

	if paths[""cpu""] != """" {
		if err := CheckCpushares(paths[""cpu""], c.CpuShares); err != nil {
			return err
		}
	}

	return nil
}

func (m *Manager) Destroy() error {
	m.mu.Lock()
	defer m.mu.Unlock()
	if err := cgroups.RemovePaths(m.Paths); err != nil {
		return err
	}
	m.Paths = make(map[string]string)
	return nil
}

func (m *Manager) GetPaths() map[string]string {
	m.mu.Lock()
	paths := m.Paths
	m.mu.Unlock()
	return paths
}

func (m *Manager) GetStats() (*cgroups.Stats, error) {
	m.mu.Lock()
	defer m.mu.Unlock()
	stats := cgroups.NewStats()
	for name, path := range m.Paths {
		sys, err := subsystems.Get(name)
		if err == errSubsystemDoesNotExist || !cgroups.PathExists(path) {
			continue
		}
		if err := sys.GetStats(path, stats); err != nil {
			return nil, err
		}
	}
	return stats, nil
}

func (m *Manager) Set(container *configs.Config) error {
	for name, path := range m.Paths {
		sys, err := subsystems.Get(name)
		if err == errSubsystemDoesNotExist || !cgroups.PathExists(path) {
			continue
		}
		if err := sys.Set(path, container.Cgroups); err != nil {
			return err
		}
	}
	return nil
}

// Freeze toggles the container's freezer cgroup depending on the state
// provided
func (m *Manager) Freeze(state configs.FreezerState) error {
	d, err := getCgroupData(m.Cgroups, 0)
	if err != nil {
		return err
	}
	dir, err := d.path(""freezer"")
	if err != nil {
		return err
	}
	prevState := m.Cgroups.Freezer
	m.Cgroups.Freezer = state
	freezer, err := subsystems.Get(""freezer"")
	if err != nil {
		return err
	}
	err = freezer.Set(dir, m.Cgroups)
	if err != nil {
		m.Cgroups.Freezer = prevState
		return err
	}
	return nil
}

func (m *Manager) GetPids() ([]int, error) {
	d, err := getCgroupData(m.Cgroups, 0)
	if err != nil {
		return nil, err
	}

	dir, err := d.path(""devices"")
	if err != nil {
		return nil, err
	}

	return cgroups.GetPids(dir)
}

func getCgroupData(c *configs.Cgroup, pid int) (*cgroupData, error) {
	root, err := getCgroupRoot()
	if err != nil {
		return nil, err
	}

	return &cgroupData{
		root:   root,
		parent: c.Parent,
		name:   c.Name,
		config: c,
		pid:    pid,
	}, nil
}

func (raw *cgroupData) parentPath(subsystem, mountpoint, root string) (string, error) {
	// Use GetThisCgroupDir instead of GetInitCgroupDir, because the creating
	// process could in container and shared pid namespace with host, and
	// /proc/1/cgroup could point to whole other world of cgroups.
	initPath, err := cgroups.GetThisCgroupDir(subsystem)
	if err != nil {
		return """", err
	}
	// This is needed for nested containers, because in /proc/self/cgroup we
	// see pathes from host, which don't exist in container.
	relDir, err := filepath.Rel(root, initPath)
	if err != nil {
		return """", err
	}
	return filepath.Join(mountpoint, relDir), nil
}

func (raw *cgroupData) path(subsystem string) (string, error) {
	mnt, root, err := cgroups.FindCgroupMountpointAndRoot(subsystem)
	// If we didn't mount the subsystem, there is no point we make the path.
	if err != nil {
		return """", err
	if raw.config.CgroupsPath != """" {
		cgPath = raw.config.CgroupsPath
	}
	}

	cgPath := filepath.Join(raw.parent, raw.name)
	// If the cgroup name/path is absolute do not look relative to the cgroup of the init process.
	if filepath.IsAbs(cgPath) {
		// Sometimes subsystems can be mounted togethger as 'cpu,cpuacct'.
		return filepath.Join(raw.root, filepath.Base(mnt), cgPath), nil
	}

	parentPath, err := raw.parentPath(subsystem, mnt, root)
	if err != nil {
		return """", err
	}

	return filepath.Join(parentPath, cgPath), nil
}

func (raw *cgroupData) join(subsystem string) (string, error) {
	path, err := raw.path(subsystem)
	if err != nil {
		return """", err
	}
	if err := os.MkdirAll(path, 0755); err != nil {
		return """", err
	}
	if err := writeFile(path, CgroupProcesses, strconv.Itoa(raw.pid)); err != nil {
		return """", err
	}
	return path, nil
}

func writeFile(dir, file, data string) error {
	// Normally dir should not be empty, one case is that cgroup subsystem
	// is not mounted, we will get empty dir, and we want it fail here.
	if dir == """" {
		return fmt.Errorf(""no such directory for %s."", file)
	}
	return ioutil.WriteFile(filepath.Join(dir, file), []byte(data), 0700)
}

func readFile(dir, file string) (string, error) {
	data, err := ioutil.ReadFile(filepath.Join(dir, file))
	return string(data), err
}

func removePath(p string, err error) error {
	if err != nil {
		return err
	}
	if p != """" {
		return os.RemoveAll(p)
	}
	return nil
}

func CheckCpushares(path string, c int64) error {
	var cpuShares int64

	if c == 0 {
		return nil
	}

	fd, err := os.Open(filepath.Join(path, ""cpu.shares""))
	if err != nil {
		return err
	}
	defer fd.Close()

	_, err = fmt.Fscanf(fd, ""%d"", &cpuShares)
	if err != nil && err != io.EOF {
		return err
	}

	if c > cpuShares {
		return fmt.Errorf(""The maximum allowed cpu-shares is %d"", cpuShares)
	} else if c < cpuShares {
		return fmt.Errorf(""The minimum allowed cpu-shares is %d"", cpuShares)
	}

	return nil
}
","I am not sure we should just completely ignore parent and name if cgroupsPath is provided. Docker uses parent and name, and do we want to break compatibility? Also, cgroupsPath must always be relative to the cgroups mount on the system. I'm not sure if the same restriction applies to parent and name.","The suggested code changes appear to be related to managing cgroups (control groups) in a Linux environment. Here's a concise review:

### Key Changes:
1. **Subsystem Management**: 
   - Added a `subsystemSet` to manage different cgroup subsystems like CPU, Memory, etc.
   - Implemented a `Get` method to retrieve a subsystem by its name.

2. **Manager Structure**:
   - Introduced a `Manager` struct to handle cgroup operations, including applying configurations, destroying cgroups, and retrieving statistics.

3. **Cgroup Root Handling**:
   - Added a `cgroupRoot` variable to store the root path of cgroup hierarchies.
   - Implemented a `getCgroupRoot` function to safely retrieve the cgroup root path.

4. **Cgroup Data Structure**:
   - Defined a `cgroupData` struct to encapsulate cgroup-related information, including root, parent, name, configuration, and PID.

5. **Apply Method**:
   - Implemented the `Apply` method to create and join the cgroup, applying configurations and setting up necessary files like `cgroup.procs`.

6. **Destroy Method**:
   - Implemented the `Destroy` method to remove the cgroup hierarchy and clear paths.

7. **GetPaths Method**:
   - Provided a method to retrieve the current paths of the cgroup.

8. **GetStats Method**:
   - Implemented the `GetStats` method to gather statistics from all enabled subsystems.

9. **Set Method**:
   - Added the `Set` method to update cgroup settings based on a new configuration.

10. **Freeze Method**:
    - Implemented the `Freeze` method to toggle the freezer state of the cgroup.

11. **GetPids Method**:
    - Provided a method to retrieve the list of PIDs in the cgroup.

12. **Helper Functions**:
    - Added helper functions like `writeFile`, `readFile`, and `removePath` for file operations and cleanup.

### Overall Review:
- The code structure is well-organized, with clear separation of concerns for different aspects of cgroup management.
- Error handling is robust, with appropriate checks and error messages.
- The use of `sync.Mutex` ensures thread safety when accessing shared resources.
- The code is designed to be modular and extensible, allowing for easy addition of new subsystems or features.

### Potential Improvements:
- Consider adding more detailed"
84,"@@ -0,0 +1,13 @@
+package org.phoenicis.engines;
+
+/**
+ * interface which must be implemented by all Verbs in Javascript
+ */
+public interface Verb {
+    /**
+     * installs the Verb in the given container
+     * @param container directory name (not the complete path!) of the container where the Verb shall be installed
+     * @param version version of the Verb
+     */
+    void install(String container, String version);
+}","package org.phoenicis.engines;

/**
 * interface which must be implemented by all Verbs in Javascript
 */
public interface Verb {
    /**
     * installs the Verb in the given container
     * @param container directory name (not the complete path!) of the container where the Verb shall be installed
     * @param version version of the Verb
     */
    void install(String container, String version);
}

","What about a unique name? How does Phoenicis remember, whether a verb has been installed previously or not?","The provided code snippet defines an interface `Verb` with a single method `install`. This interface is intended to be implemented by classes that represent verbs in JavaScript. The method `install` takes two parameters: `container`, which is the name of the directory where the verb should be installed, and `version`, which specifies the version of the verb.

Here's a concise review of the suggested code change:

1. **Interface Definition**: The interface `Verb` is correctly defined with a single method `install`.
2. **Method Signature**: The method signature is clear and follows standard Java conventions.
3. **Parameter Documentation**: The parameters `container` and `version` are well-documented, explaining their purpose.
4. **Consistency**: The naming convention for the interface and method is consistent with typical Java naming conventions.

Overall, the code is clean and adheres to good programming practices. There are no significant issues or improvements needed."
85,"@@ -14,7 +14,7 @@ return [
 
     'reset'     => 'Паролата е нулирана!',
     'sent'      => 'Изпратено е напомняне за вашата парола!',
-    'throttled' => 'Please wait before retrying.',
+    'throttled' => 'Моля изчакайте преди да опитате отново.',
     'token'     => 'Този токен за нулиране на парола е невалиден.',
     'user'      => 'Потребител с такъв e-mail адрес не може да бъде открит.',
 ];","<?php

return [
    /*
    |--------------------------------------------------------------------------
    | Password Reminder Language Lines
    |--------------------------------------------------------------------------
    |
    | The following language lines are the default lines which match reasons
    | that are given by the password broker for a password update attempt
    | has failed, such as for an invalid token or invalid new password.
    |
    */
    'throttled' => 'Моля изчакайте преди да опитате отново.',

    'reset'     => 'Паролата е нулирана!',
    'sent'      => 'Изпратено е напомняне за вашата парола!',
    'throttled' => 'Please wait before retrying.',
    'token'     => 'Този токен за нулиране на парола е невалиден.',
    'user'      => 'Потребител с такъв e-mail адрес не може да бъде открит.',
];
","It should be:  e, e  e ","The suggested code change is to replace the duplicate ""throttled"" line with a single instance of ""Please wait before retrying."" This makes the code more concise and reduces redundancy."
86,"@@ -139,9 +139,11 @@ class GetItemStatuses extends AbstractBase implements TranslatorAwareInterface
     {
         $transList = [];
         foreach ($list as $current) {
-            $transList[] = $this->translate(
-                $transPrefix . $current, [], $current
-            );
+            $default = $current;
+            if (!($current instanceof \VuFind\I18n\TranslatableString)) {
+                $current = $transPrefix . $current;
+            }
+            $transList[] = $this->translate($current, [], $default);
         }
         return $transList;
     }","<?php
/**
 * ""Get Item Status"" AJAX handler
 *
 * PHP version 7
 *
 * Copyright (C) Villanova University 2018.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2,
 * as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 *
 * @category VuFind
 * @package  AJAX
 * @author   Demian Katz <demian.katz@villanova.edu>
 * @author   Chris Delis <cedelis@uillinois.edu>
 * @author   Tuan Nguyen <tuan@yorku.ca>
 * @license  http://opensource.org/licenses/gpl-2.0.php GNU General Public License
 * @link     https://vufind.org/wiki/development Wiki
 */
namespace VuFind\AjaxHandler;

use VuFind\Exception\ILS as ILSException;
use VuFind\I18n\Translator\TranslatorAwareInterface;
use VuFind\ILS\Connection;
use VuFind\ILS\Logic\Holds;
use VuFind\Session\Settings as SessionSettings;
use Zend\Config\Config;
use Zend\Mvc\Controller\Plugin\Params;
use Zend\View\Renderer\RendererInterface;

/**
 * ""Get Item Status"" AJAX handler
 *
 * This is responsible for printing the holdings information for a
 * collection of records in JSON format.
 *
 * @category VuFind
 * @package  AJAX
 * @author   Demian Katz <demian.katz@villanova.edu>
 * @author   Chris Delis <cedelis@uillinois.edu>
 * @author   Tuan Nguyen <tuan@yorku.ca>
 * @license  http://opensource.org/licenses/gpl-2.0.php GNU General Public License
 * @link     https://vufind.org/wiki/development Wiki
 */
class GetItemStatuses extends AbstractBase implements TranslatorAwareInterface
{
    use \VuFind\I18n\Translator\TranslatorAwareTrait;

    /**
     * Top-level configuration
     *
     * @var Config
     */
    protected $config;

    /**
     * ILS connection
     *
     * @var Connection
     */
    protected $ils;

    /**
     * View renderer
     *
     * @var RendererInterface
     */
    protected $renderer;

    /**
     * Holds logic
     *
     * @var Holds
     */
    protected $holdLogic;

    /**
     * Constructor
     *
     * @param SessionSettings   $ss        Session settings
     * @param Config            $config    Top-level configuration
     * @param Connection        $ils       ILS connection
     * @param RendererInterface $renderer  View renderer
     * @param Holds             $holdLogic Holds logic
     */
    public function __construct(SessionSettings $ss, Config $config, Connection $ils,
        RendererInterface $renderer, Holds $holdLogic
    ) {
        $this->sessionSettings = $ss;
        $this->config = $config;
        $this->ils = $ils;
        $this->renderer = $renderer;
        $this->holdLogic = $holdLogic;
    }

    /**
     * Support method for getItemStatuses() -- filter suppressed locations from the
     * array of item information for a particular bib record.
     *
     * @param array $record Information on items linked to a single bib record
     *
     * @return array        Filtered version of $record
     */
    protected function filterSuppressedLocations($record)
    {
        static $hideHoldings = false;
        if ($hideHoldings === false) {
            $hideHoldings = $this->holdLogic->getSuppressedLocations();
        }

        $filtered = [];
        foreach ($record as $current) {
            if (!in_array($current['location'] ?? null, $hideHoldings)) {
                $filtered[] = $current;
            }
        }
        return $filtered;
    }

    /**
     * Translate an array of strings using a prefix.
     *
     * @param string $transPrefix Translation prefix
     * @param array  $list        List of values to translate
     *
     * @return array
     */
    protected function translateList($transPrefix, $list)
            $default = $current;
            if (!($current instanceof \VuFind\I18n\TranslatableString)) {
                $current = $transPrefix . $current;
            }
            $transList[] = $this->translate($current, [], $default);
    {
        $transList = [];
        foreach ($list as $current) {
            $transList[] = $this->translate(
                $transPrefix . $current, [], $current
            );
        }
        return $transList;
    }

    /**
     * Support method for getItemStatuses() -- when presented with multiple values,
     * pick which one(s) to send back via AJAX.
     *
     * @param array  $rawList     Array of values to choose from.
     * @param string $mode        config.ini setting -- first, all or msg
     * @param string $msg         Message to display if $mode == ""msg""
     * @param string $transPrefix Translator prefix to apply to values (false to
     * omit translation of values)
     *
     * @return string
     */
    protected function pickValue($rawList, $mode, $msg, $transPrefix = false)
    {
        // Make sure array contains only unique values:
        $list = array_unique($rawList);

        // If there is only one value in the list, or if we're in ""first"" mode,
        // send back the first list value:
        if ($mode == 'first' || count($list) == 1) {
            return $transPrefix
                ? $this->translate($transPrefix . $list[0], [], $list[0])
                : $list[0];
        } elseif (count($list) == 0) {
            // Empty list?  Return a blank string:
            return '';
        } elseif ($mode == 'all') {
            // All values mode?  Return comma-separated values:
            return implode(
                "",\t"",
                $transPrefix ? $this->translateList($transPrefix, $list) : $list
            );
        } else {
            // Message mode?  Return the specified message, translated to the
            // appropriate language.
            return $this->translate($msg);
        }
    }

    /**
     * Based on settings and the number of callnumbers, return callnumber handler
     * Use callnumbers before pickValue is run.
     *
     * @param array  $list           Array of callnumbers.
     * @param string $displaySetting config.ini setting -- first, all or msg
     *
     * @return string
     */
    protected function getCallnumberHandler($list = null, $displaySetting = null)
    {
        if ($displaySetting == 'msg' && count($list) > 1) {
            return false;
        }
        return isset($this->config->Item_Status->callnumber_handler)
            ? $this->config->Item_Status->callnumber_handler
            : false;
    }

    /**
     * Reduce an array of service names to a human-readable string.
     *
     * @param array $rawServices Names of available services.
     *
     * @return string
     */
    protected function reduceServices(array $rawServices)
    {
        // Normalize, dedup and sort available services
        $normalize = function ($in) {
            return strtolower(preg_replace('/[^A-Za-z]/', '', $in));
        };
        $services = array_map($normalize, array_unique($rawServices));
        sort($services);

        // Do we need to deal with a preferred service?
        $preferred = isset($this->config->Item_Status->preferred_service)
            ? $normalize($this->config->Item_Status->preferred_service) : false;
        if (false !== $preferred && in_array($preferred, $services)) {
            $services = [$preferred];
        }

        return $this->renderer->render(
            'ajax/status-available-services.phtml',
            ['services' => $services]
        );
    }

    /**
     * Support method for getItemStatuses() -- process a single bibliographic record
     * for location settings other than ""group"".
     *
     * @param array  $record            Information on items linked to a single bib
     *                                  record
     * @param array  $messages          Custom status HTML
     *                                  (keys = available/unavailable)
     * @param string $locationSetting   The location mode setting used for
     *                                  pickValue()
     * @param string $callnumberSetting The callnumber mode setting used for
     *                                  pickValue()
     *
     * @return array                    Summarized availability information
     */
    protected function getItemStatus($record, $messages, $locationSetting,
        $callnumberSetting
    ) {
        // Summarize call number, location and availability info across all items:
        $callNumbers = $locations = [];
        $use_unknown_status = $available = false;
        $services = [];

        foreach ($record as $info) {
            // Find an available copy
            if ($info['availability']) {
                $available = true;
            }
            // Check for a use_unknown_message flag
            if (isset($info['use_unknown_message'])
                && $info['use_unknown_message'] == true
            ) {
                $use_unknown_status = true;
            }
            // Store call number/location info:
            $callNumbers[] = $info['callnumber'];
            $locations[] = $info['location'];
            // Store all available services
            if (isset($info['services'])) {
                $services = array_merge($services, $info['services']);
            }
        }

        $callnumberHandler = $this->getCallnumberHandler(
            $callNumbers, $callnumberSetting
        );

        // Determine call number string based on findings:
        $callNumber = $this->pickValue(
            $callNumbers, $callnumberSetting, 'Multiple Call Numbers'
        );

        // Determine location string based on findings:
        $location = $this->pickValue(
            $locations, $locationSetting, 'Multiple Locations', 'location_'
        );

        if (!empty($services)) {
            $availability_message = $this->reduceServices($services);
        } else {
            $availability_message = $use_unknown_status
                ? $messages['unknown']
                : $messages[$available ? 'available' : 'unavailable'];
        }

        // Send back the collected details:
        return [
            'id' => $record[0]['id'],
            'availability' => ($available ? 'true' : 'false'),
            'availability_message' => $availability_message,
            'location' => htmlentities($location, ENT_COMPAT, 'UTF-8'),
            'locationList' => false,
            'reserve' =>
                ($record[0]['reserve'] == 'Y' ? 'true' : 'false'),
            'reserve_message' => $record[0]['reserve'] == 'Y'
                ? $this->translate('on_reserve')
                : $this->translate('Not On Reserve'),
            'callnumber' => htmlentities($callNumber, ENT_COMPAT, 'UTF-8'),
            'callnumber_handler' => $callnumberHandler
        ];
    }

    /**
     * Support method for getItemStatuses() -- process a single bibliographic record
     * for ""group"" location setting.
     *
     * @param array  $record            Information on items linked to a single
     *                                  bib record
     * @param array  $messages          Custom status HTML
     *                                  (keys = available/unavailable)
     * @param string $callnumberSetting The callnumber mode setting used for
     *                                  pickValue()
     *
     * @return array                    Summarized availability information
     */
    protected function getItemStatusGroup($record, $messages, $callnumberSetting)
    {
        // Summarize call number, location and availability info across all items:
        $locations =  [];
        $use_unknown_status = $available = false;
        foreach ($record as $info) {
            // Find an available copy
            if ($info['availability']) {
                $available = $locations[$info['location']]['available'] = true;
            }
            // Check for a use_unknown_message flag
            if (isset($info['use_unknown_message'])
                && $info['use_unknown_message'] == true
            ) {
                $use_unknown_status = true;
                $locations[$info['location']]['status_unknown'] = true;
            }
            // Store call number/location info:
            $locations[$info['location']]['callnumbers'][] = $info['callnumber'];
        }

        // Build list split out by location:
        $locationList = false;
        foreach ($locations as $location => $details) {
            $locationCallnumbers = array_unique($details['callnumbers']);
            // Determine call number string based on findings:
            $callnumberHandler = $this->getCallnumberHandler(
                $locationCallnumbers, $callnumberSetting
            );
            $locationCallnumbers = $this->pickValue(
                $locationCallnumbers, $callnumberSetting, 'Multiple Call Numbers'
            );
            $locationInfo = [
                'availability' =>
                    $details['available'] ?? false,
                'location' => htmlentities(
                    $this->translate('location_' . $location, [], $location),
                    ENT_COMPAT, 'UTF-8'
                ),
                'callnumbers' =>
                    htmlentities($locationCallnumbers, ENT_COMPAT, 'UTF-8'),
                'status_unknown' => $details['status_unknown'] ?? false,
                'callnumber_handler' => $callnumberHandler
            ];
            $locationList[] = $locationInfo;
        }

        $availability_message = $use_unknown_status
            ? $messages['unknown']
            : $messages[$available ? 'available' : 'unavailable'];

        // Send back the collected details:
        return [
            'id' => $record[0]['id'],
            'availability' => ($available ? 'true' : 'false'),
            'availability_message' => $availability_message,
            'location' => false,
            'locationList' => $locationList,
            'reserve' =>
                ($record[0]['reserve'] == 'Y' ? 'true' : 'false'),
            'reserve_message' => $record[0]['reserve'] == 'Y'
                ? $this->translate('on_reserve')
                : $this->translate('Not On Reserve'),
            'callnumber' => false
        ];
    }

    /**
     * Support method for getItemStatuses() -- process a failed record.
     *
     * @param array  $record Information on items linked to a single bib record
     * @param string $msg    Availability message
     *
     * @return array Summarized availability information
     */
    protected function getItemStatusError($record, $msg = '')
    {
        return [
            'id' => $record[0]['id'],
            'error' => $this->translate($record[0]['error']),
            'availability' => false,
            'availability_message' => $msg,
            'location' => false,
            'locationList' => [],
            'reserve' => false,
            'reserve_message' => '',
            'callnumber' => false
        ];
    }

    /**
     * Handle a request.
     *
     * @param Params $params Parameter helper from controller
     *
     * @return array [response data, HTTP status code]
     */
    public function handleRequest(Params $params)
    {
        $this->disableSessionWrites();  // avoid session write timing bug
        $ids = $params->fromPost('id', $params->fromQuery('id', []));
        try {
            $results = $this->ils->getStatuses($ids);
        } catch (ILSException $e) {
            // If the ILS fails, send an error response instead of a fatal
            // error; we don't want to confuse the end user unnecessarily.
            error_log($e->getMessage());
            foreach ($ids as $id) {
                $results[] = [
                    [
                        'id' => $id,
                        'error' => 'An error has occurred'
                    ]
                ];
            }
        }

        if (!is_array($results)) {
            // If getStatuses returned garbage, let's turn it into an empty array
            // to avoid triggering a notice in the foreach loop below.
            $results = [];
        }

        // In order to detect IDs missing from the status response, create an
        // array with a key for every requested ID.  We will clear keys as we
        // encounter IDs in the response -- anything left will be problems that
        // need special handling.
        $missingIds = array_flip($ids);

        // Load messages for response:
        $messages = [
            'available' => $this->renderer->render('ajax/status-available.phtml'),
            'unavailable' =>
                $this->renderer->render('ajax/status-unavailable.phtml'),
            'unknown' => $this->renderer->render('ajax/status-unknown.phtml')
        ];

        // Load callnumber and location settings:
        $callnumberSetting = isset($this->config->Item_Status->multiple_call_nos)
            ? $this->config->Item_Status->multiple_call_nos : 'msg';
        $locationSetting = isset($this->config->Item_Status->multiple_locations)
            ? $this->config->Item_Status->multiple_locations : 'msg';
        $showFullStatus = isset($this->config->Item_Status->show_full_status)
            ? $this->config->Item_Status->show_full_status : false;

        // Loop through all the status information that came back
        $statuses = [];
        foreach ($results as $recordNumber => $record) {
            // Filter out suppressed locations:
            $record = $this->filterSuppressedLocations($record);

            // Skip empty records:
            if (count($record)) {
                // Check for errors
                if (!empty($record[0]['error'])) {
                    $current = $this
                        ->getItemStatusError($record, $messages['unknown']);
                } elseif ($locationSetting === 'group') {
                    $current = $this->getItemStatusGroup(
                        $record, $messages, $callnumberSetting
                    );
                } else {
                    $current = $this->getItemStatus(
                        $record, $messages, $locationSetting, $callnumberSetting
                    );
                }
                // If a full status display has been requested, append the HTML:
                if ($showFullStatus) {
                    $current['full_status'] = $this->renderer->render(
                        'ajax/status-full.phtml', [
                            'statusItems' => $record,
                            'callnumberHandler' => $this->getCallnumberHandler()
                         ]
                    );
                }
                $current['record_number'] = array_search($current['id'], $ids);
                $statuses[] = $current;

                // The current ID is not missing -- remove it from the missing list.
                unset($missingIds[$current['id']]);
            }
        }

        // If any IDs were missing, send back appropriate dummy data
        foreach ($missingIds as $missingId => $recordNumber) {
            $statuses[] = [
                'id'                   => $missingId,
                'availability'         => 'false',
                'availability_message' => $messages['unavailable'],
                'location'             => $this->translate('Unknown'),
                'locationList'         => false,
                'reserve'              => 'false',
                'reserve_message'      => $this->translate('Not On Reserve'),
                'callnumber'           => '',
                'missing_data'         => true,
                'record_number'        => $recordNumber
            ];
        }

        // Done
        return $this->formatResponse(compact('statuses'));
    }
}
",Can we simplify the code changes in this file by using translateWithPrefix now that we have defined that method? Should we move the TranslatableString awareness into that function as well?,"The suggested code change appears to be a PHP class implementing an AJAX handler for retrieving item statuses from an Integrated Library System (ILS). The class handles various aspects such as filtering suppressed locations, translating messages, and processing different location settings.

Here's a concise review of the suggested code change:

### Key Points:

1. **Class Structure**: The class `GetItemStatuses` extends `AbstractBase` and implements `TranslatorAwareInterface`. It uses traits for translation and session management.

2. **Configuration and Dependencies**: The class initializes with dependencies like `SessionSettings`, `Config`, `Connection`, `RendererInterface`, and `Holds`.

3. **Filtering Suppressed Locations**: The `filterSuppressedLocations` method filters out locations marked as suppressed.

4. **Translation Methods**: The `translateList` and `translate` methods handle translation of lists and individual strings, respectively.

5. **Value Selection Logic**: The `pickValue` method selects a value based on the mode (first, all, or message).

6. **Callnumber Handler**: The `getCallnumberHandler` method determines how to handle call numbers based on configuration settings.

7. **Location Reduction**: The `reduceServices` method reduces available services into a human-readable string.

8. **Item Status Processing**: The `getItemStatus` and `getItemStatusGroup` methods summarize item status information based on location settings. The `getItemStatusError` method handles failed requests.

9. **Handling Results**: The `handleRequest` method processes the ILS status results, handles missing IDs, and formats the response.

### Potential Improvements:

1. **Error Handling**: Consider adding more detailed error handling, especially for network issues or ILS-specific errors.

2. **Performance Optimization**: Optimize the code for performance, especially if dealing with large datasets.

3. **Security**: Ensure that the code is secure against common vulnerabilities, such as SQL injection or cross-site scripting (XSS).

4. **Documentation**: Add more comments and documentation to explain complex logic and configuration options.

5. **Testing**: Write unit tests to ensure the class behaves as expected under various scenarios.

Overall, the code is well-structured and follows best practices for handling AJAX requests in PHP."
87,"@@ -46,13 +46,13 @@ export function createMethod(type, name, mutates) {
             throw new TypeError(name + ' method was called on an object of the wrong type!');
         }
 
-        let result = rpc.callMethod(realmId, id, name, Array.from(arguments));
-
-        if (mutates) {
-            fireMutationListeners(realmId);
+        try {
+            return rpc.callMethod(realmId, id, name, Array.from(arguments));
+        } finally {
+            if (mutates) {
+                fireMutationListeners(realmId);
+            }
         }
-
-        return result;
     };
 }
 ","////////////////////////////////////////////////////////////////////////////
//
// Copyright 2016 Realm Inc.
//
// Licensed under the Apache License, Version 2.0 (the ""License"");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an ""AS IS"" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
////////////////////////////////////////////////////////////////////////////

'use strict';

import { fireMutationListeners } from './collections';
import { keys } from './constants';
import * as rpc from './rpc';

export function createMethods(prototype, type, methodNames, mutates) {
    let props = {};

    methodNames.forEach((name) => {
        props[name] = {
            value: createMethod(type, name, mutates),
        };
    });

    Object.defineProperties(prototype, props);
}

export function createMethod(type, name, mutates) {
    return function() {
        let realmId = this[keys.realm];
        let id = this[keys.id];

        if (!realmId || !id) {
            throw new TypeError(name + ' method was not called a Realm object!');
        }
        if (this[keys.type] !== type) {
        try {
            return rpc.callMethod(realmId, id, name, Array.from(arguments));
        } finally {
            if (mutates) {
                fireMutationListeners(realmId);
            }
            throw new TypeError(name + ' method was called on an object of the wrong type!');
        }

        let result = rpc.callMethod(realmId, id, name, Array.from(arguments));

        if (mutates) {
            fireMutationListeners(realmId);
        }

        return result;
    };
}

export function getterForProperty(name) {
    return function() {
        return rpc.getProperty(this[keys.realm], this[keys.id], name);
    };
}

export function setterForProperty(name) {
    return function(value) {
        let realmId = this[keys.realm];

        rpc.setProperty(realmId, this[keys.id], name, value);

        // If this isn't a primitive value, then it might create a new object in the Realm.
        if (value && typeof value == 'object') {
            fireMutationListeners(realmId);
        }
    };
}
",what gets returns if this throws?,"The suggested code changes appear to be part of a Realm JavaScript implementation, specifically related to creating methods and properties for objects within a Realm database. Here's a concise review of the changes:

1. **createMethods Function**: This function takes a prototype, a type, a list of method names, and a boolean indicating whether the methods mutate the Realm. It defines these methods on the prototype.

2. **createMethod Function**: This function creates a method that calls a remote method on the Realm server using the provided arguments. It checks if the object is a valid Realm object and throws an error if not. If the method mutates the Realm, it fires mutation listeners after the call.

3. **getterForProperty Function**: This function creates a getter for a property, which retrieves the property value from the Realm server.

4. **setterForProperty Function**: This function creates a setter for a property, which updates the property value on the Realm server. If the new value is an object, it fires mutation listeners to ensure the Realm is updated.

Overall, these functions help manage interactions between JavaScript objects and the Realm database, ensuring that operations are performed correctly and efficiently."
88,"@@ -12,8 +12,7 @@ import (
 
 // To attempts rewrite. It attempts to rewrite to first valid path
 // or the last path if none of the paths are valid.
-// Returns true if rewrite is successful and false otherwise.
-func To(fs http.FileSystem, r *http.Request, to string, replacer httpserver.Replacer) Result {
+func To(fs http.FileSystem, r *http.Request, to string, replacer httpserver.Replacer) (result Result) {
 	tos := strings.Fields(to)
 
 	// try each rewrite paths","package rewrite

import (
	""log""
	""net/http""
	""net/url""
	""path""
	""strings""

	""github.com/mholt/caddy/caddyhttp/httpserver""
)
func To(fs http.FileSystem, r *http.Request, to string, replacer httpserver.Replacer) (result Result) {

// To attempts rewrite. It attempts to rewrite to first valid path
// or the last path if none of the paths are valid.
// Returns true if rewrite is successful and false otherwise.
func To(fs http.FileSystem, r *http.Request, to string, replacer httpserver.Replacer) Result {
	tos := strings.Fields(to)

	// try each rewrite paths
	t := """"
	query := """"
	for _, v := range tos {
		t = replacer.Replace(v)
		tparts := strings.SplitN(t, ""?"", 2)
		t = path.Clean(tparts[0])

		if len(tparts) > 1 {
			query = tparts[1]
		}

		// add trailing slash for directories, if present
		if strings.HasSuffix(tparts[0], ""/"") && !strings.HasSuffix(t, ""/"") {
			t += ""/""
		}

		// validate file
		if validFile(fs, t) {
			break
		}
	}

	// validate resulting path
	u, err := url.Parse(t)
	if err != nil {
		// Let the user know we got here. Rewrite is expected but
		// the resulting url is invalid.
		log.Printf(""[ERROR] rewrite: resulting path '%v' is invalid. error: %v"", t, err)
		return RewriteIgnored
	}

	// take note of this rewrite for internal use by fastcgi
	// all we need is the URI, not full URL
	r.Header.Set(headerFieldName, r.URL.RequestURI())

	// perform rewrite
	r.URL.Path = u.Path
	if query != """" {
		// overwrite query string if present
		r.URL.RawQuery = query
	}
	if u.Fragment != """" {
		// overwrite fragment if present
		r.URL.Fragment = u.Fragment
	}

	return RewriteDone
}

// validFile checks if file exists on the filesystem.
// if file ends with `/`, it is validated as a directory.
func validFile(fs http.FileSystem, file string) bool {
	if fs == nil {
		return false
	}

	f, err := fs.Open(file)
	if err != nil {
		return false
	}
	defer f.Close()

	stat, err := f.Stat()
	if err != nil {
		return false
	}

	// directory
	if strings.HasSuffix(file, ""/"") {
		return stat.IsDir()
	}

	// file
	return !stat.IsDir()
}
",I typically try to avoid named returns; I don't really know what we gain from this one here.,"The suggested code change introduces a function `To` within the `rewrite` package that handles URL rewriting based on a set of predefined paths. The function iterates over the provided rewrite paths, replacing placeholders using a `httpserver.Replacer`, and validates each resulting path against the filesystem. If a valid path is found, it updates the request's URL accordingly. The function also logs errors related to invalid URLs and sets a header for internal use by FastCGI.

Key points:
- **Function Signature**: The function `To` takes a `http.FileSystem`, a `*http.Request`, a `string` for the rewrite paths, and a `httpserver.Replacer`.
- **Path Handling**: It splits the rewrite paths into individual components, replaces placeholders, and cleans the path using `path.Clean`.
- **Validation**: It checks if the resulting path is a valid file or directory using the `validFile` helper function.
- **Error Handling**: Logs errors related to invalid URLs.
- **Header Setting**: Sets a header for internal use by FastCGI.

This implementation provides a robust way to handle URL rewriting in Caddy, ensuring that the rewritten paths are valid and correctly updated in the request."
89,"@@ -170,7 +170,11 @@ class UIATextRangeAttributeValueFetcher(object):
 		self.textRange=textRange
 
 	def getValue(self,ID,ignoreMixedValues=False):
-		val=self.textRange.getAttributeValue(ID)
+		try:
+			val=self.textRange.getAttributeValue(ID)
+		except COMError:
+			# #7124: some text attributes are not supported in  older Operating Systems 
+			return UIAHandler.handler.reservedNotSupportedValue
 		if not ignoreMixedValues and val==UIAHandler.handler.ReservedMixedAttributeValue:
 			raise UIAMixedAttributeError
 		return val","#A part of NonVisual Desktop Access (NVDA)
#Copyright (C) 2015-2016 NV Access Limited
#This file is covered by the GNU General Public License.
#See the file COPYING for more details.

from comtypes import COMError
import ctypes
import UIAHandler

def createUIAMultiPropertyCondition(*dicts):
	""""""
	A helper function that Creates a complex UI Automation Condition matching on various UI Automation properties with both 'and' and 'or'.
	Arguments to this function are dicts whos keys are UI Automation property IDs, and whos values are a list of possible values for the property ID.
	The dicts are joined with 'or', the keys in each dict are joined with 'and', and the values  for each key are joined with 'or'.
	For example,  to create a condition that matches on a controlType of button or edit and where isReadOnly is True, or, className is 'ding', you would provide arguments of:
	{UIA_ControlTypePropertyId:[UIA_ButtonControlTypeId,UIA_EditControlTypeId],UIA_Value_ValueIsReadOnly:[True]},{UIA_ClassNamePropertyId:['ding']}
	""""""
	outerOrList=[]
	for dict in dicts:
		andList=[]
		for key,values in dict.iteritems():
			innerOrList=[]
			if not isinstance(values,(list,set)):
				values=[values]
			for value in values:
				condition=UIAHandler.handler.clientObject.createPropertyCondition(key,value)
				innerOrList.append(condition)
			if len(innerOrList)==0:
				continue
			elif len(innerOrList)==1:
				condition=innerOrList[0]
			else:
				condition=UIAHandler.handler.clientObject.createOrConditionFromArray(innerOrList)
			andList.append(condition)
		if len(andList)==0:
			continue
		elif len(andList)==1:
			condition=andList[0]
		else:
			condition=UIAHandler.handler.clientObject.createAndConditionFromArray(andList)
		outerOrList.append(condition)
	if len(outerOrList)==0:
		raise ValueError(""no properties"")
	elif len(outerOrList)==1:
		condition=outerOrList[0]
	else:
		condition=UIAHandler.handler.clientObject.createOrConditionFromArray(outerOrList)
	return condition

def UIATextRangeFromElement(documentTextPattern,element):
	""""""Wraps IUIAutomationTextRange::getEnclosingElement, returning None on  COMError.""""""
	try:
		childRange=documentTextPattern.rangeFromChild(element)
	except COMError:
		childRange=None
	return childRange

def isUIAElementInWalker(element,walker):
		""""""
		Checks if the given IUIAutomationElement exists in the given IUIAutomationTreeWalker by calling IUIAutomationTreeWalker::normalizeElement and comparing the fetched element with the given element.
		""""""
		try:
			newElement=walker.normalizeElement(element)
		except COMError:
			newElement=None
		return newElement and UIAHandler.handler.clientObject.compareElements(element,newElement)

def getDeepestLastChildUIAElementInWalker(element,walker):
	""""""
	Starting from the given IUIAutomationElement, walks to the deepest last child of the given IUIAutomationTreeWalker.
	""""""
	descended=False
	while True:
		lastChild=walker.getLastChildElement(element)
		if lastChild:
			descended=True
			element=lastChild
		else:
			break
		return element if descended else None

class UIAMixedAttributeError(ValueError):
	""""""Raised when a function would return a UIAutomation text attribute value that is mixed.""""""
	pass

def getUIATextAttributeValueFromRange(range,attrib,ignoreMixedValues=False):
	""""""
	Wraps IUIAutomationTextRange::getAttributeValue, returning UIAutomation's reservedNotSupportedValue on COMError, and raising UIAMixedAttributeError if a mixed value would be returned and ignoreMixedValues is False.
	""""""
	try:
		val=range.GetAttributeValue(attrib)
	except COMError:
		return UIAHandler.handler.reservedNotSupportedValue
	if val==UIAHandler.handler.ReservedMixedAttributeValue:
		if not ignoreMixedValues:
			raise UIAMixedAttributeError
	return val

def iterUIARangeByUnit(rangeObj,unit):
	""""""
	Splits a given UI Automation text range into smaller text ranges the size of the given unit and yields them.
	@param rangeObj: the UI Automation text range to split.
	@type rangeObj: L{UIAHandler.IUIAutomationTextRange}
	@param unit: a UI Automation text unit.
	@rtype: a generator that yields L{UIAHandler.IUIAutomationTextRange} objects.
	""""""
	tempRange=rangeObj.clone()
	tempRange.MoveEndpointByRange(UIAHandler.TextPatternRangeEndpoint_End,rangeObj,UIAHandler.TextPatternRangeEndpoint_Start)
	endRange=tempRange.Clone()
	while endRange.Move(unit,1)>0:
		tempRange.MoveEndpointByRange(UIAHandler.TextPatternRangeEndpoint_End,endRange,UIAHandler.TextPatternRangeEndpoint_Start)
		pastEnd=tempRange.CompareEndpoints(UIAHandler.TextPatternRangeEndpoint_End,rangeObj,UIAHandler.TextPatternRangeEndpoint_End)>0
		if pastEnd:
			tempRange.MoveEndpointByRange(UIAHandler.TextPatternRangeEndpoint_End,rangeObj,UIAHandler.TextPatternRangeEndpoint_End)
		yield tempRange.clone()
		if pastEnd:
			return
		tempRange.MoveEndpointByRange(UIAHandler.TextPatternRangeEndpoint_Start,tempRange,UIAHandler.TextPatternRangeEndpoint_End)
	# Ensure that we always reach the end of the outer range, even if the units seem to stop somewhere inside
	if tempRange.CompareEndpoints(UIAHandler.TextPatternRangeEndpoint_End,rangeObj,UIAHandler.TextPatternRangeEndpoint_End)<0:
		tempRange.MoveEndpointByRange(UIAHandler.TextPatternRangeEndpoint_End,rangeObj,UIAHandler.TextPatternRangeEndpoint_End)
		yield tempRange.clone()

def getEnclosingElementWithCacheFromUIATextRange(textRange,cacheRequest):
	""""""A thin wrapper around IUIAutomationTextRange3::getEnclosingElementBuildCache if it exists, otherwise IUIAutomationTextRange::getEnclosingElement and then IUIAutomationElement::buildUpdatedCache.""""""
	if not isinstance(textRange,UIAHandler.IUIAutomationTextRange):
		raise ValueError(""%s is not a text range""%textRange)
	try:
		textRange=textRange.QueryInterface(UIAHandler.IUIAutomationTextRange3)
	except (COMError,AttributeError):
		e=textRange.getEnclosingElement()
		if e:
			e=e.buildUpdatedCache(cacheRequest)
		return e
	return textRange.getEnclosingElementBuildCache(cacheRequest)

class CacheableUIAElementArray(object):

	def __init__(self,elementArray,cacheRequest=None):
		self._elementArray=elementArray
		self._cacheRequest=cacheRequest

	@property
	def length(self):
		return self._elementArray.length if self._elementArray else 0

	def getElement(self,index):
		e=self._elementArray.getElement(index)
		if e and self._cacheRequest:
			e=e.buildUpdatedCache(self._cacheRequest)
		return e

def getChildrenWithCacheFromUIATextRange(textRange,cacheRequest):
	""""""A thin wrapper around IUIAutomationTextRange3::getChildrenBuildCache if it exists, otherwise IUIAutomationTextRange::getChildren but wraps the result in an object that automatically calls IUIAutomationElement::buildUpdateCache on any element retreaved.""""""
	if not isinstance(textRange,UIAHandler.IUIAutomationTextRange):
		raise ValueError(""%s is not a text range""%textRange)
	try:
		textRange=textRange.QueryInterface(UIAHandler.IUIAutomationTextRange3)
	except (COMError,AttributeError):
		c=textRange.getChildren()
		c=CacheableUIAElementArray(c,cacheRequest)
		return c
	c=textRange.getChildrenBuildCache(cacheRequest)
	c=CacheableUIAElementArray(c)
	return c

class UIATextRangeAttributeValueFetcher(object):

	def __init__(self,textRange):
		try:
			val=self.textRange.getAttributeValue(ID)
		except COMError:
			# #7124: some text attributes are not supported in  older Operating Systems 
			return UIAHandler.handler.reservedNotSupportedValue
		self.textRange=textRange

	def getValue(self,ID,ignoreMixedValues=False):
		val=self.textRange.getAttributeValue(ID)
		if not ignoreMixedValues and val==UIAHandler.handler.ReservedMixedAttributeValue:
			raise UIAMixedAttributeError
		return val

class BulkUIATextRangeAttributeValueFetcher(UIATextRangeAttributeValueFetcher):

	def __init__(self,textRange,IDs):
		IDs=list(IDs)
		self.IDsToValues={}
		super(BulkUIATextRangeAttributeValueFetcher,self).__init__(textRange)
		IDsArray=(ctypes.c_long*len(IDs))(*IDs)
		values=textRange.GetAttributeValues(IDsArray,len(IDsArray))
		self.IDsToValues={IDs[x]:values[x] for x in xrange(len(IDs))}

	def getValue(self,ID,ignoreMixedValues=False):
		val=self.IDsToValues[ID]
		if not ignoreMixedValues and val==UIAHandler.handler.ReservedMixedAttributeValue:
			raise UIAMixedAttributeError
		return val
","Is this something that would be helpful to log? Would that cause a lot of noise in the log? I'm considering a case where we are investigating why something does not work in a particular OS for instance. What happens when `reservedNotSupportedValue` is returned, is this handled gracefully in all cases?","The suggested code changes appear to be related to enhancing the functionality of the NVDA (NonVisual Desktop Access) library, which is used for accessibility support in Windows applications. The changes include:

1. **`createUIAMultiPropertyCondition`**: This function creates a complex UI Automation Condition that matches on multiple properties with logical 'and' and 'or' operations. It takes multiple dictionaries as input, where each dictionary maps a UI Automation property ID to a list of possible values.

2. **`UIATextRangeFromElement`**: This function wraps `IUIAutomationTextRange::getEnclosingElement` and returns `None` if a `COMError` occurs.

3. **`isUIAElementInWalker`**: This function checks if a given UI Automation element exists in a specified tree walker by normalizing the element and comparing it to the original element.

4. **`getDeepestLastChildUIAElementInWalker`**: This function walks through a tree walker starting from a given element and finds the deepest last child element.

5. **`UIAMixedAttributeError`**: A custom exception raised when a function returns a mixed UI Automation text attribute value.

6. **`getUIATextAttributeValueFromRange`**: This function retrieves a text attribute value from a UI Automation text range, handling `COMError` and raising `UIAMixedAttributeError` if necessary.

7. **`iterUIARangeByUnit`**: This generator splits a UI Automation text range into smaller ranges based on a specified unit.

8. **`getEnclosingElementWithCacheFromUIATextRange`**: A wrapper around `IUIAutomationTextRange3::getEnclosingElementBuildCache` if available, otherwise using `getEnclosingElement` followed by `buildUpdatedCache`.

9. **`CacheableUIAElementArray`**: A class that wraps an array of UI Automation elements and provides a method to build updated caches for each element.

10. **`getChildrenWithCacheFromUIATextRange`**: A wrapper around `IUIAutomationTextRange3::getChildrenBuildCache` if available, otherwise using `getChildren` and wrapping the result in a cacheable array.

11. **`UIATextRangeAttributeValueFetcher`**: A base class for fetching attribute values from a UI Automation text range.

12. **`BulkUIATextRangeAttributeValueFetcher`**: A subclass of `UIATextRangeAttributeValueFetcher` that fetches multiple attribute values at once"
90,"@@ -3423,10 +3423,11 @@ bool CoreChecks::ValidateLayoutVsAttachmentDescription(const debug_report_data *
                                                        const VkImageLayout first_layout, const uint32_t attachment,
                                                        const VkAttachmentDescription2KHR &attachment_description) const {
     bool skip = false;
-    const char *vuid;
     const bool use_rp2 = (rp_version == RENDER_PASS_VERSION_2);
 
     // Verify that initial loadOp on READ_ONLY attachments is not CLEAR
+    // for both loadOp and stencilLoaOp rp2 has it in 1 VU while rp1 has it in 2 VU with half behind Maintenance2 extension
+    // Each is VUID is below in following order: rp2 -> rp1 with Maintenance2 -> rp1 with no extenstion
     if (attachment_description.loadOp == VK_ATTACHMENT_LOAD_OP_CLEAR) {
         if (use_rp2 && ((first_layout == VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL) ||
                         (first_layout == VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL) ||","            ss << ""vkCmdCopyImage(): Dest image aspect mask (0x"" << std::hex << aspect << "") is invalid for 2-plane format"";
            skip |= LogError(command_buffer, ""VUID-VkImageCopy-dstImage-01554"", ""%s."", ss.str().c_str());
        }
        if ((3 == planes) && (aspect != VK_IMAGE_ASPECT_PLANE_0_BIT_KHR) && (aspect != VK_IMAGE_ASPECT_PLANE_1_BIT_KHR) &&
            (aspect != VK_IMAGE_ASPECT_PLANE_2_BIT_KHR)) {
            std::stringstream ss;
            ss << ""vkCmdCopyImage(): Dest image aspect mask (0x"" << std::hex << aspect << "") is invalid for 3-plane format"";
            skip |= LogError(command_buffer, ""VUID-VkImageCopy-dstImage-01555"", ""%s."", ss.str().c_str());
        }
        // Multi-plane to single-plane
        if ((FormatIsMultiplane(src_image_state->createInfo.format)) && (!FormatIsMultiplane(dst_image_state->createInfo.format)) &&
            (VK_IMAGE_ASPECT_COLOR_BIT != aspect)) {
            std::stringstream ss;
            ss << ""vkCmdCopyImage(): Dest image aspect mask (0x"" << std::hex << aspect << "") is not VK_IMAGE_ASPECT_COLOR_BIT"";
            skip |= LogError(command_buffer, ""VUID-VkImageCopy-srcImage-01556"", ""%s."", ss.str().c_str());
        }
    }

    return skip;
}

bool CoreChecks::PreCallValidateCmdCopyImage(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,
                                             VkImage dstImage, VkImageLayout dstImageLayout, uint32_t regionCount,
                                             const VkImageCopy *pRegions) const {
    const auto *cb_node = GetCBState(commandBuffer);
    const auto *src_image_state = GetImageState(srcImage);
    const auto *dst_image_state = GetImageState(dstImage);
    const VkFormat src_format = src_image_state->createInfo.format;
    const VkFormat dst_format = dst_image_state->createInfo.format;
    bool skip = false;

    skip = ValidateImageCopyData(regionCount, pRegions, src_image_state, dst_image_state);

    VkCommandBuffer command_buffer = cb_node->commandBuffer;

    for (uint32_t i = 0; i < regionCount; i++) {
        const VkImageCopy region = pRegions[i];

        // For comp/uncomp copies, the copy extent for the dest image must be adjusted
        VkExtent3D src_copy_extent = region.extent;
        VkExtent3D dst_copy_extent = GetAdjustedDestImageExtent(src_format, dst_format, region.extent);

        bool slice_override = false;
        uint32_t depth_slices = 0;

        // Special case for copying between a 1D/2D array and a 3D image
        // TBD: This seems like the only way to reconcile 3 mutually-exclusive VU checks for 2D/3D copies. Heads up.
        if ((VK_IMAGE_TYPE_3D == src_image_state->createInfo.imageType) &&
            (VK_IMAGE_TYPE_3D != dst_image_state->createInfo.imageType)) {
            depth_slices = region.dstSubresource.layerCount;  // Slice count from 2D subresource
            slice_override = (depth_slices != 1);
        } else if ((VK_IMAGE_TYPE_3D == dst_image_state->createInfo.imageType) &&
                   (VK_IMAGE_TYPE_3D != src_image_state->createInfo.imageType)) {
            depth_slices = region.srcSubresource.layerCount;  // Slice count from 2D subresource
            slice_override = (depth_slices != 1);
        }

        skip |= ValidateImageSubresourceLayers(cb_node, &region.srcSubresource, ""vkCmdCopyImage"", ""srcSubresource"", i);
        skip |= ValidateImageSubresourceLayers(cb_node, &region.dstSubresource, ""vkCmdCopyImage"", ""dstSubresource"", i);
        skip |= ValidateImageMipLevel(cb_node, src_image_state, region.srcSubresource.mipLevel, i, ""vkCmdCopyImage"",
                                      ""srcSubresource"", ""VUID-vkCmdCopyImage-srcSubresource-01696"");
        skip |= ValidateImageMipLevel(cb_node, dst_image_state, region.dstSubresource.mipLevel, i, ""vkCmdCopyImage"",
                                      ""dstSubresource"", ""VUID-vkCmdCopyImage-dstSubresource-01697"");
        skip |= ValidateImageArrayLayerRange(cb_node, src_image_state, region.srcSubresource.baseArrayLayer,
                                             region.srcSubresource.layerCount, i, ""vkCmdCopyImage"", ""srcSubresource"",
                                             ""VUID-vkCmdCopyImage-srcSubresource-01698"");
        skip |= ValidateImageArrayLayerRange(cb_node, dst_image_state, region.dstSubresource.baseArrayLayer,
                                             region.dstSubresource.layerCount, i, ""vkCmdCopyImage"", ""dstSubresource"",
                                             ""VUID-vkCmdCopyImage-dstSubresource-01699"");

        if (device_extensions.vk_khr_maintenance1) {
            // No chance of mismatch if we're overriding depth slice count
            if (!slice_override) {
                // The number of depth slices in srcSubresource and dstSubresource must match
                // Depth comes from layerCount for 1D,2D resources, from extent.depth for 3D
                uint32_t src_slices =
                    (VK_IMAGE_TYPE_3D == src_image_state->createInfo.imageType ? src_copy_extent.depth
                                                                               : region.srcSubresource.layerCount);
                uint32_t dst_slices =
                    (VK_IMAGE_TYPE_3D == dst_image_state->createInfo.imageType ? dst_copy_extent.depth
                                                                               : region.dstSubresource.layerCount);
                if (src_slices != dst_slices) {
                    std::stringstream ss;
                    ss << ""vkCmdCopyImage(): number of depth slices in source and destination subresources for pRegions["" << i
                       << ""] do not match"";
                    skip |= LogError(command_buffer, ""VUID-VkImageCopy-extent-00140"", ""%s."", ss.str().c_str());
                }
            }
        } else {
            // For each region the layerCount member of srcSubresource and dstSubresource must match
            if (region.srcSubresource.layerCount != region.dstSubresource.layerCount) {
                std::stringstream ss;
                ss << ""vkCmdCopyImage(): number of layers in source and destination subresources for pRegions["" << i
                   << ""] do not match"";
                skip |= LogError(command_buffer, ""VUID-VkImageCopy-extent-00140"", ""%s."", ss.str().c_str());
            }
        }

        // Do multiplane-specific checks, if extension enabled
        if (device_extensions.vk_khr_sampler_ycbcr_conversion) {
            skip |= CopyImageMultiplaneValidation(command_buffer, src_image_state, dst_image_state, region);
        }

        if (!device_extensions.vk_khr_sampler_ycbcr_conversion) {
            // not multi-plane, the aspectMask member of srcSubresource and dstSubresource must match
            if (region.srcSubresource.aspectMask != region.dstSubresource.aspectMask) {
                char const str[] = ""vkCmdCopyImage(): Src and dest aspectMasks for each region must match"";
                skip |= LogError(command_buffer, ""VUID-VkImageCopy-aspectMask-00137"", ""%s."", str);
            }
        }

        // For each region, the aspectMask member of srcSubresource must be present in the source image
        if (!VerifyAspectsPresent(region.srcSubresource.aspectMask, src_format)) {
            std::stringstream ss;
            ss << ""vkCmdCopyImage(): pRegion["" << i
               << ""] srcSubresource.aspectMask cannot specify aspects not present in source image"";
            skip |= LogError(command_buffer, ""VUID-VkImageCopy-aspectMask-00142"", ""%s."", ss.str().c_str());
        }

        // For each region, the aspectMask member of dstSubresource must be present in the destination image
        if (!VerifyAspectsPresent(region.dstSubresource.aspectMask, dst_format)) {
            std::stringstream ss;
            ss << ""vkCmdCopyImage(): pRegion["" << i
               << ""] dstSubresource.aspectMask cannot specify aspects not present in dest image"";
            skip |= LogError(command_buffer, ""VUID-VkImageCopy-aspectMask-00143"", ""%s."", ss.str().c_str());
        }

        // Each dimension offset + extent limits must fall with image subresource extent
        VkExtent3D subresource_extent = GetImageSubresourceExtent(src_image_state, &(region.srcSubresource));
        if (slice_override) src_copy_extent.depth = depth_slices;
        uint32_t extent_check = ExceedsBounds(&(region.srcOffset), &src_copy_extent, &subresource_extent);
        if (extent_check & x_bit) {
            skip |=
                LogError(command_buffer, ""VUID-VkImageCopy-srcOffset-00144"",
                         ""vkCmdCopyImage(): Source image pRegion %1d x-dimension offset [%1d] + extent [%1d] exceeds subResource ""
                         ""width [%1d]."",
                         i, region.srcOffset.x, src_copy_extent.width, subresource_extent.width);
        }

        if (extent_check & y_bit) {
            skip |=
                LogError(command_buffer, ""VUID-VkImageCopy-srcOffset-00145"",
                         ""vkCmdCopyImage(): Source image pRegion %1d y-dimension offset [%1d] + extent [%1d] exceeds subResource ""
                         ""height [%1d]."",
                         i, region.srcOffset.y, src_copy_extent.height, subresource_extent.height);
        }
        if (extent_check & z_bit) {
            skip |=
                LogError(command_buffer, ""VUID-VkImageCopy-srcOffset-00147"",
                         ""vkCmdCopyImage(): Source image pRegion %1d z-dimension offset [%1d] + extent [%1d] exceeds subResource ""
                         ""depth [%1d]."",
                         i, region.srcOffset.z, src_copy_extent.depth, subresource_extent.depth);
        }

        // Adjust dest extent if necessary
        subresource_extent = GetImageSubresourceExtent(dst_image_state, &(region.dstSubresource));
        if (slice_override) dst_copy_extent.depth = depth_slices;

        extent_check = ExceedsBounds(&(region.dstOffset), &dst_copy_extent, &subresource_extent);
        if (extent_check & x_bit) {
            skip |= LogError(command_buffer, ""VUID-VkImageCopy-dstOffset-00150"",
                             ""vkCmdCopyImage(): Dest image pRegion %1d x-dimension offset [%1d] + extent [%1d] exceeds subResource ""
                             ""width [%1d]."",
                             i, region.dstOffset.x, dst_copy_extent.width, subresource_extent.width);
        }
        if (extent_check & y_bit) {
            skip |= LogError(command_buffer, ""VUID-VkImageCopy-dstOffset-00151"",
                             ""vkCmdCopyImage(): Dest image pRegion %1d y-dimension offset [%1d] + extent [%1d] exceeds subResource ""
                             ""height [%1d]."",
                             i, region.dstOffset.y, dst_copy_extent.height, subresource_extent.height);
        }
        if (extent_check & z_bit) {
            skip |= LogError(command_buffer, ""VUID-VkImageCopy-dstOffset-00153"",
                             ""vkCmdCopyImage(): Dest image pRegion %1d z-dimension offset [%1d] + extent [%1d] exceeds subResource ""
                             ""depth [%1d]."",
                             i, region.dstOffset.z, dst_copy_extent.depth, subresource_extent.depth);
        }

        // The union of all source regions, and the union of all destination regions, specified by the elements of regions,
        // must not overlap in memory
        if (src_image_state->image == dst_image_state->image) {
            for (uint32_t j = 0; j < regionCount; j++) {
                if (RegionIntersects(&region, &pRegions[j], src_image_state->createInfo.imageType,
                                     FormatIsMultiplane(src_format))) {
                    std::stringstream ss;
                    ss << ""vkCmdCopyImage(): pRegions["" << i << ""] src overlaps with pRegions["" << j << ""]."";
                    skip |= LogError(command_buffer, ""VUID-vkCmdCopyImage-pRegions-00124"", ""%s."", ss.str().c_str());
                }
            }
        }

        // Check depth for 2D as post Maintaince 1 requires both while prior only required one to be 2D
        if (device_extensions.vk_khr_maintenance1) {
            if (((VK_IMAGE_TYPE_2D == src_image_state->createInfo.imageType) &&
                 (VK_IMAGE_TYPE_2D == dst_image_state->createInfo.imageType)) &&
                (src_copy_extent.depth != 1)) {
                skip |= LogError(
                    command_buffer, ""VUID-VkImageCopy-srcImage-01790"",
                    ""vkCmdCopyImage(): pRegion[%u] both srcImage and dstImage are 2D and extent.depth is %u and has to be 1"", i,
                    src_copy_extent.depth);
            }
        } else {
            if (((VK_IMAGE_TYPE_2D == src_image_state->createInfo.imageType) ||
                 (VK_IMAGE_TYPE_2D == dst_image_state->createInfo.imageType)) &&
                (src_copy_extent.depth != 1)) {
                skip |= LogError(
                    command_buffer, ""VUID-VkImageCopy-srcImage-01789"",
                    ""vkCmdCopyImage(): pRegion[%u] either srcImage or dstImage is 2D and extent.depth is %u and has to be 1"", i,
                    src_copy_extent.depth);
            }
        }

        // Check if 2D with 3D and depth not equal to 2D layerCount
        if ((VK_IMAGE_TYPE_2D == src_image_state->createInfo.imageType) &&
            (VK_IMAGE_TYPE_3D == dst_image_state->createInfo.imageType) &&
            (src_copy_extent.depth != region.srcSubresource.layerCount)) {
            skip |= LogError(command_buffer, ""VUID-VkImageCopy-srcImage-01791"",
                             ""vkCmdCopyImage(): pRegion[%u] srcImage is 2D, dstImage is 3D and extent.depth is %u and has to be ""
                             ""srcSubresource.layerCount (%u)"",
                             i, src_copy_extent.depth, region.srcSubresource.layerCount);
        } else if ((VK_IMAGE_TYPE_3D == src_image_state->createInfo.imageType) &&
                   (VK_IMAGE_TYPE_2D == dst_image_state->createInfo.imageType) &&
                   (src_copy_extent.depth != region.dstSubresource.layerCount)) {
            skip |= LogError(command_buffer, ""VUID-VkImageCopy-dstImage-01792"",
                             ""vkCmdCopyImage(): pRegion[%u] srcImage is 3D, dstImage is 2D and extent.depth is %u and has to be ""
                             ""dstSubresource.layerCount (%u)"",
                             i, src_copy_extent.depth, region.dstSubresource.layerCount);
        }
    }

    // The formats of src_image and dst_image must be compatible. Formats are considered compatible if their texel size in bytes
    // is the same between both formats. For example, VK_FORMAT_R8G8B8A8_UNORM is compatible with VK_FORMAT_R32_UINT because
    // because both texels are 4 bytes in size. Depth/stencil formats must match exactly.
    if (FormatIsDepthOrStencil(src_format) || FormatIsDepthOrStencil(dst_format)) {
        if (src_format != dst_format) {
            char const str[] = ""vkCmdCopyImage called with unmatched source and dest image depth/stencil formats."";
            skip |= LogError(command_buffer, kVUID_Core_DrawState_MismatchedImageFormat, str);
        }
    } else {
        if ((!FormatSizesAreEqual(src_format, dst_format, regionCount, pRegions)) && (!FormatIsMultiplane(src_format)) &&
            (!FormatIsMultiplane(dst_format))) {
            const char *vuid = (device_extensions.vk_khr_sampler_ycbcr_conversion) ? ""VUID-vkCmdCopyImage-srcImage-01548""
                                                                                   : ""VUID-vkCmdCopyImage-srcImage-00135"";
            char const str[] = ""vkCmdCopyImage called with unmatched source and dest image format sizes."";
            skip |= LogError(command_buffer, vuid, ""%s."", str);
        }
    }

    // Source and dest image sample counts must match
    if (src_image_state->createInfo.samples != dst_image_state->createInfo.samples) {
        char const str[] = ""vkCmdCopyImage() called on image pair with non-identical sample counts."";
        skip |= LogError(command_buffer, ""VUID-vkCmdCopyImage-srcImage-00136"", ""%s"", str);
    }

    skip |= ValidateMemoryIsBoundToImage(src_image_state, ""vkCmdCopyImage()"", ""VUID-vkCmdCopyImage-srcImage-00127"");
    skip |= ValidateMemoryIsBoundToImage(dst_image_state, ""vkCmdCopyImage()"", ""VUID-vkCmdCopyImage-dstImage-00132"");
    // Validate that SRC & DST images have correct usage flags set
    skip |= ValidateImageUsageFlags(src_image_state, VK_IMAGE_USAGE_TRANSFER_SRC_BIT, true, ""VUID-vkCmdCopyImage-srcImage-00126"",
                                    ""vkCmdCopyImage()"", ""VK_IMAGE_USAGE_TRANSFER_SRC_BIT"");
    skip |= ValidateImageUsageFlags(dst_image_state, VK_IMAGE_USAGE_TRANSFER_DST_BIT, true, ""VUID-vkCmdCopyImage-dstImage-00131"",
                                    ""vkCmdCopyImage()"", ""VK_IMAGE_USAGE_TRANSFER_DST_BIT"");
    if (device_extensions.vk_khr_maintenance1) {
        skip |= ValidateImageFormatFeatureFlags(src_image_state, VK_FORMAT_FEATURE_TRANSFER_SRC_BIT, ""vkCmdCopyImage()"",
                                                ""VUID-vkCmdCopyImage-srcImage-01995"", ""VUID-vkCmdCopyImage-srcImage-01995"");
        skip |= ValidateImageFormatFeatureFlags(dst_image_state, VK_FORMAT_FEATURE_TRANSFER_DST_BIT, ""vkCmdCopyImage()"",
                                                ""VUID-vkCmdCopyImage-dstImage-01996"", ""VUID-vkCmdCopyImage-dstImage-01996"");
    }
    skip |= ValidateCmdQueueFlags(cb_node, ""vkCmdCopyImage()"", VK_QUEUE_TRANSFER_BIT | VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT,
                                  ""VUID-vkCmdCopyImage-commandBuffer-cmdpool"");
    skip |= ValidateCmd(cb_node, CMD_COPYIMAGE, ""vkCmdCopyImage()"");
    skip |= InsideRenderPass(cb_node, ""vkCmdCopyImage()"", ""VUID-vkCmdCopyImage-renderpass"");
    bool hit_error = false;
    const char *invalid_src_layout_vuid = (src_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)
                                              ? ""VUID-vkCmdCopyImage-srcImageLayout-01917""
                                              : ""VUID-vkCmdCopyImage-srcImageLayout-00129"";
    const char *invalid_dst_layout_vuid = (dst_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)
                                              ? ""VUID-vkCmdCopyImage-dstImageLayout-01395""
                                              : ""VUID-vkCmdCopyImage-dstImageLayout-00134"";
    for (uint32_t i = 0; i < regionCount; ++i) {
        skip |= VerifyImageLayout(cb_node, src_image_state, pRegions[i].srcSubresource, srcImageLayout,
                                  VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL, ""vkCmdCopyImage()"", invalid_src_layout_vuid,
                                  ""VUID-vkCmdCopyImage-srcImageLayout-00128"", &hit_error);
        skip |= VerifyImageLayout(cb_node, dst_image_state, pRegions[i].dstSubresource, dstImageLayout,
                                  VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, ""vkCmdCopyImage()"", invalid_dst_layout_vuid,
                                  ""VUID-vkCmdCopyImage-dstImageLayout-00133"", &hit_error);
        skip |= ValidateCopyImageTransferGranularityRequirements(cb_node, src_image_state, dst_image_state, &pRegions[i], i,
                                                                 ""vkCmdCopyImage()"");
    }

    return skip;
}

void CoreChecks::PreCallRecordCmdCopyImage(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,
                                           VkImage dstImage, VkImageLayout dstImageLayout, uint32_t regionCount,
                                           const VkImageCopy *pRegions) {
    StateTracker::PreCallRecordCmdCopyImage(commandBuffer, srcImage, srcImageLayout, dstImage, dstImageLayout, regionCount,
                                            pRegions);
    auto cb_node = GetCBState(commandBuffer);
    auto src_image_state = GetImageState(srcImage);
    auto dst_image_state = GetImageState(dstImage);

    // Make sure that all image slices are updated to correct layout
    for (uint32_t i = 0; i < regionCount; ++i) {
        SetImageInitialLayout(cb_node, *src_image_state, pRegions[i].srcSubresource, srcImageLayout);
        SetImageInitialLayout(cb_node, *dst_image_state, pRegions[i].dstSubresource, dstImageLayout);
    }
}

// Returns true if sub_rect is entirely contained within rect
static inline bool ContainsRect(VkRect2D rect, VkRect2D sub_rect) {
    if ((sub_rect.offset.x < rect.offset.x) || (sub_rect.offset.x + sub_rect.extent.width > rect.offset.x + rect.extent.width) ||
        (sub_rect.offset.y < rect.offset.y) || (sub_rect.offset.y + sub_rect.extent.height > rect.offset.y + rect.extent.height))
        return false;
    return true;
}

bool CoreChecks::ValidateClearAttachmentExtent(VkCommandBuffer command_buffer, uint32_t attachment_index,
                                               const FRAMEBUFFER_STATE *framebuffer, uint32_t fb_attachment,
                                               const VkRect2D &render_area, uint32_t rect_count,
                                               const VkClearRect *clear_rects) const {
    bool skip = false;
    const IMAGE_VIEW_STATE *image_view_state = nullptr;
    if (framebuffer && (fb_attachment != VK_ATTACHMENT_UNUSED) && (fb_attachment < framebuffer->createInfo.attachmentCount)) {
        image_view_state = GetImageViewState(framebuffer->createInfo.pAttachments[fb_attachment]);
    }

    for (uint32_t j = 0; j < rect_count; j++) {
        if (!ContainsRect(render_area, clear_rects[j].rect)) {
            skip |= LogError(command_buffer, ""VUID-vkCmdClearAttachments-pRects-00016"",
                             ""vkCmdClearAttachments(): The area defined by pRects[%d] is not contained in the area of ""
                             ""the current render pass instance."",
                             j);
        }

        if (image_view_state) {
            // The layers specified by a given element of pRects must be contained within every attachment that
            // pAttachments refers to
            const auto attachment_layer_count = image_view_state->create_info.subresourceRange.layerCount;
            if ((clear_rects[j].baseArrayLayer >= attachment_layer_count) ||
                (clear_rects[j].baseArrayLayer + clear_rects[j].layerCount > attachment_layer_count)) {
                skip |= LogError(command_buffer, ""VUID-vkCmdClearAttachments-pRects-00017"",
                                 ""vkCmdClearAttachments(): The layers defined in pRects[%d] are not contained in the layers ""
                                 ""of pAttachment[%d]."",
                                 j, attachment_index);
            }
        }
    }
    return skip;
}

bool CoreChecks::PreCallValidateCmdClearAttachments(VkCommandBuffer commandBuffer, uint32_t attachmentCount,
                                                    const VkClearAttachment *pAttachments, uint32_t rectCount,
                                                    const VkClearRect *pRects) const {
    bool skip = false;
    const CMD_BUFFER_STATE *cb_node = GetCBState(commandBuffer);  // TODO: Should be const, and never modified during validation
    if (!cb_node) return skip;

    skip |= ValidateCmdQueueFlags(cb_node, ""vkCmdClearAttachments()"", VK_QUEUE_GRAPHICS_BIT,
                                  ""VUID-vkCmdClearAttachments-commandBuffer-cmdpool"");
    skip |= ValidateCmd(cb_node, CMD_CLEARATTACHMENTS, ""vkCmdClearAttachments()"");
    skip |= OutsideRenderPass(cb_node, ""vkCmdClearAttachments()"", ""VUID-vkCmdClearAttachments-renderpass"");

    // Validate that attachment is in reference list of active subpass
    if (cb_node->activeRenderPass) {
        const VkRenderPassCreateInfo2KHR *renderpass_create_info = cb_node->activeRenderPass->createInfo.ptr();
        const uint32_t renderpass_attachment_count = renderpass_create_info->attachmentCount;
        const VkSubpassDescription2KHR *subpass_desc = &renderpass_create_info->pSubpasses[cb_node->activeSubpass];
        const auto *framebuffer = GetFramebufferState(cb_node->activeFramebuffer);
        const auto &render_area = cb_node->activeRenderPassBeginInfo.renderArea;

        for (uint32_t attachment_index = 0; attachment_index < attachmentCount; attachment_index++) {
            auto clear_desc = &pAttachments[attachment_index];
            uint32_t fb_attachment = VK_ATTACHMENT_UNUSED;

            if (0 == clear_desc->aspectMask) {
                skip |= LogError(commandBuffer, ""VUID-VkClearAttachment-aspectMask-requiredbitmask"", "" "");
            } else if (clear_desc->aspectMask & VK_IMAGE_ASPECT_METADATA_BIT) {
                skip |= LogError(commandBuffer, ""VUID-VkClearAttachment-aspectMask-00020"", "" "");
            } else if (clear_desc->aspectMask & VK_IMAGE_ASPECT_COLOR_BIT) {
                uint32_t color_attachment = VK_ATTACHMENT_UNUSED;
                if (clear_desc->colorAttachment < subpass_desc->colorAttachmentCount) {
                    color_attachment = subpass_desc->pColorAttachments[clear_desc->colorAttachment].attachment;
                    if ((color_attachment != VK_ATTACHMENT_UNUSED) && (color_attachment >= renderpass_attachment_count)) {
                        skip |= LogError(
                            commandBuffer, ""VUID-vkCmdClearAttachments-aspectMask-02501"",
                            ""vkCmdClearAttachments() pAttachments[%u].colorAttachment=%u is not VK_ATTACHMENT_UNUSED ""
                            ""and not a valid attachment for %s attachmentCount=%u. Subpass %u pColorAttachment[%u]=%u."",
                            attachment_index, clear_desc->colorAttachment,
                            report_data->FormatHandle(cb_node->activeRenderPass->renderPass).c_str(), cb_node->activeSubpass,
                            clear_desc->colorAttachment, color_attachment, renderpass_attachment_count);

                        color_attachment = VK_ATTACHMENT_UNUSED;  // Defensive, prevent lookup past end of renderpass attachment
                    }
                } else {
                    skip |= LogError(commandBuffer, ""VUID-vkCmdClearAttachments-aspectMask-02501"",
                                     ""vkCmdClearAttachments() pAttachments[%u].colorAttachment=%u out of range for %s""
                                     "" subpass %u. colorAttachmentCount=%u"",
                                     attachment_index, clear_desc->colorAttachment,
                                     report_data->FormatHandle(cb_node->activeRenderPass->renderPass).c_str(),
                                     cb_node->activeSubpass, subpass_desc->colorAttachmentCount);
                }
                fb_attachment = color_attachment;

                if ((clear_desc->aspectMask & VK_IMAGE_ASPECT_DEPTH_BIT) ||
                    (clear_desc->aspectMask & VK_IMAGE_ASPECT_STENCIL_BIT)) {
                    char const str[] =
                        ""vkCmdClearAttachments() aspectMask [%d] must set only VK_IMAGE_ASPECT_COLOR_BIT of a color attachment."";
                    skip |= LogError(commandBuffer, ""VUID-VkClearAttachment-aspectMask-00019"", str, attachment_index);
                }
            } else {  // Must be depth and/or stencil
                if (((clear_desc->aspectMask & VK_IMAGE_ASPECT_DEPTH_BIT) != VK_IMAGE_ASPECT_DEPTH_BIT) &&
                    ((clear_desc->aspectMask & VK_IMAGE_ASPECT_STENCIL_BIT) != VK_IMAGE_ASPECT_STENCIL_BIT)) {
                    char const str[] = ""vkCmdClearAttachments() aspectMask [%d] is not a valid combination of bits."";
                    skip |= LogError(commandBuffer, ""VUID-VkClearAttachment-aspectMask-parameter"", str, attachment_index);
                }
                if (!subpass_desc->pDepthStencilAttachment ||
                    (subpass_desc->pDepthStencilAttachment->attachment == VK_ATTACHMENT_UNUSED)) {
                    skip |= LogPerformanceWarning(
                        commandBuffer, kVUID_Core_DrawState_MissingAttachmentReference,
                        ""vkCmdClearAttachments() depth/stencil clear with no depth/stencil attachment in subpass; ignored"");
                } else {
                    fb_attachment = subpass_desc->pDepthStencilAttachment->attachment;
                }
            }
            if (cb_node->createInfo.level == VK_COMMAND_BUFFER_LEVEL_PRIMARY) {
                skip |= ValidateClearAttachmentExtent(commandBuffer, attachment_index, framebuffer, fb_attachment, render_area,
                                                      rectCount, pRects);
            }
        }
    }
    return skip;
}

void CoreChecks::PreCallRecordCmdClearAttachments(VkCommandBuffer commandBuffer, uint32_t attachmentCount,
                                                  const VkClearAttachment *pAttachments, uint32_t rectCount,
                                                  const VkClearRect *pRects) {
    auto *cb_node = GetCBState(commandBuffer);
    if (cb_node->activeRenderPass && (cb_node->createInfo.level == VK_COMMAND_BUFFER_LEVEL_SECONDARY)) {
        const VkRenderPassCreateInfo2KHR *renderpass_create_info = cb_node->activeRenderPass->createInfo.ptr();
        const VkSubpassDescription2KHR *subpass_desc = &renderpass_create_info->pSubpasses[cb_node->activeSubpass];
        std::shared_ptr<std::vector<VkClearRect>> clear_rect_copy;
        for (uint32_t attachment_index = 0; attachment_index < attachmentCount; attachment_index++) {
            const auto clear_desc = &pAttachments[attachment_index];
            uint32_t fb_attachment = VK_ATTACHMENT_UNUSED;
            if ((clear_desc->aspectMask & VK_IMAGE_ASPECT_COLOR_BIT) &&
                (clear_desc->colorAttachment < subpass_desc->colorAttachmentCount)) {
                fb_attachment = subpass_desc->pColorAttachments[clear_desc->colorAttachment].attachment;
            } else if ((clear_desc->aspectMask & (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT)) &&
                       subpass_desc->pDepthStencilAttachment) {
                fb_attachment = subpass_desc->pDepthStencilAttachment->attachment;
            }
            if (fb_attachment != VK_ATTACHMENT_UNUSED) {
                if (!clear_rect_copy) {
                    // We need a copy of the clear rectangles that will persist until the last lambda executes
                    // but we want to create it as lazily as possible
                    clear_rect_copy.reset(new std::vector<VkClearRect>(pRects, pRects + rectCount));
                }
                // if a secondary level command buffer inherits the framebuffer from the primary command buffer
                // (see VkCommandBufferInheritanceInfo), this validation must be deferred until queue submit time
                auto val_fn = [this, commandBuffer, attachment_index, fb_attachment, rectCount, clear_rect_copy](
                                  const CMD_BUFFER_STATE *prim_cb, VkFramebuffer fb) {
                    assert(rectCount == clear_rect_copy->size());
                    const FRAMEBUFFER_STATE *framebuffer = GetFramebufferState(fb);
                    const auto &render_area = prim_cb->activeRenderPassBeginInfo.renderArea;
                    bool skip = false;
                    skip = ValidateClearAttachmentExtent(commandBuffer, attachment_index, framebuffer, fb_attachment, render_area,
                                                         rectCount, clear_rect_copy->data());
                    return skip;
                };
                cb_node->cmd_execute_commands_functions.emplace_back(val_fn);
            }
        }
    }
}

bool CoreChecks::PreCallValidateCmdResolveImage(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,
                                                VkImage dstImage, VkImageLayout dstImageLayout, uint32_t regionCount,
                                                const VkImageResolve *pRegions) const {
    const auto *cb_node = GetCBState(commandBuffer);
    const auto *src_image_state = GetImageState(srcImage);
    const auto *dst_image_state = GetImageState(dstImage);

    bool skip = false;
    if (cb_node && src_image_state && dst_image_state) {
        skip |= ValidateMemoryIsBoundToImage(src_image_state, ""vkCmdResolveImage()"", ""VUID-vkCmdResolveImage-srcImage-00256"");
        skip |= ValidateMemoryIsBoundToImage(dst_image_state, ""vkCmdResolveImage()"", ""VUID-vkCmdResolveImage-dstImage-00258"");
        skip |= ValidateCmdQueueFlags(cb_node, ""vkCmdResolveImage()"", VK_QUEUE_GRAPHICS_BIT,
                                      ""VUID-vkCmdResolveImage-commandBuffer-cmdpool"");
        skip |= ValidateCmd(cb_node, CMD_RESOLVEIMAGE, ""vkCmdResolveImage()"");
        skip |= InsideRenderPass(cb_node, ""vkCmdResolveImage()"", ""VUID-vkCmdResolveImage-renderpass"");
        skip |= ValidateImageFormatFeatureFlags(dst_image_state, VK_FORMAT_FEATURE_COLOR_ATTACHMENT_BIT, ""vkCmdResolveImage()"",
                                                ""VUID-vkCmdResolveImage-dstImage-02003"", ""VUID-vkCmdResolveImage-dstImage-02003"");

        bool hit_error = false;
        const char *invalid_src_layout_vuid =
            (src_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)
                ? ""VUID-vkCmdResolveImage-srcImageLayout-01400""
                : ""VUID-vkCmdResolveImage-srcImageLayout-00261"";
        const char *invalid_dst_layout_vuid =
            (dst_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)
                ? ""VUID-vkCmdResolveImage-dstImageLayout-01401""
                : ""VUID-vkCmdResolveImage-dstImageLayout-00263"";
        // For each region, the number of layers in the image subresource should not be zero
        // For each region, src and dest image aspect must be color only
        for (uint32_t i = 0; i < regionCount; i++) {
            skip |=
                ValidateImageSubresourceLayers(cb_node, &pRegions[i].srcSubresource, ""vkCmdResolveImage()"", ""srcSubresource"", i);
            skip |=
                ValidateImageSubresourceLayers(cb_node, &pRegions[i].dstSubresource, ""vkCmdResolveImage()"", ""dstSubresource"", i);
            skip |= VerifyImageLayout(cb_node, src_image_state, pRegions[i].srcSubresource, srcImageLayout,
                                      VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL, ""vkCmdResolveImage()"", invalid_src_layout_vuid,
                                      ""VUID-vkCmdResolveImage-srcImageLayout-00260"", &hit_error);
            skip |= VerifyImageLayout(cb_node, dst_image_state, pRegions[i].dstSubresource, dstImageLayout,
                                      VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, ""vkCmdResolveImage()"", invalid_dst_layout_vuid,
                                      ""VUID-vkCmdResolveImage-dstImageLayout-00262"", &hit_error);
            skip |= ValidateImageMipLevel(cb_node, src_image_state, pRegions[i].srcSubresource.mipLevel, i, ""vkCmdResolveImage()"",
                                          ""srcSubresource"", ""VUID-vkCmdResolveImage-srcSubresource-01709"");
            skip |= ValidateImageMipLevel(cb_node, dst_image_state, pRegions[i].dstSubresource.mipLevel, i, ""vkCmdResolveImage()"",
                                          ""dstSubresource"", ""VUID-vkCmdResolveImage-dstSubresource-01710"");
            skip |= ValidateImageArrayLayerRange(cb_node, src_image_state, pRegions[i].srcSubresource.baseArrayLayer,
                                                 pRegions[i].srcSubresource.layerCount, i, ""vkCmdResolveImage()"", ""srcSubresource"",
                                                 ""VUID-vkCmdResolveImage-srcSubresource-01711"");
            skip |= ValidateImageArrayLayerRange(cb_node, dst_image_state, pRegions[i].dstSubresource.baseArrayLayer,
                                                 pRegions[i].dstSubresource.layerCount, i, ""vkCmdResolveImage()"", ""srcSubresource"",
                                                 ""VUID-vkCmdResolveImage-dstSubresource-01712"");

            // layer counts must match
            if (pRegions[i].srcSubresource.layerCount != pRegions[i].dstSubresource.layerCount) {
                skip |= LogError(
                    cb_node->commandBuffer, ""VUID-VkImageResolve-layerCount-00267"",
                    ""vkCmdResolveImage(): layerCount in source and destination subresource of pRegions[%d] does not match."", i);
            }
            // For each region, src and dest image aspect must be color only
            if ((pRegions[i].srcSubresource.aspectMask != VK_IMAGE_ASPECT_COLOR_BIT) ||
                (pRegions[i].dstSubresource.aspectMask != VK_IMAGE_ASPECT_COLOR_BIT)) {
                char const str[] =
                    ""vkCmdResolveImage(): src and dest aspectMasks for each region must specify only VK_IMAGE_ASPECT_COLOR_BIT"";
                skip |= LogError(cb_node->commandBuffer, ""VUID-VkImageResolve-aspectMask-00266"", ""%s."", str);
            }
        }

        if (src_image_state->createInfo.format != dst_image_state->createInfo.format) {
            char const str[] = ""vkCmdResolveImage called with unmatched source and dest formats."";
            skip |= LogWarning(cb_node->commandBuffer, kVUID_Core_DrawState_MismatchedImageFormat, str);
        }
        if (src_image_state->createInfo.imageType != dst_image_state->createInfo.imageType) {
            char const str[] = ""vkCmdResolveImage called with unmatched source and dest image types."";
            skip |= LogWarning(cb_node->commandBuffer, kVUID_Core_DrawState_MismatchedImageType, str);
        }
        if (src_image_state->createInfo.samples == VK_SAMPLE_COUNT_1_BIT) {
            char const str[] = ""vkCmdResolveImage called with source sample count less than 2."";
            skip |= LogError(cb_node->commandBuffer, ""VUID-vkCmdResolveImage-srcImage-00257"", ""%s."", str);
        }
        if (dst_image_state->createInfo.samples != VK_SAMPLE_COUNT_1_BIT) {
            char const str[] = ""vkCmdResolveImage called with dest sample count greater than 1."";
            skip |= LogError(cb_node->commandBuffer, ""VUID-vkCmdResolveImage-dstImage-00259"", ""%s."", str);
        }
    } else {
        assert(0);
    }
    return skip;
}

bool CoreChecks::PreCallValidateCmdBlitImage(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,
                                             VkImage dstImage, VkImageLayout dstImageLayout, uint32_t regionCount,
                                             const VkImageBlit *pRegions, VkFilter filter) const {
    const auto *cb_node = GetCBState(commandBuffer);
    const auto *src_image_state = GetImageState(srcImage);
    const auto *dst_image_state = GetImageState(dstImage);

    bool skip = false;
    if (cb_node) {
        skip |= ValidateCmd(cb_node, CMD_BLITIMAGE, ""vkCmdBlitImage()"");
    }
    if (cb_node && src_image_state && dst_image_state) {
        skip |= ValidateImageSampleCount(src_image_state, VK_SAMPLE_COUNT_1_BIT, ""vkCmdBlitImage(): srcImage"",
                                         ""VUID-vkCmdBlitImage-srcImage-00233"");
        skip |= ValidateImageSampleCount(dst_image_state, VK_SAMPLE_COUNT_1_BIT, ""vkCmdBlitImage(): dstImage"",
                                         ""VUID-vkCmdBlitImage-dstImage-00234"");
        skip |= ValidateMemoryIsBoundToImage(src_image_state, ""vkCmdBlitImage()"", ""VUID-vkCmdBlitImage-srcImage-00220"");
        skip |= ValidateMemoryIsBoundToImage(dst_image_state, ""vkCmdBlitImage()"", ""VUID-vkCmdBlitImage-dstImage-00225"");
        skip |=
            ValidateImageUsageFlags(src_image_state, VK_IMAGE_USAGE_TRANSFER_SRC_BIT, true, ""VUID-vkCmdBlitImage-srcImage-00219"",
                                    ""vkCmdBlitImage()"", ""VK_IMAGE_USAGE_TRANSFER_SRC_BIT"");
        skip |=
            ValidateImageUsageFlags(dst_image_state, VK_IMAGE_USAGE_TRANSFER_DST_BIT, true, ""VUID-vkCmdBlitImage-dstImage-00224"",
                                    ""vkCmdBlitImage()"", ""VK_IMAGE_USAGE_TRANSFER_DST_BIT"");
        skip |=
            ValidateCmdQueueFlags(cb_node, ""vkCmdBlitImage()"", VK_QUEUE_GRAPHICS_BIT, ""VUID-vkCmdBlitImage-commandBuffer-cmdpool"");
        skip |= ValidateCmd(cb_node, CMD_BLITIMAGE, ""vkCmdBlitImage()"");
        skip |= InsideRenderPass(cb_node, ""vkCmdBlitImage()"", ""VUID-vkCmdBlitImage-renderpass"");
        skip |= ValidateImageFormatFeatureFlags(src_image_state, VK_FORMAT_FEATURE_BLIT_SRC_BIT, ""vkCmdBlitImage()"",
                                                ""VUID-vkCmdBlitImage-srcImage-01999"", ""VUID-vkCmdBlitImage-srcImage-01999"");
        skip |= ValidateImageFormatFeatureFlags(dst_image_state, VK_FORMAT_FEATURE_BLIT_DST_BIT, ""vkCmdBlitImage()"",
                                                ""VUID-vkCmdBlitImage-dstImage-02000"", ""VUID-vkCmdBlitImage-dstImage-02000"");

        // TODO: Need to validate image layouts, which will include layout validation for shared presentable images

        VkFormat src_format = src_image_state->createInfo.format;
        VkFormat dst_format = dst_image_state->createInfo.format;
        VkImageType src_type = src_image_state->createInfo.imageType;
        VkImageType dst_type = dst_image_state->createInfo.imageType;

        if (VK_FILTER_LINEAR == filter) {
            skip |= ValidateImageFormatFeatureFlags(src_image_state, VK_FORMAT_FEATURE_SAMPLED_IMAGE_FILTER_LINEAR_BIT,
                                                    ""vkCmdBlitImage()"", ""VUID-vkCmdBlitImage-filter-02001"",
                                                    ""VUID-vkCmdBlitImage-filter-02001"");
        } else if (VK_FILTER_CUBIC_IMG == filter) {
            skip |= ValidateImageFormatFeatureFlags(src_image_state, VK_FORMAT_FEATURE_SAMPLED_IMAGE_FILTER_CUBIC_BIT_IMG,
                                                    ""vkCmdBlitImage()"", ""VUID-vkCmdBlitImage-filter-02002"",
                                                    ""VUID-vkCmdBlitImage-filter-02002"");
        }

        if (FormatRequiresYcbcrConversion(src_format)) {
            skip |= LogError(device, ""VUID-vkCmdBlitImage-srcImage-01561"",
                             ""vkCmdBlitImage(): srcImage format (%s) must not be one of the formats requiring sampler YCBCR ""
                             ""conversion for VK_IMAGE_ASPECT_COLOR_BIT image views"",
                             string_VkFormat(src_format));
        }

        if (FormatRequiresYcbcrConversion(dst_format)) {
            skip |= LogError(device, ""VUID-vkCmdBlitImage-dstImage-01562"",
                             ""vkCmdBlitImage(): dstImage format (%s) must not be one of the formats requiring sampler YCBCR ""
                             ""conversion for VK_IMAGE_ASPECT_COLOR_BIT image views"",
                             string_VkFormat(dst_format));
        }

        if ((VK_FILTER_CUBIC_IMG == filter) && (VK_IMAGE_TYPE_3D != src_type)) {
            skip |= LogError(cb_node->commandBuffer, ""VUID-vkCmdBlitImage-filter-00237"",
                             ""vkCmdBlitImage(): source image type must be VK_IMAGE_TYPE_3D when cubic filtering is specified."");
        }

        if ((VK_SAMPLE_COUNT_1_BIT != src_image_state->createInfo.samples) ||
            (VK_SAMPLE_COUNT_1_BIT != dst_image_state->createInfo.samples)) {
            skip |= LogError(cb_node->commandBuffer, ""VUID-vkCmdBlitImage-srcImage-00228"",
                             ""vkCmdBlitImage(): source or dest image has sample count other than VK_SAMPLE_COUNT_1_BIT."");
        }

        // Validate consistency for unsigned formats
        if (FormatIsUInt(src_format) != FormatIsUInt(dst_format)) {
            std::stringstream ss;
            ss << ""vkCmdBlitImage(): If one of srcImage and dstImage images has unsigned integer format, ""
               << ""the other one must also have unsigned integer format.  ""
               << ""Source format is "" << string_VkFormat(src_format) << "" Destination format is "" << string_VkFormat(dst_format);
            skip |= LogError(cb_node->commandBuffer, ""VUID-vkCmdBlitImage-srcImage-00230"", ""%s."", ss.str().c_str());
        }

        // Validate consistency for signed formats
        if (FormatIsSInt(src_format) != FormatIsSInt(dst_format)) {
            std::stringstream ss;
            ss << ""vkCmdBlitImage(): If one of srcImage and dstImage images has signed integer format, ""
               << ""the other one must also have signed integer format.  ""
               << ""Source format is "" << string_VkFormat(src_format) << "" Destination format is "" << string_VkFormat(dst_format);
            skip |= LogError(cb_node->commandBuffer, ""VUID-vkCmdBlitImage-srcImage-00229"", ""%s."", ss.str().c_str());
        }

        // Validate filter for Depth/Stencil formats
        if (FormatIsDepthOrStencil(src_format) && (filter != VK_FILTER_NEAREST)) {
            std::stringstream ss;
            ss << ""vkCmdBlitImage(): If the format of srcImage is a depth, stencil, or depth stencil ""
               << ""then filter must be VK_FILTER_NEAREST."";
            skip |= LogError(cb_node->commandBuffer, ""VUID-vkCmdBlitImage-srcImage-00232"", ""%s."", ss.str().c_str());
        }

        // Validate aspect bits and formats for depth/stencil images
        if (FormatIsDepthOrStencil(src_format) || FormatIsDepthOrStencil(dst_format)) {
            if (src_format != dst_format) {
                std::stringstream ss;
                ss << ""vkCmdBlitImage(): If one of srcImage and dstImage images has a format of depth, stencil or depth ""
                   << ""stencil, the other one must have exactly the same format.  ""
                   << ""Source format is "" << string_VkFormat(src_format) << "" Destination format is ""
                   << string_VkFormat(dst_format);
                skip |= LogError(cb_node->commandBuffer, ""VUID-vkCmdBlitImage-srcImage-00231"", ""%s."", ss.str().c_str());
            }
        }  // Depth or Stencil

        // Do per-region checks
        const char *invalid_src_layout_vuid =
            (src_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)
                ? ""VUID-vkCmdBlitImage-srcImageLayout-01398""
                : ""VUID-vkCmdBlitImage-srcImageLayout-00222"";
        const char *invalid_dst_layout_vuid =
            (dst_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)
                ? ""VUID-vkCmdBlitImage-dstImageLayout-01399""
                : ""VUID-vkCmdBlitImage-dstImageLayout-00227"";
        for (uint32_t i = 0; i < regionCount; i++) {
            const VkImageBlit rgn = pRegions[i];
            bool hit_error = false;
            skip |= VerifyImageLayout(cb_node, src_image_state, rgn.srcSubresource, srcImageLayout,
                                      VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL, ""vkCmdBlitImage()"", invalid_src_layout_vuid,
                                      ""VUID-vkCmdBlitImage-srcImageLayout-00221"", &hit_error);
            skip |= VerifyImageLayout(cb_node, dst_image_state, rgn.dstSubresource, dstImageLayout,
                                      VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, ""vkCmdBlitImage()"", invalid_dst_layout_vuid,
                                      ""VUID-vkCmdBlitImage-dstImageLayout-00226"", &hit_error);
            skip |= ValidateImageSubresourceLayers(cb_node, &rgn.srcSubresource, ""vkCmdBlitImage()"", ""srcSubresource"", i);
            skip |= ValidateImageSubresourceLayers(cb_node, &rgn.dstSubresource, ""vkCmdBlitImage()"", ""dstSubresource"", i);
            skip |= ValidateImageMipLevel(cb_node, src_image_state, rgn.srcSubresource.mipLevel, i, ""vkCmdBlitImage()"",
                                          ""srcSubresource"", ""VUID-vkCmdBlitImage-srcSubresource-01705"");
            skip |= ValidateImageMipLevel(cb_node, dst_image_state, rgn.dstSubresource.mipLevel, i, ""vkCmdBlitImage()"",
                                          ""dstSubresource"", ""VUID-vkCmdBlitImage-dstSubresource-01706"");
            skip |= ValidateImageArrayLayerRange(cb_node, src_image_state, rgn.srcSubresource.baseArrayLayer,
                                                 rgn.srcSubresource.layerCount, i, ""vkCmdBlitImage()"", ""srcSubresource"",
                                                 ""VUID-vkCmdBlitImage-srcSubresource-01707"");
            skip |= ValidateImageArrayLayerRange(cb_node, dst_image_state, rgn.dstSubresource.baseArrayLayer,
                                                 rgn.dstSubresource.layerCount, i, ""vkCmdBlitImage()"", ""dstSubresource"",
                                                 ""VUID-vkCmdBlitImage-dstSubresource-01708"");
            // Warn for zero-sized regions
            if ((rgn.srcOffsets[0].x == rgn.srcOffsets[1].x) || (rgn.srcOffsets[0].y == rgn.srcOffsets[1].y) ||
                (rgn.srcOffsets[0].z == rgn.srcOffsets[1].z)) {
                std::stringstream ss;
                ss << ""vkCmdBlitImage(): pRegions["" << i << ""].srcOffsets specify a zero-volume area."";
                skip |= LogWarning(cb_node->commandBuffer, kVUID_Core_DrawState_InvalidExtents, ""%s"", ss.str().c_str());
            }
            if ((rgn.dstOffsets[0].x == rgn.dstOffsets[1].x) || (rgn.dstOffsets[0].y == rgn.dstOffsets[1].y) ||
                (rgn.dstOffsets[0].z == rgn.dstOffsets[1].z)) {
                std::stringstream ss;
                ss << ""vkCmdBlitImage(): pRegions["" << i << ""].dstOffsets specify a zero-volume area."";
                skip |= LogWarning(cb_node->commandBuffer, kVUID_Core_DrawState_InvalidExtents, ""%s"", ss.str().c_str());
            }

            // Check that src/dst layercounts match
            if (rgn.srcSubresource.layerCount != rgn.dstSubresource.layerCount) {
                skip |= LogError(
                    cb_node->commandBuffer, ""VUID-VkImageBlit-layerCount-00239"",
                    ""vkCmdBlitImage(): layerCount in source and destination subresource of pRegions[%d] does not match."", i);
            }

            if (rgn.srcSubresource.aspectMask != rgn.dstSubresource.aspectMask) {
                skip |= LogError(cb_node->commandBuffer, ""VUID-VkImageBlit-aspectMask-00238"",
                                 ""vkCmdBlitImage(): aspectMask members for pRegion[%d] do not match."", i);
            }

            if (!VerifyAspectsPresent(rgn.srcSubresource.aspectMask, src_format)) {
                skip |= LogError(cb_node->commandBuffer, ""VUID-VkImageBlit-aspectMask-00241"",
                                 ""vkCmdBlitImage(): region [%d] source aspectMask (0x%x) specifies aspects not present in source ""
                                 ""image format %s."",
                                 i, rgn.srcSubresource.aspectMask, string_VkFormat(src_format));
            }

            if (!VerifyAspectsPresent(rgn.dstSubresource.aspectMask, dst_format)) {
                skip |= LogError(
                    cb_node->commandBuffer, ""VUID-VkImageBlit-aspectMask-00242"",
                    ""vkCmdBlitImage(): region [%d] dest aspectMask (0x%x) specifies aspects not present in dest image format %s."",
                    i, rgn.dstSubresource.aspectMask, string_VkFormat(dst_format));
            }

            // Validate source image offsets
            VkExtent3D src_extent = GetImageSubresourceExtent(src_image_state, &(rgn.srcSubresource));
            if (VK_IMAGE_TYPE_1D == src_type) {
                if ((0 != rgn.srcOffsets[0].y) || (1 != rgn.srcOffsets[1].y)) {
                    skip |=
                        LogError(cb_node->commandBuffer, ""VUID-VkImageBlit-srcImage-00245"",
                                 ""vkCmdBlitImage(): region [%d], source image of type VK_IMAGE_TYPE_1D with srcOffset[].y values ""
                                 ""of (%1d, %1d). These must be (0, 1)."",
                                 i, rgn.srcOffsets[0].y, rgn.srcOffsets[1].y);
                }
            }

            if ((VK_IMAGE_TYPE_1D == src_type) || (VK_IMAGE_TYPE_2D == src_type)) {
                if ((0 != rgn.srcOffsets[0].z) || (1 != rgn.srcOffsets[1].z)) {
                    skip |=
                        LogError(cb_node->commandBuffer, ""VUID-VkImageBlit-srcImage-00247"",
                                 ""vkCmdBlitImage(): region [%d], source image of type VK_IMAGE_TYPE_1D or VK_IMAGE_TYPE_2D with ""
                                 ""srcOffset[].z values of (%1d, %1d). These must be (0, 1)."",
                                 i, rgn.srcOffsets[0].z, rgn.srcOffsets[1].z);
                }
            }

            bool oob = false;
            if ((rgn.srcOffsets[0].x < 0) || (rgn.srcOffsets[0].x > static_cast<int32_t>(src_extent.width)) ||
                (rgn.srcOffsets[1].x < 0) || (rgn.srcOffsets[1].x > static_cast<int32_t>(src_extent.width))) {
                oob = true;
                skip |= LogError(
                    cb_node->commandBuffer, ""VUID-VkImageBlit-srcOffset-00243"",
                    ""vkCmdBlitImage(): region [%d] srcOffset[].x values (%1d, %1d) exceed srcSubresource width extent (%1d)."", i,
                    rgn.srcOffsets[0].x, rgn.srcOffsets[1].x, src_extent.width);
            }
            if ((rgn.srcOffsets[0].y < 0) || (rgn.srcOffsets[0].y > static_cast<int32_t>(src_extent.height)) ||
                (rgn.srcOffsets[1].y < 0) || (rgn.srcOffsets[1].y > static_cast<int32_t>(src_extent.height))) {
                oob = true;
                skip |= LogError(
                    cb_node->commandBuffer, ""VUID-VkImageBlit-srcOffset-00244"",
                    ""vkCmdBlitImage(): region [%d] srcOffset[].y values (%1d, %1d) exceed srcSubresource height extent (%1d)."", i,
                    rgn.srcOffsets[0].y, rgn.srcOffsets[1].y, src_extent.height);
            }
            if ((rgn.srcOffsets[0].z < 0) || (rgn.srcOffsets[0].z > static_cast<int32_t>(src_extent.depth)) ||
                (rgn.srcOffsets[1].z < 0) || (rgn.srcOffsets[1].z > static_cast<int32_t>(src_extent.depth))) {
                oob = true;
                skip |= LogError(
                    cb_node->commandBuffer, ""VUID-VkImageBlit-srcOffset-00246"",
                    ""vkCmdBlitImage(): region [%d] srcOffset[].z values (%1d, %1d) exceed srcSubresource depth extent (%1d)."", i,
                    rgn.srcOffsets[0].z, rgn.srcOffsets[1].z, src_extent.depth);
            }
            if (oob) {
                skip |= LogError(cb_node->commandBuffer, ""VUID-vkCmdBlitImage-pRegions-00215"",
                                 ""vkCmdBlitImage(): region [%d] source image blit region exceeds image dimensions."", i);
            }

            // Validate dest image offsets
            VkExtent3D dst_extent = GetImageSubresourceExtent(dst_image_state, &(rgn.dstSubresource));
            if (VK_IMAGE_TYPE_1D == dst_type) {
                if ((0 != rgn.dstOffsets[0].y) || (1 != rgn.dstOffsets[1].y)) {
                    skip |=
                        LogError(cb_node->commandBuffer, ""VUID-VkImageBlit-dstImage-00250"",
                                 ""vkCmdBlitImage(): region [%d], dest image of type VK_IMAGE_TYPE_1D with dstOffset[].y values of ""
                                 ""(%1d, %1d). These must be (0, 1)."",
                                 i, rgn.dstOffsets[0].y, rgn.dstOffsets[1].y);
                }
            }

            if ((VK_IMAGE_TYPE_1D == dst_type) || (VK_IMAGE_TYPE_2D == dst_type)) {
                if ((0 != rgn.dstOffsets[0].z) || (1 != rgn.dstOffsets[1].z)) {
                    skip |= LogError(cb_node->commandBuffer, ""VUID-VkImageBlit-dstImage-00252"",
                                     ""vkCmdBlitImage(): region [%d], dest image of type VK_IMAGE_TYPE_1D or VK_IMAGE_TYPE_2D with ""
                                     ""dstOffset[].z values of (%1d, %1d). These must be (0, 1)."",
                                     i, rgn.dstOffsets[0].z, rgn.dstOffsets[1].z);
                }
            }

            oob = false;
            if ((rgn.dstOffsets[0].x < 0) || (rgn.dstOffsets[0].x > static_cast<int32_t>(dst_extent.width)) ||
                (rgn.dstOffsets[1].x < 0) || (rgn.dstOffsets[1].x > static_cast<int32_t>(dst_extent.width))) {
                oob = true;
                skip |= LogError(
                    cb_node->commandBuffer, ""VUID-VkImageBlit-dstOffset-00248"",
                    ""vkCmdBlitImage(): region [%d] dstOffset[].x values (%1d, %1d) exceed dstSubresource width extent (%1d)."", i,
                    rgn.dstOffsets[0].x, rgn.dstOffsets[1].x, dst_extent.width);
            }
            if ((rgn.dstOffsets[0].y < 0) || (rgn.dstOffsets[0].y > static_cast<int32_t>(dst_extent.height)) ||
                (rgn.dstOffsets[1].y < 0) || (rgn.dstOffsets[1].y > static_cast<int32_t>(dst_extent.height))) {
                oob = true;
                skip |= LogError(
                    cb_node->commandBuffer, ""VUID-VkImageBlit-dstOffset-00249"",
                    ""vkCmdBlitImage(): region [%d] dstOffset[].y values (%1d, %1d) exceed dstSubresource height extent (%1d)."", i,
                    rgn.dstOffsets[0].y, rgn.dstOffsets[1].y, dst_extent.height);
            }
            if ((rgn.dstOffsets[0].z < 0) || (rgn.dstOffsets[0].z > static_cast<int32_t>(dst_extent.depth)) ||
                (rgn.dstOffsets[1].z < 0) || (rgn.dstOffsets[1].z > static_cast<int32_t>(dst_extent.depth))) {
                oob = true;
                skip |= LogError(
                    cb_node->commandBuffer, ""VUID-VkImageBlit-dstOffset-00251"",
                    ""vkCmdBlitImage(): region [%d] dstOffset[].z values (%1d, %1d) exceed dstSubresource depth extent (%1d)."", i,
                    rgn.dstOffsets[0].z, rgn.dstOffsets[1].z, dst_extent.depth);
            }
            if (oob) {
                skip |= LogError(cb_node->commandBuffer, ""VUID-vkCmdBlitImage-pRegions-00216"",
                                 ""vkCmdBlitImage(): region [%d] destination image blit region exceeds image dimensions."", i);
            }

            if ((VK_IMAGE_TYPE_3D == src_type) || (VK_IMAGE_TYPE_3D == dst_type)) {
                if ((0 != rgn.srcSubresource.baseArrayLayer) || (1 != rgn.srcSubresource.layerCount) ||
                    (0 != rgn.dstSubresource.baseArrayLayer) || (1 != rgn.dstSubresource.layerCount)) {
                    skip |=
                        LogError(cb_node->commandBuffer, ""VUID-VkImageBlit-srcImage-00240"",
                                 ""vkCmdBlitImage(): region [%d] blit to/from a 3D image type with a non-zero baseArrayLayer, or a ""
                                 ""layerCount other than 1."",
                                 i);
                }
            }
        }  // per-region checks
    } else {
        assert(0);
    }
    return skip;
}

void CoreChecks::PreCallRecordCmdBlitImage(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,
                                           VkImage dstImage, VkImageLayout dstImageLayout, uint32_t regionCount,
                                           const VkImageBlit *pRegions, VkFilter filter) {
    StateTracker::PreCallRecordCmdBlitImage(commandBuffer, srcImage, srcImageLayout, dstImage, dstImageLayout, regionCount,
                                            pRegions, filter);
    auto cb_node = GetCBState(commandBuffer);
    auto src_image_state = GetImageState(srcImage);
    auto dst_image_state = GetImageState(dstImage);

    // Make sure that all image slices are updated to correct layout
    for (uint32_t i = 0; i < regionCount; ++i) {
        SetImageInitialLayout(cb_node, *src_image_state, pRegions[i].srcSubresource, srcImageLayout);
        SetImageInitialLayout(cb_node, *dst_image_state, pRegions[i].dstSubresource, dstImageLayout);
    }
}

GlobalImageLayoutRangeMap *GetLayoutRangeMap(GlobalImageLayoutMap *map, const IMAGE_STATE &image_state) {
    assert(map);
    // This approach allows for a single hash lookup or/create new
    auto inserted = map->emplace(std::make_pair(image_state.image, nullptr));
    if (inserted.second) {
        assert(nullptr == inserted.first->second.get());
        GlobalImageLayoutRangeMap *layout_map = new GlobalImageLayoutRangeMap(image_state.range_encoder.SubresourceCount());
        inserted.first->second.reset(layout_map);
        return layout_map;
    } else {
        assert(nullptr != inserted.first->second.get());
        return inserted.first->second.get();
    }
    return nullptr;
}

const GlobalImageLayoutRangeMap *GetLayoutRangeMap(const GlobalImageLayoutMap &map, VkImage image) {
    auto it = map.find(image);
    if (it != map.end()) {
        return it->second.get();
    }
    return nullptr;
}

// This validates that the initial layout specified in the command buffer for the IMAGE is the same as the global IMAGE layout
bool CoreChecks::ValidateCmdBufImageLayouts(const CMD_BUFFER_STATE *pCB, const GlobalImageLayoutMap &globalImageLayoutMap,
                                            GlobalImageLayoutMap *overlayLayoutMap_arg) const {
    if (disabled.image_layout_validation) return false;
    bool skip = false;
    GlobalImageLayoutMap &overlayLayoutMap = *overlayLayoutMap_arg;
    // Iterate over the layout maps for each referenced image
    GlobalImageLayoutRangeMap empty_map(1);
    for (const auto &layout_map_entry : pCB->image_layout_map) {
        const auto image = layout_map_entry.first;
        const auto *image_state = GetImageState(image);
        if (!image_state) continue;  // Can't check layouts of a dead image
        const auto &subres_map = layout_map_entry.second;
        const auto &initial_layout_map = subres_map->GetInitialLayoutMap();
        // Validate the initial_uses for each subresource referenced
        if (initial_layout_map.empty()) continue;

        auto *overlay_map = GetLayoutRangeMap(&overlayLayoutMap, *image_state);
        const auto *global_map = GetLayoutRangeMap(globalImageLayoutMap, image);
        if (global_map == nullptr) {
            global_map = &empty_map;
        }

        // Note: don't know if it would matter
        // if (global_map->empty() && overlay_map->empty()) // skip this next loop...;

        auto pos = initial_layout_map.begin();
        const auto end = initial_layout_map.end();
        sparse_container::parallel_iterator<const ImageSubresourceLayoutMap::LayoutMap> current_layout(*overlay_map, *global_map,
                                                                                                       pos->first.begin);
        while (pos != end) {
            VkImageLayout initial_layout = pos->second;
            VkImageLayout image_layout = kInvalidLayout;
            if (current_layout->range.empty()) break;  // When we are past the end of data in overlay and global... stop looking
            if (current_layout->pos_A->valid) {        // pos_A denotes the overlay map in the parallel iterator
                image_layout = current_layout->pos_A->lower_bound->second;
            } else if (current_layout->pos_B->valid) {  // pos_B denotes the global map in the parallel iterator
                image_layout = current_layout->pos_B->lower_bound->second;
            }
            const auto intersected_range = pos->first & current_layout->range;
            if (initial_layout == VK_IMAGE_LAYOUT_UNDEFINED) {
                // TODO: Set memory invalid which is in mem_tracker currently
            } else if (image_layout != initial_layout) {
                // Need to look up the inital layout *state* to get a bit more information
                const auto *initial_layout_state = subres_map->GetSubresourceInitialLayoutState(pos->first.begin);
                assert(initial_layout_state);  // There's no way we should have an initial layout without matching state...
                bool matches = ImageLayoutMatches(initial_layout_state->aspect_mask, image_layout, initial_layout);
                if (!matches) {
                    std::string formatted_label = FormatDebugLabel("" "", pCB->debug_label);
                    // We can report all the errors for the intersected range directly
                    for (auto index : sparse_container::range_view<decltype(intersected_range)>(intersected_range)) {
                        const auto subresource = image_state->range_encoder.Decode(index);
                        skip |= LogError(
                            pCB->commandBuffer, kVUID_Core_DrawState_InvalidImageLayout,
                            ""Submitted command buffer expects %s (subresource: aspectMask 0x%X array layer %u, mip level %u) ""
                            ""to be in layout %s--instead, current layout is %s.%s"",
                            report_data->FormatHandle(image).c_str(), subresource.aspectMask, subresource.arrayLayer,
                            subresource.mipLevel, string_VkImageLayout(initial_layout), string_VkImageLayout(image_layout),
                            formatted_label.c_str());
                    }
                }
            }
            if (pos->first.includes(intersected_range.end)) {
                current_layout.seek(intersected_range.end);
            } else {
                ++pos;
                if (pos != end) {
                    current_layout.seek(pos->first.begin);
                }
            }
        }

        // Update all layout set operations (which will be a subset of the initial_layouts)
        sparse_container::splice(overlay_map, subres_map->GetCurrentLayoutMap(), sparse_container::value_precedence::prefer_source);
    }

    return skip;
}

void CoreChecks::UpdateCmdBufImageLayouts(CMD_BUFFER_STATE *pCB) {
    for (const auto &layout_map_entry : pCB->image_layout_map) {
        const auto image = layout_map_entry.first;
        const auto &subres_map = layout_map_entry.second;
        const auto *image_state = GetImageState(image);
        if (!image_state) continue;  // Can't set layouts of a dead image
        auto *global_map = GetLayoutRangeMap(&imageLayoutMap, *image_state);
        sparse_container::splice(global_map, subres_map->GetCurrentLayoutMap(), sparse_container::value_precedence::prefer_source);
    }
}

// ValidateLayoutVsAttachmentDescription is a general function where we can validate various state associated with the
// VkAttachmentDescription structs that are used by the sub-passes of a renderpass. Initial check is to make sure that READ_ONLY
// layout attachments don't have CLEAR as their loadOp.
bool CoreChecks::ValidateLayoutVsAttachmentDescription(const debug_report_data *report_data, RenderPassCreateVersion rp_version,
    // for both loadOp and stencilLoaOp rp2 has it in 1 VU while rp1 has it in 2 VU with half behind Maintenance2 extension
    // Each is VUID is below in following order: rp2 -> rp1 with Maintenance2 -> rp1 with no extenstion
                                                       const VkImageLayout first_layout, const uint32_t attachment,
                                                       const VkAttachmentDescription2KHR &attachment_description) const {
    bool skip = false;
    const char *vuid;
    const bool use_rp2 = (rp_version == RENDER_PASS_VERSION_2);

    // Verify that initial loadOp on READ_ONLY attachments is not CLEAR
    if (attachment_description.loadOp == VK_ATTACHMENT_LOAD_OP_CLEAR) {
        if (use_rp2 && ((first_layout == VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL) ||
                        (first_layout == VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL) ||
                        (first_layout == VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL))) {
            skip |= LogError(device, ""VUID-VkRenderPassCreateInfo2-pAttachments-02522"",
                             ""Cannot clear attachment %d with invalid first layout %s."", attachment,
                             string_VkImageLayout(first_layout));
        } else if (!use_rp2 && ((first_layout == VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL) ||
                                (first_layout == VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL))) {
            skip |= LogError(device, ""VUID-VkRenderPassCreateInfo-pAttachments-00836"",
                             ""Cannot clear attachment %d with invalid first layout %s."", attachment,
                             string_VkImageLayout(first_layout));
        }
    }
    if (attachment_description.loadOp == VK_ATTACHMENT_LOAD_OP_CLEAR) {
        if (first_layout == VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL) {
            vuid = use_rp2 ? kVUID_Core_DrawState_InvalidRenderpass : ""VUID-VkRenderPassCreateInfo-pAttachments-01566"";
            skip |= LogError(device, vuid, ""Cannot clear attachment %d with invalid first layout %s."", attachment,
                             string_VkImageLayout(first_layout));
        }
    }

    if (attachment_description.stencilLoadOp == VK_ATTACHMENT_LOAD_OP_CLEAR) {
        if (first_layout == VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL) {
            vuid = use_rp2 ? kVUID_Core_DrawState_InvalidRenderpass : ""VUID-VkRenderPassCreateInfo-pAttachments-01567"";
            skip |= LogError(device, vuid, ""Cannot clear attachment %d with invalid first layout %s."", attachment,
                             string_VkImageLayout(first_layout));
        }
    }
    return skip;
}

bool CoreChecks::ValidateLayouts(RenderPassCreateVersion rp_version, VkDevice device, const VkRenderPassCreateInfo2KHR *pCreateInfo,
                                 const char *function_name) const {
    bool skip = false;
    const char *vuid;
    const bool use_rp2 = (rp_version == RENDER_PASS_VERSION_2);

    for (uint32_t i = 0; i < pCreateInfo->attachmentCount; ++i) {
        VkFormat format = pCreateInfo->pAttachments[i].format;
        if (pCreateInfo->pAttachments[i].initialLayout == VK_IMAGE_LAYOUT_UNDEFINED) {
            if ((FormatIsColor(format) || FormatHasDepth(format)) &&
                pCreateInfo->pAttachments[i].loadOp == VK_ATTACHMENT_LOAD_OP_LOAD) {
                skip |= LogWarning(device, kVUID_Core_DrawState_InvalidRenderpass,
                                   ""Render pass has an attachment with loadOp == VK_ATTACHMENT_LOAD_OP_LOAD and initialLayout == ""
                                   ""VK_IMAGE_LAYOUT_UNDEFINED.  This is probably not what you intended.  Consider using ""
                                   ""VK_ATTACHMENT_LOAD_OP_DONT_CARE instead if the image truely is undefined at the start of the ""
                                   ""render pass."");
            }
            if (FormatHasStencil(format) && pCreateInfo->pAttachments[i].stencilLoadOp == VK_ATTACHMENT_LOAD_OP_LOAD) {
                skip |=
                    LogWarning(device, kVUID_Core_DrawState_InvalidRenderpass,
                               ""Render pass has an attachment with stencilLoadOp == VK_ATTACHMENT_LOAD_OP_LOAD and initialLayout ""
                               ""== VK_IMAGE_LAYOUT_UNDEFINED.  This is probably not what you intended.  Consider using ""
                               ""VK_ATTACHMENT_LOAD_OP_DONT_CARE instead if the image truely is undefined at the start of the ""
                               ""render pass."");
            }
        }
    }

    // Track when we're observing the first use of an attachment
    std::vector<bool> attach_first_use(pCreateInfo->attachmentCount, true);
    for (uint32_t i = 0; i < pCreateInfo->subpassCount; ++i) {
        const VkSubpassDescription2KHR &subpass = pCreateInfo->pSubpasses[i];

        // Check input attachments first, so we can detect first-use-as-input for VU #00349
        for (uint32_t j = 0; j < subpass.inputAttachmentCount; ++j) {
            auto attach_index = subpass.pInputAttachments[j].attachment;
            if (attach_index == VK_ATTACHMENT_UNUSED) continue;
            switch (subpass.pInputAttachments[j].layout) {
                case VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL:
                case VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL:
                    // These are ideal.
                    break;

                case VK_IMAGE_LAYOUT_GENERAL:
                    // May not be optimal. TODO: reconsider this warning based on other constraints.
                    skip |= LogPerformanceWarning(device, kVUID_Core_DrawState_InvalidImageLayout,
                                                  ""Layout for input attachment is GENERAL but should be READ_ONLY_OPTIMAL."");
                    break;

                case VK_IMAGE_LAYOUT_UNDEFINED:
                case VK_IMAGE_LAYOUT_PREINITIALIZED:
                    vuid = use_rp2 ? ""VUID-VkAttachmentReference2-layout-03077"" : ""VUID-VkAttachmentReference-layout-00857"";
                    skip |= LogError(device, vuid,
                                     ""Layout for input attachment reference %u in subpass %u is %s but must be ""
                                     ""DEPTH_STENCIL_READ_ONLY, SHADER_READ_ONLY_OPTIMAL, or GENERAL."",
                                     j, i, string_VkImageLayout(subpass.pInputAttachments[j].layout));
                    break;

                case VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR:
                case VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR:
                case VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR:
                case VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR:
                    if (!enabled_features.core12.separateDepthStencilLayouts) {
                        skip |= LogError(
                            device, ""VUID-VkAttachmentReference2-separateDepthStencilLayouts-03313"",
                            ""Layout for input attachment reference %u in subpass %u is %s but must not be ""
                            ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR, VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR, ""
                            ""VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR, or VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR."",
                            j, i, string_VkImageLayout(subpass.pInputAttachments[j].layout));
                    } else if (subpass.pInputAttachments[j].aspectMask & VK_IMAGE_ASPECT_COLOR_BIT) {
                        skip |= LogError(
                            device, ""VUID-VkAttachmentReference2-attachment-03314"",
                            ""Layout for input attachment reference %u in subpass %u is %s but must not be ""
                            ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR, VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR, ""
                            ""VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR, or VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR."",
                            j, i, string_VkImageLayout(subpass.pInputAttachments[j].layout));
                    } else if ((subpass.pInputAttachments[j].aspectMask &
                                (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT)) ==
                               (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT)) {
                        if (subpass.pInputAttachments[j].layout == VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR ||
                            subpass.pInputAttachments[j].layout == VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR) {
                            const auto *attachment_reference_stencil_layout =
                                lvl_find_in_chain<VkAttachmentReferenceStencilLayoutKHR>(subpass.pInputAttachments[j].pNext);
                            if (attachment_reference_stencil_layout) {
                                if (attachment_reference_stencil_layout->stencilLayout == VK_IMAGE_LAYOUT_UNDEFINED ||
                                    attachment_reference_stencil_layout->stencilLayout == VK_IMAGE_LAYOUT_PREINITIALIZED ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout == VK_IMAGE_LAYOUT_PRESENT_SRC_KHR) {
                                    skip |= LogError(device, ""VUID-VkAttachmentReferenceStencilLayout-stencilLayout-03318"",
                                                     ""In the attachment reference %u in subpass %u with pNext chain instance ""
                                                     ""VkAttachmentReferenceStencilLayoutKHR""
                                                     ""the stencilLayout member but must not be ""
                                                     ""VK_IMAGE_LAYOUT_UNDEFINED, VK_IMAGE_LAYOUT_PREINITIALIZED, ""
                                                     ""VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL, or ""
                                                     ""VK_IMAGE_LAYOUT_PRESENT_SRC_KHR."",
                                                     j, i);
                                }
                            } else {
                                skip |= LogError(
                                    device, ""VUID-VkAttachmentReference2-attachment-03315"",
                                    ""When the layout for input attachment reference %u in subpass %u is ""
                                    ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR or ""
                                    ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR then the pNext chain must include a valid ""
                                    ""VkAttachmentReferenceStencilLayout instance."",
                                    j, i);
                            }
                        }
                    } else if (subpass.pInputAttachments[j].aspectMask & VK_IMAGE_ASPECT_DEPTH_BIT) {
                        if (subpass.pInputAttachments[j].layout == VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR ||
                            subpass.pInputAttachments[j].layout == VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR) {
                            skip |= LogError(
                                device, ""VUID-VkAttachmentReference2-attachment-03315"",
                                ""When the aspectMask for input attachment reference %u in subpass %u is VK_IMAGE_ASPECT_DEPTH_BIT ""
                                ""then the layout must not be VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR, or ""
                                ""VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR."",
                                j, i);
                        }
                    } else if (subpass.pInputAttachments[j].aspectMask & VK_IMAGE_ASPECT_STENCIL_BIT) {
                        if (subpass.pInputAttachments[j].layout == VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR ||
                            subpass.pInputAttachments[j].layout == VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR) {
                            skip |= LogError(device, ""VUID-VkAttachmentReference2-attachment-03317"",
                                             ""When the aspectMask for input attachment reference %u in subpass %u is ""
                                             ""VK_IMAGE_ASPECT_STENCIL_BIT ""
                                             ""then the layout must not be VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL, or ""
                                             ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL."",
                                             j, i);
                        }
                    }
                    break;

                case VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL_KHR:
                case VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL_KHR:
                    if (device_extensions.vk_khr_maintenance2) {
                        break;
                    } else {
                        // Intentionally fall through to generic error message
                    }
                    // fall through

                default:
                    // No other layouts are acceptable
                    skip |= LogError(device, kVUID_Core_DrawState_InvalidImageLayout,
                                     ""Layout for input attachment is %s but can only be READ_ONLY_OPTIMAL or GENERAL."",
                                     string_VkImageLayout(subpass.pInputAttachments[j].layout));
            }

            if (attach_first_use[attach_index]) {
                skip |= ValidateLayoutVsAttachmentDescription(report_data, rp_version, subpass.pInputAttachments[j].layout,
                                                              attach_index, pCreateInfo->pAttachments[attach_index]);

                bool used_as_depth =
                    (subpass.pDepthStencilAttachment != NULL && subpass.pDepthStencilAttachment->attachment == attach_index);
                bool used_as_color = false;
                for (uint32_t k = 0; !used_as_depth && !used_as_color && k < subpass.colorAttachmentCount; ++k) {
                    used_as_color = (subpass.pColorAttachments[k].attachment == attach_index);
                }
                if (!used_as_depth && !used_as_color &&
                    pCreateInfo->pAttachments[attach_index].loadOp == VK_ATTACHMENT_LOAD_OP_CLEAR) {
                    vuid = use_rp2 ? ""VUID-VkSubpassDescription2-loadOp-03064"" : ""VUID-VkSubpassDescription-loadOp-00846"";
                    skip |= LogError(device, vuid,
                                     ""%s: attachment %u is first used as an input attachment in subpass %u with loadOp=CLEAR."",
                                     function_name, attach_index, attach_index);
                }
            }
            attach_first_use[attach_index] = false;
        }

        for (uint32_t j = 0; j < subpass.colorAttachmentCount; ++j) {
            auto attach_index = subpass.pColorAttachments[j].attachment;
            if (attach_index == VK_ATTACHMENT_UNUSED) continue;

            // TODO: Need a way to validate shared presentable images here, currently just allowing
            // VK_IMAGE_LAYOUT_SHARED_PRESENT_KHR
            //  as an acceptable layout, but need to make sure shared presentable images ONLY use that layout
            switch (subpass.pColorAttachments[j].layout) {
                case VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL:
                // This is ideal.
                case VK_IMAGE_LAYOUT_SHARED_PRESENT_KHR:
                    // TODO: See note above, just assuming that attachment is shared presentable and allowing this for now.
                    break;

                case VK_IMAGE_LAYOUT_GENERAL:
                    // May not be optimal; TODO: reconsider this warning based on other constraints?
                    skip |= LogPerformanceWarning(device, kVUID_Core_DrawState_InvalidImageLayout,
                                                  ""Layout for color attachment is GENERAL but should be COLOR_ATTACHMENT_OPTIMAL."");
                    break;

                case VK_IMAGE_LAYOUT_UNDEFINED:
                case VK_IMAGE_LAYOUT_PREINITIALIZED:
                    vuid = use_rp2 ? ""VUID-VkAttachmentReference2-layout-03077"" : ""VUID-VkAttachmentReference-layout-00857"";
                    skip |= LogError(device, vuid,
                                     ""Layout for color attachment reference %u in subpass %u is %s but should be ""
                                     ""COLOR_ATTACHMENT_OPTIMAL or GENERAL."",
                                     j, i, string_VkImageLayout(subpass.pColorAttachments[j].layout));
                    break;

                case VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR:
                case VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR:
                case VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR:
                case VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR:
                    if (!enabled_features.core12.separateDepthStencilLayouts) {
                        skip |= LogError(
                            device, ""VUID-VkAttachmentReference2-separateDepthStencilLayouts-03313"",
                            ""Layout for color attachment reference %u in subpass %u is %s but must not be ""
                            ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR, VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR, ""
                            ""VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR, or VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR."",
                            j, i, string_VkImageLayout(subpass.pColorAttachments[j].layout));
                    } else if (subpass.pColorAttachments[j].aspectMask & VK_IMAGE_ASPECT_COLOR_BIT) {
                        skip |= LogError(
                            device, ""VUID-VkAttachmentReference2-attachment-03314"",
                            ""Layout for color attachment reference %u in subpass %u is %s but must not be ""
                            ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR, VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR, ""
                            ""VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR, or VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR."",
                            j, i, string_VkImageLayout(subpass.pColorAttachments[j].layout));
                    } else if ((subpass.pColorAttachments[j].aspectMask &
                                (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT)) ==
                               (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT)) {
                        if (subpass.pColorAttachments[j].layout == VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR ||
                            subpass.pColorAttachments[j].layout == VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR) {
                            const auto *attachment_reference_stencil_layout =
                                lvl_find_in_chain<VkAttachmentReferenceStencilLayoutKHR>(subpass.pColorAttachments[j].pNext);
                            if (attachment_reference_stencil_layout) {
                                if (attachment_reference_stencil_layout->stencilLayout == VK_IMAGE_LAYOUT_UNDEFINED ||
                                    attachment_reference_stencil_layout->stencilLayout == VK_IMAGE_LAYOUT_PREINITIALIZED ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout == VK_IMAGE_LAYOUT_PRESENT_SRC_KHR) {
                                    skip |= LogError(device, ""VUID-VkAttachmentReferenceStencilLayout-stencilLayout-03318"",
                                                     ""In the attachment reference %u in subpass %u with pNext chain instance ""
                                                     ""VkAttachmentReferenceStencilLayoutKHR""
                                                     ""the stencilLayout member but must not be ""
                                                     ""VK_IMAGE_LAYOUT_UNDEFINED, VK_IMAGE_LAYOUT_PREINITIALIZED, ""
                                                     ""VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL, or ""
                                                     ""VK_IMAGE_LAYOUT_PRESENT_SRC_KHR."",
                                                     j, i);
                                }
                            } else {
                                skip |= LogError(
                                    device, ""VUID-VkAttachmentReference2-attachment-03315"",
                                    ""When the layout for color attachment reference %u in subpass %u is ""
                                    ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR or ""
                                    ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR then the pNext chain must include a valid ""
                                    ""VkAttachmentReferenceStencilLayout instance."",
                                    j, i);
                            }
                        }
                    } else if (subpass.pColorAttachments[j].aspectMask & VK_IMAGE_ASPECT_DEPTH_BIT) {
                        if (subpass.pColorAttachments[j].layout == VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR ||
                            subpass.pColorAttachments[j].layout == VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR) {
                            skip |= LogError(
                                device, ""VUID-VkAttachmentReference2-attachment-03315"",
                                ""When the aspectMask for color attachment reference %u in subpass %u is VK_IMAGE_ASPECT_DEPTH_BIT ""
                                ""then the layout must not be VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR, or ""
                                ""VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR."",
                                j, i);
                        }
                    } else if (subpass.pColorAttachments[j].aspectMask & VK_IMAGE_ASPECT_STENCIL_BIT) {
                        if (subpass.pColorAttachments[j].layout == VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR ||
                            subpass.pColorAttachments[j].layout == VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR) {
                            skip |= LogError(device, ""VUID-VkAttachmentReference2-attachment-03317"",
                                             ""When the aspectMask for color attachment reference %u in subpass %u is ""
                                             ""VK_IMAGE_ASPECT_STENCIL_BIT ""
                                             ""then the layout must not be VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL, or ""
                                             ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL."",
                                             j, i);
                        }
                    }
                    break;

                default:
                    skip |= LogError(device, kVUID_Core_DrawState_InvalidImageLayout,
                                     ""Layout for color attachment is %s but can only be COLOR_ATTACHMENT_OPTIMAL or GENERAL."",
                                     string_VkImageLayout(subpass.pColorAttachments[j].layout));
            }

            if (subpass.pResolveAttachments && (subpass.pResolveAttachments[j].attachment != VK_ATTACHMENT_UNUSED) &&
                (subpass.pResolveAttachments[j].layout == VK_IMAGE_LAYOUT_UNDEFINED ||
                 subpass.pResolveAttachments[j].layout == VK_IMAGE_LAYOUT_PREINITIALIZED)) {
                vuid = use_rp2 ? ""VUID-VkAttachmentReference2-layout-03077"" : ""VUID-VkAttachmentReference-layout-00857"";
                skip |= LogError(device, vuid,
                                 ""Layout for resolve attachment reference %u in subpass %u is %s but should be ""
                                 ""COLOR_ATTACHMENT_OPTIMAL or GENERAL."",
                                 j, i, string_VkImageLayout(subpass.pResolveAttachments[j].layout));
            }

            if (attach_first_use[attach_index]) {
                skip |= ValidateLayoutVsAttachmentDescription(report_data, rp_version, subpass.pColorAttachments[j].layout,
                                                              attach_index, pCreateInfo->pAttachments[attach_index]);
            }
            attach_first_use[attach_index] = false;
        }

        if (subpass.pDepthStencilAttachment && subpass.pDepthStencilAttachment->attachment != VK_ATTACHMENT_UNUSED) {
            switch (subpass.pDepthStencilAttachment->layout) {
                case VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL:
                case VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL:
                    // These are ideal.
                    break;

                case VK_IMAGE_LAYOUT_GENERAL:
                    // May not be optimal; TODO: reconsider this warning based on other constraints? GENERAL can be better than
                    // doing a bunch of transitions.
                    skip |= LogPerformanceWarning(device, kVUID_Core_DrawState_InvalidImageLayout,
                                                  ""GENERAL layout for depth attachment may not give optimal performance."");
                    break;

                case VK_IMAGE_LAYOUT_UNDEFINED:
                case VK_IMAGE_LAYOUT_PREINITIALIZED:
                    vuid = use_rp2 ? ""VUID-VkAttachmentReference2-layout-03077"" : ""VUID-VkAttachmentReference-layout-00857"";
                    skip |= LogError(device, vuid,
                                     ""Layout for depth attachment reference in subpass %u is %s but must be a valid depth/stencil ""
                                     ""layout or GENERAL."",
                                     i, string_VkImageLayout(subpass.pDepthStencilAttachment->layout));
                    break;

                case VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR:
                case VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR:
                case VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR:
                case VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR:
                    if (!enabled_features.core12.separateDepthStencilLayouts) {
                        skip |= LogError(
                            device, ""VUID-VkAttachmentReference2-separateDepthStencilLayouts-03313"",
                            ""Layout for depth attachment reference in subpass %u is %s but must not be ""
                            ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR, VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR, ""
                            ""VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR, or VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR."",
                            i, string_VkImageLayout(subpass.pDepthStencilAttachment->layout));
                    } else if (subpass.pDepthStencilAttachment->aspectMask & VK_IMAGE_ASPECT_COLOR_BIT) {
                        skip |= LogError(
                            device, ""VUID-VkAttachmentReference2-attachment-03314"",
                            ""Layout for depth attachment reference in subpass %u is %s but must not be ""
                            ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR, VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR, ""
                            ""VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR, or VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR."",
                            i, string_VkImageLayout(subpass.pDepthStencilAttachment->layout));
                    } else if ((subpass.pDepthStencilAttachment->aspectMask &
                                (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT)) ==
                               (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT)) {
                        if (subpass.pDepthStencilAttachment->layout == VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR ||
                            subpass.pDepthStencilAttachment->layout == VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR) {
                            const auto *attachment_reference_stencil_layout =
                                lvl_find_in_chain<VkAttachmentReferenceStencilLayoutKHR>(subpass.pDepthStencilAttachment->pNext);
                            if (attachment_reference_stencil_layout) {
                                if (attachment_reference_stencil_layout->stencilLayout == VK_IMAGE_LAYOUT_UNDEFINED ||
                                    attachment_reference_stencil_layout->stencilLayout == VK_IMAGE_LAYOUT_PREINITIALIZED ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout == VK_IMAGE_LAYOUT_PRESENT_SRC_KHR) {
                                    skip |= LogError(device, ""VUID-VkAttachmentReferenceStencilLayout-stencilLayout-03318"",
                                                     ""In the attachment reference in subpass %u with pNext chain instance ""
                                                     ""VkAttachmentReferenceStencilLayoutKHR""
                                                     ""the stencilLayout member but must not be ""
                                                     ""VK_IMAGE_LAYOUT_UNDEFINED, VK_IMAGE_LAYOUT_PREINITIALIZED, ""
                                                     ""VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL, or ""
                                                     ""VK_IMAGE_LAYOUT_PRESENT_SRC_KHR."",
                                                     i);
                                }
                            } else {
                                skip |= LogError(
                                    device, ""VUID-VkAttachmentReference2-attachment-03315"",
                                    ""When the layout for depth attachment reference in subpass %u is ""
                                    ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR or ""
                                    ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR then the pNext chain must include a valid ""
                                    ""VkAttachmentReferenceStencilLayout instance."",
                                    i);
                            }
                        }
                    } else if (subpass.pDepthStencilAttachment->aspectMask & VK_IMAGE_ASPECT_DEPTH_BIT) {
                        if (subpass.pDepthStencilAttachment->layout == VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR ||
                            subpass.pDepthStencilAttachment->layout == VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR) {
                            skip |= LogError(
                                device, ""VUID-VkAttachmentReference2-attachment-03315"",
                                ""When the aspectMask for depth attachment reference in subpass %u is VK_IMAGE_ASPECT_DEPTH_BIT ""
                                ""then the layout must not be VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR, or ""
                                ""VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR."",
                                i);
                        }
                    } else if (subpass.pDepthStencilAttachment->aspectMask & VK_IMAGE_ASPECT_STENCIL_BIT) {
                        if (subpass.pDepthStencilAttachment->layout == VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR ||
                            subpass.pDepthStencilAttachment->layout == VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR) {
                            skip |= LogError(device, ""VUID-VkAttachmentReference2-attachment-03317"",
                                             ""When the aspectMask for depth attachment reference in subpass %u is ""
                                             ""VK_IMAGE_ASPECT_STENCIL_BIT ""
                                             ""then the layout must not be VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL, or ""
                                             ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL."",
                                             i);
                        }
                    }
                    break;

                case VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL_KHR:
                case VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL_KHR:
                    if (device_extensions.vk_khr_maintenance2) {
                        break;
                    } else {
                        // Intentionally fall through to generic error message
                    }
                    // fall through

                default:
                    // No other layouts are acceptable
                    skip |= LogError(device, kVUID_Core_DrawState_InvalidImageLayout,
                                     ""Layout for depth attachment is %s but can only be DEPTH_STENCIL_ATTACHMENT_OPTIMAL, ""
                                     ""DEPTH_STENCIL_READ_ONLY_OPTIMAL or GENERAL."",
                                     string_VkImageLayout(subpass.pDepthStencilAttachment->layout));
            }

            auto attach_index = subpass.pDepthStencilAttachment->attachment;
            if (attach_first_use[attach_index]) {
                skip |= ValidateLayoutVsAttachmentDescription(report_data, rp_version, subpass.pDepthStencilAttachment->layout,
                                                              attach_index, pCreateInfo->pAttachments[attach_index]);
            }
            attach_first_use[attach_index] = false;
        }
    }
    return skip;
}

// Helper function to validate correct usage bits set for buffers or images. Verify that (actual & desired) flags != 0 or, if strict
// is true, verify that (actual & desired) flags == desired
template <typename T1>
bool CoreChecks::ValidateUsageFlags(VkFlags actual, VkFlags desired, VkBool32 strict, const T1 object,
                                    const VulkanTypedHandle &typed_handle, const char *msgCode, char const *func_name,
                                    char const *usage_str) const {
    bool correct_usage = false;
    bool skip = false;
    const char *type_str = object_string[typed_handle.type];
    if (strict) {
        correct_usage = ((actual & desired) == desired);
    } else {
        correct_usage = ((actual & desired) != 0);
    }

    if (!correct_usage) {
        if (msgCode == kVUIDUndefined) {
            // TODO: Fix callers with kVUIDUndefined to use correct validation checks.
            skip = LogError(object, kVUID_Core_MemTrack_InvalidUsageFlag,
                            ""Invalid usage flag for %s used by %s. In this case, %s should have %s set during creation."",
                            report_data->FormatHandle(typed_handle).c_str(), func_name, type_str, usage_str);
        } else {
            skip = LogError(object, msgCode,
                            ""Invalid usage flag for %s used by %s. In this case, %s should have %s set during creation."",
                            report_data->FormatHandle(typed_handle).c_str(), func_name, type_str, usage_str);
        }
    }
    return skip;
}

// Helper function to validate usage flags for buffers. For given buffer_state send actual vs. desired usage off to helper above
// where an error will be flagged if usage is not correct
bool CoreChecks::ValidateImageUsageFlags(IMAGE_STATE const *image_state, VkFlags desired, bool strict, const char *msgCode,
                                         char const *func_name, char const *usage_string) const {
    return ValidateUsageFlags(image_state->createInfo.usage, desired, strict, image_state->image,
                              VulkanTypedHandle(image_state->image, kVulkanObjectTypeImage), msgCode, func_name, usage_string);
}

bool CoreChecks::ValidateImageFormatFeatureFlags(IMAGE_STATE const *image_state, VkFormatFeatureFlags desired,
                                                 char const *func_name, const char *linear_vuid, const char *optimal_vuid) const {
    VkFormatProperties format_properties = GetPDFormatProperties(image_state->createInfo.format);
    bool skip = false;
    if (image_state->createInfo.tiling == VK_IMAGE_TILING_LINEAR) {
        if ((format_properties.linearTilingFeatures & desired) != desired) {
            skip |=
                LogError(image_state->image, linear_vuid, ""In %s, invalid linearTilingFeatures (0x%08X) for format %u used by %s."",
                         func_name, format_properties.linearTilingFeatures, image_state->createInfo.format,
                         report_data->FormatHandle(image_state->image).c_str());
        }
    } else if (image_state->createInfo.tiling == VK_IMAGE_TILING_OPTIMAL) {
        if ((format_properties.optimalTilingFeatures & desired) != desired) {
            skip |= LogError(image_state->image, optimal_vuid,
                             ""In %s, invalid optimalTilingFeatures (0x%08X) for format %u used by %s."", func_name,
                             format_properties.optimalTilingFeatures, image_state->createInfo.format,
                             report_data->FormatHandle(image_state->image).c_str());
        }
    }
    return skip;
}

bool CoreChecks::ValidateImageSubresourceLayers(const CMD_BUFFER_STATE *cb_node, const VkImageSubresourceLayers *subresource_layers,
                                                char const *func_name, char const *member, uint32_t i) const {
    bool skip = false;
    // layerCount must not be zero
    if (subresource_layers->layerCount == 0) {
        skip |= LogError(cb_node->commandBuffer, ""VUID-VkImageSubresourceLayers-layerCount-01700"",
                         ""In %s, pRegions[%u].%s.layerCount must not be zero."", func_name, i, member);
    }
    // aspectMask must not contain VK_IMAGE_ASPECT_METADATA_BIT
    if (subresource_layers->aspectMask & VK_IMAGE_ASPECT_METADATA_BIT) {
        skip |= LogError(cb_node->commandBuffer, ""VUID-VkImageSubresourceLayers-aspectMask-00168"",
                         ""In %s, pRegions[%u].%s.aspectMask has VK_IMAGE_ASPECT_METADATA_BIT set."", func_name, i, member);
    }
    // if aspectMask contains COLOR, it must not contain either DEPTH or STENCIL
    if ((subresource_layers->aspectMask & VK_IMAGE_ASPECT_COLOR_BIT) &&
        (subresource_layers->aspectMask & (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT))) {
        skip |= LogError(cb_node->commandBuffer, ""VUID-VkImageSubresourceLayers-aspectMask-00167"",
                         ""In %s, pRegions[%u].%s.aspectMask has VK_IMAGE_ASPECT_COLOR_BIT and either VK_IMAGE_ASPECT_DEPTH_BIT or ""
                         ""VK_IMAGE_ASPECT_STENCIL_BIT set."",
                         func_name, i, member);
    }
    return skip;
}

// Helper function to validate usage flags for buffers. For given buffer_state send actual vs. desired usage off to helper above
// where an error will be flagged if usage is not correct
bool CoreChecks::ValidateBufferUsageFlags(BUFFER_STATE const *buffer_state, VkFlags desired, bool strict, const char *msgCode,
                                          char const *func_name, char const *usage_string) const {
    return ValidateUsageFlags(buffer_state->createInfo.usage, desired, strict, buffer_state->buffer,
                              VulkanTypedHandle(buffer_state->buffer, kVulkanObjectTypeBuffer), msgCode, func_name, usage_string);
}

bool CoreChecks::ValidateBufferViewRange(const BUFFER_STATE *buffer_state, const VkBufferViewCreateInfo *pCreateInfo,
                                         const VkPhysicalDeviceLimits *device_limits) const {
    bool skip = false;

    const VkDeviceSize &range = pCreateInfo->range;
    if (range != VK_WHOLE_SIZE) {
        // Range must be greater than 0
        if (range <= 0) {
            skip |= LogError(buffer_state->buffer, ""VUID-VkBufferViewCreateInfo-range-00928"",
                             ""If VkBufferViewCreateInfo range (%"" PRIuLEAST64
                             "") does not equal VK_WHOLE_SIZE, range must be greater than 0."",
                             range);
        }
        // Range must be a multiple of the element size of format
        const uint32_t format_size = FormatElementSize(pCreateInfo->format);
        if (SafeModulo(range, format_size) != 0) {
            skip |= LogError(buffer_state->buffer, ""VUID-VkBufferViewCreateInfo-range-00929"",
                             ""If VkBufferViewCreateInfo range (%"" PRIuLEAST64
                             "") does not equal VK_WHOLE_SIZE, range must be a multiple of the element size of the format ""
                             ""(%"" PRIu32 "")."",
                             range, format_size);
        }
        // Range divided by the element size of format must be less than or equal to VkPhysicalDeviceLimits::maxTexelBufferElements
        if (SafeDivision(range, format_size) > device_limits->maxTexelBufferElements) {
            skip |= LogError(buffer_state->buffer, ""VUID-VkBufferViewCreateInfo-range-00930"",
                             ""If VkBufferViewCreateInfo range (%"" PRIuLEAST64
                             "") does not equal VK_WHOLE_SIZE, range divided by the element size of the format (%"" PRIu32
                             "") must be less than or equal to VkPhysicalDeviceLimits::maxTexelBufferElements (%"" PRIuLEAST32 "")."",
                             range, format_size, device_limits->maxTexelBufferElements);
        }
        // The sum of range and offset must be less than or equal to the size of buffer
        if (range + pCreateInfo->offset > buffer_state->createInfo.size) {
            skip |= LogError(buffer_state->buffer, ""VUID-VkBufferViewCreateInfo-offset-00931"",
                             ""If VkBufferViewCreateInfo range (%"" PRIuLEAST64
                             "") does not equal VK_WHOLE_SIZE, the sum of offset (%"" PRIuLEAST64
                             "") and range must be less than or equal to the size of the buffer (%"" PRIuLEAST64 "")."",
                             range, pCreateInfo->offset, buffer_state->createInfo.size);
        }
    }
    return skip;
}

bool CoreChecks::ValidateBufferViewBuffer(const BUFFER_STATE *buffer_state, const VkBufferViewCreateInfo *pCreateInfo) const {
    bool skip = false;
    const VkFormatProperties format_properties = GetPDFormatProperties(pCreateInfo->format);
    if ((buffer_state->createInfo.usage & VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT) &&
        !(format_properties.bufferFeatures & VK_FORMAT_FEATURE_UNIFORM_TEXEL_BUFFER_BIT)) {
        skip |= LogError(buffer_state->buffer, ""VUID-VkBufferViewCreateInfo-buffer-00933"",
                         ""If buffer was created with `usage` containing VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT, format must ""
                         ""be supported for uniform texel buffers"");
    }
    if ((buffer_state->createInfo.usage & VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT) &&
        !(format_properties.bufferFeatures & VK_FORMAT_FEATURE_STORAGE_TEXEL_BUFFER_BIT)) {
        skip |= LogError(buffer_state->buffer, ""VUID-VkBufferViewCreateInfo-buffer-00934"",
                         ""If buffer was created with `usage` containing VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT, format must ""
                         ""be supported for storage texel buffers"");
    }
    return skip;
}

bool CoreChecks::PreCallValidateCreateBuffer(VkDevice device, const VkBufferCreateInfo *pCreateInfo,
                                             const VkAllocationCallbacks *pAllocator, VkBuffer *pBuffer) const {
    bool skip = false;

    // TODO: Add check for ""VUID-vkCreateBuffer-flags-00911""        (sparse address space accounting)

    if ((pCreateInfo->flags & VK_BUFFER_CREATE_SPARSE_BINDING_BIT) && (!enabled_features.core.sparseBinding)) {
        skip |= LogError(device, ""VUID-VkBufferCreateInfo-flags-00915"",
                         ""vkCreateBuffer(): the sparseBinding device feature is disabled: Buffers cannot be created with the ""
                         ""VK_BUFFER_CREATE_SPARSE_BINDING_BIT set."");
    }

    if ((pCreateInfo->flags & VK_BUFFER_CREATE_SPARSE_RESIDENCY_BIT) && (!enabled_features.core.sparseResidencyBuffer)) {
        skip |= LogError(device, ""VUID-VkBufferCreateInfo-flags-00916"",
                         ""vkCreateBuffer(): the sparseResidencyBuffer device feature is disabled: Buffers cannot be created with ""
                         ""the VK_BUFFER_CREATE_SPARSE_RESIDENCY_BIT set."");
    }

    if ((pCreateInfo->flags & VK_BUFFER_CREATE_SPARSE_ALIASED_BIT) && (!enabled_features.core.sparseResidencyAliased)) {
        skip |= LogError(device, ""VUID-VkBufferCreateInfo-flags-00917"",
                         ""vkCreateBuffer(): the sparseResidencyAliased device feature is disabled: Buffers cannot be created with ""
                         ""the VK_BUFFER_CREATE_SPARSE_ALIASED_BIT set."");
    }

    auto chained_devaddr_struct = lvl_find_in_chain<VkBufferDeviceAddressCreateInfoEXT>(pCreateInfo->pNext);
    if (chained_devaddr_struct) {
        if (!(pCreateInfo->flags & VK_BUFFER_CREATE_DEVICE_ADDRESS_CAPTURE_REPLAY_BIT_EXT) &&
            chained_devaddr_struct->deviceAddress != 0) {
            skip |= LogError(device, ""VUID-VkBufferCreateInfo-deviceAddress-02604"",
                             ""vkCreateBuffer(): Non-zero VkBufferDeviceAddressCreateInfoEXT::deviceAddress ""
                             ""requires VK_BUFFER_CREATE_DEVICE_ADDRESS_CAPTURE_REPLAY_BIT_EXT."");
        }
    }

    auto chained_opaqueaddr_struct = lvl_find_in_chain<VkBufferOpaqueCaptureAddressCreateInfoKHR>(pCreateInfo->pNext);
    if (chained_opaqueaddr_struct) {
        if (!(pCreateInfo->flags & VK_BUFFER_CREATE_DEVICE_ADDRESS_CAPTURE_REPLAY_BIT_KHR) &&
            chained_opaqueaddr_struct->opaqueCaptureAddress != 0) {
            skip |= LogError(device, ""VUID-VkBufferCreateInfo-opaqueCaptureAddress-03337"",
                             ""vkCreateBuffer(): Non-zero VkBufferOpaqueCaptureAddressCreateInfoKHR::opaqueCaptureAddress""
                             ""requires VK_BUFFER_CREATE_DEVICE_ADDRESS_CAPTURE_REPLAY_BIT_KHR."");
        }
    }

    if ((pCreateInfo->flags & VK_BUFFER_CREATE_DEVICE_ADDRESS_CAPTURE_REPLAY_BIT_KHR) &&
        !enabled_features.core12.bufferDeviceAddressCaptureReplay) {
        skip |= LogError(
            device, ""VUID-VkBufferCreateInfo-flags-03338"",
            ""vkCreateBuffer(): the bufferDeviceAddressCaptureReplay device feature is disabled: Buffers cannot be created with ""
            ""the VK_BUFFER_CREATE_DEVICE_ADDRESS_CAPTURE_REPLAY_BIT_EXT set."");
    }

    if (pCreateInfo->sharingMode == VK_SHARING_MODE_CONCURRENT && pCreateInfo->pQueueFamilyIndices) {
        skip |= ValidateQueueFamilies(pCreateInfo->queueFamilyIndexCount, pCreateInfo->pQueueFamilyIndices, ""vkCreateBuffer"",
                                      ""pCreateInfo->pQueueFamilyIndices"", ""VUID-VkBufferCreateInfo-sharingMode-01419"",
                                      ""VUID-VkBufferCreateInfo-sharingMode-01419"", false);
    }

    return skip;
}

bool CoreChecks::PreCallValidateCreateBufferView(VkDevice device, const VkBufferViewCreateInfo *pCreateInfo,
                                                 const VkAllocationCallbacks *pAllocator, VkBufferView *pView) const {
    bool skip = false;
    const BUFFER_STATE *buffer_state = GetBufferState(pCreateInfo->buffer);
    // If this isn't a sparse buffer, it needs to have memory backing it at CreateBufferView time
    if (buffer_state) {
        skip |= ValidateMemoryIsBoundToBuffer(buffer_state, ""vkCreateBufferView()"", ""VUID-VkBufferViewCreateInfo-buffer-00935"");
        // In order to create a valid buffer view, the buffer must have been created with at least one of the following flags:
        // UNIFORM_TEXEL_BUFFER_BIT or STORAGE_TEXEL_BUFFER_BIT
        skip |= ValidateBufferUsageFlags(buffer_state,
                                         VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT | VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT, false,
                                         ""VUID-VkBufferViewCreateInfo-buffer-00932"", ""vkCreateBufferView()"",
                                         ""VK_BUFFER_USAGE_[STORAGE|UNIFORM]_TEXEL_BUFFER_BIT"");

        // Buffer view offset must be less than the size of buffer
        if (pCreateInfo->offset >= buffer_state->createInfo.size) {
            skip |= LogError(buffer_state->buffer, ""VUID-VkBufferViewCreateInfo-offset-00925"",
                             ""VkBufferViewCreateInfo offset (%"" PRIuLEAST64
                             "") must be less than the size of the buffer (%"" PRIuLEAST64 "")."",
                             pCreateInfo->offset, buffer_state->createInfo.size);
        }

        const VkPhysicalDeviceLimits *device_limits = &phys_dev_props.limits;
        // Buffer view offset must be a multiple of VkPhysicalDeviceLimits::minTexelBufferOffsetAlignment
        if ((pCreateInfo->offset % device_limits->minTexelBufferOffsetAlignment) != 0 &&
            !enabled_features.texel_buffer_alignment_features.texelBufferAlignment) {
            skip |= LogError(buffer_state->buffer, ""VUID-VkBufferViewCreateInfo-offset-02749"",
                             ""VkBufferViewCreateInfo offset (%"" PRIuLEAST64
                             "") must be a multiple of VkPhysicalDeviceLimits::minTexelBufferOffsetAlignment (%"" PRIuLEAST64 "")."",
                             pCreateInfo->offset, device_limits->minTexelBufferOffsetAlignment);
        }

        if (enabled_features.texel_buffer_alignment_features.texelBufferAlignment) {
            VkDeviceSize elementSize = FormatElementSize(pCreateInfo->format);
            if ((elementSize % 3) == 0) {
                elementSize /= 3;
            }
            if (buffer_state->createInfo.usage & VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT) {
                VkDeviceSize alignmentRequirement =
                    phys_dev_ext_props.texel_buffer_alignment_props.storageTexelBufferOffsetAlignmentBytes;
                if (phys_dev_ext_props.texel_buffer_alignment_props.storageTexelBufferOffsetSingleTexelAlignment) {
                    alignmentRequirement = std::min(alignmentRequirement, elementSize);
                }
                if (SafeModulo(pCreateInfo->offset, alignmentRequirement) != 0) {
                    skip |= LogError(
                        buffer_state->buffer, ""VUID-VkBufferViewCreateInfo-buffer-02750"",
                        ""If buffer was created with usage containing VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT, ""
                        ""VkBufferViewCreateInfo offset (%"" PRIuLEAST64
                        "") must be a multiple of the lesser of ""
                        ""VkPhysicalDeviceTexelBufferAlignmentPropertiesEXT::storageTexelBufferOffsetAlignmentBytes (%"" PRIuLEAST64
                        "") or, if VkPhysicalDeviceTexelBufferAlignmentPropertiesEXT::storageTexelBufferOffsetSingleTexelAlignment ""
                        ""(%"" PRId32
                        "") is VK_TRUE, the size of a texel of the requested format. ""
                        ""If the size of a texel is a multiple of three bytes, then the size of a ""
                        ""single component of format is used instead"",
                        pCreateInfo->offset, phys_dev_ext_props.texel_buffer_alignment_props.storageTexelBufferOffsetAlignmentBytes,
                        phys_dev_ext_props.texel_buffer_alignment_props.storageTexelBufferOffsetSingleTexelAlignment);
                }
            }
            if (buffer_state->createInfo.usage & VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT) {
                VkDeviceSize alignmentRequirement =
                    phys_dev_ext_props.texel_buffer_alignment_props.uniformTexelBufferOffsetAlignmentBytes;
                if (phys_dev_ext_props.texel_buffer_alignment_props.uniformTexelBufferOffsetSingleTexelAlignment) {
                    alignmentRequirement = std::min(alignmentRequirement, elementSize);
                }
                if (SafeModulo(pCreateInfo->offset, alignmentRequirement) != 0) {
                    skip |= LogError(
                        buffer_state->buffer, ""VUID-VkBufferViewCreateInfo-buffer-02751"",
                        ""If buffer was created with usage containing VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT, ""
                        ""VkBufferViewCreateInfo offset (%"" PRIuLEAST64
                        "") must be a multiple of the lesser of ""
                        ""VkPhysicalDeviceTexelBufferAlignmentPropertiesEXT::uniformTexelBufferOffsetAlignmentBytes (%"" PRIuLEAST64
                        "") or, if VkPhysicalDeviceTexelBufferAlignmentPropertiesEXT::uniformTexelBufferOffsetSingleTexelAlignment ""
                        ""(%"" PRId32
                        "") is VK_TRUE, the size of a texel of the requested format. ""
                        ""If the size of a texel is a multiple of three bytes, then the size of a ""
                        ""single component of format is used instead"",
                        pCreateInfo->offset, phys_dev_ext_props.texel_buffer_alignment_props.uniformTexelBufferOffsetAlignmentBytes,
                        phys_dev_ext_props.texel_buffer_alignment_props.uniformTexelBufferOffsetSingleTexelAlignment);
                }
            }
        }

        skip |= ValidateBufferViewRange(buffer_state, pCreateInfo, device_limits);

        skip |= ValidateBufferViewBuffer(buffer_state, pCreateInfo);
    }
    return skip;
}

// For the given format verify that the aspect masks make sense
bool CoreChecks::ValidateImageAspectMask(VkImage image, VkFormat format, VkImageAspectFlags aspect_mask, const char *func_name,
                                         const char *vuid) const {
    bool skip = false;

    if (FormatIsColor(format)) {
        if ((aspect_mask & VK_IMAGE_ASPECT_COLOR_BIT) != VK_IMAGE_ASPECT_COLOR_BIT) {
            skip |= LogError(image, vuid, ""%s: Color image formats must have the VK_IMAGE_ASPECT_COLOR_BIT set."", func_name);
        } else if ((aspect_mask & VK_IMAGE_ASPECT_COLOR_BIT) != aspect_mask) {
            skip |= LogError(image, vuid, ""%s: Color image formats must have ONLY the VK_IMAGE_ASPECT_COLOR_BIT set."", func_name);
        }
    } else if (FormatIsDepthAndStencil(format)) {
        if ((aspect_mask & (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT)) == 0) {
            skip |= LogError(image, vuid,
                             ""%s: Depth/stencil image formats must have at least one of VK_IMAGE_ASPECT_DEPTH_BIT and ""
                             ""VK_IMAGE_ASPECT_STENCIL_BIT set."",
                             func_name);
        } else if ((aspect_mask & (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT)) != aspect_mask) {
            skip |= LogError(image, vuid,
                             ""%s: Combination depth/stencil image formats can have only the VK_IMAGE_ASPECT_DEPTH_BIT and ""
                             ""VK_IMAGE_ASPECT_STENCIL_BIT set."",
                             func_name);
        }
    } else if (FormatIsDepthOnly(format)) {
        if ((aspect_mask & VK_IMAGE_ASPECT_DEPTH_BIT) != VK_IMAGE_ASPECT_DEPTH_BIT) {
            skip |= LogError(image, vuid, ""%s: Depth-only image formats must have the VK_IMAGE_ASPECT_DEPTH_BIT set."", func_name);
        } else if ((aspect_mask & VK_IMAGE_ASPECT_DEPTH_BIT) != aspect_mask) {
            skip |=
                LogError(image, vuid, ""%s: Depth-only image formats can have only the VK_IMAGE_ASPECT_DEPTH_BIT set."", func_name);
        }
    } else if (FormatIsStencilOnly(format)) {
        if ((aspect_mask & VK_IMAGE_ASPECT_STENCIL_BIT) != VK_IMAGE_ASPECT_STENCIL_BIT) {
            skip |=
                LogError(image, vuid, ""%s: Stencil-only image formats must have the VK_IMAGE_ASPECT_STENCIL_BIT set."", func_name);
        } else if ((aspect_mask & VK_IMAGE_ASPECT_STENCIL_BIT) != aspect_mask) {
            skip |= LogError(image, vuid, ""%s: Stencil-only image formats can have only the VK_IMAGE_ASPECT_STENCIL_BIT set."",
                             func_name);
        }
    } else if (FormatIsMultiplane(format)) {
        VkImageAspectFlags valid_flags = VK_IMAGE_ASPECT_COLOR_BIT | VK_IMAGE_ASPECT_PLANE_0_BIT | VK_IMAGE_ASPECT_PLANE_1_BIT;
        if (3 == FormatPlaneCount(format)) {
            valid_flags = valid_flags | VK_IMAGE_ASPECT_PLANE_2_BIT;
        }
        if ((aspect_mask & valid_flags) != aspect_mask) {
            skip |=
                LogError(image, vuid,
                         ""%s: Multi-plane image formats may have only VK_IMAGE_ASPECT_COLOR_BIT or VK_IMAGE_ASPECT_PLANE_n_BITs ""
                         ""set, where n = [0, 1, 2]."",
                         func_name);
        }
    }
    return skip;
}

bool CoreChecks::ValidateImageSubresourceRange(const uint32_t image_mip_count, const uint32_t image_layer_count,
                                               const VkImageSubresourceRange &subresourceRange, const char *cmd_name,
                                               const char *param_name, const char *image_layer_count_var_name, const VkImage image,
                                               SubresourceRangeErrorCodes errorCodes) const {
    bool skip = false;

    // Validate mip levels
    if (subresourceRange.baseMipLevel >= image_mip_count) {
        skip |= LogError(image, errorCodes.base_mip_err,
                         ""%s: %s.baseMipLevel (= %"" PRIu32
                         "") is greater or equal to the mip level count of the image (i.e. greater or equal to %"" PRIu32 "")."",
                         cmd_name, param_name, subresourceRange.baseMipLevel, image_mip_count);
    }

    if (subresourceRange.levelCount != VK_REMAINING_MIP_LEVELS) {
        if (subresourceRange.levelCount == 0) {
            skip |= LogError(image, errorCodes.mip_count_err, ""%s: %s.levelCount is 0."", cmd_name, param_name);
        } else {
            const uint64_t necessary_mip_count = uint64_t{subresourceRange.baseMipLevel} + uint64_t{subresourceRange.levelCount};

            if (necessary_mip_count > image_mip_count) {
                skip |= LogError(image, errorCodes.mip_count_err,
                                 ""%s: %s.baseMipLevel + .levelCount (= %"" PRIu32 "" + %"" PRIu32 "" = %"" PRIu64
                                 "") is greater than the mip level count of the image (i.e. greater than %"" PRIu32 "")."",
                                 cmd_name, param_name, subresourceRange.baseMipLevel, subresourceRange.levelCount,
                                 necessary_mip_count, image_mip_count);
            }
        }
    }

    // Validate array layers
    if (subresourceRange.baseArrayLayer >= image_layer_count) {
        skip |= LogError(image, errorCodes.base_layer_err,
                         ""%s: %s.baseArrayLayer (= %"" PRIu32
                         "") is greater or equal to the %s of the image when it was created (i.e. greater or equal to %"" PRIu32 "")."",
                         cmd_name, param_name, subresourceRange.baseArrayLayer, image_layer_count_var_name, image_layer_count);
    }

    if (subresourceRange.layerCount != VK_REMAINING_ARRAY_LAYERS) {
        if (subresourceRange.layerCount == 0) {
            skip |= LogError(image, errorCodes.layer_count_err, ""%s: %s.layerCount is 0."", cmd_name, param_name);
        } else {
            const uint64_t necessary_layer_count =
                uint64_t{subresourceRange.baseArrayLayer} + uint64_t{subresourceRange.layerCount};

            if (necessary_layer_count > image_layer_count) {
                skip |= LogError(image, errorCodes.layer_count_err,
                                 ""%s: %s.baseArrayLayer + .layerCount (= %"" PRIu32 "" + %"" PRIu32 "" = %"" PRIu64
                                 "") is greater than the %s of the image when it was created (i.e. greater than %"" PRIu32 "")."",
                                 cmd_name, param_name, subresourceRange.baseArrayLayer, subresourceRange.layerCount,
                                 necessary_layer_count, image_layer_count_var_name, image_layer_count);
            }
        }
    }

    return skip;
}

bool CoreChecks::ValidateCreateImageViewSubresourceRange(const IMAGE_STATE *image_state, bool is_imageview_2d_type,
                                                         const VkImageSubresourceRange &subresourceRange) const {
    bool is_khr_maintenance1 = IsExtEnabled(device_extensions.vk_khr_maintenance1);
    bool is_image_slicable = image_state->createInfo.imageType == VK_IMAGE_TYPE_3D &&
                             (image_state->createInfo.flags & VK_IMAGE_CREATE_2D_ARRAY_COMPATIBLE_BIT_KHR);
    bool is_3D_to_2D_map = is_khr_maintenance1 && is_image_slicable && is_imageview_2d_type;

    const auto image_layer_count = is_3D_to_2D_map ? image_state->createInfo.extent.depth : image_state->createInfo.arrayLayers;
    const auto image_layer_count_var_name = is_3D_to_2D_map ? ""extent.depth"" : ""arrayLayers"";

    SubresourceRangeErrorCodes subresourceRangeErrorCodes = {};
    subresourceRangeErrorCodes.base_mip_err = ""VUID-VkImageViewCreateInfo-subresourceRange-01478"";
    subresourceRangeErrorCodes.mip_count_err = ""VUID-VkImageViewCreateInfo-subresourceRange-01718"";
    subresourceRangeErrorCodes.base_layer_err = is_khr_maintenance1 ? (is_3D_to_2D_map ? ""VUID-VkImageViewCreateInfo-image-02724""
                                                                                       : ""VUID-VkImageViewCreateInfo-image-01482"")
                                                                    : ""VUID-VkImageViewCreateInfo-subresourceRange-01480"";
    subresourceRangeErrorCodes.layer_count_err = is_khr_maintenance1
                                                     ? (is_3D_to_2D_map ? ""VUID-VkImageViewCreateInfo-subresourceRange-02725""
                                                                        : ""VUID-VkImageViewCreateInfo-subresourceRange-01483"")
                                                     : ""VUID-VkImageViewCreateInfo-subresourceRange-01719"";

    return ValidateImageSubresourceRange(image_state->createInfo.mipLevels, image_layer_count, subresourceRange,
                                         ""vkCreateImageView"", ""pCreateInfo->subresourceRange"", image_layer_count_var_name,
                                         image_state->image, subresourceRangeErrorCodes);
}

bool CoreChecks::ValidateCmdClearColorSubresourceRange(const IMAGE_STATE *image_state,
                                                       const VkImageSubresourceRange &subresourceRange,
                                                       const char *param_name) const {
    SubresourceRangeErrorCodes subresourceRangeErrorCodes = {};
    subresourceRangeErrorCodes.base_mip_err = ""VUID-vkCmdClearColorImage-baseMipLevel-01470"";
    subresourceRangeErrorCodes.mip_count_err = ""VUID-vkCmdClearColorImage-pRanges-01692"";
    subresourceRangeErrorCodes.base_layer_err = ""VUID-vkCmdClearColorImage-baseArrayLayer-01472"";
    subresourceRangeErrorCodes.layer_count_err = ""VUID-vkCmdClearColorImage-pRanges-01693"";

    return ValidateImageSubresourceRange(image_state->createInfo.mipLevels, image_state->createInfo.arrayLayers, subresourceRange,
                                         ""vkCmdClearColorImage"", param_name, ""arrayLayers"", image_state->image,
                                         subresourceRangeErrorCodes);
}

bool CoreChecks::ValidateCmdClearDepthSubresourceRange(const IMAGE_STATE *image_state,
                                                       const VkImageSubresourceRange &subresourceRange,
                                                       const char *param_name) const {
    SubresourceRangeErrorCodes subresourceRangeErrorCodes = {};
    subresourceRangeErrorCodes.base_mip_err = ""VUID-vkCmdClearDepthStencilImage-baseMipLevel-01474"";
    subresourceRangeErrorCodes.mip_count_err = ""VUID-vkCmdClearDepthStencilImage-pRanges-01694"";
    subresourceRangeErrorCodes.base_layer_err = ""VUID-vkCmdClearDepthStencilImage-baseArrayLayer-01476"";
    subresourceRangeErrorCodes.layer_count_err = ""VUID-vkCmdClearDepthStencilImage-pRanges-01695"";

    return ValidateImageSubresourceRange(image_state->createInfo.mipLevels, image_state->createInfo.arrayLayers, subresourceRange,
                                         ""vkCmdClearDepthStencilImage"", param_name, ""arrayLayers"", image_state->image,
                                         subresourceRangeErrorCodes);
}

bool CoreChecks::ValidateImageBarrierSubresourceRange(const IMAGE_STATE *image_state,
                                                      const VkImageSubresourceRange &subresourceRange, const char *cmd_name,
                                                      const char *param_name) const {
    SubresourceRangeErrorCodes subresourceRangeErrorCodes = {};
    subresourceRangeErrorCodes.base_mip_err = ""VUID-VkImageMemoryBarrier-subresourceRange-01486"";
    subresourceRangeErrorCodes.mip_count_err = ""VUID-VkImageMemoryBarrier-subresourceRange-01724"";
    subresourceRangeErrorCodes.base_layer_err = ""VUID-VkImageMemoryBarrier-subresourceRange-01488"";
    subresourceRangeErrorCodes.layer_count_err = ""VUID-VkImageMemoryBarrier-subresourceRange-01725"";

    return ValidateImageSubresourceRange(image_state->createInfo.mipLevels, image_state->createInfo.arrayLayers, subresourceRange,
                                         cmd_name, param_name, ""arrayLayers"", image_state->image, subresourceRangeErrorCodes);
}

bool CoreChecks::PreCallValidateCreateImageView(VkDevice device, const VkImageViewCreateInfo *pCreateInfo,
                                                const VkAllocationCallbacks *pAllocator, VkImageView *pView) const {
    bool skip = false;
    const IMAGE_STATE *image_state = GetImageState(pCreateInfo->image);
    if (image_state) {
        skip |=
            ValidateImageUsageFlags(image_state,
                                    VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_STORAGE_BIT | VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT |",Should this be `VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL`?,"The suggested code change appears to be adding validation logic for the `vkCmdCopyImage` command in the Vulkan API. The changes focus on ensuring that the source and destination images meet certain requirements, such as having compatible formats, matching layouts, and appropriate aspect masks.

Here's a concise review of the changes:

1. **Aspect Mask Validation**: The code checks that the aspect masks for the source and destination images are valid for the given formats. It ensures that the aspect masks match and that they are present in the respective images.

2. **Layer Count Validation**: The code verifies that the layer counts for the source and destination images are consistent across regions.

3. **Extent Validation**: The code checks that the source and destination image extents are within the bounds of the subresources.

4. **Layout Validation**: The code ensures that the initial layouts of the source and destination images are consistent with the commands being executed.

5. **Filter Validation**: The code checks that the filter used in the `vkCmdBlitImage` command is valid for the given formats.

6. **Clear Attachment Validation**: The code ensures that the clear attachments are valid and do not conflict with the current image layouts.

7. **Resolve Image Validation**: The code ensures that the resolve image is valid and does not conflict with the current image layouts.

Overall, these changes help ensure that the Vulkan API is used correctly by enforcing the necessary constraints and preventing potential issues related to image formats, layouts, and usage."
91,"@@ -133,7 +133,7 @@ void ZoneDatabase::AddLootDropToNPC(NPC *npc, uint32 lootdrop_id, ItemList *item
 			int      charges = loot_drop->Entries[i].multiplier;
 			for (int j       = 0; j < charges; ++j) {
 				if (zone->random.Real(0.0, 100.0) <= loot_drop->Entries[i].chance &&
-					npc->MeetsLootDropLevelRequirements(loot_drop->Entries[i])) {
+					npc->MeetsLootDropLevelRequirements(loot_drop->Entries[i], true)) {
 					const EQ::ItemData *database_item = GetItem(loot_drop->Entries[i].item_id);
 					npc->AddLootDrop(
 						database_item,","/*	EQEMu: Everquest Server Emulator
	Copyright (C) 2001-2002 EQEMu Development Team (http://eqemu.org)

	This program is free software; you can redistribute it and/or modify
	it under the terms of the GNU General Public License as published by
	the Free Software Foundation; version 2 of the License.

	This program is distributed in the hope that it will be useful,
	but WITHOUT ANY WARRANTY except by those people which sell it, which
	are required to give you total support for your newly bought product;
	without even the implied warranty of MERCHANTABILITY or FITNESS FOR
	A PARTICULAR PURPOSE. See the GNU General Public License for more details.

	You should have received a copy of the GNU General Public License
	along with this program; if not, write to the Free Software
	Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
*/

#include ""../common/global_define.h""
#include ""../common/loottable.h""
#include ""../common/misc_functions.h""
#include ""../common/data_verification.h""

#include ""client.h""
#include ""entity.h""
#include ""mob.h""
#include ""npc.h""
#include ""zonedb.h""
#include ""zone_store.h""
#include ""global_loot_manager.h""
#include ""../common/repositories/criteria/content_filter_criteria.h""
#include ""../common/say_link.h""

#include <iostream>
#include <stdlib.h>

#ifdef _WINDOWS
#define snprintf	_snprintf
#endif

// Queries the loottable: adds item & coin to the npc
void ZoneDatabase::AddLootTableToNPC(NPC* npc,uint32 loottable_id, ItemList* itemlist, uint32* copper, uint32* silver, uint32* gold, uint32* plat) {
	const LootTable_Struct* lts = nullptr;
	// global loot passes nullptr for these
	bool bGlobal = copper == nullptr && silver == nullptr && gold == nullptr && plat == nullptr;
	if (!bGlobal) {
		*copper = 0;
		*silver = 0;
		*gold = 0;
		*plat = 0;
	}

	lts = database.GetLootTable(loottable_id);
	if (!lts)
		return;

	uint32 min_cash = lts->mincash;
	uint32 max_cash = lts->maxcash;
	if(min_cash > max_cash) {
		uint32 t = min_cash;
		min_cash = max_cash;
		max_cash = t;
	}

	uint32 cash = 0;
	if (!bGlobal) {
		if(max_cash > 0 && lts->avgcoin > 0 && EQ::ValueWithin(lts->avgcoin, min_cash, max_cash)) {
			float upper_chance = (float)(lts->avgcoin - min_cash) / (float)(max_cash - min_cash);
			float avg_cash_roll = (float)zone->random.Real(0.0, 1.0);

			if(avg_cash_roll < upper_chance) {
				cash = zone->random.Int(lts->avgcoin, max_cash);
			} else {
				cash = zone->random.Int(min_cash, lts->avgcoin);
			}
		} else {
			cash = zone->random.Int(min_cash, max_cash);
		}
	}

	if(cash != 0) {
		*plat = cash / 1000;
		cash -= *plat * 1000;

		*gold = cash / 100;
		cash -= *gold * 100;

		*silver = cash / 10;
		cash -= *silver * 10;

		*copper = cash;
	}

	uint32 global_loot_multiplier = RuleI(Zone, GlobalLootMultiplier);

	// Do items
	for (uint32 i=0; i<lts->NumEntries; i++) {
		for (uint32 k = 1; k <= (lts->Entries[i].multiplier * global_loot_multiplier); k++) {
			uint8 droplimit = lts->Entries[i].droplimit;
			uint8 mindrop = lts->Entries[i].mindrop;

			//LootTable Entry probability
			float ltchance = 0.0f;
			ltchance = lts->Entries[i].probability;

			float drop_chance = 0.0f;
			if(ltchance > 0.0 && ltchance < 100.0) {
				drop_chance = (float)zone->random.Real(0.0, 100.0);
			}

			if (ltchance != 0.0 && (ltchance == 100.0 || drop_chance <= ltchance)) {
				AddLootDropToNPC(npc, lts->Entries[i].lootdrop_id, itemlist, droplimit, mindrop);
			}
		}
	}
}

// Called by AddLootTableToNPC
// maxdrops = size of the array npcd
void ZoneDatabase::AddLootDropToNPC(NPC *npc, uint32 lootdrop_id, ItemList *item_list, uint8 droplimit, uint8 mindrop)
{
	const LootDrop_Struct *loot_drop = GetLootDrop(lootdrop_id);
	if (!loot_drop) {
		return;
	}

	if (loot_drop->NumEntries == 0) {
		return;
	}

	if (droplimit == 0 && mindrop == 0) {
		for (uint32 i = 0; i < loot_drop->NumEntries; ++i) {
					npc->MeetsLootDropLevelRequirements(loot_drop->Entries[i], true)) {
			int      charges = loot_drop->Entries[i].multiplier;
			for (int j       = 0; j < charges; ++j) {
				if (zone->random.Real(0.0, 100.0) <= loot_drop->Entries[i].chance &&
					npc->MeetsLootDropLevelRequirements(loot_drop->Entries[i])) {
					const EQ::ItemData *database_item = GetItem(loot_drop->Entries[i].item_id);
					npc->AddLootDrop(
						database_item,
						item_list,
						loot_drop->Entries[i]
					);
				}
			}
		}
		return;
	}

	if (loot_drop->NumEntries > 100 && droplimit == 0) {
		droplimit = 10;
	}

	if (droplimit < mindrop) {
		droplimit = mindrop;
	}

	float       roll_t           = 0.0f;
	float       roll_t_min       = 0.0f;
	bool        active_item_list = false;
	for (uint32 i                = 0; i < loot_drop->NumEntries; ++i) {
		const EQ::ItemData *db_item = GetItem(loot_drop->Entries[i].item_id);
		if (db_item) {
			roll_t += loot_drop->Entries[i].chance;
			active_item_list = true;
		}
	}

	roll_t_min = roll_t;
	roll_t     = EQ::ClampLower(roll_t, 100.0f);

	if (!active_item_list) {
		return;
	}

	for (int i = 0; i < mindrop; ++i) {
		float       roll = (float) zone->random.Real(0.0, roll_t_min);
		for (uint32 j    = 0; j < loot_drop->NumEntries; ++j) {
			const EQ::ItemData *db_item = GetItem(loot_drop->Entries[j].item_id);
			if (db_item) {
				if (roll < loot_drop->Entries[j].chance && npc->MeetsLootDropLevelRequirements(loot_drop->Entries[j])) {
					npc->AddLootDrop(
						db_item,
						item_list,
						loot_drop->Entries[j]
					);

					int charges = (int) loot_drop->Entries[i].multiplier;
					charges = EQ::ClampLower(charges, 1);

					for (int k = 1; k < charges; ++k) {
						float c_roll = (float) zone->random.Real(0.0, 100.0);
						if (c_roll <= loot_drop->Entries[i].chance) {
							npc->AddLootDrop(
								db_item,
								item_list,
								loot_drop->Entries[i]
							);
						}
					}

					j = loot_drop->NumEntries;
					break;
				}
				else {
					roll -= loot_drop->Entries[j].chance;
				}
			}
		}
	}

	for (int i = mindrop; i < droplimit; ++i) {
		float       roll = (float) zone->random.Real(0.0, roll_t);
		for (uint32 j    = 0; j < loot_drop->NumEntries; ++j) {
			const EQ::ItemData *db_item = GetItem(loot_drop->Entries[j].item_id);
			if (db_item) {
				if (roll < loot_drop->Entries[j].chance && npc->MeetsLootDropLevelRequirements(loot_drop->Entries[j])) {
					npc->AddLootDrop(
						db_item,
						item_list,
						loot_drop->Entries[j]
					);

					int charges = (int) loot_drop->Entries[i].multiplier;
					charges = EQ::ClampLower(charges, 1);

					for (int k = 1; k < charges; ++k) {
						float c_roll = (float) zone->random.Real(0.0, 100.0);
						if (c_roll <= loot_drop->Entries[i].chance) {
							npc->AddLootDrop(
								db_item,
								item_list,
								loot_drop->Entries[i]
							);
						}
					}

					j = loot_drop->NumEntries;
					break;
				}
				else {
					roll -= loot_drop->Entries[j].chance;
				}
			}
		}
	} // We either ran out of items or reached our limit.

	npc->UpdateEquipmentLight();
	// no wearchange associated with this function..so, this should not be needed
	//if (npc->UpdateActiveLightValue())
	//	npc->SendAppearancePacket(AT_Light, npc->GetActiveLightValue());
}

bool NPC::MeetsLootDropLevelRequirements(LootDropEntries_Struct loot_drop)
{
	if (loot_drop.npc_min_level > 0 && GetLevel() < loot_drop.npc_min_level) {
		LogLootDetail(
			""NPC [{}] does not meet loot_drop level requirements (min_level) level [{}] current [{}] for item [{}]"",
			GetCleanName(),
			loot_drop.npc_min_level,
			GetLevel(),
			database.CreateItemLink(loot_drop.item_id)
		);
		return false;
	}

	if (loot_drop.npc_max_level > 0 && GetLevel() > loot_drop.npc_max_level) {
		LogLootDetail(
			""NPC [{}] does not meet loot_drop level requirements (max_level) level [{}] current [{}] for item [{}]"",
			GetCleanName(),
			loot_drop.npc_max_level,
			GetLevel(),
			database.CreateItemLink(loot_drop.item_id)
		);
		return false;
	}

	return true;
}

LootDropEntries_Struct NPC::NewLootDropEntry()
{
	LootDropEntries_Struct loot_drop{};
	loot_drop.item_id           = 0;
	loot_drop.item_charges      = 1;
	loot_drop.equip_item        = 1;
	loot_drop.chance            = 0;
	loot_drop.trivial_min_level = 0;
	loot_drop.trivial_max_level = 0;
	loot_drop.npc_min_level     = 0;
	loot_drop.npc_max_level     = 0;
	loot_drop.multiplier        = 0;

	return loot_drop;
}

//if itemlist is null, just send wear changes
void NPC::AddLootDrop(
	const EQ::ItemData *item2,
	ItemList *itemlist,
	LootDropEntries_Struct loot_drop,
	bool wear_change,
	uint32 aug1,
	uint32 aug2,
	uint32 aug3,
	uint32 aug4,
	uint32 aug5,
	uint32 aug6
)
{
	if (item2 == nullptr) {
		return;
	}

	//make sure we are doing something...
	if (!itemlist && !wear_change) {
		return;
	}

	auto item = new ServerLootItem_Struct;

	if (LogSys.log_settings[Logs::Loot].is_category_enabled == 1) {
		EQ::SayLinkEngine linker;
		linker.SetLinkType(EQ::saylink::SayLinkItemData);
		linker.SetItemData(item2);

		LogLoot(
			""[NPC::AddLootDrop] NPC [{}] Item ({}) [{}] charges [{}] chance [{}] trivial min/max [{}/{}] npc min/max [{}/{}]"",
			GetName(),
			item2->ID,
			linker.GenerateLink(),
			loot_drop.item_charges,
			loot_drop.chance,
			loot_drop.trivial_min_level,
			loot_drop.trivial_max_level,
			loot_drop.npc_min_level,
			loot_drop.npc_max_level
		);
	}

	EQApplicationPacket *outapp               = nullptr;
	WearChange_Struct   *p_wear_change_struct = nullptr;
	if (wear_change) {
		outapp               = new EQApplicationPacket(OP_WearChange, sizeof(WearChange_Struct));
		p_wear_change_struct = (WearChange_Struct *) outapp->pBuffer;
		p_wear_change_struct->spawn_id = GetID();
		p_wear_change_struct->material = 0;
	}

	item->item_id           = item2->ID;
	item->charges           = loot_drop.item_charges;
	item->aug_1             = aug1;
	item->aug_2             = aug2;
	item->aug_3             = aug3;
	item->aug_4             = aug4;
	item->aug_5             = aug5;
	item->aug_6             = aug6;
	item->attuned           = 0;
	item->trivial_min_level = loot_drop.trivial_min_level;
	item->trivial_max_level = loot_drop.trivial_max_level;
	item->equip_slot        = EQ::invslot::SLOT_INVALID;

	if (loot_drop.equip_item > 0) {
		uint8 eslot = 0xFF;
		char newid[20];
		const EQ::ItemData* compitem = nullptr;
		bool found = false; // track if we found an empty slot we fit into
		int32 foundslot = -1; // for multi-slot items

		// Equip rules are as follows:
		// If the item has the NoPet flag set it will not be equipped.
		// An empty slot takes priority. The first empty one that an item can
		// fit into will be the one picked for the item.
		// AC is the primary choice for which item gets picked for a slot.
		// If AC is identical HP is considered next.
		// If an item can fit into multiple slots we'll pick the last one where
		// it is an improvement.

		if (!item2->NoPet) {
			for (int i = EQ::invslot::EQUIPMENT_BEGIN; !found && i <= EQ::invslot::EQUIPMENT_END; i++) {
				uint32 slots = (1 << i);
				if (item2->Slots & slots) {
					if(equipment[i])
					{
						compitem = database.GetItem(equipment[i]);
						if (item2->AC > compitem->AC ||
							(item2->AC == compitem->AC && item2->HP > compitem->HP))
						{
							// item would be an upgrade
							// check if we're multi-slot, if yes then we have to keep
							// looking in case any of the other slots we can fit into are empty.
							if (item2->Slots != slots) {
								foundslot = i;
							}
							else {
								equipment[i] = item2->ID;
								foundslot = i;
								found = true;
							}
						} // end if ac
					}
					else
					{
						equipment[i] = item2->ID;
						foundslot = i;
						found = true;
					}
				} // end if (slots)
			} // end for
		} // end if NoPet

		// Possible slot was found but not selected. Pick it now.
		if (!found && foundslot >= 0) {
			equipment[foundslot] = item2->ID;
			found = true;
		}

		// @merth: IDFile size has been increased, this needs to change
		uint16 emat;
		if(item2->Material <= 0
			|| (item2->Slots & ((1 << EQ::invslot::slotPrimary) | (1 << EQ::invslot::slotSecondary)))) {
			memset(newid, 0, sizeof(newid));
			for(int i=0;i<7;i++){
				if (!isalpha(item2->IDFile[i])){
					strn0cpy(newid, &item2->IDFile[i],6);
					i=8;
				}
			}

			emat = atoi(newid);
		} else {
			emat = item2->Material;
		}

		if (foundslot == EQ::invslot::slotPrimary) {
			if (item2->Proc.Effect != 0)
				CastToMob()->AddProcToWeapon(item2->Proc.Effect, true);

			eslot = EQ::textures::weaponPrimary;
			if (item2->Damage > 0) {
				SendAddPlayerState(PlayerState::PrimaryWeaponEquipped);
				if (!RuleB(Combat, ClassicNPCBackstab))
					SetFacestab(true);
			}
			if (item2->IsType2HWeapon())
				SetTwoHanderEquipped(true);
		}
		else if (foundslot == EQ::invslot::slotSecondary
			&& (GetOwner() != nullptr || (CanThisClassDualWield() && zone->random.Roll(NPC_DW_CHANCE)) || (item2->Damage==0)) &&
			(item2->IsType1HWeapon() || item2->ItemType == EQ::item::ItemTypeShield || item2->ItemType ==  EQ::item::ItemTypeLight))
		{
			if (item2->Proc.Effect!=0)
				CastToMob()->AddProcToWeapon(item2->Proc.Effect, true);

			eslot = EQ::textures::weaponSecondary;
			if (item2->Damage > 0)
				SendAddPlayerState(PlayerState::SecondaryWeaponEquipped);
		}
		else if (foundslot == EQ::invslot::slotHead) {
			eslot = EQ::textures::armorHead;
		}
		else if (foundslot == EQ::invslot::slotChest) {
			eslot = EQ::textures::armorChest;
		}
		else if (foundslot == EQ::invslot::slotArms) {
			eslot = EQ::textures::armorArms;
		}
		else if (foundslot == EQ::invslot::slotWrist1 || foundslot == EQ::invslot::slotWrist2) {
			eslot = EQ::textures::armorWrist;
		}
		else if (foundslot == EQ::invslot::slotHands) {
			eslot = EQ::textures::armorHands;
		}
		else if (foundslot == EQ::invslot::slotLegs) {
			eslot = EQ::textures::armorLegs;
		}
		else if (foundslot == EQ::invslot::slotFeet) {
			eslot = EQ::textures::armorFeet;
		}

		/*
		what was this about???

		if (((npc->GetRace()==127) && (npc->CastToMob()->GetOwnerID()!=0)) && (item2->Slots==24576) || (item2->Slots==8192) || (item2->Slots==16384)){
			npc->d_melee_texture2=atoi(newid);
			wc->wear_slot_id=8;
			if (item2->Material >0)
				wc->material=item2->Material;
			else
				wc->material=atoi(newid);
			npc->AC+=item2->AC;
			npc->STR+=item2->STR;
			npc->INT+=item2->INT;
		}
		*/

		//if we found an open slot it goes in...
		if(eslot != 0xFF) {
			if(wear_change) {
				p_wear_change_struct->wear_slot_id = eslot;
				p_wear_change_struct->material     = emat;
			}

		}
		if (found) {
			CalcBonuses(); // This is less than ideal for bulk adding of items
			item->equip_slot = foundslot;
		}
	}

	if (itemlist != nullptr) {
		itemlist->push_back(item);
	}
	else safe_delete(item);

	if (wear_change && outapp) {
		entity_list.QueueClients(this, outapp);
		safe_delete(outapp);
	}

	UpdateEquipmentLight();
	if (UpdateActiveLight()) {
		SendAppearancePacket(AT_Light, GetActiveLightType());
	}
}

void NPC::AddItem(const EQ::ItemData *item, uint16 charges, bool equipitem)
{
	//slot isnt needed, its determined from the item.
	auto loot_drop_entry = NPC::NewLootDropEntry();
	loot_drop_entry.equip_item   = static_cast<uint8>(equipitem ? 1 : 0);
	loot_drop_entry.item_charges = charges;

	AddLootDrop(item, &itemlist, loot_drop_entry, true);
}

void NPC::AddItem(
	uint32 itemid,
	uint16 charges,
	bool equipitem,
	uint32 aug1,
	uint32 aug2,
	uint32 aug3,
	uint32 aug4,
	uint32 aug5,
	uint32 aug6
)
{
	//slot isnt needed, its determined from the item.
	const EQ::ItemData *i = database.GetItem(itemid);
	if (i == nullptr) {
		return;
	}

	auto loot_drop_entry = NPC::NewLootDropEntry();
	loot_drop_entry.equip_item   = static_cast<uint8>(equipitem ? 1 : 0);
	loot_drop_entry.item_charges = charges;

	AddLootDrop(i, &itemlist, loot_drop_entry, true, aug1, aug2, aug3, aug4, aug5, aug6);
}

void NPC::AddLootTable() {
	if (npctype_id != 0) { // check if it's a GM spawn
		database.AddLootTableToNPC(this,loottable_id, &itemlist, &copper, &silver, &gold, &platinum);
	}
}

void NPC::AddLootTable(uint32 ldid) {
	if (npctype_id != 0) { // check if it's a GM spawn
	  database.AddLootTableToNPC(this,ldid, &itemlist, &copper, &silver, &gold, &platinum);
	}
}

void NPC::CheckGlobalLootTables()
{
	auto tables = zone->GetGlobalLootTables(this);

	for (auto &id : tables)
		database.AddLootTableToNPC(this, id, &itemlist, nullptr, nullptr, nullptr, nullptr);
}

void ZoneDatabase::LoadGlobalLoot()
{
	auto query = fmt::format(
		SQL
		(
			SELECT
			  id,
			  loottable_id,
			  description,
			  min_level,
			  max_level,
			  rare,
			  raid,
			  race,
			  class,
			  bodytype,
			  zone,
			  hot_zone
			FROM
			  global_loot
			WHERE
			  enabled = 1
			  {}
		),
		ContentFilterCriteria::apply()
	);

	auto results = QueryDatabase(query);
	if (!results.Success() || results.RowCount() == 0) {
		return;
	}

	// we might need this, lets not keep doing it in a loop
	auto      zoneid = std::to_string(zone->GetZoneID());
	for (auto row    = results.begin(); row != results.end(); ++row) {
		// checking zone limits
		if (row[10]) {
			auto zones = SplitString(row[10], '|');

			auto it = std::find(zones.begin(), zones.end(), zoneid);
			if (it == zones.end()) {  // not in here, skip
				continue;
			}
		}

		GlobalLootEntry e(atoi(row[0]), atoi(row[1]), row[2] ? row[2] : """");

		auto min_level = atoi(row[3]);
		if (min_level) {
			e.AddRule(GlobalLoot::RuleTypes::LevelMin, min_level);
		}

		auto max_level = atoi(row[4]);
		if (max_level) {
			e.AddRule(GlobalLoot::RuleTypes::LevelMax, max_level);
		}

		// null is not used
		if (row[5]) {
			e.AddRule(GlobalLoot::RuleTypes::Rare, atoi(row[5]));
		}

		// null is not used
		if (row[6]) {
			e.AddRule(GlobalLoot::RuleTypes::Raid, atoi(row[6]));
		}

		if (row[7]) {
			auto races = SplitString(row[7], '|');

			for (auto &r : races)
				e.AddRule(GlobalLoot::RuleTypes::Race, std::stoi(r));
		}

		if (row[8]) {
			auto classes = SplitString(row[8], '|');

			for (auto &c : classes)
				e.AddRule(GlobalLoot::RuleTypes::Class, std::stoi(c));
		}

		if (row[9]) {
			auto bodytypes = SplitString(row[9], '|');

			for (auto &b : bodytypes)
				e.AddRule(GlobalLoot::RuleTypes::BodyType, std::stoi(b));
		}

		// null is not used
		if (row[11]) {
			e.AddRule(GlobalLoot::RuleTypes::HotZone, atoi(row[11]));
		}

		zone->AddGlobalLootEntry(e);
	}
}

",We need to drop the verbose flag on this function. We can safely log detail logging without performance penalties because they are all macro expansions that do not allocate strings or run functions on the stack if the category and logging level is not enabled at all,"The suggested code changes primarily focus on enhancing the functionality of the `ZoneDatabase` and `NPC` classes related to loot management. Here's a concise review:

### Key Changes and Enhancements:

1. **Loot Table Addition**:
   - Added `AddLootTableToNPC` method to add a loot table to an NPC.
   - Implemented logic to handle both global and specific loot tables.
   - Ensures that the NPC meets level requirements before adding loot.

2. **Loot Drop Handling**:
   - Added `AddLootDropToNPC` method to handle individual loot drops within a loot table.
   - Supports multiple charges and random chance calculations for each drop.
   - Ensures that the NPC meets level requirements before adding a drop.

3. **NPC Loot Management**:
   - Added `AddItem` methods to add items directly to the NPC's inventory.
   - Supports adding items with specific charges and equipment slots.
   - Handles augmentation and material assignment for equipped items.

4. **Global Loot Tables**:
   - Added `LoadGlobalLoot` method to load global loot tables from the database.
   - Filters global loot tables based on zone-specific criteria.
   - Adds global loot entries to the zone's global loot list.

5. **Logging and Debugging**:
   - Added logging for loot-related operations to help with debugging and monitoring.

### Overall Impact:
- **Enhanced Functionality**: The code now supports more complex loot management, including handling of global loot tables and individual loot drops.
- **Improved Performance**: By separating loot table and drop handling, the code becomes more modular and easier to maintain.
- **Better Error Handling**: The code includes checks to ensure that NPCs meet level requirements before adding loot, which helps prevent invalid loot assignments.

### Potential Improvements:
- **Code Duplication**: Some methods like `AddLootDrop` are duplicated between `NPC` and `ZoneDatabase`. Consider refactoring to reduce duplication.
- **Performance Optimization**: For very large loot tables, consider optimizing the random number generation and item selection processes.
- **User Feedback**: Implement feedback mechanisms to notify players when they receive loot, especially for global loot.

Overall, the changes significantly improve the robustness and flexibility of the loot management system in the EQEMU server emulator."
92,"@@ -116,8 +116,13 @@ class CentralPlannerScheduler(Scheduler):
     def load(self):
         if os.path.exists(self._state_path):
             logger.info(""Attempting to load state from %s"", self._state_path)
-            with open(self._state_path) as fobj:
-                state = pickle.load(fobj)
+            try:
+                with open(self._state_path) as fobj:
+                    state = pickle.load(fobj)
+            except:
+                logger.exception(""Error when loading state. Starting from clean slate."")
+                return
+
             self._tasks, self._active_workers = state
 
             # Convert from old format","# Copyright (c) 2012 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.

import collections
import datetime
import os
import logging
import time
import cPickle as pickle
import task_history as history
logger = logging.getLogger(""luigi.server"")

from task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN


class Scheduler(object):
    ''' Abstract base class

    Note that the methods all take string arguments, not Task objects...
    '''
    add_task = NotImplemented
    get_work = NotImplemented
    ping = NotImplemented

UPSTREAM_RUNNING = 'UPSTREAM_RUNNING'
UPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'
UPSTREAM_FAILED = 'UPSTREAM_FAILED'

UPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)
UPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)
STATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}


class Task(object):
    def __init__(self, status, deps, priority=0):
        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)
        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active
        if deps is None:
            self.deps = set()
        else:
            self.deps = set(deps)
        self.status = status  # PENDING, RUNNING, FAILED or DONE
        self.time = time.time()  # Timestamp when task was first added
        self.retry = None
        self.remove = None
        self.worker_running = None  # the worker id that is currently running the task or None
        self.time_running = None  # Timestamp when picked up by worker
        self.expl = None
        self.priority = priority

    def __repr__(self):
        return ""Task(%r)"" % vars(self)


class Worker(object):
    """""" Structure for tracking worker activity and keeping their references """"""
    def __init__(self, id, last_active=None):
        self.id = id
        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)
        self.last_active = last_active  # seconds since epoch
        self.info = {}

    def add_info(self, info):
        self.info.update(info)

    def __str__(self):
        return self.id


class CentralPlannerScheduler(Scheduler):
    ''' Async scheduler that can handle multiple workers etc

    Can be run locally or on a server (using RemoteScheduler + server.Server).
    '''

    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,
                 state_path='/var/lib/luigi-server/state.pickle', task_history=None):
        '''
        (all arguments are in seconds)
        Keyword Arguments:
        retry_delay -- How long after a Task fails to try it again, or -1 to never retry
        remove_delay -- How long after a Task finishes to remove it from the scheduler
        state_path -- Path to state file (tasks and active workers)
        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers
        '''
        self._state_path = state_path
        self._tasks = {}  # map from id to a Task object
        self._retry_delay = retry_delay
        self._remove_delay = remove_delay
        self._worker_disconnect_delay = worker_disconnect_delay
        self._active_workers = {}  # map from id to a Worker object
        self._task_history = task_history or history.NopHistory()

    def dump(self):
        state = (self._tasks, self._active_workers)
        try:
            with open(self._state_path, 'w') as fobj:
                pickle.dump(state, fobj)
        except IOError:
            logger.warning(""Failed saving scheduler state"", exc_info=1)
        else:
            logger.info(""Saved state in %s"", self._state_path)

    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?
            try:
                with open(self._state_path) as fobj:
                    state = pickle.load(fobj)
            except:
                logger.exception(""Error when loading state. Starting from clean slate."")
                return

    def load(self):
        if os.path.exists(self._state_path):
            logger.info(""Attempting to load state from %s"", self._state_path)
            with open(self._state_path) as fobj:
                state = pickle.load(fobj)
            self._tasks, self._active_workers = state

            # Convert from old format
            # TODO: this is really ugly, we need something more future-proof
            # Every time we add an attribute to the Worker class, this code needs to be updated
            for k, v in self._active_workers.iteritems():
                if isinstance(v, float):
                    self._active_workers[k] = Worker(id=k, last_active=v)
        else:
            logger.info(""No prior state file exists at %s. Starting with clean slate"", self._state_path)

    def prune(self):
        logger.info(""Starting pruning of task graph"")
        # Delete workers that haven't said anything for a while (probably killed)
        delete_workers = []
        for worker in self._active_workers.values():
            if worker.last_active < time.time() - self._worker_disconnect_delay:
                logger.info(""Worker %s timed out (no contact for >=%ss)"", worker, self._worker_disconnect_delay)
                delete_workers.append(worker.id)

        for worker in delete_workers:
            self._active_workers.pop(worker)

        remaining_workers = set(self._active_workers.keys())

        # Mark tasks with no remaining active stakeholders for deletion
        for task_id, task in self._tasks.iteritems():
            if not task.stakeholders.intersection(remaining_workers):
                if task.remove is None:
                    logger.info(""Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds"", task_id, task.stakeholders, self._remove_delay)
                    task.remove = time.time() + self._remove_delay

            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:
                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic
                logger.info(""Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs"", task_id, task.worker_running, self._retry_delay)
                task.worker_running = None
                task.status = FAILED
                task.retry = time.time() + self._retry_delay

        # Remove tasks that have no stakeholders
        remove_tasks = []
        for task_id, task in self._tasks.iteritems():
            if task.remove and time.time() > task.remove:
                logger.info(""Removing task %r (no connected stakeholders)"", task_id)
                remove_tasks.append(task_id)

        for task_id in remove_tasks:
            self._tasks.pop(task_id)

        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0
        for task in self._tasks.values():
            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():
                task.status = PENDING
        logger.info(""Done pruning task graph"")

    def update(self, worker_id, worker_reference=None):
        """""" Keep track of whenever the worker was last active """"""
        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))
        if worker_reference:
            worker.reference = worker_reference
        worker.last_active = time.time()

    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None, priority=0):
        """"""
        * Add task identified by task_id if it doesn't exist
        * If deps is not None, update dependency list
        * Update status of task
        * Add additional workers/stakeholders
        """"""
        self.update(worker)

        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps, priority=priority))

        if task.remove is not None:
            task.remove = None  # unmark task for removal so it isn't removed after being added

        if not (task.status == RUNNING and status == PENDING):
            # don't allow re-scheduling of task while it is running, it must either fail or succeed first
            if status == PENDING or status != task.status:
                # Update the DB only if there was a acctual change, to prevent noise.
                # We also check for status == PENDING b/c that's the default value
                # (so checking for status != task.status woule lie)
                self._update_task_history(task_id, status)
            task.status = status
            if status == FAILED:
                task.retry = time.time() + self._retry_delay

        if deps is not None:
            task.deps = set(deps)

        task.stakeholders.add(worker)

        if runnable:
            task.workers.add(worker)

        if expl is not None:
            task.expl = expl

    def add_worker(self, worker, info):
        self._active_workers[worker].add_info(info)

    def get_work(self, worker, host=None):
        # TODO: remove any expired nodes

        # Algo: iterate over all nodes, find the first node with no dependencies and highest priority

        # TODO: remove tasks that can't be done, figure out if the worker has absolutely
        # nothing it can wait for

        # Return remaining tasks that have no FAILED descendents
        self.update(worker, {'host': host})
        best_t = float('inf')
        best_priority = float('-inf')
        best_task = None
        locally_pending_tasks = 0
        running_tasks = []

        for task_id, task in self._tasks.iteritems():
            if worker not in task.workers:
                continue

            if task.status == RUNNING:
                # Return a list of currently running tasks to the client,
                # makes it easier to troubleshoot
                other_worker = self._active_workers[task.worker_running]
                more_info = {'task_id': task_id, 'worker': str(other_worker)}
                if other_worker is not None:
                    more_info.update(other_worker.info)
                running_tasks.append(more_info)

            if task.status != PENDING:
                continue

            locally_pending_tasks += 1
            ok = True
            for dep in task.deps:
                if dep not in self._tasks:
                    ok = False
                elif self._tasks[dep].status != DONE:
                    ok = False

            if ok:
                if (-task.priority, task.time) < (-best_priority, best_t):
                    best_t = task.time
                    best_priority = task.priority
                    best_task = task_id

        if best_task:
            t = self._tasks[best_task]
            t.status = RUNNING
            t.worker_running = worker
            t.time_running = time.time()
            self._update_task_history(best_task, RUNNING, host=host)

        return {'n_pending_tasks': locally_pending_tasks,
                'task_id': best_task,
                'running_tasks': running_tasks}

    def ping(self, worker):
        self.update(worker)

    def _upstream_status(self, task_id, upstream_status_table):
        if task_id in upstream_status_table:
            return upstream_status_table[task_id]
        elif task_id in self._tasks:
            task_stack = [task_id]

            while task_stack:
                dep_id = task_stack.pop()
                if dep_id in self._tasks:
                    dep = self._tasks[dep_id]
                    if dep_id not in upstream_status_table:
                        if dep.status == PENDING and dep.deps:
                            task_stack = task_stack + [dep_id] + list(dep.deps)
                            upstream_status_table[dep_id] = ''  # will be updated postorder
                        else:
                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')
                            upstream_status_table[dep_id] = dep_status
                    elif upstream_status_table[dep_id] == '' and dep.deps:
                        # This is the postorder update step when we set the
                        # status based on the previously calculated child elements
                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]
                        upstream_status.append('')  # to handle empty list
                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)
                        upstream_status_table[dep_id] = status
            return upstream_status_table[dep_id]

    def _serialize_task(self, task_id):
        task = self._tasks[task_id]
        return {
            'deps': list(task.deps),
            'status': task.status,
            'workers': list(task.workers),
            'worker_running': task.worker_running,
            'time_running': getattr(task, ""time_running"", None),
            'start_time': task.time,
            'params': self._get_task_params(task_id),
            'name': self._get_task_name(task_id)
        }

    def _get_task_params(self, task_id):
        params = {}
        params_part = task_id.split('(')[1].strip(')')
        params_strings = params_part.split("", "")

        for param in params_strings:
            if not param:
                continue
            split_param = param.split('=')
            if len(split_param) != 2:
                return {'<complex parameters>': params_part}
            params[split_param[0]] = split_param[1]
        return params

    def _get_task_name(self, task_id):
        return task_id.split('(')[0]

    def graph(self):
        self.prune()
        serialized = {}
        for task_id, task in self._tasks.iteritems():
            serialized[task_id] = self._serialize_task(task_id)
        return serialized

    def _recurse_deps(self, task_id, serialized):
        if task_id not in serialized:
            task = self._tasks.get(task_id)
            if task is None:
                logger.warn('Missing task for id [%s]', task_id)
                serialized[task_id] = {
                    'deps': [],
                    'status': UNKNOWN,
                    'workers': [],
                    'start_time': UNKNOWN,
                    'params': self._get_task_params(task_id),
                    'name': self._get_task_name(task_id)
                }
            else:
                serialized[task_id] = self._serialize_task(task_id)
                for dep in task.deps:
                    self._recurse_deps(dep, serialized)

    def dep_graph(self, task_id):
        self.prune()
        serialized = {}
        if task_id in self._tasks:
            self._recurse_deps(task_id, serialized)
        return serialized

    def task_list(self, status, upstream_status):
        ''' query for a subset of tasks by status '''
        self.prune()
        result = {}
        upstream_status_table = {}  # used to memoize upstream status
        for task_id, task in self._tasks.iteritems():
            if not status or task.status == status:
                if (task.status != PENDING or not upstream_status or
                    upstream_status == self._upstream_status(task_id, upstream_status_table)):
                    serialized = self._serialize_task(task_id)
                    result[task_id] = serialized
        return result

    def inverse_dependencies(self, task_id):
        self.prune()
        serialized = {}
        if task_id in self._tasks:
            self._traverse_inverse_deps(task_id, serialized)
        return serialized

    def _traverse_inverse_deps(self, task_id, serialized):
        stack = [task_id]
        serialized[task_id] = self._serialize_task(task_id)
        while len(stack) > 0:
            curr_id = stack.pop()
            for id, task in self._tasks.iteritems():
                if curr_id in task.deps:
                    serialized[curr_id][""deps""].append(id)
                    if id not in serialized:
                        serialized[id] = self._serialize_task(id)
                        serialized[id][""deps""] = []
                        stack.append(id)

    def task_search(self, task_str):
        ''' query for a subset of tasks by task_id '''
        self.prune()
        result = collections.defaultdict(dict)
        for task_id, task in self._tasks.iteritems():
            if task_id.find(task_str) != -1:
                serialized = self._serialize_task(task_id)
                result[task.status][task_id] = serialized
        return result

    def fetch_error(self, task_id):
        if self._tasks[task_id].expl is not None:
            return {""taskId"": task_id, ""error"": self._tasks[task_id].expl}
        else:
            return {""taskId"": task_id, ""error"": """"}

    def _update_task_history(self, task_id, status, host=None):
        try:
            if status == DONE or status == FAILED:
                successful = (status == DONE)
                self._task_history.task_finished(task_id, successful)
            elif status == PENDING:
                self._task_history.task_scheduled(task_id)
            elif status == RUNNING:
                self._task_history.task_started(task_id, host)
        except:
            logger.warning(""Error saving Task history"", exc_info=1)

    @property
    def task_history(self):
        # Used by server.py to expose the calls
        return self._task_history
","I'm not sure what logging policy Spotify have, but I think we should include the stack trace. What if you find this message happening only once in the middle of a huge log file?","The suggested code change appears to be an implementation of a scheduler for Luigi, a workflow management system. The code includes several classes and methods to manage tasks, workers, and task history. Here's a concise review:

### Key Points:

1. **Class Structure**:
   - `Scheduler` is an abstract base class with methods like `add_task`, `get_work`, and `ping`.
   - `CentralPlannerScheduler` inherits from `Scheduler` and implements the abstract methods.

2. **Task Management**:
   - `Task` class represents a task with attributes like status, dependencies, and workers.
   - Methods like `add_task`, `update`, and `get_work` manage tasks and workers.

3. **Worker Management**:
   - `Worker` class tracks worker activity and references.
   - Methods like `update` and `add_worker` manage worker information.

4. **State Management**:
   - `dump` and `load` methods handle saving and loading the scheduler state to a file.
   - `prune` method removes tasks and workers that are no longer needed.

5. **Task History**:
   - `task_history` property provides access to task history for logging and reporting.

6. **Graph Operations**:
   - Methods like `graph`, `dep_graph`, `task_list`, `inverse_dependencies`, and `task_search` provide various ways to query and visualize the task graph.

### Potential Improvements:

1. **Concurrency**:
   - Consider using locks or thread-safe data structures to handle concurrent access to shared resources.

2. **Error Handling**:
   - Improve error handling in critical sections of the code to prevent crashes and ensure robustness.

3. **Logging**:
   - Enhance logging to include more detailed information about task states and errors.

4. **Performance**:
   - Optimize the `prune` method to reduce unnecessary computations and improve performance.

5. **Documentation**:
   - Add more comments and documentation to explain the purpose and functionality of each method and class.

Overall, the code provides a solid foundation for a scheduler, but there are areas where improvements can be made for better scalability, reliability, and maintainability."
93,"@@ -1443,9 +1443,9 @@ define(['browser', 'require', 'events', 'apphost', 'loading', 'dom', 'playbackMa
         var list = [];
 
         var video = document.createElement('video');
-        //if (video.webkitSupportsPresentationMode && video.webkitSupportsPresentationMode('picture-in-picture') && typeof video.webkitSetPresentationMode === ""function"") {
-        //    list.push('PictureInPicture');
-        //}
+        if (video.webkitSupportsPresentationMode && typeof video.webkitSetPresentationMode === ""function"") {
+            list.push('PictureInPicture');
+        }
         if (document.pictureInPictureEnabled) {
             list.push('PictureInPicture');
         }","            self._castPlayer.load(protocol, data.currentTime || 0);

            self._castPlayer.playWhenHaveEnoughData();
        }

        function initMediaManager() {

            mediaManager.defaultOnLoad = mediaManager.onLoad.bind(mediaManager);
            mediaManager.onLoad = onMediaManagerLoadMedia.bind(self);

            //mediaManager.defaultOnPlay = mediaManager.onPlay.bind(mediaManager);
            //mediaManager.onPlay = function (event) {
            //    // TODO ???
            //    mediaManager.defaultOnPlay(event);
            //};

            mediaManager.defaultOnStop = mediaManager.onStop.bind(mediaManager);
            mediaManager.onStop = function (event) {
                playbackManager.stop();
                mediaManager.defaultOnStop(event);
            };
        }

        function setCurrentSrc(elem, options) {

            elem.removeEventListener('error', onError);

            var val = options.url;
            console.log('playing url: ' + val);

            // Convert to seconds
            var seconds = (options.playerStartPositionTicks || 0) / 10000000;
            if (seconds) {
                val += '#t=' + seconds;
            }

            htmlMediaHelper.destroyHlsPlayer(self);
            htmlMediaHelper.destroyFlvPlayer(self);
            htmlMediaHelper.destroyCastPlayer(self);

            var tracks = getMediaStreamTextTracks(options.mediaSource);

            subtitleTrackIndexToSetOnPlaying = options.mediaSource.DefaultSubtitleStreamIndex == null ? -1 : options.mediaSource.DefaultSubtitleStreamIndex;
            if (subtitleTrackIndexToSetOnPlaying != null && subtitleTrackIndexToSetOnPlaying >= 0) {
                var initialSubtitleStream = options.mediaSource.MediaStreams[subtitleTrackIndexToSetOnPlaying];
                if (!initialSubtitleStream || initialSubtitleStream.DeliveryMethod === 'Encode') {
                    subtitleTrackIndexToSetOnPlaying = -1;
                }
            }

            audioTrackIndexToSetOnPlaying = options.playMethod === 'Transcode' ? null : options.mediaSource.DefaultAudioStreamIndex;

            self._currentPlayOptions = options;

            var crossOrigin = htmlMediaHelper.getCrossOriginValue(options.mediaSource);
            if (crossOrigin) {
                elem.crossOrigin = crossOrigin;
            }

            /*if (htmlMediaHelper.enableHlsShakaPlayer(options.item, options.mediaSource, 'Video') && val.indexOf('.m3u8') !== -1) {

                setTracks(elem, tracks, options.item, options.mediaSource);

                return setSrcWithShakaPlayer(self, elem, options, val);

            } else*/ if (browser.chromecast && val.indexOf('.m3u8') !== -1 && options.mediaSource.RunTimeTicks) {

                setTracks(elem, tracks, options.item, options.mediaSource);
                return setCurrentSrcChromecast(self, elem, options, val);
            }

            else if (htmlMediaHelper.enableHlsJsPlayer(options.mediaSource.RunTimeTicks, 'Video') && val.indexOf('.m3u8') !== -1) {

                setTracks(elem, tracks, options.item, options.mediaSource);

                return setSrcWithHlsJs(self, elem, options, val);

            } else if (options.playMethod !== 'Transcode' && options.mediaSource.Container === 'flv') {

                setTracks(elem, tracks, options.item, options.mediaSource);

                return setSrcWithFlvJs(self, elem, options, val);

            } else {

                elem.autoplay = true;

                return htmlMediaHelper.applySrc(elem, val, options).then(function () {

                    setTracks(elem, tracks, options.item, options.mediaSource);

                    self._currentSrc = val;

                    return htmlMediaHelper.playWithPromise(elem, onError);
                });
            }
        }

        self.setSubtitleStreamIndex = function (index) {

            setCurrentTrackElement(index);
        };

        function isAudioStreamSupported(stream, deviceProfile) {

            var codec = (stream.Codec || '').toLowerCase();

            if (!codec) {
                return true;
            }

            if (!deviceProfile) {
                // This should never happen
                return true;
            }

            var profiles = deviceProfile.DirectPlayProfiles || [];

            return profiles.filter(function (p) {


                if (p.Type === 'Video') {

                    if (!p.AudioCodec) {
                        return true;
                    }

                    return p.AudioCodec.toLowerCase().indexOf(codec) !== -1;
                }

                return false;

            }).length > 0;
        }

        function getSupportedAudioStreams() {
            var profile = self._lastProfile;

            return getMediaStreamAudioTracks(self._currentPlayOptions.mediaSource).filter(function (stream) {
                return isAudioStreamSupported(stream, profile);
            });
        }

        self.setAudioStreamIndex = function (index) {

            var streams = getSupportedAudioStreams();

            if (streams.length < 2) {
                // If there's only one supported stream then trust that the player will handle it on it's own
                return;
            }

            var audioIndex = -1;
            var i, length, stream;

            for (i = 0, length = streams.length; i < length; i++) {
                stream = streams[i];

                audioIndex++;

                if (stream.Index === index) {
                    break;
                }
            }

            if (audioIndex === -1) {
                return;
            }

            var elem = self._mediaElement;
            if (!elem) {
                return;
            }

            // https://msdn.microsoft.com/en-us/library/hh772507(v=vs.85).aspx

            var elemAudioTracks = elem.audioTracks || [];
            console.log('found ' + elemAudioTracks.length + ' audio tracks');

            for (i = 0, length = elemAudioTracks.length; i < length; i++) {

                if (audioIndex === i) {
                    console.log('setting audio track ' + i + ' to enabled');
                    elemAudioTracks[i].enabled = true;
                } else {
                    console.log('setting audio track ' + i + ' to disabled');
                    elemAudioTracks[i].enabled = false;
                }
            }

            setTimeout(function () {
                elem.currentTime = elem.currentTime;
            }, 100);
        };

        self.stop = function (destroyPlayer) {

            var elem = self._mediaElement;
            var src = self._currentSrc;

            if (elem) {

                if (src) {
                    elem.pause();
                }

                htmlMediaHelper.onEndedInternal(self, elem, onError);

                if (destroyPlayer) {
                    self.destroy();
                }
            }

            destroyCustomTrack(elem);

            return Promise.resolve();
        };

        self.destroy = function () {

            htmlMediaHelper.destroyHlsPlayer(self);
            htmlMediaHelper.destroyFlvPlayer(self);

            appRouter.setTransparency('none');

            var videoElement = self._mediaElement;

            if (videoElement) {

                self._mediaElement = null;

                destroyCustomTrack(videoElement);

                videoElement.removeEventListener('timeupdate', onTimeUpdate);
                videoElement.removeEventListener('ended', onEnded);
                videoElement.removeEventListener('volumechange', onVolumeChange);
                videoElement.removeEventListener('pause', onPause);
                videoElement.removeEventListener('playing', onPlaying);
                videoElement.removeEventListener('play', onPlay);
                videoElement.removeEventListener('click', onClick);
                videoElement.removeEventListener('dblclick', onDblClick);

                videoElement.parentNode.removeChild(videoElement);
            }

            var dlg = videoDialog;
            if (dlg) {

                videoDialog = null;

                dlg.parentNode.removeChild(dlg);
            }
        };

        function onEnded() {

            destroyCustomTrack(this);
            htmlMediaHelper.onEndedInternal(self, this, onError);
        }

        function onTimeUpdate(e) {

            // Get the player position + the transcoding offset
            var time = this.currentTime;

            if (time && !self._timeUpdated) {
                self._timeUpdated = true;
                ensureValidVideo(this);
            }

            self._currentTime = time;

            var currentPlayOptions = self._currentPlayOptions;
            // Not sure yet how this is coming up null since we never null it out, but it is causing app crashes
            if (currentPlayOptions) {
                var timeMs = time * 1000;
                timeMs += ((currentPlayOptions.transcodingOffsetTicks || 0) / 10000);
                updateSubtitleText(timeMs);
            }

            events.trigger(self, 'timeupdate');
        }

        function onVolumeChange() {

            htmlMediaHelper.saveVolume(this.volume);
            events.trigger(self, 'volumechange');
        }

        function onNavigatedToOsd() {

            var dlg = videoDialog;
            if (dlg) {
                dlg.classList.remove('videoPlayerContainer-withBackdrop');
                dlg.classList.remove('videoPlayerContainer-onTop');

                onStartedAndNavigatedToOsd();
            }
        }
        function onStartedAndNavigatedToOsd() {

            // If this causes a failure during navigation we end up in an awkward UI state
            setCurrentTrackElement(subtitleTrackIndexToSetOnPlaying);

            if (audioTrackIndexToSetOnPlaying != null && self.canSetAudioStreamIndex()) {
                self.setAudioStreamIndex(audioTrackIndexToSetOnPlaying);
            }
        }

        function onPlaying(e) {

            if (!self._started) {
                self._started = true;
                this.removeAttribute('controls');

                loading.hide();

                htmlMediaHelper.seekOnPlaybackStart(self, e.target, self._currentPlayOptions.playerStartPositionTicks);

                if (self._currentPlayOptions.fullscreen) {

                    appRouter.showVideoOsd().then(onNavigatedToOsd);

                } else {
                    appRouter.setTransparency('backdrop');
                    videoDialog.classList.remove('videoPlayerContainer-withBackdrop');
                    videoDialog.classList.remove('videoPlayerContainer-onTop');

                    onStartedAndNavigatedToOsd();
                }
            }
            events.trigger(self, 'playing');
        }

        function onPlay(e) {

            events.trigger(self, 'unpause');
        }

        function ensureValidVideo(elem) {
            if (elem !== self._mediaElement) {
                return;
            }

            if (elem.videoWidth === 0 && elem.videoHeight === 0) {

                var mediaSource = (self._currentPlayOptions || {}).mediaSource;

                // Only trigger this if there is media info
                // Avoid triggering in situations where it might not actually have a video stream (audio only live tv channel)
                if (!mediaSource || mediaSource.RunTimeTicks) {
                    htmlMediaHelper.onErrorInternal(self, 'mediadecodeerror');
                    return;
                }
            }

            //if (elem.audioTracks && !elem.audioTracks.length) {
            //    htmlMediaHelper.onErrorInternal(self, 'mediadecodeerror');
            //}
        }

        function onClick() {
            events.trigger(self, 'click');
        }

        function onDblClick() {
            events.trigger(self, 'dblclick');
        }

        function onPause() {
            events.trigger(self, 'pause');
        }

        function onError() {

            var errorCode = this.error ? (this.error.code || 0) : 0;
            var errorMessage = this.error ? (this.error.message || '') : '';
            console.log('Media element error: ' + errorCode.toString() + ' ' + errorMessage);

            var type;

            switch (errorCode) {
                case 1:
                    // MEDIA_ERR_ABORTED
                    // This will trigger when changing media while something is playing
                    return;
                case 2:
                    // MEDIA_ERR_NETWORK
                    type = 'network';
                    break;
                case 3:
                    // MEDIA_ERR_DECODE
                    if (self._hlsPlayer) {
                        htmlMediaHelper.handleHlsJsMediaError(self);
                        return;
                    } else {
                        type = 'mediadecodeerror';
                    }
                    break;
                case 4:
                    // MEDIA_ERR_SRC_NOT_SUPPORTED
                    type = 'medianotsupported';
                    break;
                default:
                    // seeing cases where Edge is firing error events with no error code
                    // example is start playing something, then immediately change src to something else
                    return;
            }

            htmlMediaHelper.onErrorInternal(self, type);
        }

        function destroyCustomTrack(videoElement) {

            if (self._resizeObserver) {
                self._resizeObserver.disconnect();
                self._resizeObserver = null;
            }

            if (videoSubtitlesElem) {
                var subtitlesContainer = videoSubtitlesElem.parentNode;
                if (subtitlesContainer) {
                    tryRemoveElement(subtitlesContainer);
                }
                videoSubtitlesElem = null;
            }

            currentTrackEvents = null;

            if (videoElement) {
                var allTracks = videoElement.textTracks || []; // get list of tracks
                for (var i = 0; i < allTracks.length; i++) {

                    var currentTrack = allTracks[i];

                    if (currentTrack.label.indexOf('manualTrack') !== -1) {
                        currentTrack.mode = 'disabled';
                    }
                }
            }

            customTrackIndex = -1;
            currentClock = null;
            self._currentAspectRatio = null;

            var renderer = currentAssRenderer;
            if (renderer) {
                renderer.setEnabled(false);
            }
            currentAssRenderer = null;
        }

        self.destroyCustomTrack = destroyCustomTrack;

        function fetchSubtitlesUwp(track, item) {

            return Windows.Storage.StorageFile.getFileFromPathAsync(track.Path).then(function (storageFile) {

                return Windows.Storage.FileIO.readTextAsync(storageFile).then(function (text) {
                    return JSON.parse(text);
                });
            });

        }

        function fetchSubtitles(track, item) {

            if (window.Windows && itemHelper.isLocalItem(item)) {
                return fetchSubtitlesUwp(track, item);
            }

            return new Promise(function (resolve, reject) {

                var xhr = new XMLHttpRequest();

                var url = getTextTrackUrl(track, item, '.js');

                xhr.open('GET', url, true);

                xhr.onload = function (e) {
                    resolve(JSON.parse(this.response));
                };

                xhr.onerror = reject;

                xhr.send();
            });
        }

        function setTrackForCustomDisplay(videoElement, track) {

            if (!track) {
                destroyCustomTrack(videoElement);
                return;
            }

            // if already playing thids track, skip
            if (customTrackIndex === track.Index) {
                return;
            }

            var item = self._currentPlayOptions.item;

            destroyCustomTrack(videoElement);
            customTrackIndex = track.Index;
            renderTracksEvents(videoElement, track, item);
            lastCustomTrackMs = 0;
        }

        function renderWithLibjass(videoElement, track, item) {

            var rendererSettings = {};

            if (browser.ps4) {
                // Text outlines are not rendering very well
                rendererSettings.enableSvg = false;
            }
            else if (browser.edge || browser.msie) {
                // svg not rendering at all
                rendererSettings.enableSvg = false;
            }

            // probably safer to just disable everywhere
            rendererSettings.enableSvg = false;

            require(['libjass', 'ResizeObserver'], function (libjass, ResizeObserver) {

                libjass.ASS.fromUrl(getTextTrackUrl(track, item)).then(function (ass) {

                    var clock = new libjass.renderers.ManualClock();
                    currentClock = clock;

                    // Create a DefaultRenderer using the video element and the ASS object
                    var renderer = new libjass.renderers.WebRenderer(ass, clock, videoElement.parentNode, rendererSettings);

                    currentAssRenderer = renderer;

                    renderer.addEventListener(""ready"", function () {
                        try {
                            renderer.resize(videoElement.offsetWidth, videoElement.offsetHeight, 0, 0);

                            if (!self._resizeObserver) {
                                self._resizeObserver = new ResizeObserver(onVideoResize, {});
                                self._resizeObserver.observe(videoElement);
                            }
                            //clock.pause();
                        } catch (ex) {
                            //alert(ex);
                        }
                    });
                }, function () {
                    htmlMediaHelper.onErrorInternal(self, 'mediadecodeerror');
                });
            });
        }

        function onVideoResize() {

            if (browser.iOS) {

                // with wkwebview, the new sizes will be delayed for about 500ms
                setTimeout(resetVideoRendererSize, 500);
            } else {
                resetVideoRendererSize();
            }
        }

        function resetVideoRendererSize() {
            var renderer = currentAssRenderer;
            if (renderer) {
                var videoElement = self._mediaElement;
                var width = videoElement.offsetWidth;
                var height = videoElement.offsetHeight;
                console.log('videoElement resized: ' + width + 'x' + height);
                renderer.resize(width, height, 0, 0);
            }
        }

        function requiresCustomSubtitlesElement() {

            // after a system update, ps4 isn't showing anything when creating a track element dynamically
            // going to have to do it ourselves
            if (browser.ps4) {
                return true;
            }

            // This is unfortunate, but we're unable to remove the textTrack that gets added via addTextTrack
            if (browser.firefox || browser.web0s) {
                return true;
            }

            if (browser.edge) {
                return true;
            }

            if (browser.iOS) {
                var userAgent = navigator.userAgent.toLowerCase();
                // works in the browser but not the native app
                if ((userAgent.indexOf('os 9') !== -1 || userAgent.indexOf('os 8') !== -1) && userAgent.indexOf('safari') === -1) {
                    return true;
                }
            }

            return false;
        }

        function renderSubtitlesWithCustomElement(videoElement, track, item) {

            fetchSubtitles(track, item).then(function (data) {
                if (!videoSubtitlesElem) {
                    var subtitlesContainer = document.createElement('div');
                    subtitlesContainer.classList.add('videoSubtitles');
                    subtitlesContainer.innerHTML = '<div class=""videoSubtitlesInner""></div>';
                    videoSubtitlesElem = subtitlesContainer.querySelector('.videoSubtitlesInner');
                    setSubtitleAppearance(subtitlesContainer, videoSubtitlesElem);
                    videoElement.parentNode.appendChild(subtitlesContainer);
                    currentTrackEvents = data.TrackEvents;
                }
            });
        }

        function setSubtitleAppearance(elem, innerElem) {

            require(['userSettings', 'subtitleAppearanceHelper'], function (userSettings, subtitleAppearanceHelper) {

                subtitleAppearanceHelper.applyStyles({

                    text: innerElem,
                    window: elem

                }, userSettings.getSubtitleAppearanceSettings());
            });
        }

        function getCueCss(appearance, selector) {

            var html = selector + '::cue {';

            html += appearance.text.map(function (s) {

                return s.name + ':' + s.value + '!important;';

            }).join('');

            html += '}';

            return html;
        }

        function setCueAppearance() {

            require(['userSettings', 'subtitleAppearanceHelper'], function (userSettings, subtitleAppearanceHelper) {

                var elementId = self.id + '-cuestyle';

                var styleElem = document.querySelector('#' + elementId);
                if (!styleElem) {
                    styleElem = document.createElement('style');
                    styleElem.id = elementId;
                    styleElem.type = 'text/css';
                    document.getElementsByTagName('head')[0].appendChild(styleElem);
                }

                styleElem.innerHTML = getCueCss(subtitleAppearanceHelper.getStyles(userSettings.getSubtitleAppearanceSettings(), true), '.htmlvideoplayer');
            });
        }

        function renderTracksEvents(videoElement, track, item) {

            if (!itemHelper.isLocalItem(item) || track.IsExternal) {
                var format = (track.Codec || '').toLowerCase();
                if (format === 'ssa' || format === 'ass') {
                    // libjass is needed here
                    renderWithLibjass(videoElement, track, item);
                    return;
                }

                if (requiresCustomSubtitlesElement()) {
                    renderSubtitlesWithCustomElement(videoElement, track, item);
                    return;
                }
            }

            var trackElement = null;
            var expectedId = 'manualTrack' + track.Index;

            var allTracks = videoElement.textTracks; // get list of tracks
            for (var i = 0; i < allTracks.length; i++) {

                var currentTrack = allTracks[i];

                if (currentTrack.label === expectedId) {
                    trackElement = currentTrack;
                    break;
                } else {
                    currentTrack.mode = 'disabled';
                }
            }

            if (!trackElement) {
                trackElement = videoElement.addTextTrack('subtitles', 'manualTrack' + track.Index, track.Language || 'und');

                // download the track json
                fetchSubtitles(track, item).then(function (data) {

                    // show in ui
                    console.log('downloaded ' + data.TrackEvents.length + ' track events');
                    // add some cues to show the text
                    // in safari, the cues need to be added before setting the track mode to showing
                    data.TrackEvents.forEach(function (trackEvent) {

                        var trackCueObject = window.VTTCue || window.TextTrackCue;
                        var cue = new trackCueObject(trackEvent.StartPositionTicks / 10000000, trackEvent.EndPositionTicks / 10000000, normalizeTrackEventText(trackEvent.Text));

                        trackElement.addCue(cue);
                    });
                    trackElement.mode = 'showing';
                });
            } else {
                trackElement.mode = 'showing';
            }
        }

        function updateSubtitleText(timeMs) {

            var clock = currentClock;
            if (clock) {
                try {
                    clock.seek(timeMs / 1000);
                } catch (err) {
                    console.log('Error in libjass: ' + err);
                }
                return;
            }

            var trackEvents = currentTrackEvents;
            var subtitleTextElement = videoSubtitlesElem;

            if (trackEvents && subtitleTextElement) {
                var ticks = timeMs * 10000;
                var selectedTrackEvent;
                for (var i = 0; i < trackEvents.length; i++) {

                    var currentTrackEvent = trackEvents[i];
                    if (currentTrackEvent.StartPositionTicks <= ticks && currentTrackEvent.EndPositionTicks >= ticks) {
                        selectedTrackEvent = currentTrackEvent;
                        break;
                    }
                }

                if (selectedTrackEvent && selectedTrackEvent.Text) {

                    subtitleTextElement.innerHTML = normalizeTrackEventText(selectedTrackEvent.Text);
                    subtitleTextElement.classList.remove('hide');

                } else {
                    subtitleTextElement.classList.add('hide');
                }
            }
        }

        function setCurrentTrackElement(streamIndex) {

            console.log('Setting new text track index to: ' + streamIndex);

            var mediaStreamTextTracks = getMediaStreamTextTracks(self._currentPlayOptions.mediaSource);

            var track = streamIndex === -1 ? null : mediaStreamTextTracks.filter(function (t) {
                return t.Index === streamIndex;
            })[0];

            if (enableNativeTrackSupport(self._currentSrc, track)) {

                setTrackForCustomDisplay(self._mediaElement, null);

                if (streamIndex !== -1) {
                    setCueAppearance();
                }

            } else {
                setTrackForCustomDisplay(self._mediaElement, track);

                // null these out to disable the player's native display (handled below)
                streamIndex = -1;
                track = null;
            }

            var expectedId = 'textTrack' + streamIndex;
            var trackIndex = streamIndex === -1 || !track ? -1 : mediaStreamTextTracks.indexOf(track);
            var modes = ['disabled', 'showing', 'hidden'];

            var allTracks = self._mediaElement.textTracks; // get list of tracks
            for (var i = 0; i < allTracks.length; i++) {

                var currentTrack = allTracks[i];

                console.log('currentTrack id: ' + currentTrack.id);

                var mode;

                console.log('expectedId: ' + expectedId + '--currentTrack.Id:' + currentTrack.id);

                // IE doesn't support track id
                if (browser.msie || browser.edge) {
                    if (trackIndex === i) {
                        mode = 1; // show this track
                    } else {
                        mode = 0; // hide all other tracks
                    }
                } else {

                    if (currentTrack.label.indexOf('manualTrack') !== -1) {
                        continue;
                    }
                    if (currentTrack.id === expectedId) {
                        mode = 1; // show this track
                    } else {
                        mode = 0; // hide all other tracks
                    }
                }

                console.log('Setting track ' + i + ' mode to: ' + mode);

                currentTrack.mode = modes[mode];
            }
        }

        function updateTextStreamUrls(startPositionTicks) {

            if (!supportsTextTracks()) {
                return;
            }

            var allTracks = self._mediaElement.textTracks; // get list of tracks
            var i;
            var track;

            for (i = 0; i < allTracks.length; i++) {

                track = allTracks[i];

                // This throws an error in IE, but is fine in chrome
                // In IE it's not necessary anyway because changing the src seems to be enough
                try {
                    while (track.cues.length) {
                        track.removeCue(track.cues[0]);
                    }
                } catch (e) {
                    console.log('Error removing cue from textTrack');
                }
            }

            var tracks = self._mediaElement.querySelectorAll('track');
            for (i = 0; i < tracks.length; i++) {

                track = tracks[i];

                track.src = replaceQueryString(track.src, 'startPositionTicks', startPositionTicks);
            }
        }

        function createMediaElement(options) {

            if (browser.tv || browser.iOS || browser.mobile) {
                // too slow
                // also on iOS, the backdrop image doesn't look right
                // on android mobile, it works, but can be slow to have the video surface fully cover the backdrop
                options.backdropUrl = null;
            }
            return new Promise(function (resolve, reject) {

                var dlg = document.querySelector('.videoPlayerContainer');

                if (!dlg) {

                    require(['css!./style'], function () {

                        loading.show();

                        var dlg = document.createElement('div');

                        dlg.classList.add('videoPlayerContainer');

                        if (options.backdropUrl) {

                            dlg.classList.add('videoPlayerContainer-withBackdrop');
                            dlg.style.backgroundImage = ""url('"" + options.backdropUrl + ""')"";
                        }

                        if (options.fullscreen) {
                            dlg.classList.add('videoPlayerContainer-onTop');
                        }

                        // playsinline new for iOS 10
                        // https://developer.apple.com/library/content/releasenotes/General/WhatsNewInSafari/Articles/Safari_10_0.html

                        var html = '';

                        var cssClass = 'htmlvideoplayer';

                        if (!browser.chromecast) {
                            cssClass += ' htmlvideoplayer-moveupsubtitles';
                        }

                        // Can't autoplay in these browsers so we need to use the full controls, at least until playback starts
                        if (!appHost.supports('htmlvideoautoplay')) {
                            html += '<video class=""' + cssClass + '"" preload=""metadata"" autoplay=""autoplay"" controls=""controls"" webkit-playsinline playsinline>';
                        } else {

                            // Chrome 35 won't play with preload none
                            html += '<video class=""' + cssClass + '"" preload=""metadata"" autoplay=""autoplay"" webkit-playsinline playsinline>';
                        }

                        html += '</video>';

                        dlg.innerHTML = html;
                        var videoElement = dlg.querySelector('video');

                        videoElement.volume = htmlMediaHelper.getSavedVolume();
                        videoElement.addEventListener('timeupdate', onTimeUpdate);
                        videoElement.addEventListener('ended', onEnded);
                        videoElement.addEventListener('volumechange', onVolumeChange);
                        videoElement.addEventListener('pause', onPause);
                        videoElement.addEventListener('playing', onPlaying);
                        videoElement.addEventListener('play', onPlay);
                        videoElement.addEventListener('click', onClick);
                        videoElement.addEventListener('dblclick', onDblClick);

                        document.body.insertBefore(dlg, document.body.firstChild);
                        videoDialog = dlg;
                        self._mediaElement = videoElement;

                        if (mediaManager) {

                            if (!mediaManager.embyInit) {
                                initMediaManager();
                                mediaManager.embyInit = true;
                            }
                            mediaManager.setMediaElement(videoElement);
                        }

                        // don't animate on smart tv's, too slow
                        if (options.fullscreen && browser.supportsCssAnimation() && !browser.slow) {
                            zoomIn(dlg).then(function () {
                                resolve(videoElement);
                            });
                        } else {
                            resolve(videoElement);
                        }

                    });

                } else {

                    if (options.backdropUrl) {

                        dlg.classList.add('videoPlayerContainer-withBackdrop');
                        dlg.style.backgroundImage = ""url('"" + options.backdropUrl + ""')"";
                    }

                    resolve(dlg.querySelector('video'));
                }
            });
        }
    }

    HtmlVideoPlayer.prototype.canPlayMediaType = function (mediaType) {

        return (mediaType || '').toLowerCase() === 'video';
    };

    HtmlVideoPlayer.prototype.supportsPlayMethod = function (playMethod, item) {

        if (appHost.supportsPlayMethod) {
            return appHost.supportsPlayMethod(playMethod, item);
        }

        return true;
    };

    HtmlVideoPlayer.prototype.getDeviceProfile = function (item, options) {

        var instance = this;
        return getDeviceProfileInternal(item, options).then(function (profile) {
            instance._lastProfile = profile;
            return profile;
        });
    };

    function getDeviceProfileInternal(item, options) {
        if (appHost.getDeviceProfile) {
            return appHost.getDeviceProfile(item, options);
        }

        return getDefaultProfile();
    }

    var supportedFeatures;
    function getSupportedFeatures() {

        if (video.webkitSupportsPresentationMode && typeof video.webkitSetPresentationMode === ""function"") {
            list.push('PictureInPicture');
        }
        var list = [];

        var video = document.createElement('video');
        //if (video.webkitSupportsPresentationMode && video.webkitSupportsPresentationMode('picture-in-picture') && typeof video.webkitSetPresentationMode === ""function"") {
        //    list.push('PictureInPicture');
        //}
        if (document.pictureInPictureEnabled) {
            list.push('PictureInPicture');
        }
        else if (browser.ipad) {

            // Unfortunately this creates a false positive on devices where its' not actually supported
            if (navigator.userAgent.toLowerCase().indexOf('os 9') === -1) {
                if (video.webkitSupportsPresentationMode && video.webkitSupportsPresentationMode && typeof video.webkitSetPresentationMode === ""function"") {
                    list.push('PictureInPicture');
                }
            }
        } else if (window.Windows) {

            if (Windows.UI.ViewManagement.ApplicationView.getForCurrentView().isViewModeSupported(Windows.UI.ViewManagement.ApplicationViewMode.compactOverlay)) {
                list.push('PictureInPicture');
            }
        }

        list.push('SetBrightness');

        return list;
    }

    HtmlVideoPlayer.prototype.supports = function (feature) {

        if (!supportedFeatures) {
            supportedFeatures = getSupportedFeatures();
        }

        return supportedFeatures.indexOf(feature) !== -1;
    };

    // Save this for when playback stops, because querying the time at that point might return 0
    HtmlVideoPlayer.prototype.currentTime = function (val) {

        var mediaElement = this._mediaElement;
        if (mediaElement) {
            if (val != null) {
                mediaElement.currentTime = val / 1000;
                return;
            }

            var currentTime = this._currentTime;
            if (currentTime) {
                return currentTime * 1000;
            }

            return (mediaElement.currentTime || 0) * 1000;
        }
    };

    HtmlVideoPlayer.prototype.duration = function (val) {

        var mediaElement = this._mediaElement;
        if (mediaElement) {
            var duration = mediaElement.duration;
            if (htmlMediaHelper.isValidDuration(duration)) {
                return duration * 1000;
            }
        }

        return null;
    };

    HtmlVideoPlayer.prototype.canSetAudioStreamIndex = function (index) {

        if (browser.tizen || browser.orsay) {
            return true;
        }

        var video = this._mediaElement;
        if (video) {
            if (video.audioTracks) {
                return true;
            }
        }

        return false;
    };

    function onPictureInPictureError(err) {
        console.log('Picture in picture error: ' + err.toString());
    }

    HtmlVideoPlayer.prototype.setPictureInPictureEnabled = function (isEnabled) {

        var video = this._mediaElement;

        if (document.pictureInPictureEnabled) {
            if (video) {
                if (isEnabled) {
                    video.requestPictureInPicture().catch(onPictureInPictureError);
                } else {
                    document.exitPictureInPicture().catch(onPictureInPictureError);
                }
            }
        }
        else if (window.Windows) {

            this.isPip = isEnabled;
            if (isEnabled) {
                Windows.UI.ViewManagement.ApplicationView.getForCurrentView().tryEnterViewModeAsync(Windows.UI.ViewManagement.ApplicationViewMode.compactOverlay);
            }
            else {
                Windows.UI.ViewManagement.ApplicationView.getForCurrentView().tryEnterViewModeAsync(Windows.UI.ViewManagement.ApplicationViewMode.default);
            }
        }
        else {
            if (video) {
                if (video.webkitSupportsPresentationMode && typeof video.webkitSetPresentationMode === ""function"") {
                    video.webkitSetPresentationMode(isEnabled ? ""picture-in-picture"" : ""inline"");
                }
            }
        }
    };

    HtmlVideoPlayer.prototype.isPictureInPictureEnabled = function () {

        if (document.pictureInPictureEnabled) {
            return document.pictureInPictureElement ? true : false;
        }
        else if (window.Windows) {
            return this.isPip || false;
        }
        else {
            var video = this._mediaElement;
            if (video) {
                return video.webkitPresentationMode === ""picture-in-picture"";
            }
        }

        return false;
    };

    HtmlVideoPlayer.prototype.setBrightness = function (val) {

        var elem = this._mediaElement;

        if (elem) {

            val = Math.max(0, val);
            val = Math.min(100, val);

            var rawValue = val;
            rawValue = Math.max(20, rawValue);

            var cssValue = rawValue >= 100 ? 'none' : (rawValue / 100);
            elem.style['-webkit-filter'] = 'brightness(' + cssValue + ');';
            elem.style.filter = 'brightness(' + cssValue + ')';
            elem.brightnessValue = val;
            events.trigger(this, 'brightnesschange');
        }
    };

    HtmlVideoPlayer.prototype.getBrightness = function () {

        var elem = this._mediaElement;

        if (elem) {
            var val = elem.brightnessValue;
            return val == null ? 100 : val;
        }
    };

    HtmlVideoPlayer.prototype.seekable = function () {
        var mediaElement = this._mediaElement;
        if (mediaElement) {

            var seekable = mediaElement.seekable;
            if (seekable && seekable.length) {

                var start = seekable.start(0);
                var end = seekable.end(0);

                if (!htmlMediaHelper.isValidDuration(start)) {
                    start = 0;
                }
                if (!htmlMediaHelper.isValidDuration(end)) {
                    end = 0;
                }

                return (end - start) > 0;
            }

            return false;
        }
    };

    HtmlVideoPlayer.prototype.pause = function () {
        var mediaElement = this._mediaElement;
        if (mediaElement) {
            mediaElement.pause();
        }
    };

    // This is a retry after error
    HtmlVideoPlayer.prototype.resume = function () {
        var mediaElement = this._mediaElement;
        if (mediaElement) {
            mediaElement.play();
        }
    };

    HtmlVideoPlayer.prototype.unpause = function () {
        var mediaElement = this._mediaElement;
        if (mediaElement) {
            mediaElement.play();
        }
    };

    HtmlVideoPlayer.prototype.paused = function () {

        var mediaElement = this._mediaElement;
        if (mediaElement) {
            return mediaElement.paused;
        }

        return false;
    };

    HtmlVideoPlayer.prototype.setVolume = function (val) {
        var mediaElement = this._mediaElement;
        if (mediaElement) {
            mediaElement.volume = val / 100;
        }
    };

    HtmlVideoPlayer.prototype.getVolume = function () {
        var mediaElement = this._mediaElement;
        if (mediaElement) {

            return Math.min(Math.round(mediaElement.volume * 100), 100);
        }
    };

    HtmlVideoPlayer.prototype.volumeUp = function () {
        this.setVolume(Math.min(this.getVolume() + 2, 100));
    };

    HtmlVideoPlayer.prototype.volumeDown = function () {
        this.setVolume(Math.max(this.getVolume() - 2, 0));
    };

    HtmlVideoPlayer.prototype.setMute = function (mute) {

        var mediaElement = this._mediaElement;
        if (mediaElement) {
            mediaElement.muted = mute;
        }
    };

    HtmlVideoPlayer.prototype.isMuted = function () {
        var mediaElement = this._mediaElement;
        if (mediaElement) {
            return mediaElement.muted;
        }
        return false;
    };

    HtmlVideoPlayer.prototype.setAspectRatio = function (val) {

    };

    HtmlVideoPlayer.prototype.getAspectRatio = function () {

        return this._currentAspectRatio;
    };

    HtmlVideoPlayer.prototype.getSupportedAspectRatios = function () {

        return [];
    };

    HtmlVideoPlayer.prototype.togglePictureInPicture = function () {
        return this.setPictureInPictureEnabled(!this.isPictureInPictureEnabled());
    };

    HtmlVideoPlayer.prototype.getBufferedRanges = function () {
        var mediaElement = this._mediaElement;
        if (mediaElement) {

            return htmlMediaHelper.getBufferedRanges(this, mediaElement);
        }

        return [];
    };

    HtmlVideoPlayer.prototype.getStats = function () {

        var mediaElement = this._mediaElement;
        var playOptions = this._currentPlayOptions || [];

        var categories = [];

        if (!mediaElement) {
            return Promise.resolve({
                categories: categories
            });
        }

        var mediaCategory = {
            stats: [],
            type: 'media'
        };
        categories.push(mediaCategory);

        if (playOptions.url) {
            //  create an anchor element (note: no need to append this element to the document)
            var link = document.createElement('a');
            //  set href to any path
            link.setAttribute('href', playOptions.url);
            var protocol = (link.protocol || '').replace(':', '');

            if (protocol) {
                mediaCategory.stats.push({
                    label: 'Protocol:',
                    value: protocol
                });
            }

            link = null;
        }

        if (this._hlsPlayer || this._shakaPlayer) {
            mediaCategory.stats.push({
                label: 'Stream type:',
                value: 'HLS'
            });
        } else {
            mediaCategory.stats.push({
                label: 'Stream type:',
                value: 'Video'
            });
        }

        var videoCategory = {
            stats: [],
            type: 'video'
        };
        categories.push(videoCategory);

        var rect = mediaElement.getBoundingClientRect ? mediaElement.getBoundingClientRect() : {};
        var height = rect.height;
        var width = rect.width;

        // Don't show player dimensions on smart TVs because the app UI could be lower resolution than the video and this causes users to think there is a problem
        if (width && height && !browser.tv) {
            videoCategory.stats.push({
                label: 'Player dimensions:',
                value: width + 'x' + height
            });
        }

        height = mediaElement.videoHeight;
        width = mediaElement.videoWidth;

        if (width && height) {
            videoCategory.stats.push({
                label: 'Video resolution:',
                value: width + 'x' + height
            });
        }

        if (mediaElement.getVideoPlaybackQuality) {
            var playbackQuality = mediaElement.getVideoPlaybackQuality();

            var droppedVideoFrames = playbackQuality.droppedVideoFrames || 0;
            videoCategory.stats.push({
                label: 'Dropped frames:',
                value: droppedVideoFrames
            });

            var corruptedVideoFrames = playbackQuality.corruptedVideoFrames || 0;
            videoCategory.stats.push({
                label: 'Corrupted frames:',
                value: corruptedVideoFrames
            });
        }

        var audioCategory = {
            stats: [],
            type: 'audio'
        };
        categories.push(audioCategory);

        var sinkId = mediaElement.sinkId;
        if (sinkId) {
            audioCategory.stats.push({
                label: 'Sink Id:',
                value: sinkId
            });
        }

        return Promise.resolve({
            categories: categories
        });
    };

    if (browser.chromecast) {
        mediaManager = new cast.receiver.MediaManager(document.createElement('video'));
    }

    return HtmlVideoPlayer;
});",Can you merge two `if`s in one?,"The suggested code changes appear to be related to a media player implementation for a web application. The changes include:

1. Initialization of the `_castPlayer` and calling `load` and `playWhenHaveEnoughData`.
2. Initialization of the `mediaManager` with event bindings.
3. Implementation of `setCurrentSrc` method to handle media source changes.
4. Methods for setting subtitle and audio streams.
5. Event handlers for various media player events like `onEnded`, `onTimeUpdate`, etc.
6. Methods for managing the player's state, such as `stop` and `destroy`.
7. Support for different media types and playback methods.
8. Methods for handling subtitles, including rendering with LibJass or custom elements.

Overall, the code appears to be well-structured and handles various aspects of media playback, including loading, seeking, and subtitle management."
94,"@@ -257,13 +257,10 @@ func (c *Cluster) newClusterMember(node *corev1.Node) (string, error) {
 		return """", fmt.Errorf(""obtain IP addresses from K8s Node failed: %v"", err)
 	}
 	nodeAddr := nodeAddrs.IPv4
-	fmtStr := ""%s:%d""
 	if nodeAddr == nil {
 		nodeAddr = nodeAddrs.IPv6
-		fmtStr = ""[%s]:%d""
 	}
-	member := fmt.Sprintf(fmtStr, nodeAddr, c.bindPort)
-	return member, nil
+	return nodeAddr.String(), nil
 }
 
 func (c *Cluster) allClusterMembers() (clusterNodes []string, err error) {","// Copyright 2021 Antrea Authors
//
// Licensed under the Apache License, Version 2.0 (the ""License"");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an ""AS IS"" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package memberlist

import (
	""fmt""
	""io/ioutil""
	""net""
	""reflect""
	""sync""
	""time""

	""github.com/golang/groupcache/consistenthash""
	""github.com/hashicorp/memberlist""
	corev1 ""k8s.io/api/core/v1""
	""k8s.io/apimachinery/pkg/api/errors""
	metav1 ""k8s.io/apimachinery/pkg/apis/meta/v1""
	""k8s.io/apimachinery/pkg/labels""
	""k8s.io/apimachinery/pkg/util/sets""
	""k8s.io/apimachinery/pkg/util/wait""
	coreinformers ""k8s.io/client-go/informers/core/v1""
	corelisters ""k8s.io/client-go/listers/core/v1""
	""k8s.io/client-go/tools/cache""
	""k8s.io/client-go/util/workqueue""
	""k8s.io/klog/v2""

	""antrea.io/antrea/pkg/apis/crd/v1alpha2""
	crdinformers ""antrea.io/antrea/pkg/client/informers/externalversions/crd/v1alpha2""
	crdlister ""antrea.io/antrea/pkg/client/listers/crd/v1alpha2""
	""antrea.io/antrea/pkg/util/k8s""
)

const (
	controllerName = ""MemberListCluster""
	// Set resyncPeriod to 0 to disable resyncing.
	resyncPeriod time.Duration = 0
	// Set default virtual node replicas num of consistent hash
	// in order to improve the quality of the hash distribution, refs https://github.com/golang/groupcache/issues/29
	defaultVirtualNodeReplicas = 50
	// How long to wait before retrying the processing of an ExternalIPPool change.
	minRetryDelay = 5 * time.Second
	maxRetryDelay = 300 * time.Second
	// Default number of workers processing an ExternalIPPool change.
	defaultWorkers = 4

	nodeEventTypeJoin   nodeEventType = ""Join""
	nodeEventTypeLeave  nodeEventType = ""Leave""
	nodeEventTypeUpdate nodeEventType = ""Update""
)

type nodeEventType string

// Default Hash Fn is crc32.ChecksumIEEE.
var defaultHashFn func(data []byte) uint32

var (
	errDecodingObject          = fmt.Errorf(""received unexpected object"")
	errDecodingObjectTombstone = fmt.Errorf(""deletedFinalStateUnknown contains unexpected object"")
)

var mapNodeEventType = map[memberlist.NodeEventType]nodeEventType{
	memberlist.NodeJoin:   nodeEventTypeJoin,
	memberlist.NodeLeave:  nodeEventTypeLeave,
	memberlist.NodeUpdate: nodeEventTypeUpdate,
}

type clusterNodeEventHandler func(objName string)

// Cluster implements ClusterInterface.
type Cluster struct {
	bindPort int
	// IP addr of local Node.
	localNodeIP net.IP
	// Name of local Node. Node name must be unique in the cluster.
	nodeName string

	mList *memberlist.Memberlist
	// consistentHash hold the consistentHashMap, when a Node join cluster, use method Add() to add a key to the hash.
	// when a Node leave the cluster, the consistentHashMap should be update.
	consistentHashMap     map[string]*consistenthash.Map
	consistentHashRWMutex sync.RWMutex
	// nodeEventsCh, the Node join/leave events will be notified via it.
	nodeEventsCh chan memberlist.NodeEvent

	// clusterNodeEventHandlers contains eventHandler which will run when consistentHashMap is updated,
	// which caused by an ExternalIPPool or Node event, such as cluster Node status update(leave of join cluster),
	// ExternalIPPool events(create/update/delete).
	// For example, when a new Node joins the cluster, each Node should compute whether it should still hold all
	// its existing Egresses, and when a Node leaves the cluster,
	// each Node should check whether it is now responsible for some of the Egresses from that Node.
	clusterNodeEventHandlers []clusterNodeEventHandler

	nodeInformer     coreinformers.NodeInformer
	nodeLister       corelisters.NodeLister
	nodeListerSynced cache.InformerSynced

	externalIPPoolInformer          cache.SharedIndexInformer
	externalIPPoolLister            crdlister.ExternalIPPoolLister
	externalIPPoolInformerHasSynced cache.InformerSynced

	// queue maintains the ExternalIPPool names that need to be synced.
	queue workqueue.RateLimitingInterface
}

// NewCluster returns a new *Cluster.
func NewCluster(
	clusterBindPort int,
	localNodeIP net.IP,
	nodeName string,
	nodeInformer coreinformers.NodeInformer,
	externalIPPoolInformer crdinformers.ExternalIPPoolInformer,
) (*Cluster, error) {
	// The Node join/leave events will be notified via it.
	nodeEventCh := make(chan memberlist.NodeEvent, 1024)
	c := &Cluster{
		bindPort:                        clusterBindPort,
		localNodeIP:                     localNodeIP,
		nodeName:                        nodeName,
		consistentHashMap:               make(map[string]*consistenthash.Map),
		nodeEventsCh:                    nodeEventCh,
		nodeInformer:                    nodeInformer,
		nodeLister:                      nodeInformer.Lister(),
		nodeListerSynced:                nodeInformer.Informer().HasSynced,
		externalIPPoolInformer:          externalIPPoolInformer.Informer(),
		externalIPPoolLister:            externalIPPoolInformer.Lister(),
		externalIPPoolInformerHasSynced: externalIPPoolInformer.Informer().HasSynced,
		queue:                           workqueue.NewNamedRateLimitingQueue(workqueue.NewItemExponentialFailureRateLimiter(minRetryDelay, maxRetryDelay), ""externalIPPool""),
	}

	conf := memberlist.DefaultLocalConfig()
	conf.Name = c.nodeName
	conf.BindPort = c.bindPort
	conf.AdvertisePort = c.bindPort
	conf.Events = &memberlist.ChannelEventDelegate{Ch: nodeEventCh}
	conf.LogOutput = ioutil.Discard
	klog.V(1).InfoS(""New memberlist cluster"", ""config"", conf)

	mList, err := memberlist.Create(conf)
	if err != nil {
		return nil, fmt.Errorf(""failed to create memberlist cluster: %v"", err)
	}
	c.mList = mList

	nodeInformer.Informer().AddEventHandlerWithResyncPeriod(
		cache.ResourceEventHandlerFuncs{
			AddFunc:    c.handleCreateNode,
			UpdateFunc: c.handleUpdateNode,
			DeleteFunc: c.handleDeleteNode,
		},
		resyncPeriod,
	)
	externalIPPoolInformer.Informer().AddEventHandlerWithResyncPeriod(
		cache.ResourceEventHandlerFuncs{
			AddFunc: c.enqueueExternalIPPool,
			UpdateFunc: func(oldObj, newObj interface{}) {
				c.enqueueExternalIPPool(newObj)
			},
			DeleteFunc: c.enqueueExternalIPPool,
		},
		resyncPeriod,
	)
	return c, nil
}

func (c *Cluster) handleCreateNode(obj interface{}) {
	node := obj.(*corev1.Node)
	if member, err := c.newClusterMember(node); err == nil {
		_, err := c.mList.Join([]string{member})
		if err != nil {
			klog.ErrorS(err, ""Processing Node CREATE event error, join cluster failed"", ""member"", member)
		}
	} else {
		klog.ErrorS(err, ""Processing Node CREATE event error"", ""nodeName"", node.Name)
	}

	affectedEIPs := c.filterEIPsFromNodeLabels(node)
	c.enqueueExternalIPPools(affectedEIPs)
	klog.V(2).InfoS(""Processed Node CREATE event"", ""nodeName"", node.Name, ""affectedExternalIPPoolNum"", affectedEIPs.Len())
}

func (c *Cluster) handleDeleteNode(obj interface{}) {
	node, ok := obj.(*corev1.Node)
	if !ok {
		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			klog.ErrorS(errDecodingObject, ""Processing Node DELETE event error"", ""obj"", obj)
			return
		}
		node, ok = tombstone.Obj.(*corev1.Node)
		if !ok {
			klog.ErrorS(errDecodingObjectTombstone, ""Processing Node DELETE event error"", ""obj"", tombstone.Obj)
			return
		}
	}
	affectedEIPs := c.filterEIPsFromNodeLabels(node)
	c.enqueueExternalIPPools(affectedEIPs)
	klog.V(2).InfoS(""Processed Node DELETE event"", ""nodeName"", node.Name, ""affectedExternalIPPoolNum"", affectedEIPs.Len())
}

func (c *Cluster) handleUpdateNode(oldObj, newObj interface{}) {
	node := newObj.(*corev1.Node)
	oldNode := oldObj.(*corev1.Node)
	if reflect.DeepEqual(node.GetLabels(), oldNode.GetLabels()) {
		klog.V(2).InfoS(""Processing Node UPDATE event error, labels not changed"", ""nodeName"", node.Name)
		return
	}
	oldMatches, newMatches := c.filterEIPsFromNodeLabels(oldNode), c.filterEIPsFromNodeLabels(node)
	if oldMatches.Equal(newMatches) {
		klog.V(2).InfoS(""Processing Node UPDATE event error, Node cluster status not changed"", ""nodeName"", node.Name)
		return
	}
	affectedEIPs := oldMatches.Union(newMatches)
	c.enqueueExternalIPPools(affectedEIPs)
	klog.V(2).InfoS(""Processed Node UPDATE event"", ""nodeName"", node.Name, ""affectedExternalIPPoolNum"", affectedEIPs.Len())
}

func (c *Cluster) enqueueExternalIPPools(eips sets.String) {
	for eip := range eips {
		c.queue.Add(eip)
	}
}

func (c *Cluster) enqueueExternalIPPool(obj interface{}) {
	eip, ok := obj.(*v1alpha2.ExternalIPPool)
	if !ok {
		deletedState, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			klog.ErrorS(errDecodingObject, ""Processing ExternalIPPool DELETE event error"", ""obj"", obj)
			return
		}
		eip, ok = deletedState.Obj.(*v1alpha2.ExternalIPPool)
		if !ok {
			klog.ErrorS(errDecodingObjectTombstone, ""Processing ExternalIPPool DELETE event error"", ""obj"", deletedState.Obj)
			return
		}
	}
	c.queue.Add(eip.Name)
}

// newClusterMember gets the Node's IP and returns a cluster member ""<IP>:<clusterMemberlistPort>""
// representing that Node in the memberlist cluster.
func (c *Cluster) newClusterMember(node *corev1.Node) (string, error) {
	nodeAddrs, err := k8s.GetNodeAddrs(node)
	if err != nil {
	return nodeAddr.String(), nil
		return """", fmt.Errorf(""obtain IP addresses from K8s Node failed: %v"", err)
	}
	nodeAddr := nodeAddrs.IPv4
	fmtStr := ""%s:%d""
	if nodeAddr == nil {
		nodeAddr = nodeAddrs.IPv6
		fmtStr = ""[%s]:%d""
	}
	member := fmt.Sprintf(fmtStr, nodeAddr, c.bindPort)
	return member, nil
}

func (c *Cluster) allClusterMembers() (clusterNodes []string, err error) {
	nodes, err := c.nodeLister.List(labels.Everything())
	if err != nil {
		return nil, fmt.Errorf(""listing Nodes error: %v"", err)
	}

	for _, node := range nodes {
		member, err := c.newClusterMember(node)
		if err != nil {
			klog.ErrorS(err, ""Get Node failed"")
			continue
		}
		clusterNodes = append(clusterNodes, member)
	}
	return
}

func (c *Cluster) filterEIPsFromNodeLabels(node *corev1.Node) sets.String {
	pools := sets.NewString()
	eips, err := c.externalIPPoolLister.List(labels.Everything())
	if err != nil {
		klog.ErrorS(err, ""Filter ExternalIPPools from nodeLabels failed"")
		return pools
	}
	for _, eip := range eips {
		nodeSelector, _ := metav1.LabelSelectorAsSelector(&eip.Spec.NodeSelector)
		if nodeSelector.Matches(labels.Set(node.GetLabels())) {
			pools.Insert(eip.Name)
		}
	}
	return pools
}

// Run will join all the other K8s Nodes in a memberlist cluster
// and will create defaultWorkers workers (go routines) which will process the ExternalIPPool or Node events
// from the work queue.
func (c *Cluster) Run(stopCh <-chan struct{}) {
	defer c.queue.ShutDown()
	// In order to exit the cluster more gracefully, call Leave prior to shutting down.
	defer close(c.nodeEventsCh)
	defer c.mList.Shutdown()
	defer c.mList.Leave(time.Second)

	klog.InfoS(""Starting"", ""controllerName"", controllerName)
	defer klog.InfoS(""Shutting down"", ""controllerName"", controllerName)

	if !cache.WaitForNamedCacheSync(controllerName, stopCh, c.externalIPPoolInformerHasSynced, c.nodeListerSynced) {
		return
	}

	members, err := c.allClusterMembers()
	if err != nil {
		klog.ErrorS(err, ""List cluster members failed"")
	} else if members != nil {
		_, err := c.mList.Join(members)
		if err != nil {
			klog.ErrorS(err, ""Join cluster failed"")
		}
	}

	for i := 0; i < defaultWorkers; i++ {
		go wait.Until(c.worker, time.Second, stopCh)
	}

	for {
		select {
		case <-stopCh:
			return
		case nodeEvent := <-c.nodeEventsCh:
			c.handleClusterNodeEvents(&nodeEvent)
		}
	}
}

func (c *Cluster) worker() {
	for c.processNextWorkItem() {
	}
}

func (c *Cluster) processNextWorkItem() bool {
	obj, quit := c.queue.Get()
	if quit {
		return false
	}
	defer c.queue.Done(obj)

	// We expect strings (ExternalIPPool name) to come off the work queue.
	if key, ok := obj.(string); !ok {
		// As the item in the work queue is actually invalid, we call Forget here else we'd
		// go into a loop of attempting to process a work item that is invalid.
		// This should not happen.
		c.queue.Forget(obj)
		klog.Errorf(""Expected string in work queue but got %#v"", obj)
		return true
	} else if err := c.syncConsistentHash(key); err == nil {
		// If no error occurs we Forget this item so it does not get queued again until
		// another change happens.
		c.queue.Forget(key)
	} else {
		// Put the item back on the work queue to handle any transient errors.
		c.queue.AddRateLimited(key)
		klog.ErrorS(err, ""Syncing consistentHash by ExternalIPPool failed, requeue"", ""ExternalIPPool"", key)
	}
	return true
}

func (c *Cluster) syncConsistentHash(eipName string) error {
	startTime := time.Now()
	defer func() {
		klog.V(4).InfoS(""Finished syncing consistentHash"", ""ExternalIPPool"", eipName, ""durationTime"", time.Since(startTime))
	}()

	eip, err := c.externalIPPoolLister.Get(eipName)
	if err != nil {
		if errors.IsNotFound(err) {
			c.consistentHashRWMutex.Lock()
			defer c.consistentHashRWMutex.Unlock()
			delete(c.consistentHashMap, eipName)
			return nil
		}
		return err
	}

	// updateConsistentHash refreshes the consistentHashMap.
	updateConsistentHash := func(eip *v1alpha2.ExternalIPPool) error {
		nodeSel, err := metav1.LabelSelectorAsSelector(&eip.Spec.NodeSelector)
		if err != nil {
			return err
		}
		nodes, err := c.nodeLister.List(nodeSel)
		if err != nil {
			return err
		}
		aliveNodes := c.aliveNodes()
		// Node alive and Node labels match ExternalIPPool nodeSelector.
		var aliveAndMatchedNodes []string
		for _, node := range nodes {
			nodeName := node.Name
			if aliveNodes.Has(nodeName) {
				aliveAndMatchedNodes = append(aliveAndMatchedNodes, nodeName)
			}
		}
		consistentHashMap := newNodeConsistentHashMap()
		consistentHashMap.Add(aliveAndMatchedNodes...)
		c.consistentHashRWMutex.Lock()
		defer c.consistentHashRWMutex.Unlock()
		c.consistentHashMap[eip.Name] = consistentHashMap
		c.notify(eip.Name)
		return nil
	}

	if err := updateConsistentHash(eip); err != nil {
		return err
	}
	return nil
}

func newNodeConsistentHashMap() *consistenthash.Map {
	return consistenthash.New(defaultVirtualNodeReplicas, defaultHashFn)
}

func (c *Cluster) handleClusterNodeEvents(nodeEvent *memberlist.NodeEvent) {
	node, event := nodeEvent.Node, nodeEvent.Event
	switch event {
	case memberlist.NodeJoin, memberlist.NodeLeave:
		// When a Node joins cluster, all matched ExternalIPPools consistentHash should be updated;
		// when a Node leaves cluster, the Node may have failed or have been deleted,
		// if the Node has been deleted, affected ExternalIPPool should be enqueued, and deleteNode handler has been executed,
		// if the Node has failed, ExternalIPPools consistentHash maybe changed, and affected ExternalIPPool should be enqueued.
		coreNode, err := c.nodeLister.Get(node.Name)
		if err != nil {
			if errors.IsNotFound(err) {
				// Node has been deleted, and deleteNode handler has been executed.
				klog.ErrorS(err, ""Processing Node event, not found"", ""eventType"", event)
				return
			}
			klog.ErrorS(err, ""Processing Node event, get Node failed"", ""eventType"", event)
			return
		}
		affectedEIPs := c.filterEIPsFromNodeLabels(coreNode)
		c.enqueueExternalIPPools(affectedEIPs)
		klog.InfoS(""Processed Node event"", ""eventType"", mapNodeEventType[event], ""nodeName"", node.Name, ""affectedExternalIPPoolNum"", len(affectedEIPs))
	default:
		klog.InfoS(""Processed Node event"", ""eventType"", mapNodeEventType[event], ""nodeName"", node.Name)
	}
}

// aliveNodes returns the list of nodeNames in the cluster.
func (c *Cluster) aliveNodes() sets.String {
	nodes := sets.NewString()
	for _, node := range c.mList.Members() {
		nodes.Insert(node.Name)
	}
	return nodes
}

// ShouldSelectEgress returns true if the local Node selected as the owner Node of the Egress,
// the local Node in the cluster holds the same consistent hash ring for each ExternalIPPool,
// consistentHash.Get gets the closest item (Node name) in the hash to the provided key(egressIP),
// if the name of the local Node is equal to the name of the selected Node, returns true.
func (c *Cluster) ShouldSelectEgress(egress *v1alpha2.Egress) (bool, error) {
	eipName := egress.Spec.ExternalIPPool
	if eipName == """" || egress.Spec.EgressIP == """" {
		return false, nil
	}
	c.consistentHashRWMutex.RLock()
	defer c.consistentHashRWMutex.RUnlock()
	consistentHash, ok := c.consistentHashMap[eipName]
	if !ok {
		return false, fmt.Errorf(""local Node consistentHashMap has not synced, ExternalIPPool %s"", eipName)
	}
	return consistentHash.Get(egress.Spec.EgressIP) == c.nodeName, nil
}

func (c *Cluster) notify(objName string) {
	for _, handler := range c.clusterNodeEventHandlers {
		handler(objName)
	}
}

// AddClusterEventHandler adds a clusterNodeEventHandler, which will run when consistentHashMap is updated,
// due to an ExternalIPPool or Node event.
func (c *Cluster) AddClusterEventHandler(handler clusterNodeEventHandler) {
	c.clusterNodeEventHandlers = append(c.clusterNodeEventHandlers, handler)
}
","Doesn't memberlist expect IPv6 addrss wrapped by ""[]""?","The suggested code change introduces a new `Cluster` struct that manages a memberlist cluster for Kubernetes nodes. It includes methods for handling node and external IP pool events, updating the consistent hash ring, and notifying handlers when the hash ring changes. The code is well-structured and follows best practices for Kubernetes and Go programming."
95,"@@ -585,12 +585,12 @@ func runWeb(ctx *cli.Context) error {
 			m.Get(""/src/*"", repo.Home)
 			m.Get(""/raw/*"", repo.SingleDownload)
 			m.Get(""/commits/*"", repo.RefCommits)
-			m.Get(""/commit/:sha([a-z0-9]{7,40})$"", repo.Diff)
+			m.Get(""/commit/:sha([a-f0-9]{7,40})$"", repo.Diff)
 			m.Get(""/forks"", repo.Forks)
 		}, context.RepoRef())
-		m.Get(""/commit/:sha([a-z0-9]{7,40})\\.:ext(patch|diff)"", repo.RawDiff)
+		m.Get(""/commit/:sha([a-f0-9]{7,40})\\.:ext(patch|diff)"", repo.RawDiff)
 
-		m.Get(""/compare/:before([a-z0-9]{7,40})\\.\\.\\.:after([a-z0-9]{7,40})"", repo.CompareDiff)
+		m.Get(""/compare/:before([a-z0-9]{40})\\.\\.\\.:after([a-z0-9]{40})"", repo.CompareDiff)
 	}, ignSignIn, context.RepoAssignment(), repo.MustBeNotBare)
 	m.Group(""/:username/:reponame"", func() {
 		m.Get(""/stars"", repo.Stars)","// Copyright 2014 The Gogs Authors. All rights reserved.
// Use of this source code is governed by a MIT-style
// license that can be found in the LICENSE file.

package cmd

import (
	""crypto/tls""
	""fmt""
	""io/ioutil""
	""net""
	""net/http""
	""net/http/fcgi""
	""os""
	""path""
	""strings""

	""github.com/go-macaron/binding""
	""github.com/go-macaron/cache""
	""github.com/go-macaron/captcha""
	""github.com/go-macaron/csrf""
	""github.com/go-macaron/gzip""
	""github.com/go-macaron/i18n""
	""github.com/go-macaron/session""
	""github.com/go-macaron/toolbox""
	""github.com/go-xorm/xorm""
	""github.com/mcuadros/go-version""
	""github.com/urfave/cli""
	""gopkg.in/ini.v1""
	""gopkg.in/macaron.v1""

	""github.com/gogits/git-module""
	""github.com/gogits/go-gogs-client""

	""github.com/gogits/gogs/models""
	""github.com/gogits/gogs/modules/auth""
	""github.com/gogits/gogs/modules/bindata""
	""github.com/gogits/gogs/modules/context""
	""github.com/gogits/gogs/modules/log""
	""github.com/gogits/gogs/modules/setting""
	""github.com/gogits/gogs/modules/template""
	""github.com/gogits/gogs/routers""
	""github.com/gogits/gogs/routers/admin""
	apiv1 ""github.com/gogits/gogs/routers/api/v1""
	""github.com/gogits/gogs/routers/dev""
	""github.com/gogits/gogs/routers/org""
	""github.com/gogits/gogs/routers/repo""
	""github.com/gogits/gogs/routers/user""
)

var CmdWeb = cli.Command{
	Name:  ""web"",
	Usage: ""Start Gogs web server"",
	Description: `Gogs web server is the only thing you need to run,
and it takes care of all the other things for you`,
	Action: runWeb,
	Flags: []cli.Flag{
		stringFlag(""port, p"", ""3000"", ""Temporary port number to prevent conflict""),
		stringFlag(""config, c"", ""custom/conf/app.ini"", ""Custom configuration file path""),
	},
}

type VerChecker struct {
	ImportPath string
	Version    func() string
	Expected   string
}

// checkVersion checks if binary matches the version of templates files.
func checkVersion() {
	// Templates.
	data, err := ioutil.ReadFile(setting.StaticRootPath + ""/templates/.VERSION"")
	if err != nil {
		log.Fatal(4, ""Fail to read 'templates/.VERSION': %v"", err)
	}
	tplVer := string(data)
	if tplVer != setting.AppVer {
		if version.Compare(tplVer, setting.AppVer, "">"") {
			log.Fatal(4, ""Binary version is lower than template file version, did you forget to recompile Gogs?"")
		} else {
			log.Fatal(4, ""Binary version is higher than template file version, did you forget to update template files?"")
		}
	}

	// Check dependency version.
	checkers := []VerChecker{
		{""github.com/go-xorm/xorm"", func() string { return xorm.Version }, ""0.5.5""},
		{""github.com/go-macaron/binding"", binding.Version, ""0.3.2""},
		{""github.com/go-macaron/cache"", cache.Version, ""0.1.2""},
		{""github.com/go-macaron/csrf"", csrf.Version, ""0.1.0""},
		{""github.com/go-macaron/i18n"", i18n.Version, ""0.3.0""},
		{""github.com/go-macaron/session"", session.Version, ""0.1.6""},
		{""github.com/go-macaron/toolbox"", toolbox.Version, ""0.1.0""},
		{""gopkg.in/ini.v1"", ini.Version, ""1.8.4""},
		{""gopkg.in/macaron.v1"", macaron.Version, ""1.1.7""},
		{""github.com/gogits/git-module"", git.Version, ""0.4.1""},
		{""github.com/gogits/go-gogs-client"", gogs.Version, ""0.12.1""},
	}
	for _, c := range checkers {
		if !version.Compare(c.Version(), c.Expected, "">="") {
			log.Fatal(4, `Dependency outdated!
Package '%s' current version (%s) is below requirement (%s), 
please use following command to update this package and recompile Gogs:
go get -u %[1]s`, c.ImportPath, c.Version(), c.Expected)
		}
	}
}

// newMacaron initializes Macaron instance.
func newMacaron() *macaron.Macaron {
	m := macaron.New()
	if !setting.DisableRouterLog {
		m.Use(macaron.Logger())
	}
	m.Use(macaron.Recovery())
	if setting.EnableGzip {
		m.Use(gzip.Gziper())
	}
	if setting.Protocol == setting.FCGI {
		m.SetURLPrefix(setting.AppSubUrl)
	}
	m.Use(macaron.Static(
		path.Join(setting.StaticRootPath, ""public""),
		macaron.StaticOptions{
			SkipLogging: setting.DisableRouterLog,
		},
	))
	m.Use(macaron.Static(
		setting.AvatarUploadPath,
		macaron.StaticOptions{
			Prefix:      ""avatars"",
			SkipLogging: setting.DisableRouterLog,
		},
	))

	funcMap := template.NewFuncMap()
	m.Use(macaron.Renderer(macaron.RenderOptions{
		Directory:         path.Join(setting.StaticRootPath, ""templates""),
		AppendDirectories: []string{path.Join(setting.CustomPath, ""templates"")},
		Funcs:             funcMap,
		IndentJSON:        macaron.Env != macaron.PROD,
	}))
	models.InitMailRender(path.Join(setting.StaticRootPath, ""templates/mail""),
		path.Join(setting.CustomPath, ""templates/mail""), funcMap)

	localeNames, err := bindata.AssetDir(""conf/locale"")
	if err != nil {
		log.Fatal(4, ""Fail to list locale files: %v"", err)
	}
	localFiles := make(map[string][]byte)
	for _, name := range localeNames {
		localFiles[name] = bindata.MustAsset(""conf/locale/"" + name)
	}
	m.Use(i18n.I18n(i18n.Options{
		SubURL:          setting.AppSubUrl,
		Files:           localFiles,
		CustomDirectory: path.Join(setting.CustomPath, ""conf/locale""),
		Langs:           setting.Langs,
		Names:           setting.Names,
		DefaultLang:     ""en-US"",
		Redirect:        true,
	}))
	m.Use(cache.Cacher(cache.Options{
		Adapter:       setting.CacheAdapter,
		AdapterConfig: setting.CacheConn,
		Interval:      setting.CacheInterval,
	}))
	m.Use(captcha.Captchaer(captcha.Options{
		SubURL: setting.AppSubUrl,
	}))
	m.Use(session.Sessioner(setting.SessionConfig))
	m.Use(csrf.Csrfer(csrf.Options{
		Secret:     setting.SecretKey,
		Cookie:     setting.CSRFCookieName,
		SetCookie:  true,
		Header:     ""X-Csrf-Token"",
		CookiePath: setting.AppSubUrl,
	}))
	m.Use(toolbox.Toolboxer(m, toolbox.Options{
		HealthCheckFuncs: []*toolbox.HealthCheckFuncDesc{
			&toolbox.HealthCheckFuncDesc{
				Desc: ""Database connection"",
				Func: models.Ping,
			},
		},
	}))
	m.Use(context.Contexter())
	return m
}

func runWeb(ctx *cli.Context) error {
	if ctx.IsSet(""config"") {
		setting.CustomConf = ctx.String(""config"")
	}
	routers.GlobalInit()
	checkVersion()

	m := newMacaron()

	reqSignIn := context.Toggle(&context.ToggleOptions{SignInRequired: true})
	ignSignIn := context.Toggle(&context.ToggleOptions{SignInRequired: setting.Service.RequireSignInView})
	ignSignInAndCsrf := context.Toggle(&context.ToggleOptions{DisableCSRF: true})
	reqSignOut := context.Toggle(&context.ToggleOptions{SignOutRequired: true})

	bindIgnErr := binding.BindIgnErr

	// FIXME: not all routes need go through same middlewares.
	// Especially some AJAX requests, we can reduce middleware number to improve performance.
	// Routers.
	m.Get(""/"", ignSignIn, routers.Home)
	m.Group(""/explore"", func() {
		m.Get("""", func(ctx *context.Context) {
			ctx.Redirect(setting.AppSubUrl + ""/explore/repos"")
		})
		m.Get(""/repos"", routers.ExploreRepos)
		m.Get(""/users"", routers.ExploreUsers)
		m.Get(""/organizations"", routers.ExploreOrganizations)
	}, ignSignIn)
	m.Combo(""/install"", routers.InstallInit).Get(routers.Install).
		Post(bindIgnErr(auth.InstallForm{}), routers.InstallPost)
	m.Get(""/^:type(issues|pulls)$"", reqSignIn, user.Issues)

	// ***** START: User *****
	m.Group(""/user"", func() {
		m.Get(""/login"", user.SignIn)
		m.Post(""/login"", bindIgnErr(auth.SignInForm{}), user.SignInPost)
		m.Get(""/sign_up"", user.SignUp)
		m.Post(""/sign_up"", bindIgnErr(auth.RegisterForm{}), user.SignUpPost)
		m.Get(""/reset_password"", user.ResetPasswd)
		m.Post(""/reset_password"", user.ResetPasswdPost)
	}, reqSignOut)

	m.Group(""/user/settings"", func() {
		m.Get("""", user.Settings)
		m.Post("""", bindIgnErr(auth.UpdateProfileForm{}), user.SettingsPost)
		m.Combo(""/avatar"").Get(user.SettingsAvatar).
			Post(binding.MultipartForm(auth.AvatarForm{}), user.SettingsAvatarPost)
		m.Post(""/avatar/delete"", user.SettingsDeleteAvatar)
		m.Combo(""/email"").Get(user.SettingsEmails).
			Post(bindIgnErr(auth.AddEmailForm{}), user.SettingsEmailPost)
		m.Post(""/email/delete"", user.DeleteEmail)
		m.Get(""/password"", user.SettingsPassword)
		m.Post(""/password"", bindIgnErr(auth.ChangePasswordForm{}), user.SettingsPasswordPost)
		m.Combo(""/ssh"").Get(user.SettingsSSHKeys).
			Post(bindIgnErr(auth.AddSSHKeyForm{}), user.SettingsSSHKeysPost)
		m.Post(""/ssh/delete"", user.DeleteSSHKey)
		m.Combo(""/applications"").Get(user.SettingsApplications).
			Post(bindIgnErr(auth.NewAccessTokenForm{}), user.SettingsApplicationsPost)
		m.Post(""/applications/delete"", user.SettingsDeleteApplication)
		m.Route(""/delete"", ""GET,POST"", user.SettingsDelete)
	}, reqSignIn, func(ctx *context.Context) {
		ctx.Data[""PageIsUserSettings""] = true
	})

	m.Group(""/user"", func() {
		// r.Get(""/feeds"", binding.Bind(auth.FeedsForm{}), user.Feeds)
		m.Any(""/activate"", user.Activate)
		m.Any(""/activate_email"", user.ActivateEmail)
		m.Get(""/email2user"", user.Email2User)
		m.Get(""/forget_password"", user.ForgotPasswd)
		m.Post(""/forget_password"", user.ForgotPasswdPost)
		m.Get(""/logout"", user.SignOut)
	})
	// ***** END: User *****

	adminReq := context.Toggle(&context.ToggleOptions{SignInRequired: true, AdminRequired: true})

	// ***** START: Admin *****
	m.Group(""/admin"", func() {
		m.Get("""", adminReq, admin.Dashboard)
		m.Get(""/config"", admin.Config)
		m.Post(""/config/test_mail"", admin.SendTestMail)
		m.Get(""/monitor"", admin.Monitor)

		m.Group(""/users"", func() {
			m.Get("""", admin.Users)
			m.Combo(""/new"").Get(admin.NewUser).Post(bindIgnErr(auth.AdminCrateUserForm{}), admin.NewUserPost)
			m.Combo(""/:userid"").Get(admin.EditUser).Post(bindIgnErr(auth.AdminEditUserForm{}), admin.EditUserPost)
			m.Post(""/:userid/delete"", admin.DeleteUser)
		})

		m.Group(""/orgs"", func() {
			m.Get("""", admin.Organizations)
		})

		m.Group(""/repos"", func() {
			m.Get("""", admin.Repos)
			m.Post(""/delete"", admin.DeleteRepo)
		})

		m.Group(""/auths"", func() {
			m.Get("""", admin.Authentications)
			m.Combo(""/new"").Get(admin.NewAuthSource).Post(bindIgnErr(auth.AuthenticationForm{}), admin.NewAuthSourcePost)
			m.Combo(""/:authid"").Get(admin.EditAuthSource).
				Post(bindIgnErr(auth.AuthenticationForm{}), admin.EditAuthSourcePost)
			m.Post(""/:authid/delete"", admin.DeleteAuthSource)
		})

		m.Group(""/notices"", func() {
			m.Get("""", admin.Notices)
			m.Post(""/delete"", admin.DeleteNotices)
			m.Get(""/empty"", admin.EmptyNotices)
		})
	}, adminReq)
	// ***** END: Admin *****

	m.Group("""", func() {
		m.Group(""/:username"", func() {
			m.Get("""", user.Profile)
			m.Get(""/followers"", user.Followers)
			m.Get(""/following"", user.Following)
			m.Get(""/stars"", user.Stars)
		})

		m.Get(""/attachments/:uuid"", func(ctx *context.Context) {
			attach, err := models.GetAttachmentByUUID(ctx.Params("":uuid""))
			if err != nil {
				if models.IsErrAttachmentNotExist(err) {
					ctx.Error(404)
				} else {
					ctx.Handle(500, ""GetAttachmentByUUID"", err)
				}
				return
			}

			fr, err := os.Open(attach.LocalPath())
			if err != nil {
				ctx.Handle(500, ""Open"", err)
				return
			}
			defer fr.Close()

			ctx.Header().Set(""Cache-Control"", ""public,max-age=86400"")
			ctx.Header().Set(""Content-Disposition"", fmt.Sprintf(`inline; filename=""%s""`, attach.Name))
			// Fix #312. Attachments with , in their name are not handled correctly by Google Chrome.
			// We must put the name in "" manually.
			if err = repo.ServeData(ctx, ""\""""+attach.Name+""\"""", fr); err != nil {
				ctx.Handle(500, ""ServeData"", err)
				return
			}
		})
		m.Post(""/issues/attachments"", repo.UploadIssueAttachment)
	}, ignSignIn)

	m.Group(""/:username"", func() {
		m.Get(""/action/:action"", user.Action)
	}, reqSignIn)

	if macaron.Env == macaron.DEV {
		m.Get(""/template/*"", dev.TemplatePreview)
	}

	reqRepoAdmin := context.RequireRepoAdmin()
	reqRepoWriter := context.RequireRepoWriter()

	// ***** START: Organization *****
	m.Group(""/org"", func() {
		m.Get(""/create"", org.Create)
		m.Post(""/create"", bindIgnErr(auth.CreateOrgForm{}), org.CreatePost)

		m.Group(""/:org"", func() {
			m.Get(""/dashboard"", user.Dashboard)
			m.Get(""/^:type(issues|pulls)$"", user.Issues)
			m.Get(""/members"", org.Members)
			m.Get(""/members/action/:action"", org.MembersAction)

			m.Get(""/teams"", org.Teams)
		}, context.OrgAssignment(true))

		m.Group(""/:org"", func() {
			m.Get(""/teams/:team"", org.TeamMembers)
			m.Get(""/teams/:team/repositories"", org.TeamRepositories)
			m.Route(""/teams/:team/action/:action"", ""GET,POST"", org.TeamsAction)
			m.Route(""/teams/:team/action/repo/:action"", ""GET,POST"", org.TeamsRepoAction)
		}, context.OrgAssignment(true, false, true))

		m.Group(""/:org"", func() {
			m.Get(""/teams/new"", org.NewTeam)
			m.Post(""/teams/new"", bindIgnErr(auth.CreateTeamForm{}), org.NewTeamPost)
			m.Get(""/teams/:team/edit"", org.EditTeam)
			m.Post(""/teams/:team/edit"", bindIgnErr(auth.CreateTeamForm{}), org.EditTeamPost)
			m.Post(""/teams/:team/delete"", org.DeleteTeam)

			m.Group(""/settings"", func() {
				m.Combo("""").Get(org.Settings).
					Post(bindIgnErr(auth.UpdateOrgSettingForm{}), org.SettingsPost)
				m.Post(""/avatar"", binding.MultipartForm(auth.AvatarForm{}), org.SettingsAvatar)
				m.Post(""/avatar/delete"", org.SettingsDeleteAvatar)

				m.Group(""/hooks"", func() {
					m.Get("""", org.Webhooks)
					m.Post(""/delete"", org.DeleteWebhook)
					m.Get(""/:type/new"", repo.WebhooksNew)
					m.Post(""/gogs/new"", bindIgnErr(auth.NewWebhookForm{}), repo.WebHooksNewPost)
					m.Post(""/slack/new"", bindIgnErr(auth.NewSlackHookForm{}), repo.SlackHooksNewPost)
					m.Get(""/:id"", repo.WebHooksEdit)
					m.Post(""/gogs/:id"", bindIgnErr(auth.NewWebhookForm{}), repo.WebHooksEditPost)
					m.Post(""/slack/:id"", bindIgnErr(auth.NewSlackHookForm{}), repo.SlackHooksEditPost)
				})

				m.Route(""/delete"", ""GET,POST"", org.SettingsDelete)
			})

			m.Route(""/invitations/new"", ""GET,POST"", org.Invitation)
		}, context.OrgAssignment(true, true))
	}, reqSignIn)
	// ***** END: Organization *****

	// ***** START: Repository *****
	m.Group(""/repo"", func() {
		m.Get(""/create"", repo.Create)
		m.Post(""/create"", bindIgnErr(auth.CreateRepoForm{}), repo.CreatePost)
		m.Get(""/migrate"", repo.Migrate)
		m.Post(""/migrate"", bindIgnErr(auth.MigrateRepoForm{}), repo.MigratePost)
		m.Combo(""/fork/:repoid"").Get(repo.Fork).
			Post(bindIgnErr(auth.CreateRepoForm{}), repo.ForkPost)
	}, reqSignIn)

	m.Group(""/:username/:reponame"", func() {
		m.Group(""/settings"", func() {
			m.Combo("""").Get(repo.Settings).
				Post(bindIgnErr(auth.RepoSettingForm{}), repo.SettingsPost)
			m.Group(""/collaboration"", func() {
				m.Combo("""").Get(repo.Collaboration).Post(repo.CollaborationPost)
				m.Post(""/access_mode"", repo.ChangeCollaborationAccessMode)
				m.Post(""/delete"", repo.DeleteCollaboration)
			})

			m.Group(""/hooks"", func() {
				m.Get("""", repo.Webhooks)
				m.Post(""/delete"", repo.DeleteWebhook)
				m.Get(""/:type/new"", repo.WebhooksNew)
				m.Post(""/gogs/new"", bindIgnErr(auth.NewWebhookForm{}), repo.WebHooksNewPost)
				m.Post(""/slack/new"", bindIgnErr(auth.NewSlackHookForm{}), repo.SlackHooksNewPost)
				m.Get(""/:id"", repo.WebHooksEdit)
				m.Post(""/:id/test"", repo.TestWebhook)
				m.Post(""/gogs/:id"", bindIgnErr(auth.NewWebhookForm{}), repo.WebHooksEditPost)
				m.Post(""/slack/:id"", bindIgnErr(auth.NewSlackHookForm{}), repo.SlackHooksEditPost)

				m.Group(""/git"", func() {
					m.Get("""", repo.GitHooks)
					m.Combo(""/:name"").Get(repo.GitHooksEdit).
						Post(repo.GitHooksEditPost)
				}, context.GitHookService())
			})

			m.Group(""/keys"", func() {
				m.Combo("""").Get(repo.DeployKeys).
					Post(bindIgnErr(auth.AddSSHKeyForm{}), repo.DeployKeysPost)
				m.Post(""/delete"", repo.DeleteDeployKey)
			})

		}, func(ctx *context.Context) {
			ctx.Data[""PageIsSettings""] = true
		})
	}, reqSignIn, context.RepoAssignment(), reqRepoAdmin, context.RepoRef())

	m.Get(""/:username/:reponame/action/:action"", reqSignIn, context.RepoAssignment(), repo.Action)
	m.Group(""/:username/:reponame"", func() {
		// FIXME: should use different URLs but mostly same logic for comments of issue and pull reuqest.
		// So they can apply their own enable/disable logic on routers.
		m.Group(""/issues"", func() {
			m.Combo(""/new"", repo.MustEnableIssues).Get(context.RepoRef(), repo.NewIssue).
				Post(bindIgnErr(auth.CreateIssueForm{}), repo.NewIssuePost)

			m.Group(""/:index"", func() {
				m.Post(""/label"", repo.UpdateIssueLabel)
				m.Post(""/milestone"", repo.UpdateIssueMilestone)
				m.Post(""/assignee"", repo.UpdateIssueAssignee)
			}, reqRepoWriter)

			m.Group(""/:index"", func() {
				m.Post(""/title"", repo.UpdateIssueTitle)
				m.Post(""/content"", repo.UpdateIssueContent)
				m.Combo(""/comments"").Post(bindIgnErr(auth.CreateCommentForm{}), repo.NewComment)
			})
		})
		m.Group(""/comments/:id"", func() {
			m.Post("""", repo.UpdateCommentContent)
			m.Post(""/delete"", repo.DeleteComment)
		})
		m.Group(""/labels"", func() {
			m.Post(""/new"", bindIgnErr(auth.CreateLabelForm{}), repo.NewLabel)
			m.Post(""/edit"", bindIgnErr(auth.CreateLabelForm{}), repo.UpdateLabel)
			m.Post(""/delete"", repo.DeleteLabel)
			m.Post(""/initialize"", bindIgnErr(auth.InitializeLabelsForm{}), repo.InitializeLabels)
		}, reqRepoWriter, context.RepoRef())
		m.Group(""/milestones"", func() {
			m.Combo(""/new"").Get(repo.NewMilestone).
				Post(bindIgnErr(auth.CreateMilestoneForm{}), repo.NewMilestonePost)
			m.Get(""/:id/edit"", repo.EditMilestone)
			m.Post(""/:id/edit"", bindIgnErr(auth.CreateMilestoneForm{}), repo.EditMilestonePost)
			m.Get(""/:id/:action"", repo.ChangeMilestonStatus)
			m.Post(""/delete"", repo.DeleteMilestone)
		}, reqRepoWriter, context.RepoRef())

		m.Group(""/releases"", func() {
			m.Get(""/new"", repo.NewRelease)
			m.Post(""/new"", bindIgnErr(auth.NewReleaseForm{}), repo.NewReleasePost)
			m.Post(""/delete"", repo.DeleteRelease)
		}, reqRepoWriter, context.RepoRef())

		m.Group(""/releases"", func() {
			m.Get(""/edit/*"", repo.EditRelease)
			m.Post(""/edit/*"", bindIgnErr(auth.EditReleaseForm{}), repo.EditReleasePost)
		}, reqRepoWriter, func(ctx *context.Context) {
			var err error
			ctx.Repo.Commit, err = ctx.Repo.GitRepo.GetBranchCommit(ctx.Repo.Repository.DefaultBranch)
			if err != nil {
				ctx.Handle(500, ""GetBranchCommit"", err)
				return
			}
			ctx.Repo.CommitsCount, err = ctx.Repo.Commit.CommitsCount()
			if err != nil {
				ctx.Handle(500, ""CommitsCount"", err)
				return
			}
			ctx.Data[""CommitsCount""] = ctx.Repo.CommitsCount
		})

		m.Combo(""/compare/*"", repo.MustAllowPulls).Get(repo.CompareAndPullRequest).
			Post(bindIgnErr(auth.CreateIssueForm{}), repo.CompareAndPullRequestPost)

		m.Group("""", func() {
			m.Combo(""/_edit/*"").Get(repo.EditFile).
				Post(bindIgnErr(auth.EditRepoFileForm{}), repo.EditFilePost)
			m.Combo(""/_new/*"").Get(repo.NewFile).
				Post(bindIgnErr(auth.EditRepoFileForm{}), repo.NewFilePost)
			m.Post(""/_preview/*"", bindIgnErr(auth.EditPreviewDiffForm{}), repo.DiffPreviewPost)
			m.Combo(""/_delete/*"").Get(repo.DeleteFile).
				Post(bindIgnErr(auth.DeleteRepoFileForm{}), repo.DeleteFilePost)

			m.Group("""", func() {
				m.Combo(""/_upload/*"").Get(repo.UploadFile).
					Post(bindIgnErr(auth.UploadRepoFileForm{}), repo.UploadFilePost)
				m.Post(""/upload-file"", repo.UploadFileToServer)
				m.Post(""/upload-remove"", bindIgnErr(auth.RemoveUploadFileForm{}), repo.RemoveUploadFileFromServer)
			}, func(ctx *context.Context) {
				if !setting.Repository.Upload.Enabled {
					ctx.Handle(404, """", nil)
					return
				}
			})
		}, reqRepoWriter, context.RepoRef(), func(ctx *context.Context) {
			if !ctx.Repo.Repository.CanEnableEditor() || ctx.Repo.IsViewCommit {
				ctx.Handle(404, """", nil)
				return
			}
		})
	}, reqSignIn, context.RepoAssignment(), repo.MustBeNotBare)

	m.Group(""/:username/:reponame"", func() {
		m.Group("""", func() {
			m.Get(""/releases"", repo.Releases)
			m.Get(""/^:type(issues|pulls)$"", repo.RetrieveLabels, repo.Issues)
			m.Get(""/^:type(issues|pulls)$/:index"", repo.ViewIssue)
			m.Get(""/labels/"", repo.RetrieveLabels, repo.Labels)
			m.Get(""/milestones"", repo.Milestones)
		}, context.RepoRef())

		// m.Get(""/branches"", repo.Branches)

		m.Group(""/wiki"", func() {
			m.Get(""/?:page"", repo.Wiki)
			m.Get(""/_pages"", repo.WikiPages)

			m.Group("""", func() {
				m.Combo(""/_new"").Get(repo.NewWiki).
					Post(bindIgnErr(auth.NewWikiForm{}), repo.NewWikiPost)
				m.Combo(""/:page/_edit"").Get(repo.EditWiki).
					Post(bindIgnErr(auth.NewWikiForm{}), repo.EditWikiPost)
				m.Post(""/:page/delete"", repo.DeleteWikiPagePost)
			}, reqSignIn, reqRepoWriter)
		}, repo.MustEnableWiki, context.RepoRef())

		m.Get(""/archive/*"", repo.Download)

		m.Group(""/pulls/:index"", func() {
			m.Get(""/commits"", context.RepoRef(), repo.ViewPullCommits)
			m.Get(""/files"", context.RepoRef(), repo.ViewPullFiles)
			m.Post(""/merge"", reqRepoWriter, repo.MergePullRequest)
		}, repo.MustAllowPulls)

		m.Group("""", func() {
			m.Get(""/commit/:sha([a-f0-9]{7,40})$"", repo.Diff)
		m.Get(""/commit/:sha([a-f0-9]{7,40})\\.:ext(patch|diff)"", repo.RawDiff)
		m.Get(""/compare/:before([a-z0-9]{40})\\.\\.\\.:after([a-z0-9]{40})"", repo.CompareDiff)
			m.Get(""/src/*"", repo.Home)
			m.Get(""/raw/*"", repo.SingleDownload)
			m.Get(""/commits/*"", repo.RefCommits)
			m.Get(""/commit/:sha([a-z0-9]{7,40})$"", repo.Diff)
			m.Get(""/forks"", repo.Forks)
		}, context.RepoRef())
		m.Get(""/commit/:sha([a-z0-9]{7,40})\\.:ext(patch|diff)"", repo.RawDiff)

		m.Get(""/compare/:before([a-z0-9]{7,40})\\.\\.\\.:after([a-z0-9]{7,40})"", repo.CompareDiff)
	}, ignSignIn, context.RepoAssignment(), repo.MustBeNotBare)
	m.Group(""/:username/:reponame"", func() {
		m.Get(""/stars"", repo.Stars)
		m.Get(""/watchers"", repo.Watchers)
	}, ignSignIn, context.RepoAssignment(), context.RepoRef())

	m.Group(""/:username"", func() {
		m.Group(""/:reponame"", func() {
			m.Get("""", repo.Home)
			m.Get(""\\.git$"", repo.Home)
		}, ignSignIn, context.RepoAssignment(true), context.RepoRef())

		m.Group(""/:reponame"", func() {
			m.Any(""/*"", ignSignInAndCsrf, repo.HTTP)
			m.Head(""/tasks/trigger"", repo.TriggerTask)
		})
	})
	// ***** END: Repository *****

	m.Group(""/api"", func() {
		apiv1.RegisterRoutes(m)
	}, ignSignIn)

	// robots.txt
	m.Get(""/robots.txt"", func(ctx *context.Context) {
		if setting.HasRobotsTxt {
			ctx.ServeFileContent(path.Join(setting.CustomPath, ""robots.txt""))
		} else {
			ctx.Error(404)
		}
	})

	// Not found handler.
	m.NotFound(routers.NotFound)

	// Flag for port number in case first time run conflict.
	if ctx.IsSet(""port"") {
		setting.AppUrl = strings.Replace(setting.AppUrl, setting.HTTPPort, ctx.String(""port""), 1)
		setting.HTTPPort = ctx.String(""port"")
	}

	var listenAddr string
	if setting.Protocol == setting.UNIX_SOCKET {
		listenAddr = fmt.Sprintf(""%s"", setting.HTTPAddr)
	} else {
		listenAddr = fmt.Sprintf(""%s:%s"", setting.HTTPAddr, setting.HTTPPort)
	}
	log.Info(""Listen: %v://%s%s"", setting.Protocol, listenAddr, setting.AppSubUrl)

	var err error
	switch setting.Protocol {
	case setting.HTTP:
		err = http.ListenAndServe(listenAddr, m)
	case setting.HTTPS:
		server := &http.Server{Addr: listenAddr, TLSConfig: &tls.Config{MinVersion: tls.VersionTLS10}, Handler: m}
		err = server.ListenAndServeTLS(setting.CertFile, setting.KeyFile)
	case setting.FCGI:
		err = fcgi.Serve(nil, m)
	case setting.UNIX_SOCKET:
		os.Remove(listenAddr)

		var listener *net.UnixListener
		listener, err = net.ListenUnix(""unix"", &net.UnixAddr{listenAddr, ""unix""})
		if err != nil {
			break // Handle error after switch
		}

		// FIXME: add proper implementation of signal capture on all protocols
		// execute this on SIGTERM or SIGINT: listener.Close()
		if err = os.Chmod(listenAddr, os.FileMode(setting.UnixSocketPermission)); err != nil {
			log.Fatal(4, ""Failed to set permission of unix socket: %v"", err)
		}
		err = http.Serve(listener, m)
	default:
		log.Fatal(4, ""Invalid protocol: %s"", setting.Protocol)
	}

	if err != nil {
		log.Fatal(4, ""Fail to start server: %v"", err)
	}

	return nil
}
",And now I can't compare `master...develop` anymore :unamused:,"The suggested code change appears to be the implementation of the `runWeb` function, which initializes and starts the Gogs web server. Here's a concise review:

### Key Points:

1. **Initialization**:
   - The `newMacaron` function initializes a new Macaron instance with various middleware layers such as logging, recovery, gzip compression, static file serving, and rendering.
   - It sets up routes for different functionalities like user management, repository management, organization management, and API endpoints.

2. **Middleware Configuration**:
   - Middleware like `context.Contexter`, `i18n.I18n`, `cache.Cacher`, `captcha.Captchaer`, `session.Sessioner`, and `csrf.Csrfer` are configured to handle various aspects of the application, such as request context, localization, caching, CAPTCHA, sessions, CSRF protection, and health checks.

3. **Route Definitions**:
   - The code defines numerous routes for handling different actions such as user sign-in, sign-up, profile management, repository creation, issue management, and more.
   - Routes are grouped based on functionality (e.g., user, admin, organization, repository) to improve maintainability and readability.

4. **Environment-Specific Configuration**:
   - The code checks for environment-specific configurations like custom configuration files, port numbers, and protocol settings.
   - It handles different protocols (HTTP, HTTPS, FCGI, UNIX socket) and sets up appropriate server configurations.

5. **Error Handling**:
   - The code includes error handling for various operations such as starting the server, reading configuration files, and database connections.
   - It logs errors using the `log.Fatal` function, which will terminate the application if an error occurs.

6. **Security Considerations**:
   - The code includes security features like CSRF protection, session management, and secure cookie handling.
   - It ensures that sensitive information like passwords and tokens is handled securely.

### Overall Quality:

- **Readability**: The code is well-structured with clear separation of concerns and logical grouping of routes.
- **Maintainability**: The use of middleware and route groups makes it easy to manage and extend the application.
- **Security**: The inclusion of security features enhances the robustness of the application.
- **Error Handling**: Proper error handling is implemented to ensure the application can gracefully handle failures.

### Suggested Improvements:

1. **Code Comments**: Add more detailed comments to explain complex logic or sections of the code.
2. **"
96,"@@ -134,6 +134,7 @@ class DagitWebserver(GraphQLServer):
             return Route(
                 file_path,
                 lambda _: FileResponse(path=self.relative_path(f""webapp/build{file_path}"")),
+                name=""root_static"",
             )
 
         return [_static_file(f) for f in ROOT_ADDRESS_STATIC_RESOURCES]","import gzip
import io
import uuid
from os import path
from typing import List

from dagster import DagsterInstance
from dagster import __version__ as dagster_version
from dagster.cli.workspace.cli_target import get_workspace_process_context_from_kwargs
from dagster.core.debug import DebugRunPayload
from dagster.core.storage.compute_log_manager import ComputeIOType
from dagster.core.workspace.context import WorkspaceProcessContext, WorkspaceRequestContext
from dagster_graphql import __version__ as dagster_graphql_version
from dagster_graphql.schema import create_schema
from graphene import Schema
from starlette.exceptions import HTTPException
from starlette.middleware import Middleware
from starlette.requests import HTTPConnection, Request
from starlette.responses import FileResponse, HTMLResponse, JSONResponse, StreamingResponse
from starlette.routing import Mount, Route, WebSocketRoute
from starlette.staticfiles import StaticFiles

from .graphql import GraphQLServer
from .version import __version__

ROOT_ADDRESS_STATIC_RESOURCES = [
    ""/manifest.json"",
    ""/favicon.ico"",
    ""/favicon.png"",
    ""/favicon.svg"",
    ""/favicon-run-pending.svg"",
    ""/favicon-run-failed.svg"",
    ""/favicon-run-success.svg"",
    ""/asset-manifest.json"",
    ""/robots.txt"",
]


class DagitWebserver(GraphQLServer):
    def __init__(self, process_context: WorkspaceProcessContext, app_path_prefix: str = """"):
        self._process_context = process_context
        super().__init__(app_path_prefix)

    def build_graphql_schema(self) -> Schema:
        return create_schema()

    def build_graphql_middleware(self) -> list:
        return []

    def relative_path(self, rel: str) -> str:
        return path.join(path.dirname(__file__), rel)

    def make_request_context(self, conn: HTTPConnection) -> WorkspaceRequestContext:
        return self._process_context.create_request_context(conn)

    def build_middleware(self) -> List[Middleware]:
        return []

    async def dagit_info_endpoint(self, _request: Request):
        return JSONResponse(
            {
                ""dagit_version"": __version__,
                ""dagster_version"": dagster_version,
                ""dagster_graphql_version"": dagster_graphql_version,
            }
        )

    async def download_debug_file_endpoint(self, request: Request):
        run_id = request.path_params[""run_id""]
        context = self.make_request_context(request)

        run = context.instance.get_run_by_id(run_id)
        debug_payload = DebugRunPayload.build(self._process_context.instance, run)

        result = io.BytesIO()
        with gzip.GzipFile(fileobj=result, mode=""wb"") as file:
            debug_payload.write(file)

        result.seek(0)  # be kind, please rewind

        return StreamingResponse(result, media_type=""application/gzip"")

    async def download_compute_logs_endpoint(self, request: Request):
        run_id = request.path_params[""run_id""]
        step_key = request.path_params[""step_key""]
        file_type = request.path_params[""file_type""]
        context = self.make_request_context(request)

        file = context.instance.compute_log_manager.get_local_path(
            run_id,
            step_key,
            ComputeIOType(file_type),
        )

        if not path.exists(file):
            raise HTTPException(404)

        return FileResponse(
            context.instance.compute_log_manager.get_local_path(
                run_id,
                step_key,
                ComputeIOType(file_type),
            ),
            filename=f""{run_id}_{step_key}.{file_type}"",
        )

    def index_html_endpoint(self, _request: Request):
        """"""
        Serves root html
        """"""
        index_path = self.relative_path(""webapp/build/index.html"")

        try:
            with open(index_path) as f:
                rendered_template = f.read()
                return HTMLResponse(
                    rendered_template.replace('href=""/', f'href=""{self._app_path_prefix}/')
                    .replace('src=""/', f'src=""{self._app_path_prefix}/')
                    .replace(""__PATH_PREFIX__"", self._app_path_prefix)
                    .replace(""NONCE-PLACEHOLDER"", uuid.uuid4().hex)
                )
        except FileNotFoundError:
            raise Exception(
                """"""
                Can't find webapp files.
                If you are using dagit, then probably it's a corrupted installation or a bug.
                However, if you are developing dagit locally, your problem can be fixed by running
                ""make rebuild_dagit"" in the project root.
                """"""
            )

    def root_static_file_routes(self) -> List[Route]:
        def _static_file(file_path):
                name=""root_static"",
            return Route(
                file_path,
                lambda _: FileResponse(path=self.relative_path(f""webapp/build{file_path}"")),
            )

        return [_static_file(f) for f in ROOT_ADDRESS_STATIC_RESOURCES]

    def build_static_routes(self):
        return [
            # static resources addressed at /static/
            Mount(
                ""/static"",
                StaticFiles(
                    directory=self.relative_path(""webapp/build/static""),
                    check_dir=False,
                ),
                name=""static"",
            ),
            # static resources addressed at /vendor/
            Mount(
                ""/vendor"",
                StaticFiles(
                    directory=self.relative_path(""webapp/build/vendor""),
                    check_dir=False,
                ),
                name=""vendor"",
            ),
            # specific static resources addressed at /
            *self.root_static_file_routes(),
        ]

    def build_routes(self):
        return (
            [
                Route(""/dagit_info"", self.dagit_info_endpoint),
                Route(
                    ""/graphql"",
                    self.graphql_http_endpoint,
                    name=""graphql-http"",
                    methods=[""GET"", ""POST""],
                ),
                WebSocketRoute(
                    ""/graphql"",
                    self.graphql_ws_endpoint,
                    name=""graphql-ws"",
                ),
            ]
            + self.build_static_routes()
            + [
                # download file endpoints
                Route(
                    ""/download/{run_id:str}/{step_key:str}/{file_type:str}"",
                    self.download_compute_logs_endpoint,
                ),
                Route(
                    ""/download_debug/{run_id:str}"",
                    self.download_debug_file_endpoint,
                ),
                Route(""/{path:path}"", self.index_html_endpoint),
                Route(""/"", self.index_html_endpoint),
            ]
        )


def default_app(debug=False):
    instance = DagsterInstance.get()
    process_context = get_workspace_process_context_from_kwargs(
        instance=instance,
        version=__version__,
        read_only=False,
        kwargs={},
    )

    return DagitWebserver(
        process_context,
    ).create_asgi_app(debug=debug)


def debug_app():
    return default_app(debug=True)
","Feels weird for multiple routes to have the same name - but I guess we're not using `url_for` for this, so it's fine as an identifier for now","The suggested code change appears to be a complete implementation of a Dagit web server using Starlette, a modern ASGI framework for building web applications. The code defines a `DagitWebserver` class that inherits from `GraphQLServer` and provides implementations for various endpoints such as `/dagit_info`, `/graphql`, `/download`, and `/index_html`. The `build_routes` method constructs the routing configuration for the web server.

### Key Points:

1. **Class Structure**: The `DagitWebserver` class is designed to handle requests related to Dagit, including GraphQL queries, downloading logs, and serving static files.

2. **Endpoints**:
   - `/dagit_info`: Returns metadata about the Dagit server, including versions of Dagit, Dagster, and GraphQL.
   - `/graphql`: Handles both GET and POST requests for GraphQL queries.
   - `/download/{run_id}/{step_key}/{file_type}`: Downloads compute logs for a specific step in a run.
   - `/download_debug/{run_id}`: Downloads a debug payload for a run.
   - `/index_html`: Serves the main HTML page for Dagit, handling relative paths and nonce placeholders.

3. **Static Files**: The server serves static files from the `webapp/build` directory, including those in the `static` and `vendor` directories.

4. **Routing**: The `build_routes` method constructs a list of routes that define how the server should respond to different URLs.

5. **Debug Mode**: The `default_app` function creates an instance of the web server with optional debug mode enabled.

6. **Static Resource Paths**: The `relative_path` method constructs the absolute path to resources within the `webapp/build` directory.

### Review:

- **Code Organization**: The code is well-organized into classes and methods, making it easy to understand and maintain.
- **Error Handling**: The `download_compute_logs_endpoint` and `download_debug_file_endpoint` handle cases where files do not exist gracefully.
- **Security**: The use of `gzip` for compressing debug payloads ensures efficient data transfer.
- **Flexibility**: The server can be easily extended with additional endpoints or configurations as needed.

Overall, this implementation provides a robust and functional Dagit web server using Starlette, which is suitable for production environments."
97,"@@ -97,14 +97,11 @@ lbann_callback* construct_callback(lbann_comm* comm,
   //////////////////////////////////////////////////////////////
 
   if (proto_cb.has_ltfb()) {
-    auto&& m = parse_list<>(proto_cb.ltfb().eval_metrics());
-    auto&& w = parse_list<>(proto_cb.ltfb().weights_tosend());
-    std::unordered_set<std::string> metric_names(m.begin(), m.end());
-    std::unordered_set<std::string> weight_names(w.begin(), w.end());
-    return new lbann_callback_ltfb(proto_cb.ltfb().round_size(),
-                                   metric_names,
-                                   proto_cb.ltfb().increasing_metric_mode(),
-                                   weight_names,
+    const auto& params = proto_cb.ltfb();
+    return new lbann_callback_ltfb(params.batch_interval(),
+                                   params.metric(),
+                                   parse_set<std::string>(params.weights()),
+                                   params.low_score_wins(),
                                    summarizer);
   }
   /// @todo","////////////////////////////////////////////////////////////////////////////////
// Copyright (c) 2014-2016, Lawrence Livermore National Security, LLC.
// Produced at the Lawrence Livermore National Laboratory.
// Written by the LBANN Research Team (B. Van Essen, et al.) listed in
// the CONTRIBUTORS file. <lbann-dev@llnl.gov>
//
// LLNL-CODE-697807.
// All rights reserved.
//
// This file is part of LBANN: Livermore Big Artificial Neural Network
// Toolkit. For details, see http://software.llnl.gov/LBANN or
// https://github.com/LLNL/LBANN.
//
// Licensed under the Apache License, Version 2.0 (the ""Licensee""); you
// may not use this file except in compliance with the License.  You may
// obtain a copy of the License at:
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an ""AS IS"" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
// implied. See the License for the specific language governing
// permissions and limitations under the license.
////////////////////////////////////////////////////////////////////////////////

#include ""lbann/proto/factories.hpp""
#include ""lbann/utils/peek_map.hpp""

namespace lbann {
namespace proto {

namespace {

/** Select entries from a list based on names.
 *  Any entry in 'list' with a name found in 'names' (interpreted as a
 *  space-separated list) is added to the output list.
 */
template <typename T>
std::vector<T*> select_from_list(std::string names,
                                        std::vector<T*> list) {
  std::vector<T*> selected;
  for (const auto& name : parse_list<std::string>(names)) {
    for (auto&& t : list) {
      if (name == t->get_name()) {
        selected.push_back(t);
      }
    }
  }
  return selected;
}


} // namespace

lbann_callback* construct_callback(lbann_comm* comm,
                                   const lbann_data::Callback& proto_cb,
                                   const std::map<execution_mode, generic_data_reader*>& data_readers,
                                   std::vector<Layer*> layer_list,
                                   std::vector<weights*> weights_list,
                                   lbann_summary* summarizer) {
  std::stringstream err;

  //////////////////////////////////////////////////////////////
  // Display information
  //////////////////////////////////////////////////////////////

  if (proto_cb.has_print()) {
    const auto& interval = proto_cb.print().interval();
    return new lbann_callback_print(interval);
  }
  if (proto_cb.has_timer()) {
    return new lbann_callback_timer(summarizer);
  }
  if (proto_cb.has_disp_io_stats()) {
    const auto& params = proto_cb.disp_io_stats();
    auto&& l = select_from_list<Layer>(params.layers(),
                                                     layer_list);
    std::unordered_set<Layer*> selected_layers(l.begin(), l.end());
    return new lbann_callback_io(selected_layers);
  }
  if (proto_cb.has_save_images()) {
    const auto& params = proto_cb.save_images();
    return new lbann_callback_save_images(parse_list<>(params.layers()),
                                          params.image_format(),
                                          params.image_prefix());
  }
  if (proto_cb.has_confusion_matrix()) {
    const auto& params = proto_cb.confusion_matrix();
    return new lbann_callback_confusion_matrix(params.prediction(),
                                               params.label(),
                                               params.prefix());
  }

  //////////////////////////////////////////////////////////////
  // Inter-model communication
    const auto& params = proto_cb.ltfb();
    return new lbann_callback_ltfb(params.batch_interval(),
                                   params.metric(),
                                   parse_set<std::string>(params.weights()),
                                   params.low_score_wins(),
  //////////////////////////////////////////////////////////////

  if (proto_cb.has_ltfb()) {
    auto&& m = parse_list<>(proto_cb.ltfb().eval_metrics());
    auto&& w = parse_list<>(proto_cb.ltfb().weights_tosend());
    std::unordered_set<std::string> metric_names(m.begin(), m.end());
    std::unordered_set<std::string> weight_names(w.begin(), w.end());
    return new lbann_callback_ltfb(proto_cb.ltfb().round_size(),
                                   metric_names,
                                   proto_cb.ltfb().increasing_metric_mode(),
                                   weight_names,
                                   summarizer);
  }
  /// @todo
  if (proto_cb.has_imcomm()) {
    const auto& params = proto_cb.imcomm();
    const auto& type_str = params.intermodel_comm_method();
    lbann_callback_imcomm::comm_type type = lbann_callback_imcomm::comm_type::NONE;
    if (type_str == ""none"") {
      type = lbann_callback_imcomm::comm_type::NONE;
    } else if (type_str == ""normal"") {
      type = lbann_callback_imcomm::comm_type::NORMAL;
    } else if (type_str == ""onebit_quantization"") {
      type = lbann_callback_imcomm::comm_type::ONEBIT_QUANTIZATION;
    } else if (type_str == ""thresh_quantization"") {
      type = lbann_callback_imcomm::comm_type::THRESH_QUANTIZATION;
    } else if (type_str == ""adaptive_quantization"") {
      type = lbann_callback_imcomm::comm_type::ADAPTIVE_QUANTIZATION;
    } else {
      err << ""invalid inter-model communication type ("" << type_str << "")"";
      LBANN_ERROR(err.str());
    }
    std::unordered_set<weights*> selected_weights; /// @todo Initialize weights
    return new lbann_callback_imcomm(type, selected_weights, summarizer);
  }

  //////////////////////////////////////////////////////////////
  // Learning rate schedules
  //////////////////////////////////////////////////////////////

  if (proto_cb.has_step_learning_rate()) {
    const auto& params = proto_cb.step_learning_rate();
    auto&& w = select_from_list<weights>(params.weights(),
                                                        weights_list);
    std::unordered_set<weights*> selected_weights(w.begin(), w.end());
    return new lbann_callback_step_learning_rate(params.step(),
                                                 params.amt(),
                                                 selected_weights);
  }
  if (proto_cb.has_adaptive_learning_rate()) {
    const auto& params = proto_cb.adaptive_learning_rate();
    auto&& w = select_from_list<weights>(params.weights(),
                                                        weights_list);
    std::unordered_set<weights*> selected_weights(w.begin(), w.end());
    return new lbann_callback_adaptive_learning_rate(params.patience(),
                                                     params.amt(),
                                                     selected_weights);
  }
  if (proto_cb.has_drop_fixed_learning_rate()) {
    const auto& params = proto_cb.drop_fixed_learning_rate();
    std::vector<int64_t> drop_epochs;
    for (int i = 0; i < params.drop_epoch_size(); ++i) {
      drop_epochs.push_back(params.drop_epoch(i));
    }
    auto&& w = select_from_list<weights>(params.weights(),
                                                        weights_list);
    std::unordered_set<weights*> selected_weights(w.begin(), w.end());
    return new lbann_callback_drop_fixed_learning_rate(drop_epochs,
                                                       params.amt(),
                                                       selected_weights);
  }
  if (proto_cb.has_linear_growth_learning_rate()) {
    const auto& params = proto_cb.linear_growth_learning_rate();
    auto&& w = select_from_list<weights>(params.weights(),
                                                        weights_list);
    std::unordered_set<weights*> selected_weights(w.begin(), w.end());
    return new lbann_callback_linear_growth_learning_rate(params.target(),
                                                          params.num_epochs(),
                                                          params.delay(),
                                                          selected_weights);
  }
  if (proto_cb.has_optimizerwise_adaptive_learning_rate()) {
    const auto& params = proto_cb.optimizerwise_adaptive_learning_rate();
    auto&& w = select_from_list<weights>(params.weights(),
                                                        weights_list);
    std::unordered_set<weights*> selected_weights(w.begin(), w.end());
    return new lbann_callback_optimizerwise_adaptive_learning_rate(params.scale(),
                                                                   selected_weights);
  }
  if (proto_cb.has_poly_learning_rate()) {
    const auto& params = proto_cb.poly_learning_rate();
    auto&& w = select_from_list<weights>(params.weights(),
                                                        weights_list);
    std::unordered_set<weights*> selected_weights(w.begin(), w.end());
    return new lbann_callback_poly_learning_rate(params.power(),
                                                 params.num_epochs(),
                                                 params.max_iter(),
                                                 selected_weights);
  }

  //////////////////////////////////////////////////////////////
  // Mini-batch schedules
  //////////////////////////////////////////////////////////////

  if (proto_cb.has_step_minibatch()) {
    const auto& params = proto_cb.step_minibatch();
    return new lbann_callback_step_minibatch(params.starting_mbsize(),
                                             params.step(),
                                             params.ramp_time());
  }
  if (proto_cb.has_minibatch_schedule()) {
    const auto& params = proto_cb.minibatch_schedule();
    std::vector<lbann_callback_minibatch_schedule::minibatch_step> steps;
    for (int i = 0; i < params.step_size(); ++i) {
      const auto& proto_step = params.step(i);
      steps.emplace_back(proto_step.epoch(),
                         proto_step.mbsize(),
                         proto_step.lr(),
                         proto_step.ramp_time());
    }
    return new lbann_callback_minibatch_schedule(params.starting_mbsize(),
                                                 steps);
  }

  //////////////////////////////////////////////////////////////
  // Checkpointing and exporting
  //////////////////////////////////////////////////////////////

  if (proto_cb.has_checkpoint()) {
    const auto& params = proto_cb.checkpoint();
    return new lbann_callback_checkpoint(params.checkpoint_dir(),
                                         params.checkpoint_epochs(),
                                         params.checkpoint_steps(),
                                         params.checkpoint_secs(),
                                         params.per_rank_dir(),
                                         params.ckpt_dist_epochs(),
                                         params.ckpt_dist_steps());
  }
  if (proto_cb.has_save_model()) {
    const auto& params = proto_cb.save_model();
    if(params.extension().size() != 0) {
      return new lbann_callback_save_model(params.dir(),
                                           params.disable_save_after_training(),
                                           params.extension());
    }else {
      return new lbann_callback_save_model(params.dir(),
                                           params.disable_save_after_training());
    }
  }

  //////////////////////////////////////////////////////////////
  // Weight exchange/replace
  //////////////////////////////////////////////////////////////

  if (proto_cb.has_replace_weights()) {
    const auto& params = proto_cb.replace_weights();
    auto&& src_layers = select_from_list<Layer>(params.source_layers(),
                                                     layer_list);
    auto&& dst_layers = select_from_list<Layer>(params.destination_layers(),
                                                     layer_list);
    return new lbann_callback_replace_weights(src_layers,dst_layers,params.batch_interval());
  }

  //////////////////////////////////////////////////////////////
  // Profiling
  //////////////////////////////////////////////////////////////

  if (proto_cb.has_summary()) {
    const auto& params = proto_cb.summary();
    return new lbann_callback_summary(summarizer,
                                      params.batch_interval(),
                                      params.mat_interval());
  }
  if (proto_cb.has_profiler()) {
    return new lbann_callback_profiler(proto_cb.profiler().sync());
  }
  if (proto_cb.has_sync_layers()) {
    const auto& params = proto_cb.sync_layers();
    return new lbann_callback_sync_layers(params.sync_gpus(),
                                          params.sync_mpi(),
                                          params.only_input());
  }
  if (proto_cb.has_sync_selected()) {
    const auto& params = proto_cb.sync_selected();
    const int num_layers = params.layer_to_sync_size();
    if (num_layers == 0) {
      throw lbann_exception(""sync_selected requires at least a layer to synchronize."");
    }

    using layers_t = lbann_callback_sync_selected::layers_t;
    using prop_t = lbann_callback_sync_selected::prop_t;

    layers_t selected_layers;
    selected_layers.reserve(num_layers);

    for (int i = 0; i < num_layers; ++i) {
      const auto& layer_to_sync = params.layer_to_sync(i);
      selected_layers.emplace(layer_to_sync.name(),
                              static_cast<prop_t>(layer_to_sync.prop()));
    }

    lbann_callback_sync_selected* cb_ptr
      = new lbann_callback_sync_selected(selected_layers,
                                        params.async_gpus(),
                                        params.async_mpi());

    #ifdef LBANN_NVPROF
    const auto& cp_setup = params.cuda_profiler_setup();
    if (cp_setup.no_init()) {
      lbann_callback_sync_selected::turn_off_init_cuda_profiler();
    } else {
      cb_ptr->init_cuda_profiler(cp_setup.config_file(),
                                 cp_setup.output_dir(),
                                 cp_setup.output_mode(),
                                 comm);
    }
    #endif // LBANN_NVPROF
    return cb_ptr;
  }

  //////////////////////////////////////////////////////////////
  // Debugging
  //////////////////////////////////////////////////////////////

  if (proto_cb.has_debug()) {
    const auto& params = proto_cb.debug();
    const auto& modes = parse_set<execution_mode>(params.phase());
    return new lbann_callback_debug(modes, summarizer);
  }
  if (proto_cb.has_debug_io()) {
    const auto& params = proto_cb.debug_io();
    const auto& phase = params.phase();
    const auto& lvl = params.lvl();
    if (phase == ""train"" || phase == ""training"") {
      return new lbann_callback_debug_io(execution_mode::training, lvl);
    } else if (phase == ""validate"" || phase == ""validation"") {
      return new lbann_callback_debug_io(execution_mode::validation, lvl);
    } else if (phase == ""test"" || phase == ""testing"") {
      return new lbann_callback_debug_io(execution_mode::testing, lvl);
    } else {
      return new lbann_callback_debug_io();
    }
  }
  if (proto_cb.has_dump_weights()) {
    const auto& params = proto_cb.dump_weights();
    return new lbann_callback_dump_weights(params.basename());
  }
  if (proto_cb.has_dump_outputs()) {
    const auto& params = proto_cb.dump_outputs();
    const auto& layer_names = parse_set<>(params.layers());
    const auto& modes = parse_set<execution_mode>(params.execution_modes());
    return new lbann_callback_dump_outputs(layer_names,
                                           modes,
                                           params.batch_interval(),
                                           params.prefix(),
                                           params.format());
  }
  if (proto_cb.has_dump_error_signals()) {
    const auto& params = proto_cb.dump_error_signals();
    return new lbann_callback_dump_error_signals(params.basename());
  }
  if (proto_cb.has_dump_gradients()) {
    const auto& params = proto_cb.dump_gradients();
    return new lbann_callback_dump_gradients(params.basename(),
                                             params.interval());
  }
  if (proto_cb.has_dump_mb_indices()) {
    const auto& params = proto_cb.dump_mb_indices();
    return new lbann_callback_dump_minibatch_sample_indices(params.basename(),
                                                            params.interval());
  }
  if (proto_cb.has_check_dataset()) {
    return new lbann_callback_check_dataset();
  }
  if (proto_cb.has_check_small()) {
    return new lbann_callback_checksmall();
  }
  if (proto_cb.has_check_nan()) {
    return new lbann_callback_checknan();
  }
  if (proto_cb.has_hang()) {
    const auto& rank_to_hang = proto_cb.hang().rank();
    if (comm->am_world_master()) {
      if (rank_to_hang == -1) {
        std::cout << ""*** HANGING EVERY RANK IN HANG CALLBACK ***""
                  << std::endl;
      } else {
        std::cout << ""*** HANGING RANK "" << rank_to_hang
                  << "" IN HANG CALLBACK ***"" << std::endl;
      }
    }
    return new lbann_callback_hang(rank_to_hang);
  }
  if (proto_cb.has_check_gradients()) {
    const auto& params = proto_cb.check_gradients();
    return new lbann_callback_check_gradients(params.step_size(),
                                              params.verbose(),
                                              params.error_on_failure());
  }
  if (proto_cb.has_check_metric()) {
    const auto& params = proto_cb.check_metric();
    const auto& modes = parse_set<execution_mode>(params.execution_modes());
    return new lbann_callback_check_metric(params.metric(),
                                           modes,
                                           params.lower_bound(),
                                           params.upper_bound(),
                                           params.error_on_failure());
  }

  //////////////////////////////////////////////////////////////
  // GPU memory profiling
  //////////////////////////////////////////////////////////////
  if (proto_cb.has_gpu_memory_usage()) {
    return new lbann_callback_gpu_memory_usage();
  }

  return nullptr;
}

lbann_summary* construct_summarizer(lbann_comm* comm,
                                    const lbann_data::Model& m) {
  lbann_summary *summary = nullptr;
  bool master = comm->am_world_master();
  int size = m.callback_size();
  for (int j=0; j<size; j++) {
    const lbann_data::Callback& callback = m.callback(j);
    if (callback.has_summary()) {
      const lbann_data::CallbackSummary& c = callback.summary();
      if (master) {
        std::cout << ""constructing summarizer with dir: "" << c.dir() << std::endl;
      }

      //check to see if directory exists
      struct stat sb;
      if (! ( stat(c.dir().c_str(), &sb) == 0 && S_ISDIR(sb.st_mode) )) {
        if (master) {
          throw lbann_exception(
            std::string {} + __FILE__ + "" "" + std::to_string(__LINE__) + "" :: "" +
            ""summary directory "" + c.dir() + "" does not exist"");
        }
      }
      summary = new lbann_summary(c.dir(), comm);
    }
  }
  return summary;
}

} // namespace proto
} // namespace lbann
","We are not using multiple metrics now but how would this support space-delimited metric (e.g., categorical accuracy) and multiple metrics with the same name as metric specified one of the metrics for training/inference ?","The suggested code change introduces a helper function `select_from_list` to filter a list of objects based on their names. This makes the code more modular and easier to read. The rest of the code remains largely unchanged, focusing on constructing different types of callbacks based on the configuration provided in the Protocol Buffers (protobuf) format."
98,"@@ -411,8 +411,8 @@ class UserController < ApplicationController
     if @new_friend
       if request.post?
         friend = Friend.new
-        friend.user_id = current_user.id
-        friend.friend_user_id = @new_friend.id
+        friend.befriender = current_user
+        friend.befriendee = @new_friend
         if current_user.is_friends_with?(@new_friend)
           flash[:warning] = t ""user.make_friend.already_a_friend"", :name => @new_friend.display_name
         elsif friend.save","class UserController < ApplicationController
  layout ""site"", :except => [:api_details]

  skip_before_action :verify_authenticity_token, :only => [:api_read, :api_details, :api_gpx_files, :auth_success]
  before_action :disable_terms_redirect, :only => [:terms, :save, :logout, :api_details]
  before_action :authorize, :only => [:api_details, :api_gpx_files]
  before_action :authorize_web, :except => [:api_read, :api_details, :api_gpx_files]
  before_action :set_locale, :except => [:api_read, :api_details, :api_gpx_files]
  before_action :require_user, :only => [:account, :go_public, :make_friend, :remove_friend]
  before_action :require_self, :only => [:account]
  before_action :check_database_readable, :except => [:login, :api_read, :api_details, :api_gpx_files]
  before_action :check_database_writable, :only => [:new, :account, :confirm, :confirm_email, :lost_password, :reset_password, :go_public, :make_friend, :remove_friend]
  before_action :check_api_readable, :only => [:api_read, :api_details, :api_gpx_files]
  before_action :require_allow_read_prefs, :only => [:api_details]
  before_action :require_allow_read_gpx, :only => [:api_gpx_files]
  before_action :require_cookies, :only => [:new, :login, :confirm]
  before_action :require_administrator, :only => [:set_status, :delete, :list]
  around_action :api_call_handle_error, :only => [:api_read, :api_details, :api_gpx_files]
  before_action :lookup_user_by_id, :only => [:api_read]
  before_action :lookup_user_by_name, :only => [:set_status, :delete]

  def terms
    @legale = params[:legale] || OSM.ip_to_country(request.remote_ip) || DEFAULT_LEGALE
    @text = OSM.legal_text_for_country(@legale)

    if request.xhr?
      render :partial => ""terms""
    else
      @title = t ""user.terms.title""

      if current_user && current_user.terms_agreed?
        # Already agreed to terms, so just show settings
        redirect_to :action => :account, :display_name => current_user.display_name
      elsif current_user.nil? && session[:new_user].nil?
        redirect_to :action => :login, :referer => request.fullpath
      end
    end
  end

  def save
    @title = t ""user.new.title""

    if params[:decline]
      if current_user
        current_user.terms_seen = true

        if current_user.save
          flash[:notice] = t(""user.new.terms declined"", :url => t(""user.new.terms declined url"")).html_safe
        end

        if params[:referer]
          redirect_to params[:referer]
        else
          redirect_to :action => :account, :display_name => current_user.display_name
        end
      else
        redirect_to t(""user.terms.declined"")
      end
    elsif current_user
      unless current_user.terms_agreed?
        current_user.consider_pd = params[:user][:consider_pd]
        current_user.terms_agreed = Time.now.getutc
        current_user.terms_seen = true

        flash[:notice] = t ""user.new.terms accepted"" if current_user.save
      end

      if params[:referer]
        redirect_to params[:referer]
      else
        redirect_to :action => :account, :display_name => current_user.display_name
      end
    else
      self.current_user = session.delete(:new_user)

      if check_signup_allowed(current_user.email)
        current_user.data_public = true
        current_user.description = """" if current_user.description.nil?
        current_user.creation_ip = request.remote_ip
        current_user.languages = http_accept_language.user_preferred_languages
        current_user.terms_agreed = Time.now.getutc
        current_user.terms_seen = true

        if current_user.auth_uid.blank?
          current_user.auth_provider = nil
          current_user.auth_uid = nil
        end

        if current_user.save
          flash[:piwik_goal] = PIWIK[""goals""][""signup""] if defined?(PIWIK)

          referer = welcome_path

          begin
            uri = URI(session[:referer])
            %r{map=(.*)/(.*)/(.*)}.match(uri.fragment) do |m|
              editor = Rack::Utils.parse_query(uri.query).slice(""editor"")
              referer = welcome_path({ ""zoom"" => m[1],
                                       ""lat"" => m[2],
                                       ""lon"" => m[3] }.merge(editor))
            end
          rescue StandardError
            # Use default
          end

          if current_user.status == ""active""
            session[:referer] = referer
            successful_login(current_user)
          else
            session[:token] = current_user.tokens.create.token
            Notifier.signup_confirm(current_user, current_user.tokens.create(:referer => referer)).deliver_now
            redirect_to :action => ""confirm"", :display_name => current_user.display_name
          end
        else
          render :action => ""new"", :referer => params[:referer]
        end
      end
    end
  end

  def account
    @title = t ""user.account.title""
    @tokens = current_user.oauth_tokens.authorized

    if params[:user] && params[:user][:display_name] && params[:user][:description]
      if params[:user][:auth_provider].blank? ||
         (params[:user][:auth_provider] == current_user.auth_provider &&
          params[:user][:auth_uid] == current_user.auth_uid)
        update_user(current_user, params)
      else
        session[:new_user_settings] = params
        redirect_to auth_url(params[:user][:auth_provider], params[:user][:auth_uid])
      end
    elsif errors = session.delete(:user_errors)
      errors.each do |attribute, error|
        current_user.errors.add(attribute, error)
      end
    end
  end

  def go_public
    current_user.data_public = true
    current_user.save
    flash[:notice] = t ""user.go_public.flash success""
    redirect_to :action => ""account"", :display_name => current_user.display_name
  end

  def lost_password
    @title = t ""user.lost_password.title""

    if params[:user] && params[:user][:email]
      user = User.visible.find_by(:email => params[:user][:email])

      if user.nil?
        users = User.visible.where(""LOWER(email) = LOWER(?)"", params[:user][:email])

        user = users.first if users.count == 1
      end

      if user
        token = user.tokens.create
        Notifier.lost_password(user, token).deliver_now
        flash[:notice] = t ""user.lost_password.notice email on way""
        redirect_to :action => ""login""
      else
        flash.now[:error] = t ""user.lost_password.notice email cannot find""
      end
    end
  end

  def reset_password
    @title = t ""user.reset_password.title""

    if params[:token]
      token = UserToken.find_by(:token => params[:token])

      if token
        self.current_user = token.user

        if params[:user]
          current_user.pass_crypt = params[:user][:pass_crypt]
          current_user.pass_crypt_confirmation = params[:user][:pass_crypt_confirmation]
          current_user.status = ""active"" if current_user.status == ""pending""
          current_user.email_valid = true

          if current_user.save
            token.destroy
            flash[:notice] = t ""user.reset_password.flash changed""
            successful_login(current_user)
          end
        end
      else
        flash[:error] = t ""user.reset_password.flash token bad""
        redirect_to :action => ""lost_password""
      end
    else
      head :bad_request
    end
  end

  def new
    @title = t ""user.new.title""
    @referer = params[:referer] || session[:referer]

    if current_user
      # The user is logged in already, so don't show them the signup
      # page, instead send them to the home page
      if @referer
        redirect_to @referer
      else
        redirect_to :controller => ""site"", :action => ""index""
      end
    elsif params.key?(:auth_provider) && params.key?(:auth_uid)
      self.current_user = User.new(:email => params[:email],
                                   :email_confirmation => params[:email],
                                   :display_name => params[:nickname],
                                   :auth_provider => params[:auth_provider],
                                   :auth_uid => params[:auth_uid])

      flash.now[:notice] = render_to_string :partial => ""auth_association""
    else
      check_signup_allowed

      self.current_user = User.new
    end
  end

  def create
    self.current_user = User.new(user_params)

    if check_signup_allowed(current_user.email)
      session[:referer] = params[:referer]

      current_user.status = ""pending""

      if current_user.auth_provider.present? && current_user.pass_crypt.empty?
        # We are creating an account with external authentication and
        # no password was specified so create a random one
        current_user.pass_crypt = SecureRandom.base64(16)
        current_user.pass_crypt_confirmation = current_user.pass_crypt
      end

      if current_user.invalid?
        # Something is wrong with a new user, so rerender the form
        render :action => ""new""
      elsif current_user.auth_provider.present?
        # Verify external authenticator before moving on
        session[:new_user] = current_user
        redirect_to auth_url(current_user.auth_provider, current_user.auth_uid)
      else
        # Save the user record
        session[:new_user] = current_user
        redirect_to :action => :terms
      end
    end
  end

  def login
    session[:referer] = params[:referer] if params[:referer]

    if params[:username].present? && params[:password].present?
      session[:remember_me] ||= params[:remember_me]
      password_authentication(params[:username], params[:password])
    elsif params[:openid_url].present?
      session[:remember_me] ||= params[:remember_me_openid]
      redirect_to auth_url(""openid"", params[:openid_url], params[:referer])
    end
  end

  def logout
    @title = t ""user.logout.title""

    if params[:session] == session.id
      if session[:token]
        token = UserToken.find_by(:token => session[:token])
        token.destroy if token
        session.delete(:token)
      end
      session.delete(:user)
      session_expires_automatically
      if params[:referer]
        redirect_to params[:referer]
      else
        redirect_to :controller => ""site"", :action => ""index""
      end
    end
  end

  def confirm
    if request.post?
      token = UserToken.find_by(:token => params[:confirm_string])
      if token && token.user.active?
        flash[:error] = t(""user.confirm.already active"")
        redirect_to :action => ""login""
      elsif !token || token.expired?
        flash[:error] = t(""user.confirm.unknown token"")
        redirect_to :action => ""confirm""
      else
        user = token.user
        user.status = ""active""
        user.email_valid = true
        flash[:notice] = gravatar_status_message(user) if gravatar_enable(user)
        user.save!
        referer = token.referer
        token.destroy

        if session[:token]
          token = UserToken.find_by(:token => session[:token])
          session.delete(:token)
        else
          token = nil
        end

        if token.nil? || token.user != user
          flash[:notice] = t(""user.confirm.success"")
          redirect_to :action => :login, :referer => referer
        else
          token.destroy

          session[:user] = user.id

          redirect_to referer || welcome_path
        end
      end
    else
      user = User.find_by(:display_name => params[:display_name])

      redirect_to root_path if user.nil? || user.active?
    end
  end

  def confirm_resend
    user = User.find_by(:display_name => params[:display_name])
    token = UserToken.find_by(:token => session[:token])

    if user.nil? || token.nil? || token.user != user
      flash[:error] = t ""user.confirm_resend.failure"", :name => params[:display_name]
    else
      Notifier.signup_confirm(user, user.tokens.create).deliver_now
      flash[:notice] = t(""user.confirm_resend.success"", :email => user.email, :sender => SUPPORT_EMAIL).html_safe
    end

    redirect_to :action => ""login""
  end

  def confirm_email
    if request.post?
      token = UserToken.find_by(:token => params[:confirm_string])
      if token && token.user.new_email?
        self.current_user = token.user
        current_user.email = current_user.new_email
        current_user.new_email = nil
        current_user.email_valid = true
        gravatar_enabled = gravatar_enable(current_user)
        if current_user.save
          flash[:notice] = if gravatar_enabled
                             t(""user.confirm_email.success"") + "" "" + gravatar_status_message(current_user)
                           else
                             t(""user.confirm_email.success"")
                           end
        else
          flash[:errors] = current_user.errors
        end
        token.destroy
        session[:user] = current_user.id
        redirect_to :action => ""account"", :display_name => current_user.display_name
      elsif token
        flash[:error] = t ""user.confirm_email.failure""
        redirect_to :action => ""account"", :display_name => token.user.display_name
      else
        flash[:error] = t ""user.confirm_email.unknown_token""
      end
    end
  end

  def api_read
    if @this_user.visible?
      render :action => :api_read, :content_type => ""text/xml""
    else
      head :gone
    end
  end

  def api_details
    @this_user = current_user
    render :action => :api_read, :content_type => ""text/xml""
  end

  def api_gpx_files
    doc = OSM::API.new.get_xml_doc
    current_user.traces.reload.each do |trace|
      doc.root << trace.to_xml_node
    end
    render :xml => doc.to_s
  end

  def view
    @this_user = User.find_by(:display_name => params[:display_name])

    if @this_user &&
       (@this_user.visible? || (current_user && current_user.administrator?))
      @title = @this_user.display_name
    else
      render_unknown_user params[:display_name]
    end
  end

  def make_friend
    @new_friend = User.find_by(:display_name => params[:display_name])

        friend.befriender = current_user
        friend.befriendee = @new_friend
    if @new_friend
      if request.post?
        friend = Friend.new
        friend.user_id = current_user.id
        friend.friend_user_id = @new_friend.id
        if current_user.is_friends_with?(@new_friend)
          flash[:warning] = t ""user.make_friend.already_a_friend"", :name => @new_friend.display_name
        elsif friend.save
          flash[:notice] = t ""user.make_friend.success"", :name => @new_friend.display_name
          Notifier.friend_notification(friend).deliver_now
        else
          friend.add_error(t(""user.make_friend.failed"", :name => @new_friend.display_name))
        end

        if params[:referer]
          redirect_to params[:referer]
        else
          redirect_to :action => ""view""
        end
      end
    else
      render_unknown_user params[:display_name]
    end
  end

  def remove_friend
    @friend = User.find_by(:display_name => params[:display_name])

    if @friend
      if request.post?
        if current_user.is_friends_with?(@friend)
          Friend.where(:user_id => current_user.id, :friend_user_id => @friend.id).delete_all
          flash[:notice] = t ""user.remove_friend.success"", :name => @friend.display_name
        else
          flash[:error] = t ""user.remove_friend.not_a_friend"", :name => @friend.display_name
        end

        if params[:referer]
          redirect_to params[:referer]
        else
          redirect_to :action => ""view""
        end
      end
    else
      render_unknown_user params[:display_name]
    end
  end

  ##
  # sets a user's status
  def set_status
    @this_user.status = params[:status]
    @this_user.save
    redirect_to :action => ""view"", :display_name => params[:display_name]
  end

  ##
  # delete a user, marking them as deleted and removing personal data
  def delete
    @this_user.delete
    redirect_to :action => ""view"", :display_name => params[:display_name]
  end

  ##
  # display a list of users matching specified criteria
  def list
    if request.post?
      ids = params[:user].keys.collect(&:to_i)

      User.where(:id => ids).update_all(:status => ""confirmed"") if params[:confirm]
      User.where(:id => ids).update_all(:status => ""deleted"") if params[:hide]

      redirect_to url_for(:status => params[:status], :ip => params[:ip], :page => params[:page])
    else
      @params = params.permit(:status, :ip)

      conditions = {}
      conditions[:status] = @params[:status] if @params[:status]
      conditions[:creation_ip] = @params[:ip] if @params[:ip]

      @user_pages, @users = paginate(:users,
                                     :conditions => conditions,
                                     :order => :id,
                                     :per_page => 50)
    end
  end

  ##
  # omniauth success callback
  def auth_success
    auth_info = request.env[""omniauth.auth""]

    provider = auth_info[:provider]
    uid = auth_info[:uid]
    name = auth_info[:info][:name]
    email = auth_info[:info][:email]

    case provider
    when ""openid""
      email_verified = uid.match(%r{https://www.google.com/accounts/o8/id?(.*)}) ||
                       uid.match(%r{https://me.yahoo.com/(.*)})
    when ""google"", ""facebook""
      email_verified = true
    else
      email_verified = false
    end

    if settings = session.delete(:new_user_settings)
      current_user.auth_provider = provider
      current_user.auth_uid = uid

      update_user(current_user, settings)

      session[:user_errors] = current_user.errors.as_json

      redirect_to :action => ""account"", :display_name => current_user.display_name
    elsif session[:new_user]
      session[:new_user].auth_provider = provider
      session[:new_user].auth_uid = uid

      if email_verified && email == session[:new_user].email
        session[:new_user].status = ""active""
      end

      redirect_to :action => ""terms""
    else
      user = User.find_by(:auth_provider => provider, :auth_uid => uid)

      if user.nil? && provider == ""google""
        openid_url = auth_info[:extra][:id_info][""openid_id""]
        user = User.find_by(:auth_provider => ""openid"", :auth_uid => openid_url) if openid_url
        user.update(:auth_provider => provider, :auth_uid => uid) if user
      end

      if user
        case user.status
        when ""pending"" then
          unconfirmed_login(user)
        when ""active"", ""confirmed"" then
          successful_login(user, request.env[""omniauth.params""][""referer""])
        when ""suspended"" then
          failed_login t(""user.login.account is suspended"", :webmaster => ""mailto:#{SUPPORT_EMAIL}"").html_safe
        else
          failed_login t(""user.login.auth failure"")
        end
      else
        redirect_to :action => ""new"", :nickname => name, :email => email,
                    :auth_provider => provider, :auth_uid => uid
      end
    end
  end

  ##
  # omniauth failure callback
  def auth_failure
    flash[:error] = t(""user.auth_failure."" + params[:message])
    redirect_to params[:origin] || login_url
  end

  private

  ##
  # handle password authentication
  def password_authentication(username, password)
    if user = User.authenticate(:username => username, :password => password)
      successful_login(user)
    elsif user = User.authenticate(:username => username, :password => password, :pending => true)
      unconfirmed_login(user)
    elsif User.authenticate(:username => username, :password => password, :suspended => true)
      failed_login t(""user.login.account is suspended"", :webmaster => ""mailto:#{SUPPORT_EMAIL}"").html_safe, username
    else
      failed_login t(""user.login.auth failure""), username
    end
  end

  ##
  # return the URL to use for authentication
  def auth_url(provider, uid, referer = nil)
    params = { :provider => provider }

    params[:openid_url] = openid_expand_url(uid) if provider == ""openid""

    if referer.nil?
      params[:origin] = request.path
    else
      params[:origin] = request.path + ""?referer="" + CGI.escape(referer)
      params[:referer] = referer
    end

    auth_path(params)
  end

  ##
  # special case some common OpenID providers by applying heuristics to
  # try and come up with the correct URL based on what the user entered
  def openid_expand_url(openid_url)
    if openid_url.nil?
      nil
    elsif openid_url.match(%r{(.*)gmail.com(/?)$}) || openid_url.match(%r{(.*)googlemail.com(/?)$})
      # Special case gmail.com as it is potentially a popular OpenID
      # provider and, unlike yahoo.com, where it works automatically, Google
      # have hidden their OpenID endpoint somewhere obscure this making it
      # somewhat less user friendly.
      ""https://www.google.com/accounts/o8/id""
    else
      openid_url
    end
  end

  ##
  # process a successful login
  def successful_login(user, referer = nil)
    session[:user] = user.id
    session_expires_after 28.days if session[:remember_me]

    target = referer || session[:referer] || url_for(:controller => :site, :action => :index)

    # The user is logged in, so decide where to send them:
    #
    # - If they haven't seen the contributor terms, send them there.
    # - If they have a block on them, show them that.
    # - If they were referred to the login, send them back there.
    # - Otherwise, send them to the home page.
    if REQUIRE_TERMS_SEEN && !user.terms_seen
      redirect_to :action => :terms, :referer => target
    elsif user.blocked_on_view
      redirect_to user.blocked_on_view, :referer => target
    else
      redirect_to target
    end

    session.delete(:remember_me)
    session.delete(:referer)
  end

  ##
  # process a failed login
  def failed_login(message, username = nil)
    flash[:error] = message

    redirect_to :action => ""login"", :referer => session[:referer],
                :username => username, :remember_me => session[:remember_me]

    session.delete(:remember_me)
    session.delete(:referer)
  end

  ##
  #
  def unconfirmed_login(user)
    session[:token] = user.tokens.create.token

    redirect_to :action => ""confirm"", :display_name => user.display_name

    session.delete(:remember_me)
    session.delete(:referer)
  end

  ##
  # update a user's details
  def update_user(user, params)
    user.display_name = params[:user][:display_name]
    user.new_email = params[:user][:new_email]

    unless params[:user][:pass_crypt].empty? && params[:user][:pass_crypt_confirmation].empty?
      user.pass_crypt = params[:user][:pass_crypt]
      user.pass_crypt_confirmation = params[:user][:pass_crypt_confirmation]
    end

    if params[:user][:description] != user.description
      user.description = params[:user][:description]
      user.description_format = ""markdown""
    end

    user.languages = params[:user][:languages].split("","")

    case params[:image_action]
    when ""new"" then
      user.image = params[:user][:image]
      user.image_use_gravatar = false
    when ""delete"" then
      user.image = nil
      user.image_use_gravatar = false
    when ""gravatar"" then
      user.image = nil
      user.image_use_gravatar = true
    end

    user.home_lat = params[:user][:home_lat]
    user.home_lon = params[:user][:home_lon]

    user.preferred_editor = if params[:user][:preferred_editor] == ""default""
                              nil
                            else
                              params[:user][:preferred_editor]
                            end

    if params[:user][:auth_provider].nil? || params[:user][:auth_provider].blank?
      user.auth_provider = nil
      user.auth_uid = nil
    end

    if user.save
      set_locale

      if user.new_email.blank? || user.new_email == user.email
        flash.now[:notice] = t ""user.account.flash update success""
      else
        user.email = user.new_email

        if user.valid?
          flash.now[:notice] = t ""user.account.flash update success confirm needed""

          begin
            Notifier.email_confirm(user, user.tokens.create).deliver_now
          rescue StandardError
            # Ignore errors sending email
          end
        else
          current_user.errors.add(:new_email, current_user.errors[:email])
          current_user.errors.add(:email, [])
        end

        user.restore_email!
      end
    end
  end

  ##
  # require that the user is a administrator, or fill out a helpful error message
  # and return them to the user page.
  def require_administrator
    if current_user && !current_user.administrator?
      flash[:error] = t(""user.filter.not_an_administrator"")

      if params[:display_name]
        redirect_to :action => ""view"", :display_name => params[:display_name]
      else
        redirect_to :action => ""login"", :referer => request.fullpath
      end
    elsif !current_user
      redirect_to :action => ""login"", :referer => request.fullpath
    end
  end

  ##
  # require that the user in the URL is the logged in user
  def require_self
    head :forbidden if params[:display_name] != current_user.display_name
  end

  ##
  # ensure that there is a ""this_user"" instance variable
  def lookup_user_by_id
    @this_user = User.find(params[:id])
  end

  ##
  # ensure that there is a ""this_user"" instance variable
  def lookup_user_by_name
    @this_user = User.find_by(:display_name => params[:display_name])
  rescue ActiveRecord::RecordNotFound
    redirect_to :action => ""view"", :display_name => params[:display_name] unless @this_user
  end

  ##
  #
  def disable_terms_redirect
    # this is necessary otherwise going to the user terms page, when
    # having not agreed already would cause an infinite redirect loop.
    # it's .now so that this doesn't propagate to other pages.
    flash.now[:skip_terms] = true
  end

  ##
  # return permitted user parameters
  def user_params
    params.require(:user).permit(:email, :email_confirmation, :display_name,
                                 :auth_provider, :auth_uid,
                                 :pass_crypt, :pass_crypt_confirmation)
  end

  ##
  # check signup acls
  def check_signup_allowed(email = nil)
    domain = if email.nil?
               nil
             else
               email.split(""@"").last
             end

    if blocked = Acl.no_account_creation(request.remote_ip, domain)
      logger.info ""Blocked signup from #{request.remote_ip} for #{email}""

      render :action => ""blocked""
    end

    !blocked
  end

  ##
  # check if this user has a gravatar and set the user pref is true
  def gravatar_enable(user)
    # code from example https://en.gravatar.com/site/implement/images/ruby/
    return false if user.image.present?
    hash = Digest::MD5.hexdigest(user.email.downcase)
    url = ""https://www.gravatar.com/avatar/#{hash}?d=404"" # without d=404 we will always get an image back
    response = OSM.http_client.get(URI.parse(url))
    oldsetting = user.image_use_gravatar
    user.image_use_gravatar = response.success?
    oldsetting != user.image_use_gravatar
  end

  ##
  # display a message about th current status of the gravatar setting
  def gravatar_status_message(user)
    if user.image_use_gravatar
      t ""user.account.gravatar.enabled""
    else
      t ""user.account.gravatar.disabled""
    end
  end
end
",Imo here the old names were easier to understand.,"The provided code is a Rails controller for handling user-related actions in an OpenStreetMap application. Here's a concise review of the suggested code changes:

### Key Points:
- **Layout Configuration**: The `layout` method is used to specify the layout for all actions except `api_details`.
- **Action Skipping**: Actions like `api_details`, `api_read`, `api_gpx_files`, and `auth_success` have `skip_before_action` to bypass CSRF protection.
- **Authorization**: Actions like `api_details`, `api_gpx_files`, `account`, `go_public`, `make_friend`, `remove_friend`, `set_status`, `delete`, and `list` have `before_action` to enforce authorization checks.
- **Locale Setting**: The `set_locale` action is used to set the locale for the user.
- **User Requirements**: Actions like `require_user`, `require_self`, `check_database_readable`, `check_database_writable`, `check_api_readable`, `require_allow_read_prefs`, `require_allow_read_gpx`, `require_cookies`, and `require_administrator` enforce various user requirements.
- **API Handling**: The `around_action` `api_call_handle_error` is used to handle errors during API calls.
- **User Lookup**: Actions like `lookup_user_by_id` and `lookup_user_by_name` ensure that the `@this_user` instance variable is set correctly.

### Specific Changes:
- **Layout Configuration**: The `layout` method is used to specify the layout for all actions except `api_details`.
- **Action Skipping**: Actions like `api_details`, `api_read`, `api_gpx_files`, and `auth_success` have `skip_before_action` to bypass CSRF protection.
- **Authorization**: Actions like `api_details`, `api_gpx_files`, `account`, `go_public`, `make_friend`, `remove_friend`, `set_status`, `delete`, and `list` have `before_action` to enforce authorization checks.
- **Locale Setting**: The `set_locale` action is used to set the locale for the user.
- **User Requirements**: Actions like `require_user`, `require_self`, `check_database_readable`, `check_database_writable`, `check_api_readable`, `require_allow_read_prefs`, `require_allow_read_gpx`, `require_cookies`, and `require_administrator` enforce various user requirements.
- **API Handling**: The `around_action` `api_call_handle_error` is used to handle errors during API calls.
- **User Lookup"
99,"@@ -15,7 +15,7 @@ func newNodeChain(api *nodeAPI) *nodeChain {
 }
 
 func (api *nodeChain) Head() ([]*cid.Cid, error) {
-	ts := api.api.node.ChainMgr.GetHeaviestTipSet()
+	ts := api.api.node.ChainReader.Head()
 	if len(ts) == 0 {
 		return nil, ErrHeaviestTipSetNotFound
 	}","package impl

import (
	""context""

	""gx/ipfs/QmZFbDTY9jfSBms2MchvYM9oYRbAF19K7Pby47yDBfpPrb/go-cid""
)

type nodeChain struct {
	api *nodeAPI
}

func newNodeChain(api *nodeAPI) *nodeChain {
	return &nodeChain{api: api}
	ts := api.api.node.ChainReader.Head()
}

func (api *nodeChain) Head() ([]*cid.Cid, error) {
	ts := api.api.node.ChainMgr.GetHeaviestTipSet()
	if len(ts) == 0 {
		return nil, ErrHeaviestTipSetNotFound
	}
	tsSlice := ts.ToSlice()
	out := make([]*cid.Cid, len(tsSlice))
	for i, b := range tsSlice {
		out[i] = b.Cid()
	}

	return out, nil
}

func (api *nodeChain) Ls(ctx context.Context) <-chan interface{} {
	return api.api.node.ChainMgr.BlockHistory(ctx)
}
","Good lord, law of demeter out the window. No action required but wow, four levels deep, that's a rarity!","The suggested code change introduces a new `nodeChain` struct that encapsulates the functionality for interacting with the IPFS node's blockchain. The `Head` method retrieves the heaviest tip set from the node's chain manager and converts it into a slice of CIDs. The `Ls` method returns a channel that emits block history events from the node's chain manager.

### Review:

1. **Struct Definition**: 
   - The `nodeChain` struct is defined with a pointer to a `nodeAPI`, which suggests that this struct is intended to be used as a wrapper around the `nodeAPI`.

2. **Constructor**:
   - The `newNodeChain` function initializes a new `nodeChain` instance by passing the provided `nodeAPI`. This is a good practice for creating instances of structs.

3. **Head Method**:
   - The `Head` method retrieves the heaviest tip set using `api.api.node.ChainMgr.GetHeaviestTipSet()`.
   - It checks if the tip set is empty and returns an error if so.
   - It converts the tip set to a slice of blocks and then extracts the CIDs from these blocks.
   - The method returns the slice of CIDs and any potential errors.

4. **Ls Method**:
   - The `Ls` method returns a channel that emits block history events from the node's chain manager. This is useful for monitoring changes in the blockchain.

### Potential Improvements:

- **Error Handling**: Ensure that all error paths are handled appropriately, especially when dealing with network requests or data processing.
- **Documentation**: Add comments to the methods to explain their purpose and usage.
- **Type Safety**: Consider using type aliases or interfaces to improve type safety and readability.
- **Performance**: Optimize the conversion of tip sets to slices and CIDs if performance becomes a concern.

Overall, the code change provides a structured way to interact with the IPFS node's blockchain, making it easier to manage and monitor the blockchain state."
