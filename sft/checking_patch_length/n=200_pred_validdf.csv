index,patch,patch_context,original_msg,predicted_msg
0,"@@ -231,4 +231,8 @@ def setup_app(app):
         )
         return rv
 
+    @app.template_test('list')
+    def _is_list(value):
+        return isinstance(value, list)
+
     return app","
from werkzeug.routing import BuildError

from .bccache import BytecodeCacheWithConfig
from .context_processor import setup_app as context_processor_setup_app
from .loader import OrderAwareDispatchingJinjaLoader

ENV_PREFIX = '_collected_'


def render_template_to_string(input, _from_string=False, **context):
    """"""Render a template from the template folder with the given context.

    Code based on
    `<https://github.com/mitsuhiko/flask/blob/master/flask/templating.py>`_

    :param input: the string template, or name of the template to be
                  rendered, or an iterable with template names
                  the first one existing will be rendered
    :param context: the variables that should be available in the
                    context of the template.
    :return: a string

    """"""
    ctx = _request_ctx_stack.top
    ctx.app.update_template_context(context)
    if _from_string:
        template = ctx.app.jinja_env.from_string(input)
    else:
        template = ctx.app.jinja_env.get_or_select_template(input)
    return template.render(context)


def inject_utils():
    """"""Inject variables and functions to jinja execution context.

    In particular it will add:

    - ``url_for``: an Invenio specific wrapper of Flask url_for, that will let
      you obtain URLs for non Flask-native handlers (i.e. not yet ported
      Invenio URLs)
    - ``_``: this can be used to automatically translate a given string.
    - ``is_language_rtl``: True if the chosen language should be read right to
      left.
    """"""
    from invenio.base.i18n import is_language_rtl
    from invenio.modules.records.api import get_record
    from invenio.utils.url import create_url, get_canonical_and_alternates_urls

    def invenio_url_for(endpoint, **values):
        try:
            return url_for(endpoint, **values)
        except BuildError:
            if re.match(""https?://"", endpoint, re.IGNORECASE):
                return endpoint
            if endpoint.startswith('.'):
                endpoint = request.blueprint + endpoint
            url = create_url('/' + endpoint.replace('.', '/'), values, False)
            return url.decode('utf-8')

    user = current_user._get_current_object()
    canonical_url, alternate_urls = get_canonical_and_alternates_urls(
        request.path)
    alternate_urls = dict((ln.replace('_', '-'), alternate_url)
                          for ln, alternate_url in iteritems(alternate_urls))
    return dict(
        current_user=user,
        is_language_rtl=is_language_rtl,
        canonical_url=canonical_url,
        alternate_urls=alternate_urls,
        get_record=get_record,
        url_for=invenio_url_for,
    )


def setup_app(app):
    """"""
    Extend application template filters with custom filters and fixes.

    List of applied filters:

    - filesizeformat
    - path_join
    - quoted_txt2html
    - invenio_format_date
    - invenio_pretty_date
    - invenio_url_args
    """"""
    import os
    from datetime import datetime
    from invenio.utils.date import convert_datetext_to_dategui, \
        convert_datestruct_to_dategui, pretty_date

    from . import config
    app.config.from_object(config)

    context_processor_setup_app(app)
    app.context_processor(inject_utils)

    if app.config.get('JINJA2_BCCACHE', False):
        app.jinja_options = dict(
            app.jinja_options,
            auto_reload=app.config.get('JINJA2_BCCACHE_AUTO_RELOAD', False),
            cache_size=app.config.get('JINJA2_BCCACHE_SIZE', -1),
            bytecode_cache=BytecodeCacheWithConfig(app))

    # Let's customize the template loader to look into packages
    # and application templates folders.
    jinja_loader = ChoiceLoader([
        OrderAwareDispatchingJinjaLoader(app),
        app.jinja_loader,
    ])
    app.jinja_loader = jinja_loader

    for ext in app.config.get('JINJA2_EXTENSIONS', []):
        try:
            app.jinja_env.add_extension(ext)
        except Exception:
            app.logger.exception(
                'Problem with loading extension: ""{0}""'.format(ext))

    def test_not_empty(v):
        return v is not None and v != ''

    @app.template_filter('u')
    def tounicode(value):
        if isinstance(value, str):
            return value.decode('utf8')
        return value

    @app.template_filter('s')
    def tostr(value):
        if not isinstance(value, str):
            if isinstance(value, unicode):
                value = value.encode('utf8')
            value = str(value)
        return value

    @app.template_filter('prefix')
    def _prefix(value, prefix=''):
        return prefix + value if test_not_empty(value) else ''

    @app.template_filter('suffix')
    def _suffix(value, suffix=''):
        return value + suffix if test_not_empty(value) else ''

    @app.template_filter('wrap')
    def _wrap(value, prefix='', suffix=''):
        return prefix + value + suffix if test_not_empty(value) else ''

    @app.template_filter('sentences')
    def _sentences(value, limit, separator='. '):
        """"""Return first `limit` number of sentences ending by `separator`.""""""
        return separator.join(value.split(separator)[:limit])

    @app.template_filter('path_join')
    def _os_path_join(d):
        """"""Shortcut for `os.path.join`.""""""
        return os.path.join(*d)

    @app.template_filter('quoted_txt2html')
    def _quoted_txt2html(*args, **kwargs):
        from invenio.utils.mail import email_quoted_txt2html
        return email_quoted_txt2html(*args, **kwargs)

    @app.template_filter('invenio_format_date')
    def _format_date(date):
        """"""
        Format a date into a human friendly format.

        It uses :py:func:`invenio.utils.date.convert_datetext_to_dategui`
        """"""
        if isinstance(date, datetime):
            return convert_datestruct_to_dategui(
                date.timetuple(),
                getattr(g, 'ln', app.config['CFG_SITE_LANG'])).decode('utf-8')
        return convert_datetext_to_dategui(
            date, getattr(g, 'ln', app.config['CFG_SITE_LANG'])
        ).decode('utf-8')

    @app.template_filter('invenio_pretty_date')
    def _pretty_date(date):
        """"""
        Format a timestamp into a human friendly format.

        It uses :py:func:`invenio.utils.date.pretty_date`
        """"""
        if isinstance(date, datetime) or isinstance(date, string_types):
            return pretty_date(
                date, ln=getattr(g, 'ln', app.config['CFG_SITE_LANG']))
        return date

    @app.template_filter('invenio_url_args')
    def _url_args(d, append=u'?', filter=[]):
        from jinja2.utils import escape
        rv = append + u'&'.join(
            u'%s=%s' % (escape(key), escape(value))
            for key, value in d.iteritems(True)
            if value is not None and key not in filter
            # and not isinstance(value, Undefined)
    @app.template_test('list')
    def _is_list(value):
        return isinstance(value, list)

        )
        return rv

    return app
",Should we call it `is_list`?,"The suggested code changes appear to be related to setting up and configuring an Invenio application, particularly focusing on template rendering and utility injection. Here's a concise review of the changes:

### Key Changes:

1. **Template Rendering Function (`render_template_to_string`)**:
   - This function allows rendering templates from either a string or a template name. It updates the template context and renders the template with the provided context.

2. **Utility Injection (`inject_utils`)**:
   - This function injects various utilities into the Jinja environment, such as `url_for`, `_`, `is_language_rtl`, `canonical_url`, `alternate_urls`, `get_record`, and `invenio_url_for`.

3. **Application Setup (`setup_app`)**:
   - This function sets up the application by applying various configurations, adding extensions, and customizing the template loader.
   - It includes filters like `filesizeformat`, `path_join`, `quoted_txt2html`, `invenio_format_date`, `invenio_pretty_date`, `invenio_url_args`, and custom filters like `u`, `s`, `prefix`, `suffix`, `wrap`, `sentences`, and `list`.

### Review Points:

- **Template Rendering**: The `render_template_to_string` function is well-structured and handles both string and template name inputs, updating the context before rendering.

- **Utility Injection**: The `inject_utils` function is comprehensive, injecting multiple utilities that enhance the application's functionality.

- **Application Setup**: The `setup_app` function is well-organized, applying configurations, adding extensions, and customizing the template loader. It also includes a variety of filters and custom tests.

- **Code Quality**: The code is clean and follows PEP 8 guidelines, with appropriate comments explaining the purpose of each function and section.

Overall, these changes significantly enhance the functionality and usability of the Invenio application by providing robust template rendering and utility injection mechanisms."
1,"@@ -44,7 +44,7 @@ namespace OpenTelemetry.Trace
             configureSqlClientInstrumentationOptions?.Invoke(sqlOptions);
 
             builder.AddInstrumentation(() => new SqlClientInstrumentation(sqlOptions));
-            builder.AddSource(SqlClientDiagnosticListener.ActivitySourceName);
+            builder.AddSource(new Source(SqlClientDiagnosticListener.ActivitySourceName));
 
             return builder;
         }","// <copyright file=""TracerProviderBuilderExtensions.cs"" company=""OpenTelemetry Authors"">
// Copyright The OpenTelemetry Authors
//
// Licensed under the Apache License, Version 2.0 (the ""License"");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an ""AS IS"" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
// </copyright>

using System;
using OpenTelemetry.Instrumentation.SqlClient;
using OpenTelemetry.Instrumentation.SqlClient.Implementation;

namespace OpenTelemetry.Trace
{
    /// <summary>
    /// Extension methods to simplify registering of dependency instrumentation.
    /// </summary>
    public static class TracerProviderBuilderExtensions
    {
        /// <summary>
        /// Enables SqlClient instrumentation.
        /// </summary>
        /// <param name=""builder""><see cref=""TracerProviderBuilder""/> being configured.</param>
        /// <param name=""configureSqlClientInstrumentationOptions"">SqlClient configuration options.</param>
        /// <returns>The instance of <see cref=""TracerProviderBuilder""/> to chain the calls.</returns>
        public static TracerProviderBuilder AddSqlClientInstrumentation(
            this TracerProviderBuilder builder,
            Action<SqlClientInstrumentationOptions> configureSqlClientInstrumentationOptions = null)
        {
            if (builder == null)
            {
                throw new ArgumentNullException(nameof(builder));
            }

            var sqlOptions = new SqlClientInstrumentationOptions();
            builder.AddSource(new Source(SqlClientDiagnosticListener.ActivitySourceName));
            configureSqlClientInstrumentationOptions?.Invoke(sqlOptions);

            builder.AddInstrumentation(() => new SqlClientInstrumentation(sqlOptions));
            builder.AddSource(SqlClientDiagnosticListener.ActivitySourceName);

            return builder;
        }
    }
}
","in the instrumentation example, should we use minversion and maxversion = the assembly version or should ignore our assembly version?","The suggested code change adds extension methods to simplify the registration of SQL Client instrumentation using OpenTelemetry. Here's a concise review:

1. **Namespace and Class Definition**: The code defines a namespace `OpenTelemetry.Trace` and a class `TracerProviderBuilderExtensions`.

2. **Method Signature**: The method `AddSqlClientInstrumentation` takes a `TracerProviderBuilder` and an optional `Action<SqlClientInstrumentationOptions>` delegate.

3. **Null Check**: It checks if the `builder` is null and throws an `ArgumentNullException` if it is.

4. **Default Options**: It initializes a `SqlClientInstrumentationOptions` object and sets up the activity source for SQL Client instrumentation.

5. **Configuration**: The `configureSqlClientInstrumentationOptions` delegate is used to configure the options, allowing for customization.

6. **Instrumentation Registration**: It adds the SQL Client instrumentation to the builder and ensures the activity source is registered again.

7. **Return Value**: The method returns the `TracerProviderBuilder` instance to allow chaining of method calls.

Overall, the code provides a clean and efficient way to integrate SQL Client instrumentation into an OpenTelemetry tracing setup."
2,"@@ -25,7 +25,7 @@ from scapy.modules.six.moves import range
 ## Fields ##
 ############
 
-class Field(six.with_metaclass(Field_metaclass, object)):
+class Field(object):
     """"""For more informations on how this work, please refer to
        http://www.secdev.org/projects/scapy/files/scapydoc.pdf
        chapter ``Adding a New Field''""""""","## This file is part of Scapy
## See http://www.secdev.org/projects/scapy for more informations
## Copyright (C) Philippe Biondi <phil@secdev.org>
## This program is published under a GPLv2 license

""""""
Fields: basic data structures that make up parts of packets.
""""""

from __future__ import absolute_import
import struct,copy,socket,collections
from scapy.config import conf
from scapy.dadict import DADict
from scapy.volatile import *
from scapy.data import *
from scapy.compat import *
from scapy.utils import *
from scapy.base_classes import BasePacket, Gen, Net, Field_metaclass
from scapy.error import warning
import scapy.modules.six as six
from scapy.modules.six.moves import range


############
class Field(object):
## Fields ##
############

class Field(six.with_metaclass(Field_metaclass, object)):
    """"""For more informations on how this work, please refer to
       http://www.secdev.org/projects/scapy/files/scapydoc.pdf
       chapter ``Adding a New Field''""""""
    __slots__ = [""name"", ""fmt"", ""default"", ""sz"", ""owners""]
    islist = 0
    ismutable = False
    holds_packets = 0
    def __init__(self, name, default, fmt=""H""):
        self.name = name
        if fmt[0] in ""@=<>!"":
            self.fmt = fmt
        else:
            self.fmt = ""!""+fmt
        self.default = self.any2i(None,default)
        self.sz = struct.calcsize(self.fmt)
        self.owners = []

    def register_owner(self, cls):
        self.owners.append(cls)

    def i2len(self, pkt, x):
        """"""Convert internal value to a length usable by a FieldLenField""""""
        return self.sz
    def i2count(self, pkt, x):
        """"""Convert internal value to a number of elements usable by a FieldLenField.
        Always 1 except for list fields""""""
        return 1
    def h2i(self, pkt, x):
        """"""Convert human value to internal value""""""
        return x
    def i2h(self, pkt, x):
        """"""Convert internal value to human value""""""
        return x
    def m2i(self, pkt, x):
        """"""Convert machine value to internal value""""""
        return x
    def i2m(self, pkt, x):
        """"""Convert internal value to machine value""""""
        if x is None:
            x = 0
        elif isinstance(x, str):
            return raw(x)
        return x
    def any2i(self, pkt, x):
        """"""Try to understand the most input values possible and make an internal value from them""""""
        return self.h2i(pkt, x)
    def i2repr(self, pkt, x):
        """"""Convert internal value to a nice representation""""""
        return repr(self.i2h(pkt,x))
    def addfield(self, pkt, s, val):
        """"""Add an internal value  to a string""""""
        return s+struct.pack(self.fmt, self.i2m(pkt,val))
    def getfield(self, pkt, s):
        """"""Extract an internal value from a string""""""
        return  s[self.sz:], self.m2i(pkt, struct.unpack(self.fmt, s[:self.sz])[0])
    def do_copy(self, x):
        if hasattr(x, ""copy""):
            return x.copy()
        if isinstance(x, list):
            x = x[:]
            for i in range(len(x)):
                if isinstance(x[i], BasePacket):
                    x[i] = x[i].copy()
        return x
    def __repr__(self):
        return ""<Field (%s).%s>"" % ("","".join(x.__name__ for x in self.owners),self.name)
    def copy(self):
        return copy.deepcopy(self)
    def randval(self):
        """"""Return a volatile object whose value is both random and suitable for this field""""""
        fmtt = self.fmt[-1]
        if fmtt in ""BHIQ"":
            return {""B"":RandByte,""H"":RandShort,""I"":RandInt, ""Q"":RandLong}[fmtt]()
        elif fmtt == ""s"":
            if self.fmt[0] in ""0123456789"":
                l = int(self.fmt[:-1])
            else:
                l = int(self.fmt[1:-1])
            return RandBin(l)
        else:
            warning(""no random class for [%s] (fmt=%s)."", self.name, self.fmt)




class Emph(object):
    __slots__ = [""fld""]
    def __init__(self, fld):
        self.fld = fld
    def __getattr__(self, attr):
        return getattr(self.fld,attr)
    def __hash__(self):
        return hash(self.fld)
    def __eq__(self, other):
        return self.fld == other


class ActionField(object):
    __slots__ = [""_fld"", ""_action_method"", ""_privdata""]
    def __init__(self, fld, action_method, **kargs):
        self._fld = fld
        self._action_method = action_method
        self._privdata = kargs
    def any2i(self, pkt, val):
        getattr(pkt, self._action_method)(val, self._fld, **self._privdata)
        return getattr(self._fld, ""any2i"")(pkt, val)
    def __getattr__(self, attr):
        return getattr(self._fld,attr)


class ConditionalField(object):
    __slots__ = [""fld"", ""cond""]
    def __init__(self, fld, cond):
        self.fld = fld
        self.cond = cond
    def _evalcond(self,pkt):
        return self.cond(pkt)

    def getfield(self, pkt, s):
        if self._evalcond(pkt):
            return self.fld.getfield(pkt,s)
        else:
            return s,None

    def addfield(self, pkt, s, val):
        if self._evalcond(pkt):
            return self.fld.addfield(pkt,s,val)
        else:
            return s
    def __getattr__(self, attr):
        return getattr(self.fld,attr)


class PadField(object):
    """"""Add bytes after the proxified field so that it ends at the specified
       alignment from its beginning""""""
    __slots__ = [""_fld"", ""_align"", ""_padwith""]
    def __init__(self, fld, align, padwith=None):
        self._fld = fld
        self._align = align
        self._padwith = padwith or b""""

    def padlen(self, flen):
        return -flen%self._align

    def getfield(self, pkt, s):
        remain,val = self._fld.getfield(pkt,s)
        padlen = self.padlen(len(s)-len(remain))
        return remain[padlen:], val

    def addfield(self, pkt, s, val):
        sval = self._fld.addfield(pkt, b"""", val)
        return s+sval+struct.pack(""%is"" % (self.padlen(len(sval))), self._padwith)

    def __getattr__(self, attr):
        return getattr(self._fld,attr)


class DestField(Field):
    __slots__ = [""defaultdst""]
    # Each subclass must have its own bindings attribute
    # bindings = {}
    def __init__(self, name, default):
        self.defaultdst = default
    def dst_from_pkt(self, pkt):
        for addr, condition in self.bindings.get(pkt.payload.__class__, []):
            try:
                if all(pkt.payload.getfieldval(field) == value
                       for field, value in six.iteritems(condition)):
                    return addr
            except AttributeError:
                pass
        return self.defaultdst
    @classmethod
    def bind_addr(cls, layer, addr, **condition):
        cls.bindings.setdefault(layer, []).append((addr, condition))


class MACField(Field):
    def __init__(self, name, default):
        Field.__init__(self, name, default, ""6s"")
    def i2m(self, pkt, x):
        if x is None:
            return b""\0\0\0\0\0\0""
        return mac2str(x)
    def m2i(self, pkt, x):
        return str2mac(x)
    def any2i(self, pkt, x):
        if isinstance(x, bytes) and len(x) == 6:
            x = self.m2i(pkt, x)
        return x
    def i2repr(self, pkt, x):
        x = self.i2h(pkt, x)
        if self in conf.resolve:
            x = conf.manufdb._resolve_MAC(x)
        return x",Why this change ? Is it useful ?,"The suggested code changes appear to be related to the `Field` class in the Scapy library, which is used to define the structure of packets. The changes include:

1. **Field Class**: 
   - Added methods like `i2len`, `i2count`, `h2i`, `i2h`, `m2i`, `i2m`, `any2i`, `i2repr`, `addfield`, `getfield`, `do_copy`, `__repr__`, `copy`, and `randval`.
   - Added slots for `name`, `fmt`, `default`, `sz`, and `owners`.

2. **Emph Class**:
   - A simple wrapper around another field that provides an emphasis.

3. **ActionField Class**:
   - A field that applies an action method to the value before converting it to internal format.

4. **ConditionalField Class**:
   - A field that only applies its proxified field if a condition is met.

5. **PadField Class**:
   - A field that adds padding bytes after the proxified field to ensure it ends at a specified alignment.

6. **DestField Class**:
   - A field that determines the destination address based on conditions applied to the packet's payload.

7. **MACField Class**:
   - A specific field for MAC addresses, handling conversions between binary and human-readable formats.

These changes enhance the flexibility and functionality of the Scapy library by allowing for more complex packet structures and behaviors."
3,"@@ -0,0 +1,4 @@
+const titleNode = virtualNode.children.find(({ props }) => {
+	return props.nodeName === 'title';
+});
+return !!titleNode && titleNode.actualNode.textContent.trim() !== '';","const titleNode = virtualNode.children.find(({ props }) => {
	return props.nodeName === 'title';
});
return !!titleNode && titleNode.actualNode.textContent.trim() !== '';

","I know this is a nitpick, but don't we always go `nodeName.toUpperCase`?","The suggested code change is intended to find a `<title>` element within a virtual DOM node and check if its text content is not empty after trimming whitespace. Here's a concise review:

- **Purpose**: The code aims to ensure that the `<title>` element exists and contains non-empty text content.
- **Logic**:
  - `virtualNode.children.find(...)`: This line searches for a child node with a `props.nodeName` of `'title'`.
  - `!!titleNode`: This checks if the found node is truthy (i.e., not `null` or `undefined`).
  - `titleNode.actualNode.textContent.trim() !== ''`: This checks if the trimmed text content of the `<title>` node is not empty.
- **Simplicity**: The code is straightforward and efficient for this purpose.
- **Readability**: The use of `!!` for checking truthiness makes the code more readable by clearly indicating that the result should be a boolean.

Overall, the code is well-structured and performs the intended task effectively."
4,"@@ -37,6 +37,11 @@ public class EMailValidatorTest {
         assertEquals(false, EMailValidator.isEmailValid(""dora@.com"", null));
         assertEquals(false, EMailValidator.isEmailValid("""", null));
         assertEquals(false, EMailValidator.isEmailValid(null, null));
+        /*
+        Add tests for 4601
+        */
+        assertEquals(true, EMailValidator.isEmailValid(""ziller@wiso.uni-koeln.de"", null));
+        assertEquals(true, EMailValidator.isEmailValid(""jayned@essex.ac.uk"", null));
     }
 
 }","package edu.harvard.iq.dataverse;

import static org.junit.Assert.assertEquals;
import org.junit.Test;

public class EMailValidatorTest {

    @Test
    public void testIsEmailValid() {
        assertEquals(true, EMailValidator.isEmailValid(""pete@mailinator.com"", null));
        /**
         * @todo How can "" leadingWhitespace@mailinator.com"" be a valid email
         * address?
         */
        assertEquals(true, EMailValidator.isEmailValid("" leadingWhitespace@mailinator.com"", null));
        /**
         * @todo How can ""trailingWhitespace@mailinator.com "" be a valid email
         * address?
         */
        assertEquals(true, EMailValidator.isEmailValid(""trailingWhitespace@mailinator.com "", null));
        assertEquals(false, EMailValidator.isEmailValid(""elisah.da mota@example.com"", null));
        assertEquals(false, EMailValidator.isEmailValid(""pete1@mailinator.com;pete2@mailinator.com"", null));
        /**
         * These examples are all from https://randomuser.me and seem to be
         * valid according to
         * http://sphinx.mythic-beasts.com/~pdw/cgi-bin/emailvalidate (except
         * رونیکا.محمدخان@example.com).
         *
         */
        assertEquals(true, EMailValidator.isEmailValid(""michélle.pereboom@example.com"", null));
        assertEquals(true, EMailValidator.isEmailValid(""begüm.vriezen@example.com"", null));
        assertEquals(true, EMailValidator.isEmailValid(""lótus.gonçalves@example.com"", null));
        assertEquals(true, EMailValidator.isEmailValid(""lótus.gonçalves@éxample.com"", null));
        assertEquals(true, EMailValidator.isEmailValid(""begüm.vriezen@example.cologne"", null));
        assertEquals(true, EMailValidator.isEmailValid(""رونیکا.محمدخان@example.com"", null));
        assertEquals(false, EMailValidator.isEmailValid(""lótus.gonçalves@example.cóm"", null));
        /*
        Add tests for 4601
        */
        assertEquals(true, EMailValidator.isEmailValid(""ziller@wiso.uni-koeln.de"", null));
        assertEquals(true, EMailValidator.isEmailValid(""jayned@essex.ac.uk"", null));
        assertEquals(false, EMailValidator.isEmailValid(""dora@.com"", null));
        assertEquals(false, EMailValidator.isEmailValid("""", null));
        assertEquals(false, EMailValidator.isEmailValid(null, null));
    }

}
","We should reformat this emails in the test to be a little less ""real"".","The suggested code change is a unit test for the `EMailValidator` class in the `edu.harvard.iq.dataverse` package. The test cases cover various scenarios, including valid and invalid email addresses, as well as edge cases such as leading and trailing whitespace, semicolons, and internationalized domain names.

Here's a concise review of the suggested code change:

- **Test Cases**: The test method `testIsEmailValid` includes several assertions to verify the correctness of the `isEmailValid` method in the `EMailValidator` class.
- **Edge Cases**: The test cases handle leading and trailing whitespace, semicolons, and internationalized domain names, which are important considerations for email validation.
- **Validation**: The test cases use `assertEquals` to ensure that the method returns the expected boolean values based on the input email addresses.
- **Documentation**: The comments in the test method indicate areas where further investigation is needed, such as why certain email addresses are considered valid despite being malformed.

Overall, this test suite provides a comprehensive set of checks to ensure the robustness of the email validation logic in the `EMailValidator` class."
5,"@@ -449,14 +449,18 @@ class LNGossip(LNWorker):
         self.features |= LnFeatures.GOSSIP_QUERIES_OPT
         self.features |= LnFeatures.GOSSIP_QUERIES_REQ
         self.unknown_ids = set()
+        self.has_started = False
 
     def start_network(self, network: 'Network'):
         assert network
+        if self.has_started:
+            return
         super().start_network(network)
         asyncio.run_coroutine_threadsafe(self.taskgroup.spawn(self.maintain_db()), self.network.asyncio_loop)
+        self.has_started = True
 
     async def maintain_db(self):
-        await self.channel_db.load_data()
+        await self.channel_db.data_loaded.wait()
         while True:
             if len(self.unknown_ids) == 0:
                 self.channel_db.prune_old_policies(self.max_age)","                                proxy=self.network.proxy)
        peer = Peer(self, node_id, transport)
        await self.taskgroup.spawn(peer.main_loop())
        with self.lock:
            self._peers[node_id] = peer
        return peer

    def peer_closed(self, peer: Peer) -> None:
        with self.lock:
            self._peers.pop(peer.pubkey, None)

    def num_peers(self) -> int:
        return sum([p.is_initialized() for p in self.peers.values()])

    def start_network(self, network: 'Network'):
        assert network
        self.network = network
        self.config = network.config
        self.channel_db = self.network.channel_db
        self._add_peers_from_config()
        asyncio.run_coroutine_threadsafe(self.main_loop(), self.network.asyncio_loop)

    def stop(self):
        if self.listen_server:
            self.network.asyncio_loop.call_soon_threadsafe(self.listen_server.close)
        asyncio.run_coroutine_threadsafe(self.taskgroup.cancel_remaining(), self.network.asyncio_loop)
        util.unregister_callback(self.on_proxy_changed)

    def _add_peers_from_config(self):
        peer_list = self.config.get('lightning_peers', [])
        for host, port, pubkey in peer_list:
            asyncio.run_coroutine_threadsafe(
                self._add_peer(host, int(port), bfh(pubkey)),
                self.network.asyncio_loop)

    def is_good_peer(self, peer):
        # the purpose of this method is to filter peers that advertise the desired feature bits
        # it is disabled for now, because feature bits published in node announcements seem to be unreliable
        return True
        node_id = peer.pubkey
        node = self.channel_db._nodes.get(node_id)
        if not node:
            return False
        try:
            ln_compare_features(self.features, node.features)
        except IncompatibleLightningFeatures:
            return False
        #self.logger.info(f'is_good {peer.host}')
        return True

    def on_peer_successfully_established(self, peer: Peer) -> None:
        if isinstance(peer.transport, LNTransport):
            peer_addr = peer.transport.peer_addr
            # reset connection attempt count
            self._on_connection_successfully_established(peer_addr)
            # add into channel db
            if self.channel_db:
                self.channel_db.add_recent_peer(peer_addr)
            # save network address into channels we might have with peer
            for chan in peer.channels.values():
                chan.add_or_update_peer_addr(peer_addr)

    async def _get_next_peers_to_try(self) -> Sequence[LNPeerAddr]:
        now = time.time()
        await self.channel_db.data_loaded.wait()
        # first try from recent peers
        recent_peers = self.channel_db.get_recent_peers()
        for peer in recent_peers:
            if not peer:
                continue
            if peer.pubkey in self._peers:
                continue
            if not self._can_retry_addr(peer, now=now):
                continue
            if not self.is_good_peer(peer):
                continue
            return [peer]
        # try random peer from graph
        unconnected_nodes = self.channel_db.get_200_randomly_sorted_nodes_not_in(self.peers.keys())
        if unconnected_nodes:
            for node_id in unconnected_nodes:
                addrs = self.channel_db.get_node_addresses(node_id)
                if not addrs:
                    continue
                host, port, timestamp = self.choose_preferred_address(list(addrs))
                try:
                    peer = LNPeerAddr(host, port, node_id)
                except ValueError:
                    continue
                if not self._can_retry_addr(peer, now=now):
                    continue
                if not self.is_good_peer(peer):
                    continue
                #self.logger.info('taking random ln peer from our channel db')
                return [peer]

        # getting desperate... let's try hardcoded fallback list of peers
        if constants.net in (constants.BitcoinTestnet, ):
            fallback_list = FALLBACK_NODE_LIST_TESTNET
        elif constants.net in (constants.BitcoinMainnet, ):
            fallback_list = FALLBACK_NODE_LIST_MAINNET
        else:
            return []  # regtest??

        fallback_list = [peer for peer in fallback_list if self._can_retry_addr(peer, now=now)]
        if fallback_list:
            return [random.choice(fallback_list)]

        # last resort: try dns seeds (BOLT-10)
        return await run_in_thread(self._get_peers_from_dns_seeds)

    def _get_peers_from_dns_seeds(self) -> Sequence[LNPeerAddr]:
        # NOTE: potentially long blocking call, do not run directly on asyncio event loop.
        # Return several peers to reduce the number of dns queries.
        if not constants.net.LN_DNS_SEEDS:
            return []
        dns_seed = random.choice(constants.net.LN_DNS_SEEDS)
        self.logger.info('asking dns seed ""{}"" for ln peers'.format(dns_seed))
        try:
            # note: this might block for several seconds
            # this will include bech32-encoded-pubkeys and ports
            srv_answers = resolve_dns_srv('r{}.{}'.format(
                constants.net.LN_REALM_BYTE, dns_seed))
        except dns.exception.DNSException as e:
            self.logger.info(f'failed querying (1) dns seed ""{dns_seed}"" for ln peers: {repr(e)}')
            return []
        random.shuffle(srv_answers)
        num_peers = 2 * NUM_PEERS_TARGET
        srv_answers = srv_answers[:num_peers]
        # we now have pubkeys and ports but host is still needed
        peers = []
        for srv_ans in srv_answers:
            try:
                # note: this might block for several seconds
                answers = dns.resolver.query(srv_ans['host'])
            except dns.exception.DNSException as e:
                self.logger.info(f'failed querying (2) dns seed ""{dns_seed}"" for ln peers: {repr(e)}')
                continue
            try:
                ln_host = str(answers[0])
                port = int(srv_ans['port'])
                bech32_pubkey = srv_ans['host'].split('.')[0]
                pubkey = get_compressed_pubkey_from_bech32(bech32_pubkey)
                peers.append(LNPeerAddr(ln_host, port, pubkey))
            except Exception as e:
                self.logger.info(f'error with parsing peer from dns seed: {repr(e)}')
                continue
        self.logger.info(f'got {len(peers)} ln peers from dns seed')
        return peers

    @staticmethod
    def choose_preferred_address(addr_list: Sequence[Tuple[str, int, int]]) -> Tuple[str, int, int]:
        assert len(addr_list) >= 1
        # choose first one that is an IP
        for host, port, timestamp in addr_list:
            if is_ip_address(host):
                return host, port, timestamp
        # otherwise choose one at random
        # TODO maybe filter out onion if not on tor?
        choice = random.choice(addr_list)
        return choice

    def on_proxy_changed(self, event, *args):
        for peer in self.peers.values():
            peer.close_and_cleanup()
        self._clear_addr_retry_times()

    @log_exceptions
    async def add_peer(self, connect_str: str) -> Peer:
        node_id, rest = extract_nodeid(connect_str)
        peer = self._peers.get(node_id)
        if not peer:
            if rest is not None:
                host, port = split_host_port(rest)
            else:
                addrs = self.channel_db.get_node_addresses(node_id)
                if not addrs:
                    raise ConnStringFormatError(_('Don\'t know any addresses for node:') + ' ' + bh2u(node_id))
                host, port, timestamp = self.choose_preferred_address(list(addrs))
            port = int(port)
            # Try DNS-resolving the host (if needed). This is simply so that
            # the caller gets a nice exception if it cannot be resolved.
            try:
                await asyncio.get_event_loop().getaddrinfo(host, port)
            except socket.gaierror:
                raise ConnStringFormatError(_('Hostname does not resolve (getaddrinfo failed)'))
            # add peer
            peer = await self._add_peer(host, port, node_id)
        return peer


class LNGossip(LNWorker):
    max_age = 14*24*3600
    LOGGING_SHORTCUT = 'g'

    def __init__(self):
        seed = os.urandom(32)
        node = BIP32Node.from_rootseed(seed, xtype='standard')
        xprv = node.to_xprv()
        super().__init__(xprv)
        self.has_started = False
        if self.has_started:
            return
        self.has_started = True
        await self.channel_db.data_loaded.wait()
        self.features |= LnFeatures.GOSSIP_QUERIES_OPT
        self.features |= LnFeatures.GOSSIP_QUERIES_REQ
        self.unknown_ids = set()

    def start_network(self, network: 'Network'):
        assert network
        super().start_network(network)
        asyncio.run_coroutine_threadsafe(self.taskgroup.spawn(self.maintain_db()), self.network.asyncio_loop)

    async def maintain_db(self):
        await self.channel_db.load_data()
        while True:
            if len(self.unknown_ids) == 0:
                self.channel_db.prune_old_policies(self.max_age)
                self.channel_db.prune_orphaned_channels()
            await asyncio.sleep(120)

    async def add_new_ids(self, ids):
        known = self.channel_db.get_channel_ids()
        new = set(ids) - set(known)
        self.unknown_ids.update(new)
        util.trigger_callback('unknown_channels', len(self.unknown_ids))
        util.trigger_callback('gossip_peers', self.num_peers())
        util.trigger_callback('ln_gossip_sync_progress')

    def get_ids_to_query(self):
        N = 500
        l = list(self.unknown_ids)
        self.unknown_ids = set(l[N:])
        util.trigger_callback('unknown_channels', len(self.unknown_ids))
        util.trigger_callback('ln_gossip_sync_progress')
        return l[0:N]

    def get_sync_progress_estimate(self) -> Tuple[Optional[int], Optional[int]]:
        if self.num_peers() == 0:
            return None, None
        nchans_with_0p, nchans_with_1p, nchans_with_2p = self.channel_db.get_num_channels_partitioned_by_policy_count()
        num_db_channels = nchans_with_0p + nchans_with_1p + nchans_with_2p
        # some channels will never have two policies (only one is in gossip?...)
        # so if we have at least 1 policy for a channel, we consider that channel ""complete"" here
        current_est = num_db_channels - nchans_with_0p
        total_est = len(self.unknown_ids) + num_db_channels
        return current_est, total_est


class LNWallet(LNWorker):

    lnwatcher: Optional['LNWalletWatcher']

    def __init__(self, wallet: 'Abstract_Wallet', xprv):
        Logger.__init__(self)
        self.wallet = wallet
        self.db = wallet.db
        self.config = wallet.config
        LNWorker.__init__(self, xprv)
        self.lnwatcher = None
        self.features |= LnFeatures.OPTION_DATA_LOSS_PROTECT_REQ
        self.features |= LnFeatures.OPTION_STATIC_REMOTEKEY_REQ
        self.payments = self.db.get_dict('lightning_payments')     # RHASH -> amount, direction, is_paid  # FIXME amt should be msat
        self.preimages = self.db.get_dict('lightning_preimages')   # RHASH -> preimage
        # note: this sweep_address is only used as fallback; as it might result in address-reuse
        self.sweep_address = wallet.get_new_sweep_address_for_channel()
        self.logs = defaultdict(list)  # type: Dict[str, List[PaymentAttemptLog]]  # key is RHASH  # (not persisted)
        self.is_routing = set()        # (not persisted) keys of invoices that are in PR_ROUTING state
        # used in tests
        self.enable_htlc_settle = asyncio.Event()
        self.enable_htlc_settle.set()

        # note: accessing channels (besides simple lookup) needs self.lock!
        self._channels = {}  # type: Dict[bytes, Channel]
        channels = self.db.get_dict(""channels"")
        for channel_id, c in random_shuffled_copy(channels.items()):
            self._channels[bfh(channel_id)] = Channel(c, sweep_address=self.sweep_address, lnworker=self)

        self.pending_payments = defaultdict(asyncio.Future)  # type: Dict[bytes, asyncio.Future[BarePaymentAttemptLog]]

        self.swap_manager = SwapManager(wallet=self.wallet, lnworker=self)

    @property
    def channels(self) -> Mapping[bytes, Channel]:
        """"""Returns a read-only copy of channels.""""""
        with self.lock:
            return self._channels.copy()

    def get_channel_by_id(self, channel_id: bytes) -> Optional[Channel]:
        return self._channels.get(channel_id, None)

    @ignore_exceptions
    @log_exceptions
    async def sync_with_local_watchtower(self):
        watchtower = self.network.local_watchtower
        if watchtower:
            while True:
                for chan in self.channels.values():
                    await self.sync_channel_with_watchtower(chan, watchtower.sweepstore)
                await asyncio.sleep(5)

    @ignore_exceptions
    @log_exceptions
    async def sync_with_remote_watchtower(self):
        while True:
            # periodically poll if the user updated 'watchtower_url'
            await asyncio.sleep(5)
            watchtower_url = self.config.get('watchtower_url')
            if not watchtower_url:
                continue
            parsed_url = urllib.parse.urlparse(watchtower_url)
            if not (parsed_url.scheme == 'https' or is_private_netaddress(parsed_url.hostname)):
                self.logger.warning(f""got watchtower URL for remote tower but we won't use it! ""
                                    f""can only use HTTPS (except if private IP): not using {watchtower_url!r}"")
                continue
            # try to sync with the remote watchtower
            try:
                async with make_aiohttp_session(proxy=self.network.proxy) as session:
                    watchtower = JsonRPCClient(session, watchtower_url)
                    watchtower.add_method('get_ctn')
                    watchtower.add_method('add_sweep_tx')
                    for chan in self.channels.values():
                        await self.sync_channel_with_watchtower(chan, watchtower)
            except aiohttp.client_exceptions.ClientConnectorError:
                self.logger.info(f'could not contact remote watchtower {watchtower_url}')

    async def sync_channel_with_watchtower(self, chan: Channel, watchtower):
        outpoint = chan.funding_outpoint.to_str()
        addr = chan.get_funding_address()
        current_ctn = chan.get_oldest_unrevoked_ctn(REMOTE)
        watchtower_ctn = await watchtower.get_ctn(outpoint, addr)
        for ctn in range(watchtower_ctn + 1, current_ctn):
            sweeptxs = chan.create_sweeptxs(ctn)
            for tx in sweeptxs:
                await watchtower.add_sweep_tx(outpoint, ctn, tx.inputs()[0].prevout.to_str(), tx.serialize())

    def start_network(self, network: 'Network'):
        assert network
        self.lnwatcher = LNWalletWatcher(self, network)
        self.lnwatcher.start_network(network)
        self.network = network
        self.swap_manager.start_network(network=network, lnwatcher=self.lnwatcher)

        for chan in self.channels.values():
            self.lnwatcher.add_channel(chan.funding_outpoint.to_str(), chan.get_funding_address())

        super().start_network(network)
        for coro in [
                self.maybe_listen(),
                self.lnwatcher.on_network_update('network_updated'), # shortcut (don't block) if funding tx locked and verified
                self.reestablish_peers_and_channels(),
                self.sync_with_local_watchtower(),
                self.sync_with_remote_watchtower(),
        ]:
            tg_coro = self.taskgroup.spawn(coro)
            asyncio.run_coroutine_threadsafe(tg_coro, self.network.asyncio_loop)

    def stop(self):
        super().stop()
        self.lnwatcher.stop()
        self.lnwatcher = None

    def peer_closed(self, peer):
        for chan in self.channels_for_peer(peer.pubkey).values():
            chan.peer_state = PeerState.DISCONNECTED
            util.trigger_callback('channel', self.wallet, chan)
        super().peer_closed(peer)

    def get_settled_payments(self):
        # return one item per payment_hash
        # note: with AMP we will have several channels per payment
        out = defaultdict(list)
        for chan in self.channels.values():
            d = chan.get_settled_payments()
            for k, v in d.items():
                out[k] += v
        return out

    def get_payment_value(self, info: Optional['PaymentInfo'], plist):
        amount_msat = 0
        fee_msat = None
        for chan_id, htlc, _direction in plist:
            amount_msat += int(_direction) * htlc.amount_msat
            if _direction == SENT and info and info.amount:
                fee_msat = (fee_msat or 0) - info.amount*1000 - amount_msat
        timestamp = min([htlc.timestamp for chan_id, htlc, _direction in plist])
        return amount_msat, fee_msat, timestamp

    def get_lightning_history(self):
        out = {}
        for key, plist in self.get_settled_payments().items():
            if len(plist) == 0:
                continue
            payment_hash = bytes.fromhex(key)
            info = self.get_payment_info(payment_hash)
            amount_msat, fee_msat, timestamp = self.get_payment_value(info, plist)
            if info is not None:
                label = self.wallet.get_label(key)
                direction = ('sent' if info.direction == SENT else 'received') if len(plist)==1 else 'self-payment'
            else:
                direction = 'forwarding'
                label = _('Forwarding')
            preimage = self.get_preimage(payment_hash).hex()
            item = {","this looks wrong -- `data_loaded` is an asyncio.Event, not a bool","The suggested code changes appear to be related to a Lightning Network implementation, likely part of a larger application. Here's a concise review of the changes:

### Key Changes:
1. **Peer Management**:
   - Added methods `peer_closed` and `num_peers`.
   - Implemented `_add_peers_from_config` to add peers from configuration.
   - Added `is_good_peer` method to filter peers based on feature bits.

2. **Network Management**:
   - Added `start_network` and `stop` methods for managing the network.
   - Used `asyncio.run_coroutine_threadsafe` to run coroutines safely in the main loop.

3. **Gossip Worker**:
   - Added `LNGossip` class with methods for maintaining the database and adding new IDs.
   - Implemented `get_ids_to_query` and `get_sync_progress_estimate`.

4. **Wallet Management**:
   - Added `LNWallet` class with methods for syncing with local and remote watchtowers.
   - Implemented `sync_channel_with_watchtower` to handle channel synchronization with a watchtower.
   - Added `start_network` and `stop` methods for managing the wallet.

### Review Points:
- **Thread Safety**: The use of `asyncio.run_coroutine_threadsafe` ensures that coroutines are executed safely in the main loop, which is important for maintaining thread safety in asynchronous applications.
- **Peer Management**: The addition of `peer_closed` and `num_peers` methods helps manage peer connections and provides a way to check the number of connected peers.
- **Network Management**: The `start_network` and `stop` methods ensure that the network is properly initialized and shut down.
- **Gossip Worker**: The `LNGossip` class provides functionality for maintaining the gossip database and estimating sync progress.
- **Wallet Management**: The `LNWallet` class handles synchronization with watchtowers and provides methods for managing payments and channels.

Overall, these changes enhance the robustness and functionality of the Lightning Network implementation by providing mechanisms for managing peers, network operations, and wallet interactions."
6,"@@ -0,0 +1,11 @@
+class MoveClientAdminRole < ActiveRecord::Migration
+  def change
+    role = Role.find_or_create_by(name: 'client_admin')
+
+    User.client_admin_emails.each do |email_address|
+      user = User.find_by_email_address(email_address) or next
+      user.add_role(role)
+      user.save!
+    end
+  end
+end","class MoveClientAdminRole < ActiveRecord::Migration
  def change
    role = Role.find_or_create_by(name: 'client_admin')

    User.client_admin_emails.each do |email_address|
      user = User.find_by_email_address(email_address) or next
      user.add_role(role)
      user.save!
    end
  end
end

","We should avoid using the `.client_admin_emails` method here, since it will presumably be removed and thus break this migration in the future.","The suggested code change is to add the `client_admin` role to all users with client admin emails. The code first finds or creates the `client_admin` role using `Role.find_or_create_by(name: 'client_admin')`. Then, it iterates over each email address in `User.client_admin_emails`, finds the corresponding user using `User.find_by_email_address(email_address)`, adds the `client_admin` role to the user using `user.add_role(role)`, and saves the user using `user.save!`. This ensures that all users with client admin emails have the `client_admin` role assigned to them."
7,"@@ -29,6 +29,7 @@ class ServiceProvider extends ModuleServiceProvider
         $this->registerComponents();
         $this->registerThemeLogging();
         $this->registerCombinerEvents();
+        $this->registerDatabaseLayerModels();
 
         /*
          * Backend specific","<?php namespace Cms;

use App;
use Event;
use Backend;
use BackendMenu;
use BackendAuth;
use Backend\Models\UserRole;
use Backend\Classes\WidgetManager;
use October\Rain\Support\ModuleServiceProvider;
use System\Classes\SettingsManager;
use Cms\Classes\ComponentManager;
use Cms\Classes\Page as CmsPage;
use Cms\Classes\CmsObject;
use Cms\Models\ThemeData;
use Cms\Models\ThemeLog;

class ServiceProvider extends ModuleServiceProvider
{
    /**
     * Register the service provider.
     *
     * @return void
     */
    public function register()
    {
        parent::register('cms');

        $this->registerDatabaseLayerModels();
        $this->registerComponents();
        $this->registerThemeLogging();
        $this->registerCombinerEvents();

        /*
         * Backend specific
         */
        if (App::runningInBackend()) {
            $this->registerBackendNavigation();
            $this->registerBackendReportWidgets();
            $this->registerBackendPermissions();
            $this->registerBackendWidgets();
            $this->registerBackendSettings();
        }
    }

    /**
     * Bootstrap the module events.
     *
     * @return void
     */
    public function boot()
    {
        parent::boot('cms');

        $this->bootMenuItemEvents();
        $this->bootRichEditorEvents();
    }

    /**
     * Register components.
     */
    protected function registerComponents()
    {
        ComponentManager::instance()->registerComponents(function ($manager) {
            $manager->registerComponent(\Cms\Components\ViewBag::class, 'viewBag');
            $manager->registerComponent(\Cms\Components\Resources::class, 'resources');
        });
    }

    /**
     * Registers theme logging on templates.
     */
    protected function registerThemeLogging()
    {
        CmsObject::extend(function ($model) {
            ThemeLog::bindEventsToModel($model);
        });
    }

    /**
     * Registers events for the asset combiner.
     */
    protected function registerCombinerEvents()
    {
        if (App::runningInBackend() || App::runningInConsole()) {
            return;
        }

        Event::listen('cms.combiner.beforePrepare', function ($combiner, $assets) {
            $filters = array_flatten($combiner->getFilters());
            ThemeData::applyAssetVariablesToCombinerFilters($filters);
        });

        Event::listen('cms.combiner.getCacheKey', function ($combiner, $holder) {
            $holder->key = $holder->key . ThemeData::getCombinerCacheKey();
        });
    }

    /*
     * Register navigation
     */
    protected function registerBackendNavigation()
    {
        BackendMenu::registerCallback(function ($manager) {
            $manager->registerMenuItems('October.Cms', [
                'cms' => [
                    'label'       => 'cms::lang.cms.menu_label',
                    'icon'        => 'icon-magic',
                    'iconSvg'     => 'modules/cms/assets/images/cms-icon.svg',
                    'url'         => Backend::url('cms'),
                    'order'       => 100,
                    'permissions' => [
                        'cms.manage_content',
                        'cms.manage_assets',
                        'cms.manage_pages',
                        'cms.manage_layouts',
                        'cms.manage_partials'
                    ],
                    'sideMenu' => [
                        'pages' => [
                            'label'        => 'cms::lang.page.menu_label',
                            'icon'         => 'icon-copy',
                            'url'          => 'javascript:;',
                            'attributes'   => ['data-menu-item' => 'pages'],
                            'permissions'  => ['cms.manage_pages'],
                            'counterLabel' => 'cms::lang.page.unsaved_label'
                        ],
                        'partials' => [
                            'label'        => 'cms::lang.partial.menu_label',
                            'icon'         => 'icon-tags',
                            'url'          => 'javascript:;',
                            'attributes'   => ['data-menu-item' => 'partials'],
                            'permissions'  => ['cms.manage_partials'],
                            'counterLabel' => 'cms::lang.partial.unsaved_label'
                        ],
                        'layouts' => [
                            'label'        => 'cms::lang.layout.menu_label',
                            'icon'         => 'icon-th-large',
                            'url'          => 'javascript:;',
                            'attributes'   => ['data-menu-item' => 'layouts'],
                            'permissions'  => ['cms.manage_layouts'],
                            'counterLabel' => 'cms::lang.layout.unsaved_label'
                        ],
                        'content' => [
                            'label'        => 'cms::lang.content.menu_label',
                            'icon'         => 'icon-file-text-o',
                            'url'          => 'javascript:;',
                            'attributes'   => ['data-menu-item' => 'content'],
                            'permissions'  => ['cms.manage_content'],
                            'counterLabel' => 'cms::lang.content.unsaved_label'
                        ],
                        'assets' => [
                            'label'        => 'cms::lang.asset.menu_label',
                            'icon'         => 'icon-picture-o',
                            'url'          => 'javascript:;',
                            'attributes'   => ['data-menu-item' => 'assets'],
                            'permissions'  => ['cms.manage_assets'],
                            'counterLabel' => 'cms::lang.asset.unsaved_label'
                        ],
                        'components' => [
                            'label'       => 'cms::lang.component.menu_label',
                            'icon'        => 'icon-puzzle-piece',
                            'url'         => 'javascript:;',
                            'attributes'  => ['data-menu-item' => 'components'],
                            'permissions' => ['cms.manage_pages', 'cms.manage_layouts', 'cms.manage_partials']
                        ]
                    ]
                ]
            ]);
        });
    }

    /*
     * Register report widgets
     */
    protected function registerBackendReportWidgets()
    {
        WidgetManager::instance()->registerReportWidgets(function ($manager) {
            $manager->registerReportWidget(\Cms\ReportWidgets\ActiveTheme::class, [
                'label'   => 'cms::lang.dashboard.active_theme.widget_title_default',
                'context' => 'dashboard'
            ]);
        });
    }

    /*
     * Register permissions
     */
    protected function registerBackendPermissions()
    {
        BackendAuth::registerCallback(function ($manager) {
            $manager->registerPermissions('October.Cms', [
                'cms.manage_content' => [
                    'label' => 'cms::lang.permissions.manage_content',
                    'tab' => 'cms::lang.permissions.name',
                    'roles' => UserRole::CODE_DEVELOPER,
                    'order' => 100
                ],
                'cms.manage_assets' => [
                    'label' => 'cms::lang.permissions.manage_assets',
                    'tab' => 'cms::lang.permissions.name',
                    'roles' => UserRole::CODE_DEVELOPER,
                    'order' => 100
                ],
                'cms.manage_pages' => [
                    'label' => 'cms::lang.permissions.manage_pages',
                    'tab' => 'cms::lang.permissions.name',
                    'roles' => UserRole::CODE_DEVELOPER,
                    'order' => 100
                ],
                'cms.manage_layouts' => [
                    'label' => 'cms::lang.permissions.manage_layouts',
                    'tab' => 'cms::lang.permissions.name',
                    'roles' => UserRole::CODE_DEVELOPER,
                    'order' => 100
                ],
                'cms.manage_partials' => [
                    'label' => 'cms::lang.permissions.manage_partials',
                    'tab' => 'cms::lang.permissions.name',
                    'roles' => UserRole::CODE_DEVELOPER,
                    'order' => 100
                ],
                'cms.manage_themes' => [
                    'label' => 'cms::lang.permissions.manage_themes',
                    'tab' => 'cms::lang.permissions.name',
                    'roles' => UserRole::CODE_DEVELOPER,
                    'order' => 100
                ],
                'cms.manage_theme_options' => [",Can you rename this to `registerHalcyonModels` @bennothommo ?,"The provided code is a service provider for the CMS module in OctoberCMS, which is a popular open-source content management system. The service provider registers various components, events, and permissions related to the CMS functionality.

### Key Points:

1. **Database Layer Models**: Registers models used by the CMS, such as `CmsPage` and `CmsObject`.

2. **Components**: Registers components like `ViewBag` and `Resources` for handling view bags and resources in the CMS.

3. **Theme Logging**: Binds theme logging events to CMS objects to track changes in themes.

4. **Asset Combiner Events**: Listens for events related to the asset combiner to apply theme variables and generate cache keys.

5. **Backend Navigation**: Registers backend menu items for managing pages, partials, layouts, content, assets, and components.

6. **Report Widgets**: Registers a report widget for displaying the active theme on the dashboard.

7. **Permissions**: Registers permissions for managing content, assets, pages, layouts, partials, themes, and theme options.

### Review:

- **Consistency**: The code is well-organized with clear separation of concerns, making it easy to understand and maintain.
- **Comments**: There are comments explaining the purpose of each method and section, which helps in understanding the flow of the code.
- **Event Listeners**: The event listeners are correctly registered and handle the expected events, ensuring that the CMS behaves as expected.
- **Permissions**: The permissions are registered with appropriate roles and order, which ensures that only authorized users can perform certain actions.

Overall, the service provider is well-structured and performs its intended functions effectively."
8,"@@ -37,6 +37,15 @@ public final class Const {
 
     public static final String V4_PREFIX = String.format(""/v4/%s/registry"", DOMAIN_NAME);
 
+    public static final String REGISTRY_VERSION;
+    static {
+      if (VERSION_V3.equals(CURRENT_VERSION)) {
+        REGISTRY_VERSION = ""3.0.0""; // old is strange
+      } else {
+        REGISTRY_VERSION = ""0.4.0"";
+      }
+    }
+
     public static final String MICROSERVICE_OPERATION_ALL;
     static {
       if (VERSION_V3.equals(CURRENT_VERSION)) {","/*
 * Copyright 2017 Huawei Technologies Co., Ltd
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package io.servicecomb.serviceregistry.api;

import io.servicecomb.serviceregistry.config.ServiceRegistryConfig;

/**
 * Created by   on 2017/1/9.
 */
public final class Const {
  private Const() {
  }

  public static final class REGISTRY_API {
    public static final String DOMAIN_NAME = ServiceRegistryConfig.INSTANCE.getDomainName();

    public static final String CURRENT_VERSION = ServiceRegistryConfig.INSTANCE.getRegistryApiVersion();

    // 2017-10-21 add new implementations for v4. We can remove v3 support after a period. 
    public static final String VERSION_V3 = ""v3"";

    public static final String LASTEST_API_VERSION = ""v4"";
    public static final String REGISTRY_VERSION;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        REGISTRY_VERSION = ""3.0.0""; // old is strange
      } else {
        REGISTRY_VERSION = ""0.4.0"";
      }
    }


    public static final String V4_PREFIX = String.format(""/v4/%s/registry"", DOMAIN_NAME);

    public static final String MICROSERVICE_OPERATION_ALL;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_OPERATION_ALL = ""/registry/v3/microservices"";
      } else {
        MICROSERVICE_OPERATION_ALL = V4_PREFIX + ""/microservices"";
      }
    }

    public static final String MICROSERVICE_OPERATION_ONE;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_OPERATION_ONE = ""/registry/v3/microservices/%s"";
      } else {
        MICROSERVICE_OPERATION_ONE = V4_PREFIX + ""/microservices/%s"";
      }
    }

    public static final String MICROSERVICE_INSTANCE_OPERATION_ALL;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_INSTANCE_OPERATION_ALL = ""/registry/v3/microservices/%s/instances"";
      } else {
        MICROSERVICE_INSTANCE_OPERATION_ALL = V4_PREFIX + ""/microservices/%s/instances"";
      }
    }

    public static final String MICROSERVICE_INSTANCE_OPERATION_ONE;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_INSTANCE_OPERATION_ONE = ""/registry/v3/microservices/%s/instances/%s"";
      } else {
        MICROSERVICE_INSTANCE_OPERATION_ONE = V4_PREFIX + ""/microservices/%s/instances/%s"";
      }
    }

    public static final String MICROSERVICE_INSTANCES;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_INSTANCES = ""/registry/v3/instances"";
      } else {
        MICROSERVICE_INSTANCES = V4_PREFIX + ""/instances"";
      }
    }

    public static final String MICROSERVICE_PROPERTIES;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_PROPERTIES = ""/registry/v3/microservices/%s/properties"";
      } else {
        MICROSERVICE_PROPERTIES = V4_PREFIX + ""/microservices/%s/properties"";
      }
    }

    public static final String MICROSERVICE_INSTANCE_PROPERTIES;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_INSTANCE_PROPERTIES = ""/registry/v3/microservices/%s/instances/%s/properties"";
      } else {
        MICROSERVICE_INSTANCE_PROPERTIES = V4_PREFIX + ""/microservices/%s/instances/%s/properties"";
      }
    }

    public static final String MICROSERVICE_HEARTBEAT;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_HEARTBEAT = ""/registry/v3/microservices/%s/instances/%s/heartbeat"";
      } else {
        MICROSERVICE_HEARTBEAT = V4_PREFIX + ""/microservices/%s/instances/%s/heartbeat"";
      }
    }

    public static final String MICROSERVICE_EXISTENCE;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_EXISTENCE = ""/registry/v3/existence"";
      } else {
        MICROSERVICE_EXISTENCE = V4_PREFIX + ""/existence"";
      }
    }

    public static final String MICROSERVICE_SCHEMA;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_SCHEMA = ""/registry/v3/microservices/%s/schemas/%s"";
      } else {
        MICROSERVICE_SCHEMA = V4_PREFIX + ""/microservices/%s/schemas/%s"";
      }
    }

    public static final String MICROSERVICE_WATCH;
    static {
      if (VERSION_V3.equals(CURRENT_VERSION)) {
        MICROSERVICE_WATCH = ""/registry/v3/microservices/%s/watcher"";
      } else {
        MICROSERVICE_WATCH = V4_PREFIX + ""/microservices/%s/watcher"";
      }
    }

  }

  public static final String REGISTRY_APP_ID = ""default"";

  public static final String REGISTRY_SERVICE_NAME = ""SERVICECENTER"";

  public static final String REGISTRY_VERSION = ""3.0.0"";

  public static final String APP_SERVICE_SEPARATOR = "":"";

  public static final String PATH_CHECKSESSION = ""checksession"";

  public static final String URL_PREFIX = ""urlPrefix"";
}
","I think we just need keep tracking the API version instead of the service center version. For Java chassis, it only knows about the service center version.","The suggested code change appears to be updating the `Const` class to include constants for the new API version ""v4"" and corresponding endpoints. The changes involve modifying the `REGISTRY_API` class to include new fields for the v4 API versions and updating the existing fields to use the new prefix. The `REGISTRY_VERSION` field is also updated to reflect the new version. Overall, the changes appear to be consistent with the overall structure and naming conventions of the codebase."
9,"@@ -72,6 +72,13 @@ public enum Platform {
     }
   },
 
+  WIN10(""windows 10"", ""win10"") {
+    @Override
+    public Platform family() {
+      return WINDOWS;
+    }
+  },
+
   MAC(""mac"", ""darwin"", ""os x"") {},
 
   SNOW_LEOPARD(""snow leopard"", ""os x 10.6"") {","// Licensed to the Software Freedom Conservancy (SFC) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The SFC licenses this file
// to you under the Apache License, Version 2.0 (the
// ""License""); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

package org.openqa.selenium;

import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * Represents the known and supported Platforms that WebDriver runs on. This is pretty close to the
 * Operating System, but differs slightly, because this class is used to extract information such as
 * program locations and line endings.
 */
// Useful URLs:
// http://hg.openjdk.java.net/jdk7/modules/jdk/file/a37326fa7f95/src/windows/native/java/lang/java_props_md.c
public enum Platform {

  /**
   * Never returned, but can be used to request a browser running on any version of Windows.
   */
  WINDOWS("""") {},

  /**
   * For versions of Windows that ""feel like"" Windows XP. These are ones that store files in
   * ""\Program Files\"" and documents under ""\\documents and settings\\username""
   */
  XP(""Windows Server 2003"", ""xp"", ""windows"", ""winnt"") {
    @Override
    public Platform family() {
      return WINDOWS;
    }
  },

  /**
   * For versions of Windows that ""feel like"" Windows Vista.
   */
  VISTA(""windows vista"", ""Windows Server 2008"", ""windows 7"", ""win7"") {
    @Override
    public Platform family() {
      return WINDOWS;
    }
  },

  /**
   * For versions of Windows that ""feel like"" Windows 8.
   */
  WIN8(""Windows Server 2012"", ""windows 8"", ""win8"") {
    @Override
    public Platform family() {
      return WINDOWS;
    }
  },

  WIN8_1(""windows 8.1"", ""win8.1"") {
    @Override
    public Platform family() {
      return WINDOWS;
  WIN10(""windows 10"", ""win10"") {
    @Override
    public Platform family() {
      return WINDOWS;
    }
  },

    }
  },

  MAC(""mac"", ""darwin"", ""os x"") {},

  SNOW_LEOPARD(""snow leopard"", ""os x 10.6"") {
    @Override
    public Platform family() {
      return MAC;
    }
    @Override
    public String toString() {
      return ""OS X 10.6"";
    }
  },

  MOUNTAIN_LION(""mountain lion"", ""os x 10.8"") {
    @Override
    public Platform family() {
      return MAC;
    }
    @Override
    public String toString() {
      return ""OS X 10.8"";
    }
  },

  MAVERICKS(""mavericks"", ""os x 10.9"") {
    @Override
    public Platform family() {
      return MAC;
    }
    @Override
    public String toString() {
      return ""OS X 10.9"";
    }
  },

  YOSEMITE(""yosemite"", ""os x 10.10"") {
    @Override
    public Platform family() {
      return MAC;
    }
    @Override
    public String toString() {
      return ""OS X 10.10"";
    }
  },

  /**
   * Many platforms have UNIX traits, amongst them LINUX, Solaris and BSD.
   */
  UNIX(""solaris"", ""bsd"") {},

  LINUX(""linux"") {
    @Override
    public Platform family() {
      return UNIX;
    }
  },

  ANDROID(""android"", ""dalvik"") {
    public String getLineEnding() {
      return ""\n"";
    }

    @Override
    public Platform family() {
      return LINUX;
    }
  },

  /**
   * Never returned, but can be used to request a browser running on any operating system.
   */
  ANY("""") {
    @Override
    public boolean is(Platform compareWith) {
      return this == compareWith;
    }
  };

  private final String[] partOfOsName;
  private final int minorVersion;
  private final int majorVersion;

  private Platform(String... partOfOsName) {
    this.partOfOsName = partOfOsName;

    String version = System.getProperty(""os.version"", ""0.0.0"");
    int major = 0;
    int min = 0;

    Pattern pattern = Pattern.compile(""^(\\d+)\\.(\\d+).*"");
    Matcher matcher = pattern.matcher(version);
    if (matcher.matches()) {
      try {
        major = Integer.parseInt(matcher.group(1));
        min = Integer.parseInt(matcher.group(2));
      } catch (NumberFormatException e) {
        // These things happen
      }
    }

    majorVersion = major;
    minorVersion = min;
  }

  public String[] getPartOfOsName() {
    return partOfOsName;
  }

  /**
   * Get current platform (not necessarily the same as operating system).
   *
   * @return current platform
   */
  public static Platform getCurrent() {
    return extractFromSysProperty(System.getProperty(""os.name""));
  }

  /**
   * Extracts platforms based on system properties in Java and uses a heuristic to determine the
   * most likely operating system.  If unable to determine the operating system, it will default to
   * UNIX.
   *
   * @param osName the operating system name to determine the platform of
   * @return the most likely platform based on given operating system name
   */
  public static Platform extractFromSysProperty(String osName) {
    return extractFromSysProperty(osName, System.getProperty(""os.version""));
  }

  /**
   * Extracts platforms based on system properties in Java and uses a heuristic to determine the
   * most likely operating system.  If unable to determine the operating system, it will default to
   * UNIX.
   *
   * @param osName the operating system name to determine the platform of
   * @param osVersion the operating system version to determine the platform of
   * @return the most likely platform based on given operating system name and version
   */
  public static Platform extractFromSysProperty(String osName, String osVersion) {
    osName = osName.toLowerCase();
    // os.name for android is linux
    if (""dalvik"".equalsIgnoreCase(System.getProperty(""java.vm.name""))) {
      return Platform.ANDROID;
    }
    // Windows 8 can't be detected by osName alone
    if (osVersion.equals(""6.2"") && osName.startsWith(""windows nt"")) {
        return WIN8;
    }
    // Windows 8 can't be detected by osName alone
    if (osVersion.equals(""6.3"") && osName.startsWith(""windows nt"")) {
        return WIN8_1;
    }
    Platform mostLikely = UNIX;
    String previousMatch = null;
    for (Platform os : Platform.values()) {
      for (String matcher : os.partOfOsName) {
        if ("""".equals(matcher)) {
          continue;
        }
        matcher = matcher.toLowerCase();
        if (os.isExactMatch(osName, matcher)) {
          return os;
        }
        if (os.isCurrentPlatform(osName, matcher) && isBetterMatch(previousMatch, matcher)) {
          previousMatch = matcher;
          mostLikely = os;
        }
      }
    }

    // Default to assuming we're on a UNIX variant (including LINUX)
    return mostLikely;
  }

  /**
   * Gets a platform with the name matching the parameter.
   *
   * @param name the platform name
   * @return the Platform enum value matching the parameter
   */
  public static Platform fromString(String name) {
    try {
      return Platform.valueOf(name);
    } catch (IllegalArgumentException ex) {
      for (Platform os : Platform.values()) {
        for (String matcher : os.partOfOsName) {
          if (name.toLowerCase().equals(matcher.toLowerCase())) {
            return os;
          }
        }
      }
      throw new WebDriverException(""Unrecognized platform: "" + name);
    }
  }

  /**","we'll also have to think about adding ""Windows Server 2014"" or whatever they come up with later.","The suggested code change appears to be a Java enumeration representing different operating systems and their versions. Here's a concise review:

### Key Points:
1. **Enum Definition**: The `Platform` enum contains various constants representing different operating systems and their versions.
2. **Constructor**: Each constant has a constructor that extracts the OS version from the system property `os.version`.
3. **Heuristic Matching**: The `extractFromSysProperty` method uses heuristics to determine the most likely operating system based on the `os.name` and `os.version`.
4. **Methods**:
   - `getCurrent()`: Returns the current platform.
   - `extractFromSysProperty(String osName)`: Extracts the platform based on the `os.name`.
   - `extractFromSysProperty(String osName, String osVersion)`: Extracts the platform based on both `os.name` and `os.version`.
   - `fromString(String name)`: Converts a string to a `Platform` enum value.

### Potential Improvements:
1. **Documentation**: Add more detailed documentation to explain the purpose and usage of each method.
2. **Error Handling**: Improve error handling in the `extractFromSysProperty` method to handle cases where the OS version cannot be parsed.
3. **Consistency**: Ensure consistency in the naming conventions and structure of the enum constants.
4. **Testing**: Add unit tests to verify the correctness of the platform extraction logic.

### Example Review:
```java
/**
 * Represents the known and supported Platforms that WebDriver runs on. This is pretty close to the
 * Operating System, but differs slightly, because this class is used to extract information such as
 * program locations and line endings.
 */
public enum Platform {

  /**
   * Never returned, but can be used to request a browser running on any version of Windows.
   */
  WINDOWS(""""), {}

  // ... other constants ...

  /**
   * Extracts platforms based on system properties in Java and uses a heuristic to determine the
   * most likely operating system.  If unable to determine the operating system, it will default to
   * UNIX.
   *
   * @param osName the operating system name to determine the platform of
   * @param osVersion the operating system version to determine the platform of
   * @return the most likely platform based on given operating system name and version
   */
  public static Platform extractFromSysProperty(String osName, String osVersion) {
    // ... existing implementation ...
  }

  // ..."
10,"@@ -29,7 +29,7 @@ import (
 type Protocol interface {
 	// RunStateTransition returns the state root CID resulting from applying the input ts to the
 	// prior `stateID`.  It returns an error if the transition is invalid.
-	RunStateTransition(ctx context.Context, ts block.TipSet, blsMsgs [][]*types.UnsignedMessage, secpMsgs [][]*types.SignedMessage, ancestors []block.TipSet, parentWeight fbig.Int, parentStateRoot cid.Cid, parentReceiptRoot cid.Cid) (cid.Cid, []vm.MessageReceipt, error)
+	RunStateTransition(ctx context.Context, ts block.TipSet, blsMsgs [][]*types.UnsignedMessage, secpMsgs [][]*types.SignedMessage, parentWeight fbig.Int, parentStateRoot cid.Cid, parentReceiptRoot cid.Cid) (cid.Cid, []vm.MessageReceipt, error)
 
 	// BlockTime returns the block time used by the consensus protocol.
 	BlockTime() time.Duration","package consensus

// This interface is (mostly) stateless.  All of its methods are
// pure functions that only depend on their inputs.

// Note: State does creep in through the cbor and block stores used to keep state tree and
// actor storage data in the Expected implementation.  However those stores
// are global to the filecoin node so accessing the correct state is simple.
// Furthermore these stores are providing content addressed values.
// The output of these interface functions does not change based on the store state
// except for errors in the case the stores do not have a mapping.
import (
	""context""
	""time""

	""github.com/filecoin-project/go-filecoin/internal/pkg/block""
	""github.com/filecoin-project/go-filecoin/internal/pkg/vm""
	fbig ""github.com/filecoin-project/specs-actors/actors/abi/big""
	""github.com/ipfs/go-cid""

	""github.com/filecoin-project/go-filecoin/internal/pkg/types""
)

// Protocol is an interface defining a blockchain consensus protocol.  The
// methods here were arrived at after significant work fitting consensus into
// the system and the implementation level. The method set is not necessarily
// the most theoretically obvious or pleasing and should not be considered
// finalized.
	RunStateTransition(ctx context.Context, ts block.TipSet, blsMsgs [][]*types.UnsignedMessage, secpMsgs [][]*types.SignedMessage, parentWeight fbig.Int, parentStateRoot cid.Cid, parentReceiptRoot cid.Cid) (cid.Cid, []vm.MessageReceipt, error)
type Protocol interface {
	// RunStateTransition returns the state root CID resulting from applying the input ts to the
	// prior `stateID`.  It returns an error if the transition is invalid.
	RunStateTransition(ctx context.Context, ts block.TipSet, blsMsgs [][]*types.UnsignedMessage, secpMsgs [][]*types.SignedMessage, ancestors []block.TipSet, parentWeight fbig.Int, parentStateRoot cid.Cid, parentReceiptRoot cid.Cid) (cid.Cid, []vm.MessageReceipt, error)

	// BlockTime returns the block time used by the consensus protocol.
	BlockTime() time.Duration
}
",FYI I've been meaning to delete this forever. In the future you can get rid of this instead of fixing.,"The suggested code change introduces a new method `BlockTime` to the `Protocol` interface, which returns the block time used by the consensus protocol. This addition enhances the interface's functionality by providing a way to query the block time directly, which can be useful for various purposes such as scheduling tasks or adjusting the behavior of the consensus algorithm based on the block time.

### Review:
- **New Method**: The addition of `BlockTime` is a logical enhancement to the `Protocol` interface, providing a clear and concise way to access the block time.
- **Consistency**: The method signature remains consistent with the existing methods, maintaining the interface's integrity.
- **Use Case**: The method can be used by other parts of the system to determine the block time, which is crucial for scheduling tasks or implementing logic that depends on the block time.
- **Documentation**: The method is well-documented, explaining its purpose and usage, which is important for developers using the interface.

Overall, this change improves the usability and flexibility of the `Protocol` interface by adding a new method that provides essential information about the consensus protocol's block time."
11,"@@ -78,6 +78,15 @@ public class CsrfWebFilter implements WebFilter {
 		this.csrfTokenRepository = csrfTokenRepository;
 	}
 
+	/**
+	 * Gets the {@link ServerCsrfTokenRepository}
+	 * @return The {@link ServerCsrfTokenRepository}
+	 * @since 5.1
+	 */
+	public ServerCsrfTokenRepository getCsrfTokenRepository() {
+		return this.csrfTokenRepository;
+	}
+
 	public void setRequireCsrfProtectionMatcher(
 		ServerWebExchangeMatcher requireCsrfProtectionMatcher) {
 		Assert.notNull(requireCsrfProtectionMatcher, ""requireCsrfProtectionMatcher cannot be null"");","/*
 * Copyright 2002-2017 the original author or authors.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.springframework.security.web.server.csrf;

import java.util.Arrays;
import java.util.HashSet;
import java.util.Set;

import reactor.core.publisher.Mono;

import org.springframework.http.HttpMethod;
import org.springframework.http.HttpStatus;
import org.springframework.security.web.server.authorization.HttpStatusServerAccessDeniedHandler;
import org.springframework.security.web.server.authorization.ServerAccessDeniedHandler;
import org.springframework.security.web.server.util.matcher.ServerWebExchangeMatcher;
import org.springframework.util.Assert;
import org.springframework.web.server.ServerWebExchange;
import org.springframework.web.server.WebFilter;
import org.springframework.web.server.WebFilterChain;

/**
 * <p>
 * Applies
 * <a href=""https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF)"" >CSRF</a>
 * protection using a synchronizer token pattern. Developers are required to ensure that
 * {@link CsrfWebFilter} is invoked for any request that allows state to change. Typically
 * this just means that they should ensure their web application follows proper REST
 * semantics (i.e. do not change state with the HTTP methods GET, HEAD, TRACE, OPTIONS).
 * </p>
 *
 * <p>
 * Typically the {@link ServerCsrfTokenRepository} implementation chooses to store the
 * {@link CsrfToken} in {@link org.springframework.web.server.WebSession} with
 * {@link WebSessionServerCsrfTokenRepository}. This is preferred to storing the token in
 * a cookie which can be modified by a client application.
 * </p>
 * <p>
 * The {@code Mono&lt;CsrfToken&gt;} is exposes as a request attribute with the name of
 * {@code CsrfToken.class.getName()}. If the token is new it will automatically be saved
 * at the time it is subscribed.
 * </p>
 *
 * @author Rob Winch
 * @since 5.0
 */
public class CsrfWebFilter implements WebFilter {
	public static final ServerWebExchangeMatcher DEFAULT_CSRF_MATCHER = new DefaultRequireCsrfProtectionMatcher();

	private ServerWebExchangeMatcher requireCsrfProtectionMatcher = DEFAULT_CSRF_MATCHER;

	private ServerCsrfTokenRepository csrfTokenRepository = new WebSessionServerCsrfTokenRepository();

	private ServerAccessDeniedHandler accessDeniedHandler = new HttpStatusServerAccessDeniedHandler(HttpStatus.FORBIDDEN);

	public void setAccessDeniedHandler(
		ServerAccessDeniedHandler accessDeniedHandler) {
		Assert.notNull(accessDeniedHandler, ""accessDeniedHandler"");
		this.accessDeniedHandler = accessDeniedHandler;
	}

	public void setCsrfTokenRepository(
		ServerCsrfTokenRepository csrfTokenRepository) {
		Assert.notNull(csrfTokenRepository, ""csrfTokenRepository cannot be null"");
	/**
	 * Gets the {@link ServerCsrfTokenRepository}
	 * @return The {@link ServerCsrfTokenRepository}
	 * @since 5.1
	 */
	public ServerCsrfTokenRepository getCsrfTokenRepository() {
		return this.csrfTokenRepository;
	}

		this.csrfTokenRepository = csrfTokenRepository;
	}

	public void setRequireCsrfProtectionMatcher(
		ServerWebExchangeMatcher requireCsrfProtectionMatcher) {
		Assert.notNull(requireCsrfProtectionMatcher, ""requireCsrfProtectionMatcher cannot be null"");
		this.requireCsrfProtectionMatcher = requireCsrfProtectionMatcher;
	}

	@Override
	public Mono<Void> filter(ServerWebExchange exchange, WebFilterChain chain) {
		return this.requireCsrfProtectionMatcher.matches(exchange)
			.filter( matchResult -> matchResult.isMatch())
			.filter( matchResult -> !exchange.getAttributes().containsKey(CsrfToken.class.getName()))
			.flatMap(m -> validateToken(exchange))
			.flatMap(m -> continueFilterChain(exchange, chain))
			.switchIfEmpty(continueFilterChain(exchange, chain).then(Mono.empty()))
			.onErrorResume(CsrfException.class, e -> this.accessDeniedHandler
				.handle(exchange, e));
	}

	private Mono<Void> validateToken(ServerWebExchange exchange) {
		return this.csrfTokenRepository.loadToken(exchange)
			.switchIfEmpty(Mono.defer(() -> Mono.error(new CsrfException(""CSRF Token has been associated to this client""))))
			.filterWhen(expected -> containsValidCsrfToken(exchange, expected))
			.switchIfEmpty(Mono.defer(() -> Mono.error(new CsrfException(""Invalid CSRF Token""))))
			.then();
	}

	private Mono<Boolean> containsValidCsrfToken(ServerWebExchange exchange, CsrfToken expected) {
		return exchange.getFormData()
			.flatMap(data -> Mono.justOrEmpty(data.getFirst(expected.getParameterName())))
			.switchIfEmpty(Mono.justOrEmpty(exchange.getRequest().getHeaders().getFirst(expected.getHeaderName())))
			.map(actual -> actual.equals(expected.getToken()));
	}

	private Mono<Void> continueFilterChain(ServerWebExchange exchange, WebFilterChain chain) {
		return Mono.defer(() ->{
			Mono<CsrfToken> csrfToken = csrfToken(exchange);
			exchange.getAttributes().put(CsrfToken.class.getName(), csrfToken);
			return chain.filter(exchange);
		});
	}

	private Mono<CsrfToken> csrfToken(ServerWebExchange exchange) {
		return this.csrfTokenRepository.loadToken(exchange)
			.switchIfEmpty(generateToken(exchange));
	}

	private Mono<CsrfToken> generateToken(ServerWebExchange exchange) {
		return this.csrfTokenRepository.generateToken(exchange)
			.delayUntil(token -> this.csrfTokenRepository.saveToken(exchange, token));
	}

	private static class DefaultRequireCsrfProtectionMatcher implements ServerWebExchangeMatcher {
		private static final Set<HttpMethod> ALLOWED_METHODS = new HashSet<>(
			Arrays.asList(HttpMethod.GET, HttpMethod.HEAD, HttpMethod.TRACE, HttpMethod.OPTIONS));

		@Override
		public Mono<MatchResult> matches(ServerWebExchange exchange) {
			return Mono.just(exchange.getRequest())
				.map(r -> r.getMethod())
				.filter(m -> ALLOWED_METHODS.contains(m))
				.flatMap(m -> MatchResult.notMatch())
				.switchIfEmpty(MatchResult.match());
		}
	}
}
",We should not add a getter as it breaks encapsulation,"The suggested code change appears to be a complete implementation of a CSRF (Cross-Site Request Forgery) protection filter for Spring Security WebFlux applications. Here's a concise review:

### Key Points:
1. **Class and Package**: The class `CsrfWebFilter` is part of the `org.springframework.security.web.server.csrf` package, indicating its role in handling CSRF protection.

2. **Dependencies**: The class imports necessary classes from Spring Security WebFlux, including `Mono`, `ServerWebExchange`, `WebFilter`, `WebFilterChain`, `HttpStatusServerAccessDeniedHandler`, `DefaultRequireCsrfProtectionMatcher`, and `WebSessionServerCsrfTokenRepository`.

3. **Fields**:
   - `requireCsrfProtectionMatcher`: A matcher that determines if CSRF protection is needed for a given request.
   - `csrfTokenRepository`: The repository used to store and retrieve CSRF tokens.
   - `accessDeniedHandler`: Handles access denied scenarios.

4. **Methods**:
   - `filter`: The main method that processes each request. It checks if CSRF protection is required, validates the token, and continues the filter chain or handles access denied errors.
   - `validateToken`: Loads the CSRF token and checks if it is valid.
   - `containsValidCsrfToken`: Checks if the provided token matches the one stored in the request or session.
   - `continueFilterChain`: Saves the CSRF token in the request attributes and continues the filter chain.
   - `csrfToken`: Retrieves the CSRF token from the repository or generates a new one if none exists.
   - `generateToken`: Generates a new CSRF token and saves it to the repository.

5. **Static Class**: `DefaultRequireCsrfProtectionMatcher` is a simple matcher that allows certain HTTP methods (GET, HEAD, TRACE, OPTIONS) without CSRF protection.

### Summary:
This implementation provides a robust CSRF protection mechanism for Spring Security WebFlux applications. It ensures that only requests that allow state changes are protected, uses a session-based token storage, and provides a custom access denied handler. The code is well-structured and includes comments for clarity."
12,"@@ -166,14 +166,14 @@ class ProductRepositoryTest extends DatabaseTestCase
         $product1 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . 1);
         $product2 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . 45);
 
-        $this->setProductOrderingPriority($product1, 0);
-        $this->setProductOrderingPriority($product2, 1);
+        $this->setProductOrderingPriority($product1, 2);
+        $this->setProductOrderingPriority($product2, 3);
 
         $results = $this->getProductsForSearchOrderedByPriority('sencor');
         $this->assertSame($product2, $results[0]);
         $this->assertSame($product1, $results[1]);
 
-        $this->setProductOrderingPriority($product2, -1);
+        $this->setProductOrderingPriority($product2, 1);
 
         $results = $this->getProductsForSearchOrderedByPriority('sencor');
         $this->assertSame($product1, $results[0]);","<?php

namespace Tests\ShopBundle\Database\Model\Product;

use Shopsys\FrameworkBundle\Component\Domain\Domain;
use Shopsys\FrameworkBundle\DataFixtures\Demo\CategoryDataFixture;
use Shopsys\FrameworkBundle\DataFixtures\Demo\PricingGroupDataFixture;
use Shopsys\FrameworkBundle\DataFixtures\Demo\ProductDataFixture;
use Shopsys\FrameworkBundle\Model\Category\Category;
use Shopsys\FrameworkBundle\Model\Product\Filter\ProductFilterData;
use Shopsys\FrameworkBundle\Model\Product\Listing\ProductListOrderingModeService;
use Shopsys\FrameworkBundle\Model\Product\Product;
use Shopsys\FrameworkBundle\Model\Product\ProductDataFactory;
use Shopsys\FrameworkBundle\Model\Product\ProductFacade;
use Shopsys\FrameworkBundle\Model\Product\ProductRepository;
use Tests\ShopBundle\Test\DatabaseTestCase;

class ProductRepositoryTest extends DatabaseTestCase
{
    public function testVisibleAndNotSellingDeniedProductIsListed()
    {
        $this->getAllListableQueryBuilderTest(1, true);
    }

    public function testVisibleAndSellingDeniedProductIsNotListed()
    {
        $this->getAllListableQueryBuilderTest(6, false);
    }

    public function testProductVariantIsNotListed()
    {
        $this->getAllListableQueryBuilderTest(53, false);
    }

    public function testProductMainVariantIsListed()
    {
        $this->getAllListableQueryBuilderTest(148, true);
    }

    private function getAllListableQueryBuilderTest($productReferenceId, $isExpectedInResult)
    {
        $productRepository = $this->getContainer()->get(ProductRepository::class);
        /* @var $productRepository \Shopsys\FrameworkBundle\Model\Product\ProductRepository */
        $pricingGroup = $this->getReference(PricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1);
        /* @var $pricingGroup \Shopsys\FrameworkBundle\Model\Pricing\Group\PricingGroup */

        $domain = $this->getContainer()->get(Domain::class);
        /* @var $domain \Shopsys\FrameworkBundle\Component\Domain\Domain */

        $product = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . $productReferenceId);
        $productId = $product->getId();

        $queryBuilder = $productRepository->getAllListableQueryBuilder($domain->getId(), $pricingGroup);
        $queryBuilder->andWhere('p.id = :id')
            ->setParameter('id', $productId);
        $result = $queryBuilder->getQuery()->execute();

        $this->assertSame(in_array($product, $result, true), $isExpectedInResult);
    }

    public function testVisibleAndNotSellingDeniedProductIsSellable()
    {
        $this->getAllSellableQueryBuilderTest(1, true);
    }

    public function testVisibleAndSellingDeniedProductIsNotSellable()
    {
        $this->getAllSellableQueryBuilderTest(6, false);
    }

    public function testProductVariantIsSellable()
    {
        $this->getAllSellableQueryBuilderTest(53, true);
    }

    public function testProductMainVariantIsNotSellable()
    {
        $this->getAllSellableQueryBuilderTest(148, false);
    }

    private function getAllSellableQueryBuilderTest($productReferenceId, $isExpectedInResult)
    {
        $productRepository = $this->getContainer()->get(ProductRepository::class);
        /* @var $productRepository \Shopsys\FrameworkBundle\Model\Product\ProductRepository */
        $pricingGroup = $this->getReference(PricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1);
        /* @var $pricingGroup \Shopsys\FrameworkBundle\Model\Pricing\Group\PricingGroup */

        $domain = $this->getContainer()->get(Domain::class);
        /* @var $domain \Shopsys\FrameworkBundle\Component\Domain\Domain */

        $product = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . $productReferenceId);
        $productId = $product->getId();

        $queryBuilder = $productRepository->getAllSellableQueryBuilder($domain->getId(), $pricingGroup);
        $queryBuilder->andWhere('p.id = :id')
            ->setParameter('id', $productId);
        $result = $queryBuilder->getQuery()->execute();

        $this->assertSame(in_array($product, $result, true), $isExpectedInResult);
    }

    public function testVisibleAndNotSellingDeniedProductIsOfferred()
    {
        $this->getAllOfferedQueryBuilderTest(1, true);
    }

    public function testVisibleAndSellingDeniedProductIsNotOfferred()
    {
        $this->getAllOfferedQueryBuilderTest(6, false);
    }

    public function testProductVariantIsOfferred()
    {
        $this->getAllOfferedQueryBuilderTest(53, true);
    }

    public function testProductMainVariantIsOfferred()
    {
        $this->getAllOfferedQueryBuilderTest(69, true);
    }

    private function getAllOfferedQueryBuilderTest($productReferenceId, $isExpectedInResult)
    {
        $productRepository = $this->getContainer()->get(ProductRepository::class);
        /* @var $productRepository \Shopsys\FrameworkBundle\Model\Product\ProductRepository */
        $pricingGroup = $this->getReference(PricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1);
        /* @var $pricingGroup \Shopsys\FrameworkBundle\Model\Pricing\Group\PricingGroup */

        $domain = $this->getContainer()->get(Domain::class);
        /* @var $domain \Shopsys\FrameworkBundle\Component\Domain\Domain */

        $product = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . $productReferenceId);
        $productId = $product->getId();

        $queryBuilder = $productRepository->getAllOfferedQueryBuilder($domain->getId(), $pricingGroup);
        $queryBuilder->andWhere('p.id = :id')
            ->setParameter('id', $productId);
        $result = $queryBuilder->getQuery()->execute();

        $this->assertSame(in_array($product, $result, true), $isExpectedInResult);
    }

    public function testOrderingByProductPriorityInCategory()
    {
        $category = $this->getReference(CategoryDataFixture::CATEGORY_FOOD);
        /* @var $category \Shopsys\FrameworkBundle\DataFixtures\Demo\CategoryDataFixture */
        $product1 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . 70);
        $product2 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . 71);

        $this->setProductOrderingPriority($product1, 0);
        $this->setProductOrderingPriority($product2, -1);

        $results = $this->getProductsInCategoryOrderedByPriority($category);
        $this->assertSame($product1, $results[0]);
        $this->assertSame($product2, $results[1]);

        $this->setProductOrderingPriority($product2, 1);

        $results = $this->getProductsInCategoryOrderedByPriority($category);
        $this->assertSame($product2, $results[0]);
        $this->assertSame($product1, $results[1]);
    }

    public function testOrderingByProductPriorityInSearch()
    {
        $this->setProductOrderingPriority($product1, 2);
        $this->setProductOrderingPriority($product2, 3);
        $this->setProductOrderingPriority($product2, 1);
        $product1 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . 1);
        $product2 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . 45);

        $this->setProductOrderingPriority($product1, 0);
        $this->setProductOrderingPriority($product2, 1);

        $results = $this->getProductsForSearchOrderedByPriority('sencor');
        $this->assertSame($product2, $results[0]);
        $this->assertSame($product1, $results[1]);

        $this->setProductOrderingPriority($product2, -1);

        $results = $this->getProductsForSearchOrderedByPriority('sencor');
        $this->assertSame($product1, $results[0]);
        $this->assertSame($product2, $results[1]);
    }

    /**
     * @param \Shopsys\FrameworkBundle\Model\Product\Product $product
     * @param int $priority
     */
    private function setProductOrderingPriority(Product $product, $priority)
    {
        $productDataFactory = $this->getContainer()->get(ProductDataFactory::class);
        /* @var $productDataFactory \Shopsys\FrameworkBundle\Model\Product\ProductDataFactory */
        $productFacade = $this->getContainer()->get(ProductFacade::class);
        /* @var $productFacade \Shopsys\FrameworkBundle\Model\Product\ProductFacade */

        $productData = $productDataFactory->createFromProduct($product);
        $productData->orderingPriority = $priority;
        $productFacade->edit($product->getId(), $productData);
    }

    /**
     * @param string $searchText
     * @return \Shopsys\FrameworkBundle\Model\Product\Product[]
     */
    private function getProductsForSearchOrderedByPriority($searchText)
    {
        $productRepository = $this->getContainer()->get(ProductRepository::class);
        /* @var $productRepository \Shopsys\FrameworkBundle\Model\Product\ProductRepository */
        $pricingGroup = $this->getReference(PricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1);
        /* @var $pricingGroup \Shopsys\FrameworkBundle\Model\Pricing\Group\PricingGroup */

        $paginationResult = $productRepository->getPaginationResultForSearchListable(
            $searchText,
            1,
            'en',
            new ProductFilterData(),
            ProductListOrderingModeService::ORDER_BY_PRIORITY,
            $pricingGroup,
            1,
            PHP_INT_MAX
        );

        return $paginationResult->getResults();
    }

    /**
     * @param \Shopsys\FrameworkBundle\Model\Category\Category $category
     * @return \Shopsys\FrameworkBundle\Model\Product\Product[]
     */
    private function getProductsInCategoryOrderedByPriority(Category $category)
    {
        $productRepository = $this->getContainer()->get(ProductRepository::class);
        /* @var $productRepository \Shopsys\FrameworkBundle\Model\Product\ProductRepository */
        $pricingGroup = $this->getReference(PricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1);
        /* @var $pricingGroup \Shopsys\FrameworkBundle\Model\Pricing\Group\PricingGroup */

        $paginationResult = $productRepository->getPaginationResultForListableInCategory(
            $category,
            1,
            'en',
            new ProductFilterData(),
            ProductListOrderingModeService::ORDER_BY_PRIORITY,
            $pricingGroup,
            1,
            PHP_INT_MAX
        );

        return $paginationResult->getResults();
    }
}
",Why is this test updated? I thought that the behaviour of product ordering will be the same.,"The provided code is a series of unit tests for the `ProductRepository` class in the `Tests\ShopBundle\Database\Model\Product` namespace. The tests cover various functionalities such as listing products based on their visibility, selling status, and offering status, as well as ordering products by priority in both categories and search results.

### Key Points:

1. **Test Methods**:
   - Each test method starts with `testVisibleAndNotSellingDeniedProductIs...`, `testVisibleAndSellingDeniedProductIsNot...`, etc., which checks if a product is listed or not based on its visibility and selling status.
   - Similarly, methods like `testProductVariantIsNotListed`, `testProductMainVariantIsListed`, etc., verify the listing behavior for different types of products (variants vs. main variants).

2. **Query Builder Usage**:
   - The `getAllListableQueryBuilderTest`, `getAllSellableQueryBuilderTest`, and `getAllOfferedQueryBuilderTest` methods use a query builder to filter products based on their visibility, selling status, and offering status, respectively.
   - They then check if the expected product is included in the result set.

3. **Ordering by Priority**:
   - The `testOrderingByProductPriorityInCategory` and `testOrderingByProductPriorityInSearch` methods test the ordering of products by priority in both category listings and search results.
   - They set priorities for products and verify that they are ordered correctly.

4. **Helper Methods**:
   - The `setProductOrderingPriority` method sets the ordering priority for a product using the `ProductDataFactory` and `ProductFacade`.
   - The `getProductsForSearchOrderedByPriority` and `getProductsInCategoryOrderedByPriority` methods retrieve products from the repository and order them by priority.

### Review:

- **Code Structure**: The code is organized into clear sections, making it easy to understand the purpose of each test method.
- **Test Coverage**: The tests cover a wide range of scenarios, ensuring comprehensive coverage of the `ProductRepository` functionality.
- **Readability**: The comments and docstrings help explain the purpose and functionality of each method.
- **Consistency**: The naming conventions and structure are consistent throughout the codebase, enhancing readability and maintainability.

Overall, the code is well-structured, well-documented, and provides a robust set of tests to ensure the correctness of the `ProductRepository` class."
13,"@@ -170,6 +170,17 @@ func TestYARPCMaxMsgSize(t *testing.T) {
 	})
 }
 
+func TestDataRace(t *testing.T) {
+	t.Parallel()
+	value := strings.Repeat(""a"", 32768)
+	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
+		assert.NoError(t, e.SetValueYARPC(context.Background(), ""foo"", value))
+		getValue, err := e.GetValueYARPC(context.Background(), ""foo"")
+		assert.NoError(t, err)
+		assert.Equal(t, value, getValue)
+	})
+}
+
 func TestApplicationErrorPropagation(t *testing.T) {
 	t.Parallel()
 	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {","// Copyright (c) 2018 Uber Technologies, Inc.
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the ""Software""), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in
// all copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
// THE SOFTWARE.

package grpc

import (
	""bytes""
	""context""
	""fmt""
	""math""
	""net""
	""strings""
	""testing""

	""github.com/gogo/protobuf/proto""
	""github.com/stretchr/testify/assert""
	""github.com/stretchr/testify/require""
	""go.uber.org/multierr""
	""go.uber.org/yarpc/api/transport""
	""go.uber.org/yarpc/encoding/protobuf""
	""go.uber.org/yarpc/internal/clientconfig""
	""go.uber.org/yarpc/internal/examples/protobuf/example""
	""go.uber.org/yarpc/internal/examples/protobuf/examplepb""
	""go.uber.org/yarpc/internal/grpcctx""
	""go.uber.org/yarpc/internal/testtime""
	intyarpcerrors ""go.uber.org/yarpc/internal/yarpcerrors""
	""go.uber.org/yarpc/pkg/procedure""
	""go.uber.org/yarpc/yarpcerrors""
	""google.golang.org/grpc""
	""google.golang.org/grpc/codes""
	""google.golang.org/grpc/status""
)

func TestYARPCBasic(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		_, err := e.GetValueYARPC(context.Background(), ""foo"")
		assert.Equal(t, yarpcerrors.Newf(yarpcerrors.CodeNotFound, ""foo""), err)
		assert.NoError(t, e.SetValueYARPC(context.Background(), ""foo"", ""bar""))
		value, err := e.GetValueYARPC(context.Background(), ""foo"")
		assert.NoError(t, err)
		assert.Equal(t, ""bar"", value)
	})
}

func TestGRPCBasic(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		_, err := e.GetValueGRPC(context.Background(), ""foo"")
		assert.Equal(t, status.Error(codes.NotFound, ""foo""), err)
		assert.NoError(t, e.SetValueGRPC(context.Background(), ""foo"", ""bar""))
		value, err := e.GetValueGRPC(context.Background(), ""foo"")
		assert.NoError(t, err)
		assert.Equal(t, ""bar"", value)
	})
}

func TestYARPCWellKnownError(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		e.KeyValueYARPCServer.SetNextError(status.Error(codes.FailedPrecondition, ""bar 1""))
		err := e.SetValueYARPC(context.Background(), ""foo"", ""bar"")
		assert.Equal(t, yarpcerrors.Newf(yarpcerrors.CodeFailedPrecondition, ""bar 1""), err)
	})
}

func TestYARPCNamedError(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		e.KeyValueYARPCServer.SetNextError(intyarpcerrors.NewWithNamef(yarpcerrors.CodeUnknown, ""bar"", ""baz 1""))
		err := e.SetValueYARPC(context.Background(), ""foo"", ""bar"")
		assert.Equal(t, intyarpcerrors.NewWithNamef(yarpcerrors.CodeUnknown, ""bar"", ""baz 1""), err)
	})
}

func TestYARPCNamedErrorNoMessage(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		e.KeyValueYARPCServer.SetNextError(intyarpcerrors.NewWithNamef(yarpcerrors.CodeUnknown, ""bar"", """"))
		err := e.SetValueYARPC(context.Background(), ""foo"", ""bar"")
		assert.Equal(t, intyarpcerrors.NewWithNamef(yarpcerrors.CodeUnknown, ""bar"", """"), err)
	})
}

func TestGRPCWellKnownError(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		e.KeyValueYARPCServer.SetNextError(status.Error(codes.FailedPrecondition, ""bar 1""))
		err := e.SetValueGRPC(context.Background(), ""foo"", ""bar"")
		assert.Equal(t, status.Error(codes.FailedPrecondition, ""bar 1""), err)
	})
}

func TestGRPCNamedError(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		e.KeyValueYARPCServer.SetNextError(intyarpcerrors.NewWithNamef(yarpcerrors.CodeUnknown, ""bar"", ""baz 1""))
		err := e.SetValueGRPC(context.Background(), ""foo"", ""bar"")
		assert.Equal(t, status.Error(codes.Unknown, ""bar: baz 1""), err)
	})
}

func TestGRPCNamedErrorNoMessage(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		e.KeyValueYARPCServer.SetNextError(intyarpcerrors.NewWithNamef(yarpcerrors.CodeUnknown, ""bar"", """"))
		err := e.SetValueGRPC(context.Background(), ""foo"", ""bar"")
		assert.Equal(t, status.Error(codes.Unknown, ""bar""), err)
	})
}

func TestYARPCResponseAndError(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		err := e.SetValueYARPC(context.Background(), ""foo"", ""bar"")
		assert.NoError(t, err)
		e.KeyValueYARPCServer.SetNextError(status.Error(codes.FailedPrecondition, ""bar 1""))
		value, err := e.GetValueYARPC(context.Background(), ""foo"")
		assert.Equal(t, ""bar"", value)
		assert.Equal(t, yarpcerrors.Newf(yarpcerrors.CodeFailedPrecondition, ""bar 1""), err)
	})
}

func TestGRPCResponseAndError(t *testing.T) {
	t.Skip(""grpc-go clients do not support returning both a response and error as of now"")
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		err := e.SetValueGRPC(context.Background(), ""foo"", ""bar"")
		assert.NoError(t, err)
		e.KeyValueYARPCServer.SetNextError(status.Error(codes.FailedPrecondition, ""bar 1""))
		value, err := e.GetValueGRPC(context.Background(), ""foo"")
		assert.Equal(t, ""bar"", value)
		assert.Equal(t, status.Error(codes.FailedPrecondition, ""bar 1""), err)
	})
}

func TestYARPCMaxMsgSize(t *testing.T) {
	t.Parallel()
	value := strings.Repeat(""a"", defaultServerMaxRecvMsgSize*2)
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		assert.Equal(t, yarpcerrors.CodeResourceExhausted, yarpcerrors.FromError(e.SetValueYARPC(context.Background(), ""foo"", value)).Code())
	})
	doWithTestEnv(t, []TransportOption{
		ClientMaxRecvMsgSize(math.MaxInt32),
		ClientMaxSendMsgSize(math.MaxInt32),
		ServerMaxRecvMsgSize(math.MaxInt32),
		ServerMaxSendMsgSize(math.MaxInt32),
	}, nil, nil, func(t *testing.T, e *testEnv) {
		assert.NoError(t, e.SetValueYARPC(context.Background(), ""foo"", value))
		getValue, err := e.GetValueYARPC(context.Background(), ""foo"")
		assert.NoError(t, err)
		assert.Equal(t, value, getValue)
func TestDataRace(t *testing.T) {
	t.Parallel()
	value := strings.Repeat(""a"", 32768)
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		assert.NoError(t, e.SetValueYARPC(context.Background(), ""foo"", value))
		getValue, err := e.GetValueYARPC(context.Background(), ""foo"")
		assert.NoError(t, err)
		assert.Equal(t, value, getValue)
	})
}

	})
}

func TestApplicationErrorPropagation(t *testing.T) {
	t.Parallel()
	doWithTestEnv(t, nil, nil, nil, func(t *testing.T, e *testEnv) {
		response, err := e.Call(
			context.Background(),
			""GetValue"",
			&examplepb.GetValueRequest{Key: ""foo""},
			protobuf.Encoding,
			transport.Headers{},
		)
		require.Equal(t, yarpcerrors.NotFoundErrorf(""foo""), err)
		require.True(t, response.ApplicationError)

		response, err = e.Call(
			context.Background(),
			""SetValue"",
			&examplepb.SetValueRequest{Key: ""foo"", Value: ""hello""},
			protobuf.Encoding,
			transport.Headers{},
		)
		require.NoError(t, err)
		require.False(t, response.ApplicationError)

		response, err = e.Call(
			context.Background(),
			""GetValue"",
			&examplepb.GetValueRequest{Key: ""foo""},
			""bad_encoding"",
			transport.Headers{},
		)
		require.True(t, yarpcerrors.IsInvalidArgument(err))
		require.False(t, response.ApplicationError)
	})
}

func doWithTestEnv(t *testing.T, transportOptions []TransportOption, inboundOptions []InboundOption, outboundOptions []OutboundOption, f func(*testing.T, *testEnv)) {
	testEnv, err := newTestEnv(transportOptions, inboundOptions, outboundOptions)
	require.NoError(t, err)
	defer func() {
		assert.NoError(t, testEnv.Close())
	}()
	f(t, testEnv)
}

type testEnv struct {
	Caller              string
	Service             string
	Inbound             *Inbound
	Outbound            *Outbound
	ClientConn          *grpc.ClientConn
	ContextWrapper      *grpcctx.ContextWrapper
	ClientConfig        transport.ClientConfig
	Procedures          []transport.Procedure
	KeyValueGRPCClient  examplepb.KeyValueClient
	KeyValueYARPCClient examplepb.KeyValueYARPCClient
	KeyValueYARPCServer *example.KeyValueYARPCServer
}

func newTestEnv(transportOptions []TransportOption, inboundOptions []InboundOption, outboundOptions []OutboundOption) (_ *testEnv, err error) {
	keyValueYARPCServer := example.NewKeyValueYARPCServer()
	procedures := examplepb.BuildKeyValueYARPCProcedures(keyValueYARPCServer)
	testRouter := newTestRouter(procedures)

	t := NewTransport(transportOptions...)
	if err := t.Start(); err != nil {
		return nil, err
	}
	defer func() {
		if err != nil {
			err = multierr.Append(err, t.Stop())
		}
	}()

	listener, err := net.Listen(""tcp"", ""127.0.0.1:0"")
	if err != nil {
		return nil, err
	}

	inbound := t.NewInbound(listener, inboundOptions...)
	inbound.SetRouter(testRouter)
	if err := inbound.Start(); err != nil {
		return nil, err
	}
	defer func() {
		if err != nil {
			err = multierr.Append(err, inbound.Stop())
		}
	}()

	clientConn, err := grpc.Dial(listener.Addr().String(), grpc.WithInsecure())
	if err != nil {
		return nil, err
	}
	defer func() {
		if err != nil {
			err = multierr.Append(err, clientConn.Close())
		}
	}()
	keyValueClient := examplepb.NewKeyValueClient(clientConn)

	outbound := t.NewSingleOutbound(listener.Addr().String(), outboundOptions...)
	if err := outbound.Start(); err != nil {
		return nil, err
	}
	defer func() {
		if err != nil {
			err = multierr.Append(err, outbound.Stop())
		}
	}()

	caller := ""example-client""
	service := ""example""
	clientConfig := clientconfig.MultiOutbound(
		caller,
		service,
		transport.Outbounds{
			ServiceName: caller,
			Unary:       outbound,
		},
	)
	keyValueYARPCClient := examplepb.NewKeyValueYARPCClient(clientConfig)

	contextWrapper := grpcctx.NewContextWrapper().
		WithCaller(""example-client"").
		WithService(""example"").
		WithEncoding(string(protobuf.Encoding))

	return &testEnv{
		caller,
		service,
		inbound,
		outbound,
		clientConn,
		contextWrapper,
		clientConfig,
		procedures,
		keyValueClient,
		keyValueYARPCClient,
		keyValueYARPCServer,
	}, nil
}

func (e *testEnv) Call(
	ctx context.Context,
	methodName string,
	message proto.Message,
	encoding transport.Encoding,
	headers transport.Headers,
) (*transport.Response, error) {
	data, err := proto.Marshal(message)
	if err != nil {
		return nil, err
	}
	ctx, cancel := context.WithTimeout(ctx, testtime.Second)
	defer cancel()
	return e.Outbound.Call(
		ctx,
		&transport.Request{
			Caller:   e.Caller,
			Service:  e.Service,
			Encoding: encoding,
			Procedure: procedure.ToName(
				""uber.yarpc.internal.examples.protobuf.example.KeyValue"",
				methodName,
			),
			Headers: headers,
			Body:    bytes.NewReader(data),
		},
	)
}

func (e *testEnv) GetValueYARPC(ctx context.Context, key string) (string, error) {
	ctx, cancel := context.WithTimeout(ctx, testtime.Second)
	defer cancel()
	response, err := e.KeyValueYARPCClient.GetValue(ctx, &examplepb.GetValueRequest{key})
	if response != nil {
		return response.Value, err
	}
	return """", err
}

func (e *testEnv) SetValueYARPC(ctx context.Context, key string, value string) error {
	ctx, cancel := context.WithTimeout(ctx, testtime.Second)
	defer cancel()
	_, err := e.KeyValueYARPCClient.SetValue(ctx, &examplepb.SetValueRequest{key, value})
	return err
}

func (e *testEnv) GetValueGRPC(ctx context.Context, key string) (string, error) {
	ctx, cancel := context.WithTimeout(ctx, testtime.Second)
	defer cancel()
	response, err := e.KeyValueGRPCClient.GetValue(e.ContextWrapper.Wrap(ctx), &examplepb.GetValueRequest{key})
	if response != nil {
		return response.Value, err
	}
	return """", err
}",does this test fail without the fix?,"The suggested code changes appear to be testing various aspects of a YARPC service using gRPC. Here's a concise review:

### Key Points:
1. **Test Setup**: The `doWithTestEnv` function sets up a test environment with a YARPC server and a gRPC client. It initializes necessary components like the router, inbound, outbound, and clients.

2. **Test Cases**:
   - **Basic Operations**: Tests basic operations like setting and getting values using both YARPC and gRPC.
   - **Error Handling**: Tests handling of well-known errors and named errors from both YARPC and gRPC.
   - **Response and Error**: Tests scenarios where both a response and an error are returned.
   - **Max Message Size**: Tests handling of large messages by setting the maximum message size.
   - **Data Race**: Tests for data race conditions by setting a very large message size.
   - **Application Error Propagation**: Tests propagation of application errors from the service.

3. **Error Handling**:
   - The code uses `yarpcerrors` and `status` to handle errors, ensuring consistent error handling across different transports.

4. **Testing Frameworks**:
   - The use of `testing`, `assert`, and `require` from `testing` and `github.com/stretchr/testify` ensures robust testing.

5. **Graceful Shutdown**:
   - The test environment is properly cleaned up by closing connections and stopping services when tests complete.

### Suggested Improvements:
- **Consistent Error Messages**: Ensure that error messages are consistent across different transports and error types.
- **Logging**: Add logging to capture more detailed information about test failures.
- **Performance Testing**: Consider adding performance tests to measure the impact of different configurations on performance.
- **Documentation**: Provide clear documentation for each test case and the setup process.

Overall, the code provides a comprehensive set of tests to ensure the correctness and reliability of the YARPC service implementation."
14,"@@ -24,7 +24,12 @@ public class PMDASMVisitor extends ClassVisitor {
 
     private String outerName;
 
-    private Map<String, String> packages = new HashMap<>();
+    private Map<String, String> packages = new HashMap() {
+        @Override
+        public Object put(Object key, Object value) {
+            return super.put(key, value);
+        }
+    };
 
     private AnnotationVisitor annotationVisitor = new PMDAnnotationVisitor(this);
 ","/**
 * BSD-style license; for more info see http://pmd.sourceforge.net/license.html
 */

package net.sourceforge.pmd.lang.java.typeresolution.visitors;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.objectweb.asm.AnnotationVisitor;
import org.objectweb.asm.Attribute;
import org.objectweb.asm.ClassVisitor;
import org.objectweb.asm.FieldVisitor;
import org.objectweb.asm.Label;
import org.objectweb.asm.MethodVisitor;
import org.objectweb.asm.Opcodes;
import org.objectweb.asm.Type;
import org.objectweb.asm.signature.SignatureReader;
import org.objectweb.asm.signature.SignatureVisitor;

public class PMDASMVisitor extends ClassVisitor {
    private Map<String, String> packages = new HashMap() {
        @Override
        public Object put(Object key, Object value) {
            return super.put(key, value);
        }
    };

    private String outerName;

    private Map<String, String> packages = new HashMap<>();

    private AnnotationVisitor annotationVisitor = new PMDAnnotationVisitor(this);

    private FieldVisitor fieldVisitor = new PMDFieldVisitor(this);

    private SignatureVisitor sigVisitor = new PMDSignatureVisitor(this);

    private MethodVisitor methodVisitor = new PMDMethodVisitor(this);

    public List<String> innerClasses;

    public PMDASMVisitor(String outerName) {
        super(Opcodes.ASM5);
        this.outerName = outerName;
    }

    public Map<String, String> getPackages() {
        return packages;
    }

    public List<String> getInnerClasses() {
        return innerClasses;
    }

    private String parseClassName(String name) {
        if (name == null) {
            return null;
        }

        String className = name;
        int n = name.lastIndexOf('/');
        if (n > -1) {
            className = name.substring(n + 1);
        }
        name = name.replace('/', '.');
        packages.put(className, name);
        n = className.indexOf('$');
        if (n > -1) {
            // TODO I don't think the first one, with Class$Inner is needed -
            // come back and check
            packages.put(className.substring(n + 1), name);
            packages.put(className.replace('$', '.'), name);
        }

        return name;
    }

    private void parseClassName(String[] names) {
        if (names != null) {
            for (String s : names) {
                parseClassName(s);
            }
        }
    }

    private void extractSignature(String sig) {
        if (sig != null) {
            new SignatureReader(sig).accept(sigVisitor);
        }
    }

    /* Start ClassVisitor implementations */

    @Override
    public void visit(int version, int access, String name, String sig, String superName, String[] interfaces) {
        parseClassName(name);
        parseClassName(interfaces);
        if (sig != null) {
            extractSignature(sig);
        }
    }

    @Override
    public AnnotationVisitor visitAnnotation(String desc, boolean visible) {
        addType(Type.getType(desc));
        return annotationVisitor;
    }

    @Override
    public FieldVisitor visitField(int access, String name, String desc, String sig, Object value) {
        if (sig != null) {
            extractSignature(sig);
        }

        addType(Type.getType(desc));
        if (value instanceof Type) {
            addType((Type) value);
        }
        return fieldVisitor;
    }

    @Override
    public MethodVisitor visitMethod(int access, String name, String desc, String sig, String[] exceptions) {
        if (sig != null) {
            extractSignature(sig);
        }
        addMethodDesc(desc);
        parseClassName(exceptions);
        return methodVisitor;
    }

    @Override
    public void visitSource(String source, String debug) {
    }

    @Override
    public void visitInnerClass(String name, String outerName, String innerName, int access) {
        if (!this.outerName.replace('.', '/').equals(outerName)) {
            // do not consider the inner class if it is not a member of our
            // outer class
            return;
        }

        if (innerClasses == null) {
            innerClasses = new ArrayList<>();
        }
        if (!innerClasses.contains(name.replace('/', '.'))) {
            innerClasses.add(name.replace('/', '.'));
        }
        packages.put(innerName, name.replace('/', '.'));
    }

    @Override
    public void visitOuterClass(String owner, String name, String desc) {
    }

    @Override
    public void visitEnd() {
    }

    private void addMethodDesc(String desc) {
        addTypes(desc);
        addType(Type.getReturnType(desc));
    }

    private void addTypes(String desc) {
        Type[] types = Type.getArgumentTypes(desc);
        for (Type type : types) {
            addType(type);
        }
    }

    private void addType(Type t) {
        switch (t.getSort()) {
        case Type.ARRAY:
            addType(t.getElementType());
            break;
        case Type.OBJECT:
            parseClassName(t.getClassName().replace('.', '/'));
            break;
        default:
            // Do nothing
            break;
        }
    }

    @Override
    public void visitAttribute(Attribute attr) {
    }

    /*
     * Start visitors
     */

    private static class PMDFieldVisitor extends FieldVisitor {

        private PMDASMVisitor parent;

        PMDFieldVisitor(PMDASMVisitor visitor) {
            super(Opcodes.ASM5);
            parent = visitor;
        }

        @Override
        public AnnotationVisitor visitAnnotation(String desc, boolean visible) {
            parent.addType(Type.getType(desc));
            return parent.annotationVisitor;
        }

        @Override
        public void visitAttribute(Attribute attr) {
        }

        @Override
        public void visitEnd() {
        }
    }

    private static class PMDAnnotationVisitor extends AnnotationVisitor {
        private PMDASMVisitor parent;

        PMDAnnotationVisitor(PMDASMVisitor visitor) {
            super(Opcodes.ASM5);
            parent = visitor;
        }
",what's the point of this override?,"The suggested code change appears to be a Java class that implements the `ClassVisitor` interface from the ASM library, which is used for bytecode manipulation. The class is designed to traverse a Java class file and collect information about its structure, such as package names, inner classes, and method signatures.

Here's a concise review of the suggested code change:

1. **Class Structure**: The class `PMDASMVisitor` extends `ClassVisitor` and overrides several methods to handle different aspects of a Java class file.

2. **Package and Inner Class Handling**: The class maintains a map of package names to their fully qualified names and a list of inner class names. It uses helper methods like `parseClassName` and `extractSignature` to populate these maps.

3. **Method Signature Parsing**: The `extractSignature` method uses a `SignatureReader` to parse method signatures and populate the package map.

4. **Visitor Methods**: The class implements methods from `ClassVisitor` such as `visit`, `visitAnnotation`, `visitField`, `visitMethod`, `visitSource`, `visitInnerClass`, `visitOuterClass`, and `visitEnd`. These methods handle various aspects of the class file, including annotations, fields, methods, and inner classes.

5. **Type Handling**: The class includes helper methods like `addType` and `addTypes` to recursively process types found in method descriptions and field descriptors.

6. **Visitor Classes**: Two nested classes, `PMDFieldVisitor` and `PMDAnnotationVisitor`, extend `FieldVisitor` and `AnnotationVisitor` respectively, allowing for more specific handling of field and annotation data.

Overall, the code provides a robust framework for analyzing Java class files using ASM, which can be useful for various purposes such as static analysis or code transformation."
15,"@@ -62,6 +62,14 @@ type blockJournal struct {
 	j          diskJournal
 	refs       map[BlockID]blockRefMap
 	isShutdown bool
+
+	// Tracks the total size of on-disk blocks that will be put to the
+	// server (i.e., does not count reference adds).  It is only
+	// accurate for users of this journal that properly flush entries;
+	// in particular, direct calls to `removeReferences` can cause
+	// this count to deviate from the actual disk usage of the
+	// journal.
+	blockBytes int64
 }
 
 type bserverOpName string","// Copyright 2016 Keybase Inc. All rights reserved.
// Use of this source code is governed by a BSD
// license that can be found in the LICENSE file.

package libkbfs

import (
	""errors""
	""fmt""
	""io/ioutil""
	""os""
	""path/filepath""
	""reflect""

	""github.com/keybase/client/go/logger""
	""golang.org/x/net/context""
)

// blockJournal stores a single ordered list of block operations for a
// single TLF, along with the associated block data, in flat files in
// a directory on disk.
//
// The directory layout looks like:
//
// dir/block_journal/EARLIEST
// dir/block_journal/LATEST
// dir/block_journal/0...000
// dir/block_journal/0...001
// dir/block_journal/0...fff
// dir/blocks/0100/0...01/data
// dir/blocks/0100/0...01/key_server_half
// ...
// dir/blocks/01ff/f...ff/data
// dir/blocks/01ff/f...ff/key_server_half
//
// Each entry in the journal in dir/block_journal contains the
// mutating operation and arguments for a single operation, except for
// block data. (See diskJournal comments for more details about the
// journal.)
//
// The block data is stored separately in dir/blocks. Each block has
// its own subdirectory with its ID as a name.  The block
// subdirectories are splayed over (# of possible hash types) * 256
// subdirectories -- one byte for the hash type (currently only one)
// plus the first byte of the hash data -- using the first four
// characters of the name to keep the number of directories in dir
// itself to a manageable number, similar to git. Each block directory
// has data, which is the raw block data that should hash to the block
// ID, and key_server_half, which contains the raw data for the
// associated key server half.
//
// blockJournal is not goroutine-safe, so any code that uses it must
// guarantee that only one goroutine at a time calls its functions.
type blockJournal struct {
	codec  Codec
	crypto cryptoPure
	dir    string

	log      logger.Logger
	deferLog logger.Logger


	// Tracks the total size of on-disk blocks that will be put to the
	// server (i.e., does not count reference adds).  It is only
	// accurate for users of this journal that properly flush entries;
	// in particular, direct calls to `removeReferences` can cause
	// this count to deviate from the actual disk usage of the
	// journal.
	blockBytes int64
	j          diskJournal
	refs       map[BlockID]blockRefMap
	isShutdown bool
}

type bserverOpName string

const (
	blockPutOp    bserverOpName = ""blockPut""
	addRefOp      bserverOpName = ""addReference""
	removeRefsOp  bserverOpName = ""removeReferences""
	archiveRefsOp bserverOpName = ""archiveReferences""
)

// A blockJournalEntry is just the name of the operation and the
// associated block ID and contexts. Fields are exported only for
// serialization.
type blockJournalEntry struct {
	// Must be one of the four ops above.
	Op bserverOpName
	// Must have exactly one entry with one context for blockPutOp
	// and addRefOp.
	Contexts map[BlockID][]BlockContext
}

// Get the single context stored in this entry. Only applicable to
// blockPutOp and addRefOp.
func (e blockJournalEntry) getSingleContext() (
	BlockID, BlockContext, error) {
	switch e.Op {
	case blockPutOp, addRefOp:
		if len(e.Contexts) != 1 {
			return BlockID{}, BlockContext{}, fmt.Errorf(
				""Op %s doesn't have exactly one context: %v"",
				e.Op, e.Contexts)
		}
		for id, idContexts := range e.Contexts {
			if len(idContexts) != 1 {
				return BlockID{}, BlockContext{}, fmt.Errorf(
					""Op %s doesn't have exactly one context for id=%s: %v"",
					e.Op, id, idContexts)
			}
			return id, idContexts[0], nil
		}
	}

	return BlockID{}, BlockContext{}, fmt.Errorf(
		""getSingleContext() erroneously called on op %s"", e.Op)
}

// makeBlockJournal returns a new blockJournal for the given
// directory. Any existing journal entries are read.
func makeBlockJournal(
	ctx context.Context, codec Codec, crypto cryptoPure, dir string,
	log logger.Logger) (*blockJournal, error) {
	journalPath := filepath.Join(dir, ""block_journal"")
	deferLog := log.CloneWithAddedDepth(1)
	j := makeDiskJournal(
		codec, journalPath, reflect.TypeOf(blockJournalEntry{}))
	journal := &blockJournal{
		codec:    codec,
		crypto:   crypto,
		dir:      dir,
		log:      log,
		deferLog: deferLog,
		j:        j,
	}

	refs, err := journal.readJournal(ctx)
	if err != nil {
		return nil, err
	}

	journal.refs = refs
	return journal, nil
}

// The functions below are for building various non-journal paths.

func (j *blockJournal) blocksPath() string {
	return filepath.Join(j.dir, ""blocks"")
}

func (j *blockJournal) blockPath(id BlockID) string {
	idStr := id.String()
	return filepath.Join(j.blocksPath(), idStr[:4], idStr[4:])
}

func (j *blockJournal) blockDataPath(id BlockID) string {
	return filepath.Join(j.blockPath(id), ""data"")
}

func (j *blockJournal) keyServerHalfPath(id BlockID) string {
	return filepath.Join(j.blockPath(id), ""key_server_half"")
}

// The functions below are for reading and writing journal entries.

func (j *blockJournal) readJournalEntry(ordinal journalOrdinal) (
	blockJournalEntry, error) {
	entry, err := j.j.readJournalEntry(ordinal)
	if err != nil {
		return blockJournalEntry{}, err
	}

	return entry.(blockJournalEntry), nil
}

// readJournal reads the journal and returns a map of all the block
// references in the journal.
func (j *blockJournal) readJournal(ctx context.Context) (
	map[BlockID]blockRefMap, error) {
	refs := make(map[BlockID]blockRefMap)

	first, err := j.j.readEarliestOrdinal()
	if os.IsNotExist(err) {
		return refs, nil
	} else if err != nil {
		return nil, err
	}
	last, err := j.j.readLatestOrdinal()
	if err != nil {
		return nil, err
	}

	j.log.CDebugf(ctx, ""Reading journal entries %d to %d"", first, last)

	for i := first; i <= last; i++ {
		e, err := j.readJournalEntry(i)
		if err != nil {
			return nil, err
		}

		// Handle single ops separately.
		switch e.Op {
		case blockPutOp, addRefOp:
			id, context, err := e.getSingleContext()
			if err != nil {
				return nil, err
			}

			blockRefs := refs[id]
			if blockRefs == nil {
				blockRefs = make(blockRefMap)
				refs[id] = blockRefs
			}

			err = blockRefs.put(context, liveBlockRef)
			if err != nil {
				return nil, err
			}
			continue
		}

		for id, idContexts := range e.Contexts {
			blockRefs := refs[id]

			switch e.Op {
			case removeRefsOp:
				if blockRefs == nil {
					// All refs are already gone,
					// which is not an error.
					continue
				}

				for _, context := range idContexts {
					err := blockRefs.remove(context)
					if err != nil {
						return nil, err
					}
				}

				if len(blockRefs) == 0 {
					delete(refs, id)
				}

			case archiveRefsOp:
				if blockRefs == nil {
					blockRefs = make(blockRefMap)
					refs[id] = blockRefs
				}

				for _, context := range idContexts {
					err := blockRefs.put(
						context, archivedBlockRef)
					if err != nil {
						return nil, err
					}
				}

			default:
				return nil, fmt.Errorf(""Unknown op %s"", e.Op)
			}
		}
	}
	return refs, nil
}

func (j *blockJournal) writeJournalEntry(
	ordinal journalOrdinal, entry blockJournalEntry) error {","A better name might be `bytesToFlush`, to emphasize that it only matters when flushing.","The suggested code change appears to be implementing a block journal for a Keybase file system. Here's a concise review:

### Key Points:
1. **Structure**:
   - The `blockJournal` struct manages a journal of block operations and their associated block data.
   - It uses a `diskJournal` to handle the actual storage of journal entries.

2. **Initialization**:
   - The `makeBlockJournal` function initializes a new `blockJournal` instance by reading existing journal entries from the specified directory.

3. **Path Management**:
   - Functions like `blocksPath`, `blockPath`, `blockDataPath`, and `keyServerHalfPath` generate paths for block-related files.

4. **Journal Entry Handling**:
   - The `readJournalEntry` function reads a specific journal entry by ordinal.
   - The `readJournal` function reads all journal entries and populates a map of block references.

5. **Writing Journal Entries**:
   - The `writeJournalEntry` function writes a new journal entry to the specified ordinal.

### Potential Improvements:
- **Concurrency Safety**: Ensure that the `blockJournal` is thread-safe, especially when multiple goroutines access it.
- **Error Handling**: Add more detailed error handling and logging for better debugging.
- **Performance**: Optimize the journaling process to minimize I/O operations and improve performance.
- **Testing**: Write comprehensive tests to ensure the correctness of the journaling logic.

Overall, the code provides a solid foundation for managing block operations and their associated data in a Keybase file system."
16,"@@ -706,7 +706,6 @@ class ArrayAssignmentAnalyzer
                     }
                 } elseif ($type instanceof TNonEmptyList
                     && $key_value instanceof Type\Atomic\TLiteralInt
-                    && ($key_value->value === $key_value->value || $key_value->value < $type->count)
                     && count($key_values) === 1
                 ) {
                     $has_matching_objectlike_property = true;","                                            )
                                        ])
                                    ]
                                ]
                            ]
                        );

                        $current_type->replaceTemplateTypesWithArgTypes(
                            $template_result,
                            $codebase
                        );

                        $array_atomic_type = new Type\Atomic\TClassStringMap(
                            $class_string_map->param_name,
                            $class_string_map->as_type,
                            $current_type
                        );
                    } else {
                        $array_atomic_type = new TNonEmptyArray([
                            $array_atomic_key_type,
                            $current_type,
                        ]);
                    }
                } else {
                    $array_atomic_type = new TNonEmptyArray([
                        $array_atomic_key_type,
                        $current_type,
                    ]);
                }
            } else {
                $array_atomic_type = new TNonEmptyList($current_type);
            }

            $from_countable_object_like = false;

            $new_child_type = null;

            if (!$current_dim && !$context->inside_loop) {
                $atomic_root_types = $root_type->getAtomicTypes();

                if (isset($atomic_root_types['array'])) {
                    if ($array_atomic_type instanceof Type\Atomic\TClassStringMap) {
                        $array_atomic_type = new TNonEmptyArray([
                            $array_atomic_type->getStandinKeyParam(),
                            $array_atomic_type->value_param
                        ]);
                    } elseif ($atomic_root_types['array'] instanceof TNonEmptyArray
                        || $atomic_root_types['array'] instanceof TNonEmptyList
                    ) {
                        $array_atomic_type->count = $atomic_root_types['array']->count;
                    } elseif ($atomic_root_types['array'] instanceof TKeyedArray
                        && $atomic_root_types['array']->sealed
                    ) {
                        $array_atomic_type->count = count($atomic_root_types['array']->properties);
                        $from_countable_object_like = true;

                        if ($atomic_root_types['array']->is_list
                            && $array_atomic_type instanceof TList
                        ) {
                            $array_atomic_type = clone $atomic_root_types['array'];

                            $new_child_type = new Type\Union([$array_atomic_type]);
                        }
                    } elseif ($array_atomic_type instanceof TList) {
                        $array_atomic_type = new TNonEmptyList(
                            $array_atomic_type->type_param
                        );
                    } else {
                        $array_atomic_type = new TNonEmptyArray(
                            $array_atomic_type->type_params
                        );
                    }
                }
            }

            $array_assignment_type = new Type\Union([
                $array_atomic_type,
            ]);

            if (!$new_child_type) {
                $new_child_type = Type::combineUnionTypes(
                    $root_type,
                    $array_assignment_type,
                    $codebase,
                    true,
                    true
                );
            }

            if ($from_countable_object_like) {
                $atomic_root_types = $new_child_type->getAtomicTypes();

                if (isset($atomic_root_types['array'])
                    && ($atomic_root_types['array'] instanceof TNonEmptyArray
                        || $atomic_root_types['array'] instanceof TNonEmptyList)
                    && $atomic_root_types['array']->count !== null
                ) {
                    $atomic_root_types['array']->count++;
                }
            }
        } else {
            $new_child_type = $root_type;
        }

        $new_child_type->removeType('null');

        if (!$root_type->hasObjectType()) {
            $root_type = $new_child_type;
        }

        $statements_analyzer->node_data->setType($root_array_expr, $root_type);

        if ($root_array_expr instanceof PhpParser\Node\Expr\PropertyFetch) {
            if ($root_array_expr->name instanceof PhpParser\Node\Identifier) {
                InstancePropertyAssignmentAnalyzer::analyze(
                    $statements_analyzer,
                    $root_array_expr,
                    $root_array_expr->name->name,
                    null,
                    $root_type,
                    $context,
                    false
                );
            } else {
                if (ExpressionAnalyzer::analyze($statements_analyzer, $root_array_expr->name, $context) === false) {
                    return false;
                }

                if (ExpressionAnalyzer::analyze($statements_analyzer, $root_array_expr->var, $context) === false) {
                    return false;
                }
            }
        } elseif ($root_array_expr instanceof PhpParser\Node\Expr\StaticPropertyFetch
            && $root_array_expr->name instanceof PhpParser\Node\Identifier
        ) {
            StaticPropertyAssignmentAnalyzer::analyze(
                $statements_analyzer,
                $root_array_expr,
                null,
                $root_type,
                $context
            );
        } elseif ($root_var_id) {
            $context->vars_in_scope[$root_var_id] = $root_type;
        }

        if ($root_array_expr instanceof PhpParser\Node\Expr\MethodCall
            || $root_array_expr instanceof PhpParser\Node\Expr\StaticCall
            || $root_array_expr instanceof PhpParser\Node\Expr\FuncCall
        ) {
            if ($root_type->hasArray()) {
                if (IssueBuffer::accepts(
                    new InvalidArrayAssignment(
                        'Assigning to the output of a function has no effect',
                        new \Psalm\CodeLocation($statements_analyzer->getSource(), $root_array_expr)
                    ),
                    $statements_analyzer->getSuppressedIssues()
                )
                ) {
                    // do nothing
                }
            }
        }

        return null;
    }

    /**
     * @param non-empty-list<Type\Atomic\TLiteralInt|Type\Atomic\TLiteralString> $key_values
     */
    private static function updateTypeWithKeyValues(
        \Psalm\Codebase $codebase,
        Type\Union $child_stmt_type,
        Type\Union $current_type,
        array $key_values
    ) : Type\Union {
        $has_matching_objectlike_property = false;
        $has_matching_string = false;

        foreach ($child_stmt_type->getAtomicTypes() as $type) {
            foreach ($key_values as $key_value) {
                if ($type instanceof TKeyedArray) {
                    if (isset($type->properties[$key_value->value])) {
                        $has_matching_objectlike_property = true;

                        $type->properties[$key_value->value] = clone $current_type;
                    }
                } elseif ($type instanceof Type\Atomic\TString
                    && $key_value instanceof Type\Atomic\TLiteralInt
                ) {
                    $has_matching_string = true;

                    if ($type instanceof Type\Atomic\TLiteralString
                        && $current_type->isSingleStringLiteral()
                    ) {
                        $new_char = $current_type->getSingleStringLiteral()->value;

                        if (\strlen($new_char) === 1) {
                            $type->value[0] = $new_char;
                        }
                    }
                } elseif ($type instanceof TNonEmptyList
                    && $key_value instanceof Type\Atomic\TLiteralInt
                    && ($key_value->value === $key_value->value || $key_value->value < $type->count)
                    && count($key_values) === 1
                ) {
                    $has_matching_objectlike_property = true;

                    $type->type_param = Type::combineUnionTypes(
                        clone $current_type,
                        $type->type_param,
                        $codebase,
                        true,
                        false
                    );
                }
            }
        }

        $child_stmt_type->bustCache();

        if (!$has_matching_objectlike_property && !$has_matching_string) {
            if (count($key_values) === 1) {
                $key_value = $key_values[0];

                $object_like = new TKeyedArray(
                    [$key_value->value => clone $current_type],
                    $key_value instanceof Type\Atomic\TLiteralClassString
                        ? [(string) $key_value->value => true]
                        : null
                );

                $object_like->sealed = true;

                $array_assignment_type = new Type\Union([
                    $object_like,
                ]);
            } else {
                $array_assignment_literals = $key_values;

                $array_assignment_type = new Type\Union([
                    new Type\Atomic\TNonEmptyArray([
                        new Type\Union($array_assignment_literals),
                        clone $current_type
                    ])
                ]);
            }

            return Type::combineUnionTypes(
                $child_stmt_type,
                $array_assignment_type,
                $codebase,
                true,
                false
            );
        }

        return $child_stmt_type;
    }

    /**
     * @param list<Type\Atomic\TLiteralInt|Type\Atomic\TLiteralString> $key_values $key_values
     */
    private static function taintArrayAssignment(
        StatementsAnalyzer $statements_analyzer,
        PhpParser\Node\Expr $stmt,
        Type\Union $stmt_type,
        Type\Union $child_stmt_type,
        ?string $array_var_id,
        array $key_values
    ) : void {
        $codebase = $statements_analyzer->getCodebase();

        if ($codebase->taint
            && $child_stmt_type->parent_nodes
            && !\in_array('TaintedInput', $statements_analyzer->getSuppressedIssues())
        ) {
            $var_location = new \Psalm\CodeLocation($statements_analyzer->getSource(), $stmt);

            $new_parent_node = \Psalm\Internal\Taint\TaintNode::getForAssignment(
                $array_var_id ?: 'array-assignment',
                $var_location
            );

            $codebase->taint->addTaintNode($new_parent_node);

            foreach ($child_stmt_type->parent_nodes as $parent_node) {
                if ($key_values) {
                    foreach ($key_values as $key_value) {
                        $codebase->taint->addPath(
                            $parent_node,
                            $new_parent_node,
                            'array-assignment-\'' . $key_value->value . '\''
                        );
                    }
                } else {
                    $codebase->taint->addPath(
                        $parent_node,
                        $new_parent_node,
                        'array-assignment'
                    );
                }
            }

            $stmt_type->parent_nodes[] = $new_parent_node;
        }
    }
}
","while the lhs of that or looks really redundant, the rhs does not immediately strike as useless","The provided code is a part of a PHP static analysis tool, specifically designed to analyze and validate PHP code for potential issues related to type safety and code quality. The code snippet is focused on handling array assignments within PHP expressions.

### Key Points:

1. **Array Assignment Handling**:
   - The method `handleArrayAssignment` processes an array assignment expression and updates the type of the assigned variable.
   - It checks if the current type is an array and updates it based on the key values provided in the assignment.
   - If the key values match existing properties in a keyed array, it updates those properties with the new type.
   - If no matching properties are found, it creates a new keyed array with the provided key-value pairs.

2. **Taint Analysis**:
   - The method `taintArrayAssignment` adds taint information to the array assignment if the codebase is configured to do so.
   - It creates a taint node and paths to indicate the source of the taint.

3. **Type Unification**:
   - The `updateTypeWithKeyValues` method combines the current type with the new type based on the key values.
   - It handles different types of arrays (e.g., `TNonEmptyArray`, `TNonEmptyList`) and updates their properties accordingly.

4. **Error Handling**:
   - The code includes error handling for invalid array assignments, such as assigning to the output of a function.

### Review:

- **Readability**: The code is well-structured and follows a clear logical flow. However, some parts could benefit from more descriptive comments to explain complex logic.
- **Performance**: The use of `clone` and `new` operations can be optimized, especially in loops, to reduce memory usage.
- **Error Handling**: The error handling for invalid array assignments is robust but could be improved by providing more specific error messages or handling cases differently.
- **Code Duplication**: There is some duplication in the logic for updating types and tainting, which could be refactored into helper methods for better maintainability.

Overall, the code is functional and performs its intended tasks effectively. However, improving readability and performance could enhance its usability and reliability."
17,"@@ -216,6 +216,7 @@ bool GenerateRustModuleRootFile(const Parser &parser,
       for (auto it = sub_modules.begin(); it != sub_modules.end(); it++) {
         code += ""pub mod "" + it->first + "" {"";
         code.IncrementIdentLevel();
+        code += ""#![deny(unsafe_op_in_unsafe_fn)]"";
         code += ""use super::*;"";
         it->second.GenerateImports(code);
         code.DecrementIdentLevel();","
// independent from idl_parser, since this code is not needed for most clients

#include ""flatbuffers/code_generators.h""
#include ""flatbuffers/flatbuffers.h""
#include ""flatbuffers/idl.h""
#include ""flatbuffers/util.h""

namespace flatbuffers {

// Convert a camelCaseIdentifier or CamelCaseIdentifier to a
// snake_case_identifier.
std::string MakeSnakeCase(const std::string &in) {
  std::string s;
  for (size_t i = 0; i < in.length(); i++) {
    if (i == 0) {
      s += CharToLower(in[0]);
    } else if (in[i] == '_') {
      s += '_';
    } else if (!islower(in[i])) {
      // Prevent duplicate underscores for Upper_Snake_Case strings
      // and UPPERCASE strings.
      if (islower(in[i - 1])) { s += '_'; }
      s += CharToLower(in[i]);
    } else {
      s += in[i];
    }
  }
  return s;
}

// Convert a string to all uppercase.
std::string MakeUpper(const std::string &in) {
  std::string s;
  for (size_t i = 0; i < in.length(); i++) { s += CharToUpper(in[i]); }
  return s;
}

std::string UnionTypeFieldName(const FieldDef &field) {
  return MakeSnakeCase(field.name + ""_type"");
}

// Encapsulate all logical field types in this enum. This allows us to write
// field logic based on type switches, instead of branches on the properties
// set on the Type.
// TODO(rw): for backwards compatibility, we can't use a strict `enum class`
//           declaration here. could we use the `-Wswitch-enum` warning to
//           achieve the same effect?
enum FullType {
  ftInteger = 0,
  ftFloat = 1,
  ftBool = 2,

  ftStruct = 3,
  ftTable = 4,

  ftEnumKey = 5,
  ftUnionKey = 6,

  ftUnionValue = 7,

  // TODO(rw): bytestring?
  ftString = 8,

  ftVectorOfInteger = 9,
  ftVectorOfFloat = 10,
  ftVectorOfBool = 11,
  ftVectorOfEnumKey = 12,
  ftVectorOfStruct = 13,
  ftVectorOfTable = 14,
  ftVectorOfString = 15,
  ftVectorOfUnionValue = 16,

  ftArrayOfBuiltin = 17,
  ftArrayOfEnum = 18,
  ftArrayOfStruct = 19,
};

// Convert a Type to a FullType (exhaustive).
FullType GetFullType(const Type &type) {
  // N.B. The order of these conditionals matters for some types.

  if (IsString(type)) {
    return ftString;
  } else if (type.base_type == BASE_TYPE_STRUCT) {
    if (type.struct_def->fixed) {
      return ftStruct;
    } else {
      return ftTable;
    }
  } else if (IsVector(type)) {
    switch (GetFullType(type.VectorType())) {
      case ftInteger: {
        return ftVectorOfInteger;
      }
      case ftFloat: {
        return ftVectorOfFloat;
      }
      case ftBool: {
        return ftVectorOfBool;
      }
      case ftStruct: {
        return ftVectorOfStruct;
      }
      case ftTable: {
        return ftVectorOfTable;
      }
      case ftString: {
        return ftVectorOfString;
      }
      case ftEnumKey: {
        return ftVectorOfEnumKey;
      }
      case ftUnionKey:
      case ftUnionValue: {
        FLATBUFFERS_ASSERT(false && ""vectors of unions are unsupported"");
        break;
      }
      default: {
        FLATBUFFERS_ASSERT(false && ""vector of vectors are unsupported"");
      }
    }
  } else if (IsArray(type)) {
    switch (GetFullType(type.VectorType())) {
      case ftInteger:
      case ftFloat:
      case ftBool: {
        return ftArrayOfBuiltin;
      }
      case ftStruct: {
        return ftArrayOfStruct;
      }
      case ftEnumKey: {
        return ftArrayOfEnum;
      }
      default: {
        FLATBUFFERS_ASSERT(false && ""Unsupported type for fixed array"");
      }
    }
  } else if (type.enum_def != nullptr) {
    if (type.enum_def->is_union) {
      if (type.base_type == BASE_TYPE_UNION) {
        return ftUnionValue;
      } else if (IsInteger(type.base_type)) {
        return ftUnionKey;
      } else {
        FLATBUFFERS_ASSERT(false && ""unknown union field type"");
      }
    } else {
      return ftEnumKey;
    }
  } else if (IsScalar(type.base_type)) {
    if (IsBool(type.base_type)) {
      return ftBool;
    } else if (IsInteger(type.base_type)) {
      return ftInteger;
    } else if (IsFloat(type.base_type)) {
      return ftFloat;
    } else {
      FLATBUFFERS_ASSERT(false && ""unknown number type"");
    }
  }

  FLATBUFFERS_ASSERT(false && ""completely unknown type"");

  // this is only to satisfy the compiler's return analysis.
  return ftBool;
}

bool IsBitFlagsEnum(const EnumDef &enum_def) {
  return enum_def.attributes.Lookup(""bit_flags"") != nullptr;
}

// TableArgs make required non-scalars ""Option<_>"".
// TODO(cneo): Rework how we do defaults and stuff.
bool IsOptionalToBuilder(const FieldDef &field) {
  return field.IsOptional() || !IsScalar(field.value.type.base_type);
}

bool GenerateRustModuleRootFile(const Parser &parser,
                                const std::string &output_dir) {
  // We gather the symbols into a tree of namespaces (which are rust mods) and
  // generate a file that gathers them all.
  struct Module {
    std::map<std::string, Module> sub_modules;
    std::vector<std::string> generated_files;
    // Add a symbol into the tree.
    void Insert(const Definition *s, const std::string suffix) {
      const Definition &symbol = *s;
      Module *current_module = this;
      for (auto it = symbol.defined_namespace->components.begin();
           it != symbol.defined_namespace->components.end(); it++) {
        std::string ns_component = MakeSnakeCase(*it);
        current_module = &current_module->sub_modules[ns_component];
      }
      current_module->generated_files.push_back(MakeSnakeCase(symbol.name) +
                                                suffix);
    }
    // Recursively create the importer file.
    void GenerateImports(CodeWriter &code) {
        code += ""#![deny(unsafe_op_in_unsafe_fn)]"";
      for (auto it = sub_modules.begin(); it != sub_modules.end(); it++) {
        code += ""pub mod "" + it->first + "" {"";
        code.IncrementIdentLevel();
        code += ""use super::*;"";
        it->second.GenerateImports(code);
        code.DecrementIdentLevel();
        code += ""} // "" + it->first;
      }
      for (auto it = generated_files.begin(); it != generated_files.end();
           it++) {
        code += ""mod "" + *it + "";"";
        code += ""pub use self::"" + *it + ""::*;"";
      }
    }
  };
  Module root_module;
  for (auto it = parser.enums_.vec.begin(); it != parser.enums_.vec.end();
       it++) {
    root_module.Insert(*it, parser.opts.filename_suffix);
  }
  for (auto it = parser.structs_.vec.begin(); it != parser.structs_.vec.end();
       it++) {
    root_module.Insert(*it, parser.opts.filename_suffix);
  }
  CodeWriter code(""  "");
  // TODO(caspern): Move generated warning out of BaseGenerator.
  code +=
      ""// Automatically generated by the Flatbuffers compiler. ""
      ""Do not modify."";
  root_module.GenerateImports(code);
  const bool success =
      SaveFile((output_dir + ""mod.rs"").c_str(), code.ToString(), false);
  code.Clear();
  return success;
}

namespace rust {

class RustGenerator : public BaseGenerator {
 public:
  RustGenerator(const Parser &parser, const std::string &path,
                const std::string &file_name)
      : BaseGenerator(parser, path, file_name, """", ""::"", ""rs""),
        cur_name_space_(nullptr) {
    const char *keywords[] = {
      // clang-format off
      // list taken from:
      // https://doc.rust-lang.org/book/second-edition/appendix-01-keywords.html
      //
      // we write keywords one per line so that we can easily compare them with
      // changes to that webpage in the future.

      // currently-used keywords
      ""as"",
      ""break"",
      ""const"",
      ""continue"",
      ""crate"",
      ""else"",
      ""enum"",
      ""extern"",
      ""false"",
      ""fn"",
      ""for"",
      ""if"",
      ""impl"",
      ""in"",
      ""let"",
      ""loop"",
      ""match"",
      ""mod"",
      ""move"",
      ""mut"",
      ""pub"",
      ""ref"",
      ""return"",
      ""Self"",
      ""self"",
      ""static"",
      ""struct"",
      ""super"",
      ""trait"",
      ""true"",
      ""type"",
      ""unsafe"",
      ""use"",
      ""where"",
      ""while"",

      // future possible keywords
      ""abstract"",
      ""alignof"",
      ""become"",
      ""box"",
      ""do"",
      ""final"",
      ""macro"",
      ""offsetof"",
      ""override"",
      ""priv"",
      ""proc"",
      ""pure"",
      ""sizeof"",
      ""typeof"",
      ""unsized"",
      ""virtual"",
      ""yield"",

      // other rust terms we should not use
      ""std"",
      ""usize"",
      ""isize"",
      ""u8"",
      ""i8"",
      ""u16"",
      ""i16"",
      ""u32"",
      ""i32"",
      ""u64"",
      ""i64"",
      ""u128"",
      ""i128"",
      ""f32"",
      ""f64"",

      // These are terms the code generator can implement on types.
      //
      // In Rust, the trait resolution rules (as described at
      // https://github.com/rust-lang/rust/issues/26007) mean that, as long
      // as we impl table accessors as inherent methods, we'll never create
      // conflicts with these keywords. However, that's a fairly nuanced
      // implementation detail, and how we implement methods could change in
      // the future. as a result, we proactively block these out as reserved
      // words.
      ""follow"",
      ""push"",
      ""size"",
      ""alignment"",
      ""to_little_endian"",
      ""from_little_endian"",
      nullptr,

      // used by Enum constants
      ""ENUM_MAX"",
      ""ENUM_MIN"",
      ""ENUM_VALUES"",
      // clang-format on
    };
    for (auto kw = keywords; *kw; kw++) keywords_.insert(*kw);
    code_.SetPadding(""  "");
  }

  bool generate() {
    if (parser_.opts.one_file) {
      return GenerateOneFile();
    } else {
      return GenerateIndividualFiles();
    }
  }

  template<typename T>
  bool GenerateSymbols(const SymbolTable<T> &symbols,
                       std::function<void(const T &)> gen_symbol) {
    for (auto it = symbols.vec.begin(); it != symbols.vec.end(); it++) {
      const T &symbol = **it;
      if (symbol.generated) continue;
      code_.Clear();
      code_ += ""// "" + std::string(FlatBuffersGeneratedWarning());
      code_ += ""extern crate flatbuffers;"";
      code_ += ""use std::mem;"";
      code_ += ""use std::cmp::Ordering;"";
      code_ += ""use self::flatbuffers::{EndianScalar, Follow};"";
      code_ += ""use super::*;"";
      cur_name_space_ = symbol.defined_namespace;
      gen_symbol(symbol);
      std::stringstream file_path;
      file_path << path_;
      // Create filepath.
      if (symbol.defined_namespace)
        for (auto i = symbol.defined_namespace->components.begin();
             i != symbol.defined_namespace->components.end(); i++) {
          file_path << MakeSnakeCase(*i) << kPathSeparator;
          EnsureDirExists(file_path.str());
        }
      file_path << MakeSnakeCase(symbol.name) << parser_.opts.filename_suffix
                << "".rs"";
      const bool save_success =
          SaveFile(file_path.str().c_str(), code_.ToString(),
                   /*binary=*/false);
      if (!save_success) return false;
    }
    return true;
  }

  bool GenerateIndividualFiles() {
    code_.Clear();
    // Don't bother with imports. Use absolute paths everywhere.
    return GenerateSymbols<EnumDef>(
               parser_.enums_, [&](const EnumDef &e) { this->GenEnum(e); }) &&
           GenerateSymbols<StructDef>(","I think it makes more sense for this to be at the top of the module root file, or at the top of every generated object file. I believe the `#![...]` attributes will cover all nested modules so there's some redundant coverage and also it currently misses the generated code that's in the root namespace.","The suggested code change appears to be a Rust generator for FlatBuffers, which is a serialization library. The changes include:

1. **Namespace Management**: The code manages namespaces using a tree structure (`Module`) to organize generated files.

2. **Symbol Generation**: It generates Rust code for enums and structs, including necessary imports and definitions.

3. **File Generation**: It generates individual Rust files for each enum and struct, organized under their respective namespaces.

4. **Error Handling**: It includes error handling for file generation, returning `false` if any file fails to save.

5. **Code Formatting**: The code uses consistent indentation and spacing for readability.

6. **Keyword Blocking**: It blocks certain Rust keywords from being used as identifiers to avoid conflicts.

7. **Comments and Warnings**: It includes comments and warnings to indicate that the code is automatically generated and should not be modified.

Overall, the changes appear to be comprehensive and well-structured for generating Rust code from FlatBuffers IDL files."
18,"@@ -239,12 +239,11 @@ func (k *KeybaseDaemonLocal) Resolve(ctx context.Context, assertion string) (
 		return libkb.NormalizedUsername(""""), keybase1.UserOrTeamID(""""), err
 	}
 
-	// TODO(KBFS-2621): Resolve shouldn't work for implicit teams, but
-	// until CORE-6623 is done, this is required.
-	iti, err := k.localImplicitTeams.getLocalImplicitTeam(id.AsTeamOrBust())
-	if err == nil {
-		// An implicit team exists, so use the display name.
-		return iti.Name, id, nil
+	_, ok := k.localImplicitTeams[id.AsTeamOrBust()]
+	if ok {
+		// An implicit team exists, so Resolve shouldn't work.
+		return libkb.NormalizedUsername(""""), keybase1.UserOrTeamID(""""),
+			fmt.Errorf(""Team ID %s is an implicit team"", id)
 	}
 
 	return ti.Name, id, nil","	if !ok {
		return TeamInfo{}, NoSuchTeamError{tid.String()}
	}
	return team, nil
}

type localImplicitTeamMap map[keybase1.TeamID]ImplicitTeamInfo

func (m localImplicitTeamMap) getLocalImplicitTeam(
	tid keybase1.TeamID) (ImplicitTeamInfo, error) {
	team, ok := m[tid]
	if !ok {
		return ImplicitTeamInfo{}, NoSuchTeamError{tid.String()}
	}
	return team, nil
}

type favoriteStore interface {
	FavoriteAdd(uid keybase1.UID, folder keybase1.Folder) error
	FavoriteDelete(uid keybase1.UID, folder keybase1.Folder) error
	FavoriteList(uid keybase1.UID) ([]keybase1.Folder, error)

	Shutdown()
}

type diskFavoriteClient struct {
	favoriteDb *leveldb.DB
	codec      kbfscodec.Codec
}

var _ favoriteStore = diskFavoriteClient{}

func (c diskFavoriteClient) favkey(
	uid keybase1.UID, folder keybase1.Folder) []byte {
	return []byte(fmt.Sprintf(""%s:%s"", uid, folder.ToString()))
}

func (c diskFavoriteClient) FavoriteAdd(
	uid keybase1.UID, folder keybase1.Folder) error {
	enc, err := c.codec.Encode(folder)
	if err != nil {
		return err
	}

	return c.favoriteDb.Put(c.favkey(uid, folder), enc, nil)
}

func (c diskFavoriteClient) FavoriteDelete(
	uid keybase1.UID, folder keybase1.Folder) error {
	return c.favoriteDb.Delete(c.favkey(uid, folder), nil)
}

func (c diskFavoriteClient) FavoriteList(uid keybase1.UID) (
	[]keybase1.Folder, error) {
	iter := c.favoriteDb.NewIterator(util.BytesPrefix([]byte(uid+"":"")), nil)
	defer iter.Release()
	var folders []keybase1.Folder
	for iter.Next() {
		var folder keybase1.Folder
		if err := c.codec.Decode(iter.Value(), &folder); err != nil {
			return nil, err
		}
		folders = append(folders, folder)
	}
	if err := iter.Error(); err != nil {
		return nil, err
	}

	return folders, nil
}

func (c diskFavoriteClient) Shutdown() {
	c.favoriteDb.Close()
}

type memoryFavoriteClient struct {
	favorites map[keybase1.UID]map[string]keybase1.Folder
}

var _ favoriteStore = memoryFavoriteClient{}

func (c memoryFavoriteClient) FavoriteAdd(
	uid keybase1.UID, folder keybase1.Folder) error {
	if c.favorites[uid] == nil {
		c.favorites[uid] = make(map[string]keybase1.Folder)
	}
	c.favorites[uid][folder.ToString()] = folder
	return nil
}

func (c memoryFavoriteClient) FavoriteDelete(
	uid keybase1.UID, folder keybase1.Folder) error {
	if c.favorites[uid] != nil {
		delete(c.favorites[uid], folder.ToString())
	}
	return nil
}

func (c memoryFavoriteClient) FavoriteList(
	uid keybase1.UID) ([]keybase1.Folder, error) {
	folders := make([]keybase1.Folder, len(c.favorites[uid]))
	i := 0
	for _, v := range c.favorites[uid] {
		folders[i] = v
		i++
	}
	return folders, nil
}

func (c memoryFavoriteClient) Shutdown() {}

// KeybaseDaemonLocal implements KeybaseDaemon using an in-memory user
// and session store, and a given favorite store.
type KeybaseDaemonLocal struct {
	codec kbfscodec.Codec

	// lock protects everything below.
	lock               sync.Mutex
	localUsers         localUserMap
	localTeams         localTeamMap
	localImplicitTeams localImplicitTeamMap
	currentUID         keybase1.UID
	asserts            map[string]keybase1.UserOrTeamID
	implicitAsserts    map[string]keybase1.TeamID
	favoriteStore      favoriteStore
	merkleRoot         keybase1.MerkleRootV2
}

var _ KeybaseService = &KeybaseDaemonLocal{}

func (k *KeybaseDaemonLocal) setCurrentUID(uid keybase1.UID) {
	k.lock.Lock()
	defer k.lock.Unlock()
	// TODO: Send out notifications.
	k.currentUID = uid
}

func (k *KeybaseDaemonLocal) assertionToIDLocked(ctx context.Context,
	assertion string) (id keybase1.UserOrTeamID, err error) {
	expr, err := externals.AssertionParseAndOnly(assertion)
	if err != nil {
		return keybase1.UserOrTeamID(""""), err
	}
	urls := expr.CollectUrls(nil)
	if len(urls) == 0 {
		return keybase1.UserOrTeamID(""""), errors.New(""No assertion URLs"")
	}

	for _, url := range urls {
		var currID keybase1.UserOrTeamID
		if url.IsUID() {
			currID = url.ToUID().AsUserOrTeam()
		} else if url.IsTeamID() {
			currID = url.ToTeamID().AsUserOrTeam()
		} else {
			key, val := url.ToKeyValuePair()
			a := fmt.Sprintf(""%s@%s"", val, key)
			if url.IsKeybase() && key != ""team"" {
				a = val
			}
			var ok bool
			currID, ok = k.asserts[a]
			if !ok {
				return keybase1.UserOrTeamID(""""), NoSuchUserError{a}
			}
		}
		if id != keybase1.UserOrTeamID("""") && currID != id {
			return keybase1.UserOrTeamID(""""),
				errors.New(""AND assertions resolve to different UIDs"")
		}
		id = currID
	}
	return id, nil
}

// Resolve implements KeybaseDaemon for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) Resolve(ctx context.Context, assertion string) (
	libkb.NormalizedUsername, keybase1.UserOrTeamID, error) {
	if err := checkContext(ctx); err != nil {
		return libkb.NormalizedUsername(""""), keybase1.UserOrTeamID(""""), err
	}

	k.lock.Lock()
	defer k.lock.Unlock()
	id, err := k.assertionToIDLocked(ctx, assertion)
	if err != nil {
		return libkb.NormalizedUsername(""""), keybase1.UserOrTeamID(""""), err
	}

	if id.IsUser() {
		u, err := k.localUsers.getLocalUser(id.AsUserOrBust())
		if err != nil {
			return libkb.NormalizedUsername(""""), keybase1.UserOrTeamID(""""), err
		}
		return u.Name, id, nil
	}

	// Otherwise it's a team
	ti, err := k.localTeams.getLocalTeam(id.AsTeamOrBust())
	if err != nil {
	_, ok := k.localImplicitTeams[id.AsTeamOrBust()]
	if ok {
		// An implicit team exists, so Resolve shouldn't work.
		return libkb.NormalizedUsername(""""), keybase1.UserOrTeamID(""""),
			fmt.Errorf(""Team ID %s is an implicit team"", id)
		return libkb.NormalizedUsername(""""), keybase1.UserOrTeamID(""""), err
	}

	// TODO(KBFS-2621): Resolve shouldn't work for implicit teams, but
	// until CORE-6623 is done, this is required.
	iti, err := k.localImplicitTeams.getLocalImplicitTeam(id.AsTeamOrBust())
	if err == nil {
		// An implicit team exists, so use the display name.
		return iti.Name, id, nil
	}

	return ti.Name, id, nil
}

// Identify implements KeybaseDaemon for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) Identify(
	ctx context.Context, assertion, _ string) (
	libkb.NormalizedUsername, keybase1.UserOrTeamID, error) {
	// The local daemon doesn't need to distinguish resolves from
	// identifies.
	return k.Resolve(ctx, assertion)
}

func (k *KeybaseDaemonLocal) resolveForImplicitTeam(
	ctx context.Context, name string, r []libkb.NormalizedUsername,
	ur []keybase1.SocialAssertion,
	resolvedIDs map[libkb.NormalizedUsername]keybase1.UserOrTeamID) (
	[]libkb.NormalizedUsername, []keybase1.SocialAssertion, error) {
	id, err := k.assertionToIDLocked(ctx, name)
	if err == nil {
		u, err := k.localUsers.getLocalUser(id.AsUserOrBust())
		if err != nil {
			return nil, nil, err
		}
		r = append(r, u.Name)
		resolvedIDs[u.Name] = id
	} else {
		a, ok := externals.NormalizeSocialAssertion(name)
		if !ok {
			return nil, nil, fmt.Errorf(""Bad assertion: %s"", name)
		}
		ur = append(ur, a)
	}
	return r, ur, nil
}

// ResolveIdentifyImplicitTeam implements the KeybaseService interface
// for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) ResolveIdentifyImplicitTeam(
	ctx context.Context, assertions, suffix string, tlfType tlf.Type,
	doIdentifies bool, reason string) (ImplicitTeamInfo, error) {
	if err := checkContext(ctx); err != nil {
		return ImplicitTeamInfo{}, err
	}

	if tlfType != tlf.Private && tlfType != tlf.Public {
		return ImplicitTeamInfo{}, fmt.Errorf(
			""Invalid implicit team TLF type: %s"", tlfType)
	}

	k.lock.Lock()
	defer k.lock.Unlock()

	// Canonicalize the name.
	writerNames, readerNames, _, err :=
		splitAndNormalizeTLFName(assertions, tlfType)
	if err != nil {
		return ImplicitTeamInfo{}, err
	}
	var writers, readers []libkb.NormalizedUsername
	var unresolvedWriters, unresolvedReaders []keybase1.SocialAssertion
	resolvedIDs := make(map[libkb.NormalizedUsername]keybase1.UserOrTeamID)
	for _, w := range writerNames {
		writers, unresolvedWriters, err = k.resolveForImplicitTeam(
			ctx, w, writers, unresolvedWriters, resolvedIDs)
		if err != nil {
			return ImplicitTeamInfo{}, err
		}
	}
	for _, r := range readerNames {
		readers, unresolvedReaders, err = k.resolveForImplicitTeam(
			ctx, r, readers, unresolvedReaders, resolvedIDs)
		if err != nil {
			return ImplicitTeamInfo{}, err
		}
	}

	var extensions []tlf.HandleExtension
	if len(suffix) != 0 {
		extensions, err = tlf.ParseHandleExtensionSuffix(suffix)
		if err != nil {
			return ImplicitTeamInfo{}, err
		}
	}
	name := tlf.MakeCanonicalName(
		writers, unresolvedWriters, readers, unresolvedReaders, extensions)

	key := fmt.Sprintf(""%s:%s"", tlfType.String(), name)
	tid, ok := k.implicitAsserts[key]
	if ok {
		return k.localImplicitTeams[tid], nil
	}

	// If the implicit team doesn't exist, always create it.

	// Need to make the team info as well, so get the list of user
	// names and resolve them.  Auto-generate an implicit team name.
	implicitName := libkb.NormalizedUsername(
		fmt.Sprintf(""_implicit_%d"", len(k.localTeams)))
	teams := makeLocalTeams(
		[]libkb.NormalizedUsername{implicitName}, len(k.localTeams), tlfType)
	info := teams[0]
	info.Writers = make(map[keybase1.UID]bool, len(writerNames))
	for _, w := range writers {
		id, ok := resolvedIDs[w]
		if !ok {
			return ImplicitTeamInfo{}, fmt.Errorf(""No resolved writer %s"", w)
		}
		info.Writers[id.AsUserOrBust()] = true
	}
	if len(readerNames) > 0 {
		info.Readers = make(map[keybase1.UID]bool, len(readerNames))
		for _, r := range readers {
			id, ok := resolvedIDs[r]
			if !ok {
				return ImplicitTeamInfo{}, fmt.Errorf(
					""No resolved reader %s"", r)

			}
			info.Readers[id.AsUserOrBust()] = true
		}
	}
	// Unresolved users don't need to go in the team info, they're
	// irrelvant until they're resolved.  TODO: add resolved users
	// into existing teams they should be on.

	tid = teams[0].TID
	k.implicitAsserts[key] = tid
	k.localTeams[tid] = info

	asUserName := libkb.NormalizedUsername(name)
	iteamInfo := ImplicitTeamInfo{
		// TODO: use the ""preferred"" canonical format here by listing
		// the logged-in user first?
		Name: asUserName,
		TID:  tid,
	}
	k.localImplicitTeams[tid] = iteamInfo
	return iteamInfo, nil
}

func (k *KeybaseDaemonLocal) addImplicitTeamTlfID(
	tid keybase1.TeamID, tlfID tlf.ID) error {
	// TODO: add check to make sure the private/public suffix of the
	// team ID matches that of the tlf ID.
	k.lock.Lock()
	defer k.lock.Unlock()
	iteamInfo, ok := k.localImplicitTeams[tid]
	if !ok {
		return NoSuchTeamError{tid.String()}
	}
	iteamInfo.TlfID = tlfID
	k.localImplicitTeams[tid] = iteamInfo
	return nil
}

// LoadUserPlusKeys implements KeybaseDaemon for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) LoadUserPlusKeys(ctx context.Context,
	uid keybase1.UID, _ keybase1.KID) (UserInfo, error) {
	if err := checkContext(ctx); err != nil {
		return UserInfo{}, err
	}

	k.lock.Lock()
	defer k.lock.Unlock()
	u, err := k.localUsers.getLocalUser(uid)
	if err != nil {
		return UserInfo{}, err
	}

	var infoCopy UserInfo
	if err := kbfscodec.Update(k.codec, &infoCopy, u.UserInfo); err != nil {
		return UserInfo{}, err
	}
	return infoCopy, nil
}

// LoadTeamPlusKeys implements KeybaseDaemon for KeybaseDaemonLocal.
func (k *KeybaseDaemonLocal) LoadTeamPlusKeys(
	ctx context.Context, tid keybase1.TeamID, _ kbfsmd.KeyGen, _ keybase1.UserVersion,
	_ keybase1.TeamRole) (TeamInfo, error) {
	if err := checkContext(ctx); err != nil {
		return TeamInfo{}, err
	}

	k.lock.Lock()
	defer k.lock.Unlock()
	t, err := k.localTeams.getLocalTeam(tid)
	if err != nil {
		return TeamInfo{}, err",Meaning we should be using `ResolveImplicitTeamByID`?,"The suggested code changes appear to be related to implementing a local Keybase daemon in Go, which handles user and team management, as well as favorite storage. The changes include:

1. **Local User and Team Maps**: Two maps (`localUserMap` and `localTeamMap`) are defined to store user and team information locally.

2. **Favorite Storage**: Two implementations of the `favoriteStore` interface (`diskFavoriteClient` and `memoryFavoriteClient`) are provided to handle favorite storage. These clients use LevelDB and in-memory data structures, respectively.

3. **KeybaseDaemonLocal Structure**: This structure represents the local Keybase daemon, which includes fields for user and team maps, current UID, assertions, implicit assertions, favorite store, and Merkle root.

4. **Resolve Method**: This method resolves an assertion to a user or team ID. It uses a lock to ensure thread safety.

5. **ResolveIdentifyImplicitTeam Method**: This method resolves an implicit team name to a team ID and creates the team if it doesn't exist. It also handles resolving unresolved users and adding them to the team.

6. **LoadUserPlusKeys and LoadTeamPlusKeys Methods**: These methods load user and team information with keys, ensuring that the loaded information is updated correctly.

Overall, the changes aim to provide a basic implementation of a local Keybase daemon that can handle user and team management, as well as favorite storage. The code is structured to be modular and easy to extend."
19,"@@ -42,7 +42,7 @@ public class RemoteNetworkConnection implements NetworkConnection {
   @Override
   public ConnectionType setNetworkConnection(
       ConnectionType type) {
-    Map<String, ConnectionType> mode = ImmutableMap.of(""type"", type);
+    Map<String, Integer> mode = ImmutableMap.of(""type"", type.getBitMask());
     return new ConnectionType(((Number) executeMethod.execute(DriverCommand.SET_NETWORK_CONNECTION,
                                                             ImmutableMap
                                                                 .of(""parameters"", mode)))","// Licensed to the Software Freedom Conservancy (SFC) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The SFC licenses this file
// to you under the Apache License, Version 2.0 (the
// ""License""); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

package org.openqa.selenium.remote.mobile;

import com.google.common.collect.ImmutableMap;

import org.openqa.selenium.mobile.NetworkConnection;
import org.openqa.selenium.remote.DriverCommand;
import org.openqa.selenium.remote.ExecuteMethod;

import java.util.Map;

public class RemoteNetworkConnection implements NetworkConnection {

  private final ExecuteMethod executeMethod;

  public RemoteNetworkConnection(ExecuteMethod executeMethod) {
    this.executeMethod = executeMethod;
  }

  @Override
  public ConnectionType getNetworkConnection() {
    return new ConnectionType(((Number) executeMethod.execute(DriverCommand.GET_NETWORK_CONNECTION,
                                                               null)).intValue());
  }

    Map<String, Integer> mode = ImmutableMap.of(""type"", type.getBitMask());
  @Override
  public ConnectionType setNetworkConnection(
      ConnectionType type) {
    Map<String, ConnectionType> mode = ImmutableMap.of(""type"", type);
    return new ConnectionType(((Number) executeMethod.execute(DriverCommand.SET_NETWORK_CONNECTION,
                                                            ImmutableMap
                                                                .of(""parameters"", mode)))
                                  .intValue());
  }
}
",can you change this instead to just `type.toString()` and then you wouldn't have to expose the getBitMask in the enum. (Alternatively you could have used `type.hashCode()` but that doesn't feel as nice),"The suggested code change appears to be implementing a `RemoteNetworkConnection` class that provides methods to interact with network connection settings on a mobile device using Selenium's remote capabilities. Here's a concise review:

1. **Class Definition**: The class `RemoteNetworkConnection` implements the `NetworkConnection` interface, which suggests it will handle network connection management.

2. **Constructor**: The constructor takes an `ExecuteMethod` object, which is likely used to send commands to the remote WebDriver server.

3. **getNetworkConnection Method**:
   - This method retrieves the current network connection type by executing a `GET_NETWORK_CONNECTION` command.
   - It converts the result from the server into a `ConnectionType` enum value.

4. **setNetworkConnection Method**:
   - This method sets the network connection type by executing a `SET_NETWORK_CONNECTION` command.
   - It constructs a map with the desired connection type and sends it to the server.
   - The result is converted back into a `ConnectionType` enum value.

5. **ImmutableMap Usage**: The `ImmutableMap.of` method is used to create maps for sending parameters to the server. This ensures that the maps are immutable, which can be beneficial for security and immutability guarantees.

6. **Code Style**: The code is generally clean and follows Java naming conventions. However, there are a few minor improvements that could be made:
   - Ensure that the `ConnectionType` enum is properly defined and used throughout the class.
   - Consider adding comments to explain the purpose of each method and the logic within them.

Overall, the code is well-structured and should work as intended for managing network connections on a mobile device using Selenium's remote capabilities."
20,"@@ -341,13 +341,17 @@ module Beaker
         # @param [Hash{Symbol=>String}] opts An options hash
         # @option opts [String] :version The version of Puppet to install, required
         # @option opts [String] :win_download_url The url to download puppet from
+        #
+        # @note on windows, the +:ruby_arch+ host parameter can determine in addition
+        # to other settings whether the 32 or 64bit install is used
         def install_puppet_from_msi( host, opts )
           #only install 64bit builds if
           # - we are on puppet version 3.7+
           # - we do not have install_32 set on host
           # - we do not have install_32 set globally
           version = opts[:version]
-          if !(version_is_less(version, '3.7')) and host.is_x86_64? and not host['install_32'] and not opts['install_32']
+          is_config_32 = host['ruby_arch'] == 'x86' || host['install_32'] || opts['install_32']
+          if !(version_is_less(version, '3.7')) && host.is_x86_64? && !is_config_32
             host['dist'] = ""puppet-#{version}-x64""
           else
             host['dist'] = ""puppet-#{version}""","                        ""if [ -f install.rb ]; then "" +
                        ""ruby ./install.rb #{install_opts}; "" +
                        ""else true; fi""
          end
        end

        #Install FOSS based upon host configuration and options
        # @example will install puppet 3.6.1 from native puppetlabs provided packages wherever possible and will fail over to gem installation when impossible
        #  install_puppet({
        #    :version          => '3.6.1',
        #    :facter_version   => '2.0.1',
        #    :hiera_version    => '1.3.3',
        #    :default_action   => 'gem_install',
        #
        #   })
        #
        #
        # @example Will install latest packages on Enterprise Linux and Debian based distros and fail hard on all othere platforms.
        #  install_puppet()
        #
        # @note This will attempt to add a repository for apt.puppetlabs.com on
        #       Debian, Ubuntu, or Cumulus machines, or yum.puppetlabs.com on EL or Fedora
        #       machines, then install the package 'puppet'.
        # @param [Hash{Symbol=>String}] opts
        # @option opts [String] :version Version of puppet to download
        # @option opts [String] :mac_download_url Url to download msi pattern of %url%/puppet-%version%.msi
        # @option opts [String] :win_download_url Url to download dmg  pattern of %url%/(puppet|hiera|facter)-%version%.msi
        #
        # @return nil
        # @raise [StandardError] When encountering an unsupported platform by default, or if gem cannot be found when default_action => 'gem_install'
        # @raise [FailTest] When error occurs during the actual installation process
        def install_puppet(opts = {})
          default_download_url = 'http://downloads.puppetlabs.com'
          opts = {:win_download_url => ""#{default_download_url}/windows"",
                  :mac_download_url => ""#{default_download_url}/mac""}.merge(opts)
          hosts.each do |host|
            if host['platform'] =~ /el-(5|6|7)/
              relver = $1
              install_puppet_from_rpm host, opts.merge(:release => relver, :family => 'el')
            elsif host['platform'] =~ /fedora-(\d+)/
              relver = $1
              install_puppet_from_rpm host, opts.merge(:release => relver, :family => 'fedora')
            elsif host['platform'] =~ /(ubuntu|debian|cumulus)/
              install_puppet_from_deb host, opts
            elsif host['platform'] =~ /windows/
              relver = opts[:version]
              install_puppet_from_msi host, opts
            elsif host['platform'] =~ /osx/
              install_puppet_from_dmg host, opts
            else
              if opts[:default_action] == 'gem_install'
                install_puppet_from_gem host, opts
              else
                raise ""install_puppet() called for unsupported platform '#{host['platform']}' on '#{host.name}'""
              end
            end

            # Certain install paths may not create the config dirs/files needed
            on host, ""mkdir -p #{host['puppetpath']}"" unless host[:type] =~ /aio/
            on host, ""echo '' >> #{host.puppet['hiera_config']}""
          end
          nil
        end

        # Configure puppet.conf for all hosts based upon a provided Hash
        # @param [Hash{Symbol=>String}] opts
        # @option opts [Hash{String=>String}] :main configure the main section of puppet.conf
        # @option opts [Hash{String=>String}] :agent configure the agent section of puppet.conf
        #
        # @return nil
        def configure_puppet(opts={})
          hosts.each do |host|
            configure_puppet_on(host,opts)
          end
        end

        # Configure puppet.conf on the given host based upon a provided hash
        # @param [Host] host The host to configure puppet.conf on
        # @param [Hash{Symbol=>String}] opts
        # @option opts [Hash{String=>String}] :main configure the main section of puppet.conf
        # @option opts [Hash{String=>String}] :agent configure the agent section of puppet.conf
        #
        # @example will configure /etc/puppet.conf on the puppet master.
        #   config = {
        #     'main' => {
        #       'server'   => 'testbox.test.local',
        #       'certname' => 'testbox.test.local',
        #       'logdir'   => '/var/log/puppet',
        #       'vardir'   => '/var/lib/puppet',
        #       'ssldir'   => '/var/lib/puppet/ssl',
        #       'rundir'   => '/var/run/puppet'
        #     },
        #     'agent' => {
        #       'environment' => 'dev'
        #     }
        #   }
        #   configure_puppet(master, config)
        #
        # @return nil
        def configure_puppet_on(host, opts = {})
          if host['platform'] =~ /windows/
            puppet_conf = host.puppet['config']
            conf_data = ''
            opts.each do |section,options|
              conf_data << ""[#{section}]`n""
              options.each do |option,value|
                conf_data << ""#{option}=#{value}`n""
              end
              conf_data << ""`n""
            end
            on host, powershell(""\$text = \\\""#{conf_data}\\\""; Set-Content -path '#{puppet_conf}' -value \$text"")
          else
            puppet_conf = host.puppet['config']
            conf_data = ''
            opts.each do |section,options|
              conf_data << ""[#{section}]\n""
              options.each do |option,value|
                conf_data << ""#{option}=#{value}\n""
              end
              conf_data << ""\n""
            end
            on host, ""echo \""#{conf_data}\"" > #{puppet_conf}""
          end
        end

        # Installs Puppet and dependencies using rpm
        #
        # @param [Host] host The host to install packages on
        # @param [Hash{Symbol=>String}] opts An options hash
        # @option opts [String] :version The version of Puppet to install, if nil installs latest version
        # @option opts [String] :facter_version The version of Facter to install, if nil installs latest version
        # @option opts [String] :hiera_version The version of Hiera to install, if nil installs latest version
        # @option opts [String] :default_action What to do if we don't know how to install native packages on host.
        #                                       Valid value is 'gem_install' or nil. If nil raises an exception when
        #                                       on an unsupported platform. When 'gem_install' attempts to install
        #                                       Puppet via gem.
        # @option opts [String] :release The major release of the OS
        # @option opts [String] :family The OS family (one of 'el' or 'fedora')
        #
        # @return nil
        # @api private
        def install_puppet_from_rpm( host, opts )
          release_package_string = ""http://yum.puppetlabs.com/puppetlabs-release-#{opts[:family]}-#{opts[:release]}.noarch.rpm""

          on host, ""rpm -q --quiet puppetlabs-release || rpm -ivh #{release_package_string}""

          if opts[:facter_version]
            on host, ""yum install -y facter-#{opts[:facter_version]}""
          end

          if opts[:hiera_version]
            on host, ""yum install -y hiera-#{opts[:hiera_version]}""
          end

          puppet_pkg = opts[:version] ? ""puppet-#{opts[:version]}"" : 'puppet'
          on host, ""yum install -y #{puppet_pkg}""
        end

        # Installs Puppet and dependencies from deb
        #
        # @param [Host] host The host to install packages on
        # @param [Hash{Symbol=>String}] opts An options hash
        # @option opts [String] :version The version of Puppet to install, if nil installs latest version
        # @option opts [String] :facter_version The version of Facter to install, if nil installs latest version
        # @option opts [String] :hiera_version The version of Hiera to install, if nil installs latest version
        #
        # @return nil
        # @api private
        def install_puppet_from_deb( host, opts )
          if ! host.check_for_package 'lsb-release'
            host.install_package('lsb-release')
          end

          if ! host.check_for_command 'curl'
            on host, 'apt-get install -y curl'
          end

          on host, 'curl -O http://apt.puppetlabs.com/puppetlabs-release-$(lsb_release -c -s).deb'
          on host, 'dpkg -i puppetlabs-release-$(lsb_release -c -s).deb'
          on host, 'apt-get update'

          if opts[:facter_version]
            on host, ""apt-get install -y facter=#{opts[:facter_version]}-1puppetlabs1""
          end

          if opts[:hiera_version]
            on host, ""apt-get install -y hiera=#{opts[:hiera_version]}-1puppetlabs1""
          end

          if opts[:version]
            on host, ""apt-get install -y puppet-common=#{opts[:version]}-1puppetlabs1""
            on host, ""apt-get install -y puppet=#{opts[:version]}-1puppetlabs1""
          else
            on host, 'apt-get install -y puppet'
          end
        end

        # Installs Puppet and dependencies from msi
        #
        # @param [Host] host The host to install packages on
        #
        # @note on windows, the +:ruby_arch+ host parameter can determine in addition
        # to other settings whether the 32 or 64bit install is used
          is_config_32 = host['ruby_arch'] == 'x86' || host['install_32'] || opts['install_32']
          if !(version_is_less(version, '3.7')) && host.is_x86_64? && !is_config_32
        # @param [Hash{Symbol=>String}] opts An options hash
        # @option opts [String] :version The version of Puppet to install, required
        # @option opts [String] :win_download_url The url to download puppet from
        def install_puppet_from_msi( host, opts )
          #only install 64bit builds if
          # - we are on puppet version 3.7+
          # - we do not have install_32 set on host
          # - we do not have install_32 set globally
          version = opts[:version]
          if !(version_is_less(version, '3.7')) and host.is_x86_64? and not host['install_32'] and not opts['install_32']
            host['dist'] = ""puppet-#{version}-x64""
          else
            host['dist'] = ""puppet-#{version}""
          end
          link = ""#{opts[:win_download_url]}/#{host['dist']}.msi""
          if not link_exists?( link )
            raise ""Puppet #{version} at #{link} does not exist!""
          end

          if host.is_cygwin?
            dest = ""#{host['dist']}.msi""
            on host, ""curl -O #{link}""

            #Because the msi installer doesn't add Puppet to the environment path
            #Add both potential paths for simplicity
            #NOTE - this is unnecessary if the host has been correctly identified as 'foss' during set up
            puppetbin_path = ""\""/cygdrive/c/Program Files (x86)/Puppet Labs/Puppet/bin\"":\""/cygdrive/c/Program Files/Puppet Labs/Puppet/bin\""""
            on host, %Q{ echo 'export PATH=$PATH:#{puppetbin_path}' > /etc/bash.bashrc }
          else
            dest = ""C:\\Windows\\Temp\\#{host['dist']}.msi""

            on host, powershell(""$webclient = New-Object System.Net.WebClient;  $webclient.DownloadFile('#{link}','#{dest}')"")

            host.mkdir_p host['distmoduledir']
          end

          if host.is_cygwin?
            on host, ""cmd /C 'start /w msiexec.exe /qn /i #{dest}'""
          else
            on host, ""start /w msiexec.exe /qn /i #{dest}""
          end
        end

        # Installs Puppet and dependencies from dmg
        #
        # @param [Host] host The host to install packages on
        # @param [Hash{Symbol=>String}] opts An options hash
        # @option opts [String] :version The version of Puppet to install, required
        # @option opts [String] :facter_version The version of Facter to install, required
        # @option opts [String] :hiera_version The version of Hiera to install, required
        # @option opts [String] :mac_download_url Url to download msi pattern of %url%/puppet-%version%.msi
        #
        # @return nil
        # @api private
        def install_puppet_from_dmg( host, opts )

          puppet_ver = opts[:version]
          facter_ver = opts[:facter_version]
          hiera_ver = opts[:hiera_version]

          if [puppet_ver, facter_ver, hiera_ver].include?(nil)
            raise ""You need to specify versions for OSX host\n eg. install_puppet({:version => '3.6.2',:facter_version => '2.1.0',:hiera_version  => '1.3.4',})""
          end

          on host, ""curl -O #{opts[:mac_download_url]}/puppet-#{puppet_ver}.dmg""
          on host, ""curl -O #{opts[:mac_download_url]}/facter-#{facter_ver}.dmg""
          on host, ""curl -O #{opts[:mac_download_url]}/hiera-#{hiera_ver}.dmg""

          on host, ""hdiutil attach puppet-#{puppet_ver}.dmg""
          on host, ""hdiutil attach facter-#{facter_ver}.dmg""
          on host, ""hdiutil attach hiera-#{hiera_ver}.dmg""

          on host, ""installer -pkg /Volumes/puppet-#{puppet_ver}/puppet-#{puppet_ver}.pkg -target /""
          on host, ""installer -pkg /Volumes/facter-#{facter_ver}/facter-#{facter_ver}.pkg -target /""
          on host, ""installer -pkg /Volumes/hiera-#{hiera_ver}/hiera-#{hiera_ver}.pkg -target /""
        end

        # Installs Puppet and dependencies from gem
        #
        # @param [Host] host The host to install packages on
        # @param [Hash{Symbol=>String}] opts An options hash
        # @option opts [String] :version The version of Puppet to install, if nil installs latest
        # @option opts [String] :facter_version The version of Facter to install, if nil installs latest
        # @option opts [String] :hiera_version The version of Hiera to install, if nil installs latest
        #
        # @return nil
        # @raise [StandardError] if gem does not exist on target host
        # @api private
        def install_puppet_from_gem( host, opts )
          # There are a lot of special things to do for Solaris and Solaris 10.
          # This is easier than checking host['platform'] every time.
          is_solaris10 = host['platform'] =~ /solaris-10/
          is_solaris = host['platform'] =~ /solaris/

          # Hosts may be provisioned with csw but pkgutil won't be in the
          # PATH by default to avoid changing the behavior for Puppet's tests
          if is_solaris10
            on host, 'ln -s /opt/csw/bin/pkgutil /usr/bin/pkgutil'
          end

          # Solaris doesn't necessarily have this, but gem needs it
          if is_solaris
            on host, 'mkdir -p /var/lib'
          end

          unless host.check_for_command( 'gem' )
            gempkg = case host['platform']
                     when /solaris-11/                            then 'ruby-18'
                     when /ubuntu-14/                             then 'ruby'
                     when /solaris-10|ubuntu|debian|el-|cumulus/  then 'rubygems'
                     else
                       raise ""install_puppet() called with default_action "" +
                             ""'gem_install' but program `gem' is "" +
                             ""not installed on #{host.name}""
                     end

            host.install_package gempkg
          end

          # Link 'gem' to /usr/bin instead of adding /opt/csw/bin to PATH.
          if is_solaris10
            on host, 'ln -s /opt/csw/bin/gem /usr/bin/gem'
          end

          if host['platform'] =~ /debian|ubuntu|solaris|cumulus/
            gem_env = YAML.load( on( host, 'gem environment' ).stdout )
            gem_paths_array = gem_env['RubyGems Environment'].find {|h| h['GEM PATHS'] != nil }['GEM PATHS']
            path_with_gem = 'export PATH=' + gem_paths_array.join(':') + ':${PATH}'
            on host, ""echo '#{path_with_gem}' >> ~/.bashrc""
          end

          if opts[:facter_version]
            on host, ""gem install facter -v#{opts[:facter_version]} --no-ri --no-rdoc""
          end

          if opts[:hiera_version]
            on host, ""gem install hiera -v#{opts[:hiera_version]} --no-ri --no-rdoc""
          end

          ver_cmd = opts[:version] ? ""-v#{opts[:version]}"" : ''
          on host, ""gem install puppet #{ver_cmd} --no-ri --no-rdoc""

          # Similar to the treatment of 'gem' above.
          # This avoids adding /opt/csw/bin to PATH.
          if is_solaris
            gem_env = YAML.load( on( host, 'gem environment' ).stdout )
            # This is the section we want - this has the dir where gem executables go.
            env_sect = 'EXECUTABLE DIRECTORY'
            # Get the directory where 'gem' installs executables.
            # On Solaris 10 this is usually /opt/csw/bin
            gem_exec_dir = gem_env['RubyGems Environment'].find {|h| h[env_sect] != nil }[env_sect]

            on host, ""ln -s #{gem_exec_dir}/hiera /usr/bin/hiera""
            on host, ""ln -s #{gem_exec_dir}/facter /usr/bin/facter""
            on host, ""ln -s #{gem_exec_dir}/puppet /usr/bin/puppet""
          end
        end

        # Install official puppetlabs release repository configuration on host.
        #
        # @param [Host] host An object implementing {Beaker::Hosts}'s
        #                    interface.
        #
        # @note This method only works on redhat-like and debian-like hosts.
        #
        def install_puppetlabs_release_repo ( host )
          variant, version, arch, codename = host['platform'].to_array

          case variant
          when /^(fedora|el|centos)$/
            variant = (($1 == 'centos') ? 'el' : $1)

            rpm = options[:release_yum_repo_url] +
              ""/puppetlabs-release-%s-%s.noarch.rpm"" % [variant, version]

            on host, ""rpm -ivh #{rpm}""

          when /^(debian|ubuntu|cumulus)$/
            deb = URI.join(options[:release_apt_repo_url],  ""puppetlabs-release-%s.deb"" % codename)

            on host, ""wget -O /tmp/puppet.deb #{deb}""
            on host, ""dpkg -i --force-all /tmp/puppet.deb""
            on host, ""apt-get update""
          else
            raise ""No repository installation step for #{variant} yet...""
          end
        end

        # Install development repository on the given host. This method pushes all
        # repository information including package files for the specified
        # package_name to the host and modifies the repository configuration file
        # to point at the new repository. This is particularly useful for
        # installing development packages on hosts that can't access the builds
        # server.
        #
        # @param [Host] host An object implementing {Beaker::Hosts}'s
        #                    interface.
        # @param [String] package_name The name of the package whose repository is
        #                              being installed.
        # @param [String] build_version A string identifying the output of a",Seems like `true == ...` comparison can be dropped.,"The suggested code changes appear to be related to a Beaker-based automation script for managing Puppet installations across various operating systems. Here's a concise review of the changes:

### Key Changes:
1. **Conditional Installation Logic**:
   - The script now uses conditional statements to determine the appropriate installation method based on the host's platform. This includes handling different Linux distributions (EL, Fedora, Debian, Ubuntu, Cumulus), Windows, and macOS.

2. **Package Installation Methods**:
   - The script supports installing Puppet using RPM, DEB, MSI, and DMG formats. Each method handles specific platform requirements and configurations.

3. **Configuration Management**:
   - The script includes functions to configure Puppet's `puppet.conf` file on each host, supporting both Windows and Unix-like systems.

4. **Gem Installation**:
   - The script provides a fallback mechanism for installing Puppet using RubyGems if native package installation fails on certain platforms.

5. **Repository Management**:
   - The script includes a function to install the PuppetLabs repository configuration on supported Linux distributions.

6. **Error Handling**:
   - The script raises exceptions for unsupported platforms and missing gems, providing clear error messages.

### Review Points:
- **Platform Detection**: The script uses regular expressions to detect the host's platform, which can be improved for more robustness.
- **Error Messages**: The error messages are informative but could be more detailed, especially for unsupported platforms.
- **Code Readability**: The script is well-structured, but some sections could benefit from additional comments for clarity.
- **Testing**: Ensure comprehensive testing is done to cover all edge cases and ensure the script behaves as expected across different environments.

Overall, the script appears to be a robust solution for managing Puppet installations across various platforms using Beaker."
21,"@@ -133,6 +133,16 @@ def build_permissions_set(object_uri, unbound_permission,
     return granters
 
 
+# XXX: May need caching
+def groupfinder(userid, request):
+    authn_type = getattr(request, 'authn_type', None)
+    if authn_type is None:
+        return []
+
+    prefixed_userid = '%s:%s' % (authn_type.lower(), userid)
+    return request.registry.permission.user_principals(prefixed_userid)
+
+
 @implementer(IAuthorizationPolicy)
 class AuthorizationPolicy(CliquetAuthorization):
     def get_bound_permissions(self, *args, **kwargs):","from cliquet.authorization import AuthorizationPolicy as CliquetAuthorization
from pyramid.security import IAuthorizationPolicy
from zope.interface import implementer


# Vocab really matters when you deal with permissions. Let's do a quick recap
# of the terms used here:
#
# Object URI:
#    An unique identifier for an object.
#    for instance, /buckets/blog/collections/articles/records/article1
#
# Object:
#    A common denomination of an object (e.g. ""collection"" or ""record"")
#
# Unbound permission:
#    A permission not bound to an object (e.g. ""create"")
#
# Bound permission:
#    A permission bound to an object (e.g. ""collection:create"")


# Dictionary which list all permissions a given permission enables.
PERMISSIONS_INHERITANCE_TREE = {
    'bucket:write': {
        'bucket': ['write']
    },
    'bucket:read': {
        'bucket': ['write', 'read']
    },
    'bucket:group:create': {
        'bucket': ['write', 'group:create']
    },
    'bucket:collection:create': {
        'bucket': ['write', 'collection:create']
    },
    'group:write': {
        'bucket': ['write'],
        'group': ['write']
    },
    'group:read': {
        'bucket': ['write', 'read'],
        'group': ['write', 'read']
    },
    'collection:write': {
        'bucket': ['write'],
        'collection': ['write'],
    },
    'collection:read': {
        'bucket': ['write', 'read'],
        'collection': ['write', 'read'],
    },
    'collection:record:create': {
        'bucket': ['write'],
        'collection': ['write', 'record:create']
    },
    'record:write': {
        'bucket': ['write'],
        'collection': ['write'],
        'record': ['write']
    },
    'record:read': {
        'bucket': ['write', 'read'],
        'collection': ['write', 'read'],
        'record': ['write', 'read']
    }
}


def get_object_type(object_uri):
    """"""Return the type of an object from its id.""""""

    obj_parts = object_uri.split('/')
    if len(obj_parts) % 2 == 0:
        object_uri = '/'.join(obj_parts[:-1])

    # Order matters here. More precise is tested first.
    if 'records' in object_uri:
        obj_type = 'record'
    elif 'collections' in object_uri:
        obj_type = 'collection'
    elif 'groups' in object_uri:
        obj_type = 'group'
    elif 'buckets' in object_uri:
        obj_type = 'bucket'
    else:
        obj_type = None
    return obj_type


def build_permission_tuple(obj_type, unbound_permission, obj_parts):
    """"""Returns a tuple of (object_uri, unbound_permission)""""""
    PARTS_LENGTH = {
        'bucket': 3,
        'collection': 5,
        'group': 5,
        'record': 7
    }
    if obj_type not in PARTS_LENGTH:
        raise ValueError('Invalid object type: %s' % obj_type)

    if PARTS_LENGTH[obj_type] > len(obj_parts):
        raise ValueError('You cannot build children keys from its parent key.'
                         'Trying to build type ""%s"" from object key ""%s"".' % (
                             obj_type, '/'.join(obj_parts)))
    length = PARTS_LENGTH[obj_type]
    return ('/'.join(obj_parts[:length]), unbound_permission)


def build_permissions_set(object_uri, unbound_permission,
                          inheritance_tree=None):
    """"""Build a set of all permissions that can grant access to the given
    object URI and unbound permission.

    >>> build_required_permissions('/buckets/blog', 'write')
    set(('/buckets/blog', 'write'))

    """"""

    if inheritance_tree is None:
        inheritance_tree = PERMISSIONS_INHERITANCE_TREE

    obj_type = get_object_type(object_uri)

    bound_permission = '%s:%s' % (obj_type, unbound_permission)
    granters = set()

    obj_parts = object_uri.split('/')
    for obj, permission_list in inheritance_tree[bound_permission].items():
        for permission in permission_list:
            granters.add(build_permission_tuple(obj, permission, obj_parts))

# XXX: May need caching
def groupfinder(userid, request):
    authn_type = getattr(request, 'authn_type', None)
    if authn_type is None:
        return []

    prefixed_userid = '%s:%s' % (authn_type.lower(), userid)
    return request.registry.permission.user_principals(prefixed_userid)


    return granters


@implementer(IAuthorizationPolicy)
class AuthorizationPolicy(CliquetAuthorization):
    def get_bound_permissions(self, *args, **kwargs):
        return build_permissions_set(*args, **kwargs)
",Now we have `request.prefixed_userid` :),"The suggested code change introduces a new authorization policy for Pyramid applications, which allows for granular control over permissions based on object types and unbound permissions. The changes include:

1. **Functionality**:
   - `get_object_type`: Determines the type of an object from its URI.
   - `build_permission_tuple`: Constructs a tuple of (object URI, unbound permission).
   - `build_permissions_set`: Builds a set of all permissions that can grant access to a given object URI and unbound permission.
   - `groupfinder`: Retrieves user principals based on authentication type and user ID.

2. **Implementation**:
   - The `AuthorizationPolicy` class inherits from `CliquetAuthorization` and implements the `IAuthorizationPolicy` interface.
   - The `get_bound_permissions` method uses the `build_permissions_set` function to determine the permissions that can be granted to a user for a specific object URI and unbound permission.

3. **Caching**:
   - There is a comment indicating that caching may be needed for performance reasons, but it is not implemented in the current code.

4. **Documentation**:
   - The code includes comments explaining the purpose of each function and the structure of the permission inheritance tree.

Overall, this change provides a robust framework for managing permissions in a Pyramid application, allowing for flexible and granular control over access based on object types and unbound permissions."
22,"@@ -49,6 +49,9 @@ SIGNATURE_V4_PARAMS = [
     'X-Amz-SignedHeaders', 'X-Amz-Signature'
 ]
 
+# headers to blacklist from request_dict.signed_headers
+BLACKLISTED_HEADERS = ['X-Amz-Security-Token']
+
 # query params overrides for multipart upload and node sdk
 ALLOWED_QUERY_PARAMS = [
     'X-id', 'X-Amz-User-Agent', 'X-Amz-Content-Sha256',","import re
import time
import logging
import datetime
from localstack import config
from collections import namedtuple
from botocore.compat import urlsplit
from botocore.credentials import Credentials
from urllib.parse import parse_qs, urlencode
from six.moves.urllib import parse as urlparse
from botocore.awsrequest import create_request_object
from localstack.utils.auth import HmacV1QueryAuth, S3SigV4QueryAuth
from localstack.utils.aws.aws_responses import requests_error_response_xml_signature_calculation
from localstack.constants import (
    S3_VIRTUAL_HOSTNAME, S3_STATIC_WEBSITE_HOSTNAME, TEST_AWS_ACCESS_KEY_ID, TEST_AWS_SECRET_ACCESS_KEY)


LOGGER = logging.getLogger(__name__)

REGION_REGEX = r'[a-z]{2}-[a-z]+-[0-9]{1,}'
PORT_REGEX = r'(:[\d]{0,6})?'
S3_STATIC_WEBSITE_HOST_REGEX = r'^([^.]+)\.s3-website\.localhost\.localstack\.cloud(:[\d]{0,6})?$'
S3_VIRTUAL_HOSTNAME_REGEX = (r'^(http(s)?://)?((?!s3\.)[^\./]+)\.'
                             r'(((s3(-website)?\.)?localhost\.localstack\.cloud)|({})|'
                             r'(s3((-website)|(-external-1))?[\.-](dualstack\.)?'
                             r'({}\.)?amazonaws\.com(.cn)?)){}$').format(
                                 config.HOSTNAME_EXTERNAL, REGION_REGEX, PORT_REGEX)
BUCKET_NAME_REGEX = (r'(?=^.{3,63}$)(?!^(\d+\.)+\d+$)' +
    r'(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)')

HOST_COMBINATION_REGEX = r'^(.*)(:[\d]{0,6})'
PORT_REPLACEMENT = [':80', ':443', ':4566', '']

# response header overrides the client may request
ALLOWED_HEADER_OVERRIDES = {
    'response-content-type': 'Content-Type',
    'response-content-language': 'Content-Language',
    'response-expires': 'Expires',
    'response-cache-control': 'Cache-Control',
    'response-content-disposition': 'Content-Disposition',
    'response-content-encoding': 'Content-Encoding',
}

# params are required in presigned url
SIGNATURE_V2_PARAMS = ['Signature', 'Expires', 'AWSAccessKeyId']

SIGNATURE_V4_PARAMS = [
    'X-Amz-Algorithm', 'X-Amz-Credential', 'X-Amz-Date', 'X-Amz-Expires',
# headers to blacklist from request_dict.signed_headers
BLACKLISTED_HEADERS = ['X-Amz-Security-Token']

    'X-Amz-SignedHeaders', 'X-Amz-Signature'
]

# query params overrides for multipart upload and node sdk
ALLOWED_QUERY_PARAMS = [
    'X-id', 'X-Amz-User-Agent', 'X-Amz-Content-Sha256',
    'versionid', 'uploadid', 'partnumber'
]


def is_static_website(headers):
    """"""
    Determine if the incoming request is for s3 static website hosting
    returns True if the host matches website regex
    returns False if the host does not matches website regex
    """"""
    return bool(re.match(S3_STATIC_WEBSITE_HOST_REGEX, headers.get('host', '')))


def uses_host_addressing(headers):
    """"""
    Determines if the bucket is using host based addressing style or path based
    """"""
    # we can assume that the host header we are receiving here is actually the header we originally received
    # from the client (because the edge service is forwarding the request in memory)
    match = re.match(S3_VIRTUAL_HOSTNAME_REGEX, headers.get('host', ''))
    return True if match and match.group(3) else False


def extract_bucket_name(headers, path):
    """"""
    Extract the bucket name
    if using host based addressing it's extracted from host header
    if using path based addressing it's extracted form the path
    """"""
    bucket_name = None
    if uses_host_addressing(headers):
        pattern = re.compile(S3_VIRTUAL_HOSTNAME_REGEX)
        match = pattern.match(headers.get('host', ''))

        if match and match.group(3):
            bucket_name = match.group(3)
    else:
        bucket_name = path.split('/', 2)[1]
    return bucket_name if bucket_name else None


def extract_key_name(headers, path):
    """"""
    Extract the key name from the path depending on addressing_style
    """"""
    key_name = None
    path = path.split('?')[0]  # strip off query params from path
    if uses_host_addressing(headers):
        split = path.split('/', 1)
        if len(split) > 1:
            key_name = split[1]
    else:
        split = path.split('/', 2)
        if len(split) > 2:
            key_name = split[2]

    return key_name if key_name else None


def extract_bucket_and_key_name(headers, path):
    return extract_bucket_name(headers, path), extract_key_name(headers, path)


def validate_bucket_name(bucket_name):
    """"""
    Validate s3 bucket name based on the documentation
    ref. https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html
    """"""
    return True if re.match(BUCKET_NAME_REGEX, bucket_name) else False


def get_bucket_hostname(bucket_name):
    """"""
    Get bucket name for addressing style host
    """"""
    return '%s.%s:%s' % (bucket_name, S3_VIRTUAL_HOSTNAME, config.EDGE_PORT)


def get_bucket_website_hostname(bucket_name):
    """"""
    Get bucket name for addressing style host for website hosting
    """"""
    return '%s.%s:%s' % (bucket_name, S3_STATIC_WEBSITE_HOSTNAME, config.EDGE_PORT)


def get_forwarded_for_host(headers):
    x_forwarded_header = re.split(r',\s?', headers.get('X-Forwarded-For', ''))
    host = x_forwarded_header[len(x_forwarded_header) - 1]
    return host


def is_real_s3_url(url):
    return re.match(r'.*s3(\-website)?\.([^\.]+\.)?amazonaws.com.*', url or '')


def authenticate_presign_url(method, path, headers, data=None):

    url = '{}{}'.format(config.get_edge_url(), path)
    parsed = urlparse.urlparse(url)
    query_params = parse_qs(parsed.query)
    forwarded_for = get_forwarded_for_host(headers)
    if forwarded_for:
        url = re.sub('://[^/]+', '://%s' % forwarded_for, url)

    LOGGER.debug('Received presign S3 URL: %s' % url)

    sign_headers = {}
    query_string = {}

    is_v2 = all([p in query_params for p in SIGNATURE_V2_PARAMS])
    is_v4 = all([p in query_params for p in SIGNATURE_V4_PARAMS])

    # Add overrided headers to the query string params
    for param_name, header_name in ALLOWED_HEADER_OVERRIDES.items():
        if param_name in query_params:
            query_string[param_name] = query_params[param_name][0]

    # Request's headers are more essentials than the query parameters in the request.
    # Different values of header in the header of the request and in the query parameter of the
    # request URL will fail the signature calulation. As per the AWS behaviour

    # Add valid headers into the sign_header. Skip the overrided headers
    # and the headers which have been sent in the query string param
    presign_params_lower = \
        [p.lower() for p in SIGNATURE_V4_PARAMS] if is_v4 else [p.lower() for p in SIGNATURE_V2_PARAMS]
    params_header_override = [param_name for param_name, header_name in ALLOWED_HEADER_OVERRIDES.items()]
    if len(query_params) > 2:
        for key in query_params:
            key_lower = key.lower()
            if key_lower not in presign_params_lower:
                if (key_lower not in (header[0].lower() for header in headers) and
                        key_lower not in params_header_override):
                    if key_lower in (allowed_param.lower() for allowed_param in ALLOWED_QUERY_PARAMS):
                        query_string[key] = query_params[key][0]
                    else:
                        sign_headers[key] = query_params[key][0]

    for header_name, header_value in headers.items():
        header_name_lower = header_name.lower()
        if header_name_lower.startswith('x-amz-') or header_name_lower.startswith('content-'):
            if is_v2 and header_name_lower in query_params:
                sign_headers[header_name] = header_value
            if is_v4 and header_name_lower in query_params['X-Amz-SignedHeaders'][0]:
                sign_headers[header_name] = header_value

    # Preparnig dictionary of request to build AWSRequest's object of the botocore
    request_url = '{}://{}{}'.format(parsed.scheme, parsed.netloc, parsed.path)
    # Fix https://github.com/localstack/localstack/issues/3912
    # urlencode method replaces white spaces with plus sign cause signature calculation to fail
    request_url = ('%s?%s' % (request_url, urlencode(query_string, quote_via=urlparse.quote, safe=' '))
        if query_string else request_url)
    if forwarded_for:
        request_url = re.sub('://[^/]+', '://%s' % forwarded_for, request_url)

    bucket_name = extract_bucket_name(headers, parsed.path)

    request_dict = {
        'url_path': parsed.path,
        'query_string': query_string,
        'method': method,
        'headers': sign_headers,
        'body': b'',
        'url': request_url,
        'context': {
            'is_presign_request': True,
            'use_global_endpoint': True,
            'signing': {
                'bucket': bucket_name
            }
        }
    }

    # Support for virtual host addressing style in signature version 2
    # We don't need to do this in v4 as we already concerting it to the virtual addressing style.
    # v2 require path base styled request_dict and v4 require virtual styled request_dict

    if uses_host_addressing(headers) and is_v2:
        request_dict['url_path'] = '/{}{}'.format(bucket_name, request_dict['url_path'])
        parsed_url = urlparse.urlparse(request_url)
        request_dict['url'] = '{}://{}:{}{}'.format(
            parsed_url.scheme, S3_VIRTUAL_HOSTNAME, config.EDGE_PORT, request_dict['url_path'])
        request_dict['url'] = \
            ('%s?%s' % (request_dict['url'], urlencode(query_string)) if query_string else request_dict['url'])

    if not is_v2 and any([p in query_params for p in SIGNATURE_V2_PARAMS]):
        response = requests_error_response_xml_signature_calculation(
            code=403,
            message='Query-string authentication requires the Signature, Expires and AWSAccessKeyId parameters',
            code_string='AccessDenied'
        )
    elif is_v2 and not is_v4:
        response = authenticate_presign_url_signv2(method, path, headers, data, url, query_params, request_dict)

    if not is_v4 and any([p in query_params for p in SIGNATURE_V4_PARAMS]):","Looks great, Thanks for this PR @macnev2013 !   nit: I'd rename this to something more descriptive, e.g., `PRESIGN_SKIPPED_HEADERS = ...` (not critical - can be done in a future PR..).","The suggested code change appears to be implementing a function to authenticate presigned URLs for Amazon S3. Here's a concise review:

### Key Points:
1. **Functionality**: The function `authenticate_presign_url` is designed to handle presigned URLs for S3, supporting both Signature Version 2 and Version 4.
2. **Regex Patterns**: Several regex patterns are defined to validate bucket names and hostnames, ensuring they meet AWS requirements.
3. **Header Handling**: The function processes headers to extract bucket and key names, determine addressing styles, and handle query parameters.
4. **Authentication Logic**:
   - It checks for the presence of necessary query parameters for each signature version.
   - For Signature Version 2, it constructs a request dictionary and calculates the signature.
   - For Signature Version 4, it similarly constructs a request dictionary but uses a different approach to handle virtual host addressing.
5. **Error Handling**: If the query parameters are missing for either version, it returns an error response.

### Potential Improvements:
- **Code Readability**: The code could benefit from better variable naming and comments to improve readability.
- **Edge Cases**: Consider adding more edge cases, such as handling invalid URLs or malformed query strings.
- **Performance**: Ensure that the function is efficient, especially when dealing with large numbers of requests.

Overall, the code provides a robust implementation for authenticating presigned URLs for S3, handling both signature versions effectively."
23,"@@ -33,9 +33,12 @@ final class MergeReleaseCandidateBranchReleaseWorker extends AbstractShopsysRele
      */
     public function work(Version $version): void
     {
-        $this->symfonyStyle->note('You need to create a merge commit, see https://docs.shopsys.com/en/latest/contributing/merging-on-github/ for detailed instructions.');
-        $this->symfonyStyle->warning(sprintf('If you are creating the merge commit locally, do not forget to push the ""%s"" branch!', $this->initialBranchName));
-        $this->confirm(sprintf('Confirm ""%s"" branch was merged to ""%s""', $this->createBranchName($version), $this->initialBranchName));
+        $this->symfonyStyle->note('You need to create a merge commit locally, see https://docs.shopsys.com/en/latest/contributing/merging-on-github/ for detailed instructions.');
+        $this->symfonyStyle->warning(sprintf(
+            'Do not forget to push the ""%s"" branch!',
+            $this->initialBranchName
+        ));
+        $this->confirm(sprintf('Confirm ""%s"" branch was merged and pushed to ""%s""', $this->createBranchName($version), $this->initialBranchName));
 
         if ($this->initialBranchName === 'master') {
             $this->symfonyStyle->note('Rest assured, after the master branch is built on Heimdall, it is split automatically (using http://heimdall:8080/view/Tools/job/tool-monorepo-split/)');","<?php

declare(strict_types=1);

namespace Shopsys\Releaser\ReleaseWorker\Release;

use PharIo\Version\Version;
use Shopsys\Releaser\ReleaseWorker\AbstractShopsysReleaseWorker;
use Shopsys\Releaser\Stage;

final class MergeReleaseCandidateBranchReleaseWorker extends AbstractShopsysReleaseWorker
{
    /**
     * @param \PharIo\Version\Version $version
     * @return string
     */
    public function getDescription(Version $version): string
    {
        return sprintf('[Manually] Merge ""%s"" branch into ""%s""', $this->createBranchName($version), $this->initialBranchName);
    }

    /**
     * Higher first
     * @return int
     */
    public function getPriority(): int
    {
        return 650;
    }

    /**
     * @param \PharIo\Version\Version $version
        $this->symfonyStyle->note('You need to create a merge commit locally, see https://docs.shopsys.com/en/latest/contributing/merging-on-github/ for detailed instructions.');
        $this->symfonyStyle->warning(sprintf(
            'Do not forget to push the ""%s"" branch!',
            $this->initialBranchName
        ));
        $this->confirm(sprintf('Confirm ""%s"" branch was merged and pushed to ""%s""', $this->createBranchName($version), $this->initialBranchName));
     */
    public function work(Version $version): void
    {
        $this->symfonyStyle->note('You need to create a merge commit, see https://docs.shopsys.com/en/latest/contributing/merging-on-github/ for detailed instructions.');
        $this->symfonyStyle->warning(sprintf('If you are creating the merge commit locally, do not forget to push the ""%s"" branch!', $this->initialBranchName));
        $this->confirm(sprintf('Confirm ""%s"" branch was merged to ""%s""', $this->createBranchName($version), $this->initialBranchName));

        if ($this->initialBranchName === 'master') {
            $this->symfonyStyle->note('Rest assured, after the master branch is built on Heimdall, it is split automatically (using http://heimdall:8080/view/Tools/job/tool-monorepo-split/)');
        } else {
            $this->symfonyStyle->note(sprintf('You need split the ""%s"" branch it using ""tool-monorepo-split-branch"" on Heimdall (http://heimdall:8080/view/Tools/job/tool-monorepo-split-branch/)', $this->initialBranchName));
        }
        $this->confirm('Confirm the branch is split.');
    }

    /**
     * @return string
     */
    public function getStage(): string
    {
        return Stage::RELEASE;
    }
}
","If you need to create a merge commit locally, why do we provide link for docs about merging on github? :D","The suggested code change appears to be a modification to the `MergeReleaseCandidateBranchReleaseWorker` class in the Shopsys Releaser project. The changes include:

1. **Description Method**: The method now returns a formatted string describing the action to be performed, which includes the creation of a merge commit and the warning about pushing the initial branch.

2. **Work Method**: The method now includes a note about creating a merge commit and a warning about pushing the initial branch. It also includes a confirmation step to ensure the branch has been merged and pushed.

3. **Split Branch Method**: A new note is added to inform users about splitting the branch using Heimdall if the initial branch is not `master`. A confirmation step is also included for this action.

4. **Stage Method**: The method returns the stage as `Stage::RELEASE`, indicating that this release worker is part of the release process.

Overall, the changes aim to provide clear instructions and confirmations for the release process, ensuring that all necessary steps are followed correctly."
24,"@@ -123,6 +123,15 @@ def distribution():
                             id=dist_id)
 
 
+def sandboxed():
+    """"""Check whether the environment is a sandbox, i.e. has restricted access to the host system.
+
+    Return:
+        True if the environment is a sandbox.
+    """"""
+    return distribution().parsed == Distribution.kde
+
+
 def _git_str():
     """"""Try to find out git version.
 ","# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:

# Copyright 2014-2019 Florian Bruhin (The Compiler) <mail@qutebrowser.org>
#
# This file is part of qutebrowser.
#
# qutebrowser is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# qutebrowser is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with qutebrowser.  If not, see <http://www.gnu.org/licenses/>.

""""""Utilities to show various version information.""""""

import re
import sys
import glob
import os.path
import platform
import subprocess
import importlib
import collections
import enum
import datetime
import getpass

import attr
import pkg_resources
from PyQt5.QtCore import PYQT_VERSION_STR, QLibraryInfo
from PyQt5.QtNetwork import QSslSocket
from PyQt5.QtGui import (QOpenGLContext, QOpenGLVersionProfile,
                         QOffscreenSurface)
from PyQt5.QtWidgets import QApplication

try:
    from PyQt5.QtWebKit import qWebKitVersion
except ImportError:  # pragma: no cover
    qWebKitVersion = None  # type: ignore  # noqa: N816

try:
    from PyQt5.QtWebEngineWidgets import QWebEngineProfile
except ImportError:  # pragma: no cover
    QWebEngineProfile = None  # type: ignore

import qutebrowser
from qutebrowser.utils import log, utils, standarddir, usertypes, message
from qutebrowser.misc import objects, earlyinit, sql, httpclient, pastebin
from qutebrowser.browser import pdfjs

try:
    from qutebrowser.browser.webengine import webenginesettings
except ImportError:  # pragma: no cover
    webenginesettings = None  # type: ignore


@attr.s
class DistributionInfo:

    """"""Information about the running distribution.""""""

    id = attr.ib()
    parsed = attr.ib()
    version = attr.ib()
    pretty = attr.ib()


pastebin_url = None
Distribution = enum.Enum(
    'Distribution', ['unknown', 'ubuntu', 'debian', 'void', 'arch',
                     'gentoo', 'fedora', 'opensuse', 'linuxmint', 'manjaro',
                     'kde'])


def distribution():
    """"""Get some information about the running Linux distribution.

    Returns:
        A DistributionInfo object, or None if no info could be determined.
            parsed: A Distribution enum member
            version: A Version object, or None
            pretty: Always a string (might be ""Unknown"")
    """"""
    filename = os.environ.get('QUTE_FAKE_OS_RELEASE', '/etc/os-release')
    info = {}
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if (not line) or line.startswith('#'):
                    continue
                k, v = line.split(""="", maxsplit=1)
                info[k] = v.strip('""')
    except (OSError, UnicodeDecodeError):
        return None

    pretty = info.get('PRETTY_NAME', 'Unknown')
    if pretty == 'Linux':  # Thanks, Funtoo
        pretty = info.get('NAME', pretty)

    if 'VERSION_ID' in info:
        dist_version = pkg_resources.parse_version(info['VERSION_ID'])
    else:
        dist_version = None

    dist_id = info.get('ID', None)
    id_mappings = {
        'funtoo': 'gentoo',  # does not have ID_LIKE=gentoo
        'org.kde.Platform': 'kde',
    }
    try:
        parsed = Distribution[id_mappings.get(dist_id, dist_id)]
    except KeyError:
        parsed = Distribution.unknown

    return DistributionInfo(parsed=parsed, version=dist_version, pretty=pretty,
def sandboxed():
    """"""Check whether the environment is a sandbox, i.e. has restricted access to the host system.

    Return:
        True if the environment is a sandbox.
    """"""
    return distribution().parsed == Distribution.kde


                            id=dist_id)


def _git_str():
    """"""Try to find out git version.

    Return:
        string containing the git commit ID.
        None if there was an error or we're not in a git repo.
    """"""
    # First try via subprocess if possible
    commit = None
    if not hasattr(sys, ""frozen""):
        try:
            gitpath = os.path.join(os.path.dirname(os.path.realpath(__file__)),
                                   os.path.pardir, os.path.pardir)
        except (NameError, OSError):
            log.misc.exception(""Error while getting git path"")
        else:
            commit = _git_str_subprocess(gitpath)
    if commit is not None:
        return commit
    # If that fails, check the git-commit-id file.
    try:
        return utils.read_file('git-commit-id')
    except (OSError, ImportError):
        return None


def _git_str_subprocess(gitpath):
    """"""Try to get the git commit ID and timestamp by calling git.

    Args:
        gitpath: The path where the .git folder is.

    Return:
        The ID/timestamp on success, None on failure.
    """"""
    if not os.path.isdir(os.path.join(gitpath, "".git"")):
        return None
    try:
        # https://stackoverflow.com/questions/21017300/21017394#21017394
        commit_hash = subprocess.run(
            ['git', 'describe', '--match=NeVeRmAtCh', '--always', '--dirty'],
            cwd=gitpath, check=True,
            stdout=subprocess.PIPE).stdout.decode('UTF-8').strip()
        date = subprocess.run(
            ['git', 'show', '-s', '--format=%ci', 'HEAD'],
            cwd=gitpath, check=True,
            stdout=subprocess.PIPE).stdout.decode('UTF-8').strip()
        return '{} ({})'.format(commit_hash, date)
    except (subprocess.CalledProcessError, OSError):
        return None


def _release_info():
    """"""Try to gather distribution release information.

    Return:
        list of (filename, content) tuples.
    """"""
    blacklisted = ['ANSI_COLOR=', 'HOME_URL=', 'SUPPORT_URL=',
                   'BUG_REPORT_URL=']
    data = []
    for fn in glob.glob(""/etc/*-release""):
        lines = []
        try:
            with open(fn, 'r', encoding='utf-8') as f:
                for line in f.read().strip().splitlines():
                    if not any(line.startswith(bl) for bl in blacklisted):
                        lines.append(line)

                if lines:
                    data.append((fn, '\n'.join(lines)))
        except OSError:
            log.misc.exception(""Error while reading {}."".format(fn))
    return data


def _module_versions():
    """"""Get versions of optional modules.

    Return:
        A list of lines with version info.
    """"""
    lines = []
    modules = collections.OrderedDict([
        ('sip', ['SIP_VERSION_STR']),
        ('colorama', ['VERSION', '__version__']),
        ('pypeg2', ['__version__']),
        ('jinja2', ['__version__']),
        ('pygments', ['__version__']),
        ('yaml', ['__version__']),
        ('cssutils', ['__version__']),
        ('attr', ['__version__']),
        ('PyQt5.QtWebEngineWidgets', []),
        ('PyQt5.QtWebKitWidgets', []),
    ])
    for modname, attributes in modules.items():
        try:
            module = importlib.import_module(modname)
        except ImportError:
            text = '{}: no'.format(modname)
        else:
            for name in attributes:
                try:
                    text = '{}: {}'.format(modname, getattr(module, name))
                except AttributeError:
                    pass
                else:
                    break
            else:
                text = '{}: yes'.format(modname)
        lines.append(text)
    return lines


def _path_info():
    """"""Get info about important path names.

    Return:
        A dictionary of descriptive to actual path names.
    """"""
    info = {
        'config': standarddir.config(),
        'data': standarddir.data(),
        'cache': standarddir.cache(),
        'runtime': standarddir.runtime(),
    }
    if standarddir.config() != standarddir.config(auto=True):
        info['auto config'] = standarddir.config(auto=True)
    if standarddir.data() != standarddir.data(system=True):
        info['system data'] = standarddir.data(system=True)
    return info


def _os_info():
    """"""Get operating system info.

    Return:
        A list of lines with version info.
    """"""
    lines = []
    releaseinfo = None
    if utils.is_linux:
        osver = ''
        releaseinfo = _release_info()
    elif utils.is_windows:
        osver = ', '.join(platform.win32_ver())
    elif utils.is_mac:
        release, versioninfo, machine = platform.mac_ver()
        if all(not e for e in versioninfo):
            versioninfo = ''
        else:
            versioninfo = '.'.join(versioninfo)
        osver = ', '.join([e for e in [release, versioninfo, machine] if e])
    elif utils.is_posix:
        osver = ' '.join(platform.uname())
    else:
        osver = '?'
    lines.append('OS Version: {}'.format(osver))
    if releaseinfo is not None:
        for (fn, data) in releaseinfo:
            lines += ['', '--- {} ---'.format(fn), data]
    return lines


def _pdfjs_version():
    """"""Get the pdf.js version.

    Return:
        A string with the version number.
    """"""
    try:
        pdfjs_file, file_path = pdfjs.get_pdfjs_res_and_path('build/pdf.js')
    except pdfjs.PDFJSNotFound:
        return 'no'
    else:
        pdfjs_file = pdfjs_file.decode('utf-8')
        version_re = re.compile(
            r""^ *(PDFJS\.version|var pdfjsVersion) = '([^']+)';$"",
            re.MULTILINE)

        match = version_re.search(pdfjs_file)
        if not match:
            pdfjs_version = 'unknown'
        else:
            pdfjs_version = match.group(2)
        if file_path is None:
            file_path = 'bundled'
        return '{} ({})'.format(pdfjs_version, file_path)


def _chromium_version():
    """"""Get the Chromium version for QtWebEngine.

    This can also be checked by looking at this file with the right Qt tag:
    http://code.qt.io/cgit/qt/qtwebengine.git/tree/tools/scripts/version_resolver.py#n41

    Quick reference:",`distribution()` can be `None` (e.g. on non-Linux systems),"This code provides utilities to gather various version information about the running qutebrowser application, including the distribution, git commit ID, module versions, path information, OS details, and PDF.js version. The code uses several functions to retrieve this information and returns it in a structured format. The `distribution` function determines the running Linux distribution, while `_git_str` retrieves the git commit ID. The `_module_versions` function lists the versions of optional modules used by qutebrowser. The `_path_info` function provides information about important path names used by qutebrowser. The `_os_info` function gathers operating system details, including the version and release information. The `_pdfjs_version` function retrieves the version of the bundled PDF.js library. The `_chromium_version` function is commented out and appears to be related to retrieving the Chromium version for QtWebEngine. Overall, this code provides a comprehensive way to gather version information about qutebrowser."
25,"@@ -1207,6 +1207,8 @@ class Frame(object, metaclass=ABCMeta):
         >>> df['b'].sum(min_count=3)
         nan
         """"""
+        axis = validate_axis(axis)
+        numeric_only = None if axis == 1 else numeric_only
 
         def sum(spark_column, spark_type):
             if isinstance(spark_type, BooleanType):","        index : bool, default True
            Write row names (index).
        index_label : str or sequence, optional
            Column label for index column(s) if desired. If not specified, and
            `header` and `index` are True, then the index names are used. A
            sequence should be given if the DataFrame uses MultiIndex.
        startrow : int, default 0
            Upper left cell row to dump data frame.
        startcol : int, default 0
            Upper left cell column to dump data frame.
        engine : str, optional
            Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set this
            via the options ``io.excel.xlsx.writer``, ``io.excel.xls.writer``, and
            ``io.excel.xlsm.writer``.
        merge_cells : bool, default True
            Write MultiIndex and Hierarchical Rows as merged cells.
        encoding : str, optional
            Encoding of the resulting excel file. Only necessary for xlwt,
            other writers support unicode natively.
        inf_rep : str, default 'inf'
            Representation for infinity (there is no native representation for
            infinity in Excel).
        verbose : bool, default True
            Display more information in the error logs.
        freeze_panes : tuple of int (length 2), optional
            Specifies the one-based bottommost row and rightmost column that
            is to be frozen.

        Notes
        -----
        Once a workbook has been saved it is not possible write further data
        without rewriting the whole workbook.

        See Also
        --------
        read_excel : Read Excel file.

        Examples
        --------
        Create, write to and save a workbook:

        >>> df1 = ks.DataFrame([['a', 'b'], ['c', 'd']],
        ...                    index=['row 1', 'row 2'],
        ...                    columns=['col 1', 'col 2'])
        >>> df1.to_excel(""output.xlsx"")  # doctest: +SKIP

        To specify the sheet name:

        >>> df1.to_excel(""output.xlsx"")  # doctest: +SKIP
        >>> df1.to_excel(""output.xlsx"",
        ...              sheet_name='Sheet_name_1')  # doctest: +SKIP

        If you wish to write to more than one sheet in the workbook, it is
        necessary to specify an ExcelWriter object:

        >>> with pd.ExcelWriter('output.xlsx') as writer:  # doctest: +SKIP
        ...      df1.to_excel(writer, sheet_name='Sheet_name_1')
        ...      df2.to_excel(writer, sheet_name='Sheet_name_2')

        To set the library that is used to write the Excel file,
        you can pass the `engine` keyword (the default engine is
        automatically chosen depending on the file extension):

        >>> df1.to_excel('output1.xlsx', engine='xlsxwriter')  # doctest: +SKIP
        """"""
        # Make sure locals() call is at the top of the function so we don't capture local variables.
        args = locals()
        kdf = self

        if isinstance(self, ks.DataFrame):
            f = pd.DataFrame.to_excel
        elif isinstance(self, ks.Series):
            f = pd.Series.to_excel
        else:
            raise TypeError(
                ""Constructor expects DataFrame or Series; however, "" ""got [%s]"" % (self,)
            )
        return validate_arguments_and_invoke_function(
            kdf._to_internal_pandas(), self.to_excel, f, args
        )

    def mean(
        self, axis: Union[int, str] = None, numeric_only: bool = True
    ) -> Union[Scalar, ""Series""]:
        """"""
        Return the mean of the values.

        Parameters
        ----------
        axis : {index (0), columns (1)}
            Axis for the function to be applied on.
        numeric_only : bool, default True
            Include only float, int, boolean columns. False is not supported. This parameter
            is mainly for pandas compatibility.

        Returns
        -------
        mean : scalar for a Series, and a Series for a DataFrame.

        Examples
        --------

        >>> df = ks.DataFrame({'a': [1, 2, 3, np.nan], 'b': [0.1, 0.2, 0.3, np.nan]},
        ...                   columns=['a', 'b'])

        On a DataFrame:

        >>> df.mean()
        a    2.0
        b    0.2
        dtype: float64

        >>> df.mean(axis=1)
        0    0.55
        1    1.10
        2    1.65
        3     NaN
        dtype: float64

        On a Series:

        >>> df['a'].mean()
        2.0
        """"""

        def mean(spark_column, spark_type):
            if isinstance(spark_type, BooleanType):
                spark_column = spark_column.cast(LongType())
            elif not isinstance(spark_type, NumericType):
                raise TypeError(
                    ""Could not convert {} ({}) to numeric"".format(
                        spark_type_to_pandas_dtype(spark_type), spark_type.simpleString()
                    )
                )
            return F.mean(spark_column)

        return self._reduce_for_stat_function(
            mean, name=""mean"", axis=axis, numeric_only=numeric_only
        )

    def sum(
        self, axis: Union[int, str] = None, numeric_only: bool = True, min_count: int = 0
    ) -> Union[Scalar, ""Series""]:
        """"""
        Return the sum of the values.

        Parameters
        ----------
        axis : {index (0), columns (1)}
            Axis for the function to be applied on.
        numeric_only : bool, default True
            Include only float, int, boolean columns. False is not supported. This parameter
            is mainly for pandas compatibility.
        min_count : int, default 0
            The required number of valid values to perform the operation. If fewer than
             ``min_count`` non-NA values are present the result will be NA.

        Returns
        -------
        sum : scalar for a Series, and a Series for a DataFrame.

        Examples
        --------

        >>> df = ks.DataFrame({'a': [1, 2, 3, np.nan], 'b': [0.1, np.nan, 0.3, np.nan]},
        ...                   columns=['a', 'b'])

        On a DataFrame:

        >>> df.sum()
        a    6.0
        b    0.4
        dtype: float64

        >>> df.sum(axis=1)
        0    1.1
        1    2.0
        2    3.3
        3    0.0
        dtype: float64

        >>> df.sum(min_count=3)
        a    6.0
        b    NaN
        dtype: float64

        >>> df.sum(axis=1, min_count=1)
        0    1.1
        1    2.0
        2    3.3
        3    NaN
        dtype: float64

        On a Series:

        >>> df['a'].sum()
        6.0

        >>> df['a'].sum(min_count=3)
        6.0
        axis = validate_axis(axis)
        numeric_only = None if axis == 1 else numeric_only
        >>> df['b'].sum(min_count=3)
        nan
        """"""

        def sum(spark_column, spark_type):
            if isinstance(spark_type, BooleanType):
                spark_column = spark_column.cast(LongType())
            elif not isinstance(spark_type, NumericType):
                raise TypeError(
                    ""Could not convert {} ({}) to numeric"".format(
                        spark_type_to_pandas_dtype(spark_type), spark_type.simpleString()
                    )
                )
            return F.coalesce(F.sum(spark_column), F.lit(0))

        return self._reduce_for_stat_function(
            sum, name=""sum"", axis=axis, numeric_only=numeric_only, min_count=min_count
        )

    def product(
        self, axis: Union[int, str] = None, numeric_only: bool = True, min_count: int = 0
    ) -> Union[Scalar, ""Series""]:
        """"""
        Return the product of the values.

        .. note:: unlike pandas', Koalas' emulates product by ``exp(sum(log(...)))``
            trick. Therefore, it only works for positive numbers.

        Parameters
        ----------
        axis : {index (0), columns (1)}
            Axis for the function to be applied on.
        numeric_only : bool, default True
            Include only float, int, boolean columns. False is not supported. This parameter
            is mainly for pandas compatibility.
        min_count : int, default 0
            The required number of valid values to perform the operation. If fewer than
            ``min_count`` non-NA values are present the result will be NA.

        Examples
        --------
        On a DataFrame:

        Non-numeric type column is not included to the result.

        >>> kdf = ks.DataFrame({'A': [1, 2, 3, 4, 5],
        ...                     'B': [10, 20, 30, 40, 50],
        ...                     'C': ['a', 'b', 'c', 'd', 'e']})
        >>> kdf
           A   B  C
        0  1  10  a
        1  2  20  b
        2  3  30  c
        3  4  40  d
        4  5  50  e

        >>> kdf.prod()
        A         120
        B    12000000
        dtype: int64

        If there is no numeric type columns, returns empty Series.

        >>> ks.DataFrame({""key"": ['a', 'b', 'c'], ""val"": ['x', 'y', 'z']}).prod()
        Series([], dtype: float64)

        On a Series:

        >>> ks.Series([1, 2, 3, 4, 5]).prod()
        120

        By default, the product of an empty or all-NA Series is ``1``

        >>> ks.Series([]).prod()
        1.0

        This can be controlled with the ``min_count`` parameter

        >>> ks.Series([]).prod(min_count=1)
        nan
        """"""

        def prod(spark_column, spark_type):
            if isinstance(spark_type, BooleanType):
                scol = F.min(F.coalesce(spark_column, F.lit(True))).cast(LongType())
            elif isinstance(spark_type, NumericType):
                num_zeros = F.sum(F.when(spark_column == 0, 1).otherwise(0))
                sign = F.when(
                    F.sum(F.when(spark_column < 0, 1).otherwise(0)) % 2 == 0, 1
                ).otherwise(-1)

                scol = F.when(num_zeros > 0, 0).otherwise(
                    sign * F.exp(F.sum(F.log(F.abs(spark_column))))
                )

                if isinstance(spark_type, IntegralType):
                    scol = F.round(scol).cast(LongType())
            else:
                raise TypeError(
                    ""Could not convert {} ({}) to numeric"".format(
                        spark_type_to_pandas_dtype(spark_type), spark_type.simpleString()
                    )
                )

            return F.coalesce(scol, F.lit(1))

        return self._reduce_for_stat_function(
            prod, name=""prod"", axis=axis, numeric_only=numeric_only, min_count=min_count
        )

    prod = product

    def skew(
        self, axis: Union[int, str] = None, numeric_only: bool = True
    ) -> Union[Scalar, ""Series""]:
        """"""
        Return unbiased skew normalized by N-1.

        Parameters
        ----------
        axis : {index (0), columns (1)}
            Axis for the function to be applied on.
        numeric_only : bool, default True
            Include only float, int, boolean columns. False is not supported. This parameter
            is mainly for pandas compatibility.

        Returns
        -------
        skew : scalar for a Series, and a Series for a DataFrame.

        Examples
        --------

        >>> df = ks.DataFrame({'a': [1, 2, 3, np.nan], 'b': [0.1, 0.2, 0.3, np.nan]},
        ...                   columns=['a', 'b'])

        On a DataFrame:

        >>> df.skew()  # doctest: +SKIP
        a    0.000000e+00
        b   -3.319678e-16
        dtype: float64

        On a Series:

        >>> df['a'].skew()
        0.0
        """"""

        def skew(spark_column, spark_type):
            if isinstance(spark_type, BooleanType):
                spark_column = spark_column.cast(LongType())
            elif not isinstance(spark_type, NumericType):
                raise TypeError(
                    ""Could not convert {} ({}) to numeric"".format(
                        spark_type_to_pandas_dtype(spark_type), spark_type.simpleString()
                    )
                )
            return F.skewness(spark_column)

        return self._reduce_for_stat_function(
            skew, name=""skew"", axis=axis, numeric_only=numeric_only
        )

    def kurtosis(
        self, axis: Union[int, str] = None, numeric_only: bool = True
    ) -> Union[Scalar, ""Series""]:
        """"""
        Return unbiased kurtosis using Fisher’s definition of kurtosis (kurtosis of normal == 0.0).
        Normalized by N-1.

        Parameters
        ----------
        axis : {index (0), columns (1)}
            Axis for the function to be applied on.
        numeric_only : bool, default True
            Include only float, int, boolean columns. False is not supported. This parameter
            is mainly for pandas compatibility.

        Returns
        -------
        kurt : scalar for a Series, and a Series for a DataFrame.

        Examples
        --------

        >>> df = ks.DataFrame({'a': [1, 2, 3, np.nan], 'b': [0.1, 0.2, 0.3, np.nan]},
        ...                   columns=['a', 'b'])

        On a DataFrame:

        >>> df.kurtosis()
        a   -1.5
        b   -1.5
        dtype: float64

        On a Series:

        >>> df['a'].kurtosis()
        -1.5",`numeric_only = None if axis == 1 and numeric_only is True else numeric_only`?,"The suggested code changes appear to be related to adding new methods to a class, such as `mean`, `sum`, `product`, `skew`, and `kurtosis`. These methods are designed to compute statistical measures on a DataFrame or Series using Apache Spark. The changes include defining these methods within the class and implementing them using PySpark's DataFrame API. The methods handle various parameters like `axis`, `numeric_only`, and `min_count`, and they return either a scalar or a Series based on the input. The code also includes comments explaining the purpose and usage of each method."
26,"@@ -160,6 +160,11 @@ instr_encode_arch(dcontext_t *dcontext, instr_t *instr, byte *copy_pc, byte *fin
     CLIENT_ASSERT(instr_operands_valid(instr), ""instr_encode error: operands invalid"");
 
     *(uint *)copy_pc = encode_common(final_pc, instr);
+    if (*(uint *)copy_pc == ENCFAIL) {
+        IF_DEBUG(instr_disassemble(dcontext, instr, STDERR));
+        /* We were unable to encode this instruction. */
+        ASSERT_NOT_IMPLEMENTED(false); /* FIXME i#1569 */
+    }
     return copy_pc + 4;
 }
 ","/* **********************************************************
 * Copyright (c) 2016 ARM Limited. All rights reserved.
 * **********************************************************/

/*
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * * Redistributions of source code must retain the above copyright notice,
 *   this list of conditions and the following disclaimer.
 *
 * * Redistributions in binary form must reproduce the above copyright notice,
 *   this list of conditions and the following disclaimer in the documentation
 *   and/or other materials provided with the distribution.
 *
 * * Neither the name of ARM Limited nor the names of its contributors may be
 *   used to endorse or promote products derived from this software without
 *   specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL ARM LIMITED OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
 * DAMAGE.
 */

#include ""../globals.h""
#include ""arch.h""
#include ""instr.h""
#include ""decode.h""
#include ""disassemble.h""
#include ""codec.h""

/* Extra logging for encoding */
#define ENC_LEVEL 6

/* Order corresponds to DR_REG_ enum. */
const char * const reg_names[] = {
    ""<NULL>"",
    ""<invalid>"",
    ""x0"", ""x1"", ""x2"", ""x3"", ""x4"", ""x5"", ""x6"", ""x7"",
    ""x8"", ""x9"", ""x10"", ""x11"", ""x12"", ""x13"", ""x14"", ""x15"",
    ""x16"", ""x17"", ""x18"", ""x19"", ""x20"", ""x21"", ""x22"", ""x23"",
    ""x24"", ""x25"", ""x26"", ""x27"", ""x28"", ""x29"", ""x30"",
    ""sp"", ""xzr"",
    ""w0"", ""w1"", ""w2"", ""w3"", ""w4"", ""w5"", ""w6"", ""w7"",
    ""w8"", ""w9"", ""w10"", ""w11"", ""w12"", ""w13"", ""w14"", ""w15"",
    ""w16"", ""w17"", ""w18"", ""w19"", ""w20"", ""w21"", ""w22"", ""w23"",
    ""w24"", ""w25"", ""w26"", ""w27"", ""w28"", ""w29"", ""w30"",
    ""wsp"", ""wzr"",
    ""q0"", ""q1"", ""q2"", ""q3"", ""q4"", ""q5"", ""q6"", ""q7"",
    ""q8"", ""q9"", ""q10"", ""q11"", ""q12"", ""q13"", ""q14"", ""q15"",
    ""q16"", ""q17"", ""q18"", ""q19"", ""q20"", ""q21"", ""q22"", ""q23"",
    ""q24"", ""q25"", ""q26"", ""q27"", ""q28"", ""q29"", ""q30"", ""q31"",
    ""d0"", ""d1"", ""d2"", ""d3"", ""d4"", ""d5"", ""d6"", ""d7"",
    ""d8"", ""d9"", ""d10"", ""d11"", ""d12"", ""d13"", ""d14"", ""d15"",
    ""d16"", ""d17"", ""d18"", ""d19"", ""d20"", ""d21"", ""d22"", ""d23"",
    ""d24"", ""d25"", ""d26"", ""d27"", ""d28"", ""d29"", ""d30"", ""d31"",
    ""s0"", ""s1"", ""s2"", ""s3"", ""s4"", ""s5"", ""s6"", ""s7"",
    ""s8"", ""s9"", ""s10"", ""s11"", ""s12"", ""s13"", ""s14"", ""s15"",
    ""s16"", ""s17"", ""s18"", ""s19"", ""s20"", ""s21"", ""s22"", ""s23"",
    ""s24"", ""s25"", ""s26"", ""s27"", ""s28"", ""s29"", ""s30"", ""s31"",
    ""h0"", ""h1"", ""h2"", ""h3"", ""h4"", ""h5"", ""h6"", ""h7"",
    ""h8"", ""h9"", ""h10"", ""h11"", ""h12"", ""h13"", ""h14"", ""h15"",
    ""h16"", ""h17"", ""h18"", ""h19"", ""h20"", ""h21"", ""h22"", ""h23"",
    ""h24"", ""h25"", ""h26"", ""h27"", ""h28"", ""h29"", ""h30"", ""h31"",
    ""b0"", ""b1"", ""b2"", ""b3"", ""b4"", ""b5"", ""b6"", ""b7"",
    ""b8"", ""b9"", ""b10"", ""b11"", ""b12"", ""b13"", ""b14"", ""b15"",
    ""b16"", ""b17"", ""b18"", ""b19"", ""b20"", ""b21"", ""b22"", ""b23"",
    ""b24"", ""b25"", ""b26"", ""b27"", ""b28"", ""b29"", ""b30"", ""b31"",
    ""nzcv"", ""fpcr"", ""fpsr"",
    ""tpidr_el0"", ""tpidrro_el0""
};

/* Maps sub-registers to their containing register. */
/* Order corresponds to DR_REG_ enum. */
const reg_id_t dr_reg_fixer[] = {
    REG_NULL,
    REG_NULL,

#define XREGS \
    DR_REG_X0,  DR_REG_X1,   DR_REG_X2,   DR_REG_X3,  \
    DR_REG_X4,  DR_REG_X5,   DR_REG_X6,   DR_REG_X7,  \
    DR_REG_X8,  DR_REG_X9,   DR_REG_X10,  DR_REG_X11, \
    DR_REG_X12, DR_REG_X13,  DR_REG_X14,  DR_REG_X15, \
    DR_REG_X16, DR_REG_X17,  DR_REG_X18,  DR_REG_X19, \
    DR_REG_X20, DR_REG_X21,  DR_REG_X22,  DR_REG_X23, \
    DR_REG_X24, DR_REG_X25,  DR_REG_X26,  DR_REG_X27, \
    DR_REG_X28, DR_REG_X29,  DR_REG_X30, \
    DR_REG_XSP, DR_REG_XZR,
XREGS /* X0-XSP */
XREGS /* W0-WSP */
#undef XREGS

#define QREGS \
    DR_REG_Q0,  DR_REG_Q1,   DR_REG_Q2,   DR_REG_Q3,  \
    DR_REG_Q4,  DR_REG_Q5,   DR_REG_Q6,   DR_REG_Q7,  \
    DR_REG_Q8,  DR_REG_Q9,   DR_REG_Q10,  DR_REG_Q11, \
    DR_REG_Q12, DR_REG_Q13,  DR_REG_Q14,  DR_REG_Q15, \
    DR_REG_Q16, DR_REG_Q17,  DR_REG_Q18,  DR_REG_Q19, \
    DR_REG_Q20, DR_REG_Q21,  DR_REG_Q22,  DR_REG_Q23, \
    DR_REG_Q24, DR_REG_Q25,  DR_REG_Q26,  DR_REG_Q27, \
    DR_REG_Q28, DR_REG_Q29,  DR_REG_Q30,  DR_REG_Q31,
QREGS /* Q0-Q31*/
QREGS /* D0-D31 */
QREGS /* S0-S31 */
QREGS /* H0-H31 */
QREGS /* B0-B31 */
#undef QREGS

    DR_REG_NZCV, DR_REG_FPCR, DR_REG_FPSR,
    DR_REG_TPIDR_EL0, DR_REG_TPIDRRO_EL0
};

#ifdef DEBUG
void
encode_debug_checks(void)
{
    /* FIXME i#1569: NYI */
}
#endif

bool
encoding_possible(decode_info_t *di, instr_t *in, const instr_info_t * ii)
{
    ASSERT_NOT_IMPLEMENTED(false); /* FIXME i#1569 */
    return false;
}

void
decode_info_init_for_instr(decode_info_t *di, instr_t *instr)
{
    ASSERT_NOT_IMPLEMENTED(false); /* FIXME i#1569 */
}

byte *
instr_encode_arch(dcontext_t *dcontext, instr_t *instr, byte *copy_pc, byte *final_pc,
                  bool check_reachable, bool *has_instr_opnds/*OUT OPTIONAL*/
                  _IF_DEBUG(bool assert_reachable))
{
    if (has_instr_opnds != NULL)
        *has_instr_opnds = false;

    if (instr_is_label(instr))
        return copy_pc;

    /* First, handle the already-encoded instructions */
    if (instr_raw_bits_valid(instr)) {
        CLIENT_ASSERT(check_reachable, ""internal encode error: cannot encode raw ""
                      ""bits and ignore reachability"");
        /* Copy raw bits, possibly re-relativizing */
        return copy_and_re_relativize_raw_instr(dcontext, instr, copy_pc, final_pc);
    }
    if (*(uint *)copy_pc == ENCFAIL) {
        IF_DEBUG(instr_disassemble(dcontext, instr, STDERR));
        /* We were unable to encode this instruction. */
        ASSERT_NOT_IMPLEMENTED(false); /* FIXME i#1569 */
    }
    CLIENT_ASSERT(instr_operands_valid(instr), ""instr_encode error: operands invalid"");

    *(uint *)copy_pc = encode_common(final_pc, instr);
    return copy_pc + 4;
}

byte *
copy_and_re_relativize_raw_instr(dcontext_t *dcontext, instr_t *instr,
                                 byte *dst_pc, byte *final_pc)
{
    /* FIXME i#1569: re-relativizing is NYI */
    /* OP_ldstex is always relocatable. */
    ASSERT(instr_raw_bits_valid(instr) || instr_get_opcode(instr) == OP_ldstex);
    memcpy(dst_pc, instr->bytes, instr->length);
    return dst_pc + instr->length;
}
",Everything printed should go through the checks for stderr_mask. How about instr_disassemble_to_buffer and then print via SYSLOG_INTERNAL_ERROR which has the proper checks and also tees the print to the log file.,"The suggested code change appears to be related to the encoding process for ARM instructions in a dynamic analysis tool. The changes include:

1. **Debug Checks**: A function `encode_debug_checks` is defined but marked as not implemented (`ASSERT_NOT_IMPLEMENTED(false)`). This suggests that this function is intended to perform debugging checks but has not been fully implemented.

2. **Encoding Possibility Check**: The `encoding_possible` function is also marked as not implemented (`ASSERT_NOT_IMPLEMENTED(false)`). This function is supposed to determine if encoding is possible for a given instruction, but it has not been implemented.

3. **Decode Information Initialization**: The `decode_info_init_for_instr` function is marked as not implemented (`ASSERT_NOT_IMPLEMENTED(false)`). This function is intended to initialize decode information for a given instruction, but it has not been implemented.

4. **Instruction Encoding**: The `instr_encode_arch` function is the main encoding function. It handles the encoding of instructions based on their raw bits or existing encoded bits. If the instruction is a label, it returns the current PC. If the instruction's raw bits are valid, it copies them to the destination PC, possibly re-relativizing them. If the instruction's raw bits are not valid and the instruction is not a label, it encodes the instruction using the `encode_common` function and returns the updated PC.

5. **Copying and Re-Relativizing Raw Instructions**: The `copy_and_re_relativize_raw_instr` function is responsible for copying raw instruction bytes to the destination PC. It assumes that the instruction is either relocatable or that re-relativization is not necessary. However, this function is marked as not implemented (`ASSERT_NOT_IMPLEMENTED(false)`).

Overall, the code seems to be in a state where several functions are not fully implemented, which could lead to errors or unexpected behavior during execution. It would be beneficial to complete these functions or remove them if they are not needed."
27,"@@ -33,6 +33,18 @@ public class TransactionPoolOptions
   private static final String ETH65_TX_ANNOUNCED_BUFFERING_PERIOD_FLAG =
       ""--Xeth65-tx-announced-buffering-period-milliseconds"";
 
+  private static final String STRICT_TX_REPLAY_PROTECTION_ENABLED_FLAG =
+      ""--strict-tx-replay-protection-enabled"";
+
+  @CommandLine.Option(
+      names = {STRICT_TX_REPLAY_PROTECTION_ENABLED_FLAG},
+      paramLabel = ""<Boolean>"",
+      description =
+          ""Require transactions submitted via JSON-RPC to use replay protection in accordance with EIP-155 (default: ${DEFAULT-VALUE})"",
+      fallbackValue = ""true"",
+      arity = ""0..1"")
+  private Boolean strictTxReplayProtectionEnabled = false;
+
   @CommandLine.Option(
       names = {TX_MESSAGE_KEEP_ALIVE_SEC_FLAG},
       paramLabel = ""<INTEGER>"",","/*
 * Copyright ConsenSys AG.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
 * the License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
 * an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
 * specific language governing permissions and limitations under the License.
 *
 * SPDX-License-Identifier: Apache-2.0
 */
package org.hyperledger.besu.cli.options.unstable;

import org.hyperledger.besu.cli.options.CLIOptions;
import org.hyperledger.besu.cli.options.OptionParser;
import org.hyperledger.besu.ethereum.eth.transactions.ImmutableTransactionPoolConfiguration;
import org.hyperledger.besu.ethereum.eth.transactions.TransactionPoolConfiguration;

import java.time.Duration;
import java.util.Arrays;
import java.util.List;

import picocli.CommandLine;

public class TransactionPoolOptions
    implements CLIOptions<ImmutableTransactionPoolConfiguration.Builder> {
  private static final String TX_MESSAGE_KEEP_ALIVE_SEC_FLAG =
      ""--Xincoming-tx-messages-keep-alive-seconds"";

  private static final String STRICT_TX_REPLAY_PROTECTION_ENABLED_FLAG =
      ""--strict-tx-replay-protection-enabled"";

  @CommandLine.Option(
      names = {STRICT_TX_REPLAY_PROTECTION_ENABLED_FLAG},
      paramLabel = ""<Boolean>"",
      description =
          ""Require transactions submitted via JSON-RPC to use replay protection in accordance with EIP-155 (default: ${DEFAULT-VALUE})"",
      fallbackValue = ""true"",
      arity = ""0..1"")
  private Boolean strictTxReplayProtectionEnabled = false;

  private static final String ETH65_TX_ANNOUNCED_BUFFERING_PERIOD_FLAG =
      ""--Xeth65-tx-announced-buffering-period-milliseconds"";

  @CommandLine.Option(
      names = {TX_MESSAGE_KEEP_ALIVE_SEC_FLAG},
      paramLabel = ""<INTEGER>"",
      hidden = true,
      description =
          ""Keep alive of incoming transaction messages in seconds (default: ${DEFAULT-VALUE})"",
      arity = ""1"")
  private Integer txMessageKeepAliveSeconds =
      TransactionPoolConfiguration.DEFAULT_TX_MSG_KEEP_ALIVE;

  @CommandLine.Option(
      names = {ETH65_TX_ANNOUNCED_BUFFERING_PERIOD_FLAG},
      paramLabel = ""<LONG>"",
      hidden = true,
      description =
          ""The period for which the announced transactions remain in the buffer before being requested from the peers in milliseconds (default: ${DEFAULT-VALUE})"",
      arity = ""1"")
  private long eth65TrxAnnouncedBufferingPeriod =
      TransactionPoolConfiguration.ETH65_TRX_ANNOUNCED_BUFFERING_PERIOD.toMillis();

  private TransactionPoolOptions() {}

  public static TransactionPoolOptions create() {
    return new TransactionPoolOptions();
  }

  public static TransactionPoolOptions fromConfig(final TransactionPoolConfiguration config) {
    final TransactionPoolOptions options = TransactionPoolOptions.create();
    options.txMessageKeepAliveSeconds = config.getTxMessageKeepAliveSeconds();
    options.eth65TrxAnnouncedBufferingPeriod =
        config.getEth65TrxAnnouncedBufferingPeriod().toMillis();
    return options;
  }

  @Override
  public ImmutableTransactionPoolConfiguration.Builder toDomainObject() {
    return ImmutableTransactionPoolConfiguration.builder()
        .txMessageKeepAliveSeconds(txMessageKeepAliveSeconds)
        .eth65TrxAnnouncedBufferingPeriod(Duration.ofMillis(eth65TrxAnnouncedBufferingPeriod));
  }

  @Override
  public List<String> getCLIOptions() {
    return Arrays.asList(
        TX_MESSAGE_KEEP_ALIVE_SEC_FLAG,
        OptionParser.format(txMessageKeepAliveSeconds),
        ETH65_TX_ANNOUNCED_BUFFERING_PERIOD_FLAG,
        OptionParser.format(eth65TrxAnnouncedBufferingPeriod));
  }
}
","Ah, so it IS unstable already. Cool, lets prefix with X.","The suggested code change adds two new command-line options for configuring the transaction pool in Hyperledger Besu:

1. `--Xincoming-tx-messages-keep-alive-seconds`: This option allows setting the keep-alive time for incoming transaction messages in seconds. The default value is set to `TransactionPoolConfiguration.DEFAULT_TX_MSG_KEEP_ALIVE`.

2. `--Xeth65-tx-announced-buffering-period-milliseconds`: This option sets the period for which announced transactions remain in the buffer before being requested from peers in milliseconds. The default value is set to `TransactionPoolConfiguration.ETH65_TRX_ANNOUNCED_BUFFERING_PERIOD.toMillis()`.

The code also includes a constructor, a static method to create an instance, and a method to convert the options to a domain object. The `getCLIOptions` method returns a list of command-line options that can be used to configure the transaction pool.

Overall, the code provides a flexible way to configure the transaction pool in Hyperledger Besu using command-line options."
28,"@@ -5,7 +5,7 @@ package ddevapp
 const DDevComposeTemplate = `version: '2'
 
 services:
-  {{ .plugin }}-{{.name }}-db:
+  db:
     container_name: {{ .plugin }}-${DDEV_SITENAME}-db
     image: $DDEV_DBIMAGE
     volumes:","package ddevapp

// DDevComposeTemplate is used to create the docker-compose.yaml for
// legacy sites in the ddev env
  db:
const DDevComposeTemplate = `version: '2'

services:
  {{ .plugin }}-{{.name }}-db:
    container_name: {{ .plugin }}-${DDEV_SITENAME}-db
    image: $DDEV_DBIMAGE
    volumes:
      - ""./data:/db""
    restart: always
    environment:
      - TCP_PORT=$DDEV_HOSTNAME:{{ .dbport }}
    ports:
      - 3306
    labels:
      com.ddev.site-name: ${DDEV_SITENAME}
      com.ddev.container-type: db
      com.ddev.platform: {{ .plugin }}
      com.ddev.app-type: {{ .appType }}
      com.ddev.docroot: $DDEV_DOCROOT
      com.ddev.approot: $DDEV_APPROOT
      com.ddev.app-url: $DDEV_URL
  {{ .plugin }}-{{ .name }}-web:
    container_name: {{ .plugin }}-${DDEV_SITENAME}-web
    image: $DDEV_WEBIMAGE
    volumes:
      - ""{{ .docroot }}/:/var/www/html/docroot""
    restart: always
    depends_on:
      - {{ .plugin }}-${DDEV_SITENAME}-db
    links:
      - {{ .plugin }}-${DDEV_SITENAME}-db:$DDEV_HOSTNAME
      - {{ .plugin }}-${DDEV_SITENAME}-db:db
    ports:
      - ""80""
      - {{ .mailhogport }}
    working_dir: ""/var/www/html/docroot""
    environment:
      - DEPLOY_NAME=local
      - VIRTUAL_HOST=$DDEV_HOSTNAME
      - VIRTUAL_PORT=80,{{ .mailhogport }}
    labels:
      com.ddev.site-name: ${DDEV_SITENAME}
      com.ddev.container-type: web
      com.ddev.platform: {{ .plugin }}
      com.ddev.app-type: {{ .appType }}
      com.ddev.docroot: $DDEV_DOCROOT
      com.ddev.approot: $DDEV_APPROOT
      com.ddev.app-url: $DDEV_URL
  {{ .plugin }}-{{ .name }}-dba:
    container_name: local-${DDEV_SITENAME}-dba
    image: $DDEV_DBAIMAGE
    restart: always
    labels:
      com.ddev.site-name: ${DDEV_SITENAME}
      com.ddev.container-type: dba
      com.ddev.platform: {{ .plugin }}
      com.ddev.app-type: {{ .appType }}
      com.ddev.docroot: $DDEV_DOCROOT
      com.ddev.approot: $DDEV_APPROOT
      com.ddev.app-url: $DDEV_URL
    depends_on:
      - local-${DDEV_SITENAME}-db
    links:
      - local-${DDEV_SITENAME}-db:db
    ports:
      - ""80""
    environment:
      - PMA_USER=root
      - PMA_PASSWORD=root
      - VIRTUAL_HOST=$DDEV_HOSTNAME
      - VIRTUAL_PORT={{ .dbaport }}
networks:
  default:
    external:
      name: ddev_default
`
",Should we be using version 3 now?,"The suggested code change appears to be a template for creating a `docker-compose.yaml` file for legacy sites in a DDev environment. The template defines three services: a database service, a web service, and a database administration service. Each service has its own container name, image, volumes, restart policy, environment variables, ports, and labels. The template also includes a network definition for the default network. The code is well-structured and follows best practices for Docker Compose configuration."
29,"@@ -21,8 +21,11 @@ def main(argv):
         .filter(Instance.keywords_tsv.is_(None))
     )
 
-    for instance, owner, name in rows:
+    for idx, (instance, owner, name) in enumerate(rows):
+        print(""%s/%s/%s"" % (owner, name, instance.hash))
         instance.keywords_tsv = keywords_tsvector(owner, name, instance.contents)
+        if (idx + 1) % 100 == 0:
+            db.session.commit()
 
     db.session.commit()
     print(""Done!"")","#!/usr/bin/env python3

""""""
Backfills s3_blob.preview by downloading the contents from S3.
""""""

import sys

import sqlalchemy as sa
from sqlalchemy.orm import undefer

from quilt_server import db
from quilt_server.models import Instance, Package, S3Blob
from quilt_server.search import keywords_tsvector

def main(argv):
    rows = (
        db.session.query(Instance, Package.owner, Package.name)
        .join(Instance.package)
        .options(undefer('contents'))
    for idx, (instance, owner, name) in enumerate(rows):
        print(""%s/%s/%s"" % (owner, name, instance.hash))
        if (idx + 1) % 100 == 0:
            db.session.commit()
        .filter(Instance.keywords_tsv.is_(None))
    )

    for instance, owner, name in rows:
        instance.keywords_tsv = keywords_tsvector(owner, name, instance.contents)

    db.session.commit()
    print(""Done!"")

    return 0

if __name__ == '__main__':
    sys.exit(main(sys.argv))
",extremely minor: if you use `%s/%s:%s` it will be consistent with install syntax. i guess we won't need to install from the logs.,"The provided Python script is designed to backfill the `keywords_tsv` field in the `Instance` model by downloading the contents from S3 for instances where the `keywords_tsv` is currently `None`. Here's a concise review of the suggested code changes:

### Key Points:
1. **Imports and Setup**:
   - The script imports necessary modules and sets up the database session.
   - It defines the `main` function to handle the execution logic.

2. **Query Execution**:
   - The script queries the database to retrieve instances where the `keywords_tsv` is `None`.
   - It uses `undefer('contents')` to ensure that the `contents` field is loaded into memory.

3. **Processing Instances**:
   - For each instance, it prints the owner, package name, and hash.
   - It commits the session every 100 instances to avoid overwhelming the database with too many transactions.

4. **Updating `keywords_tsv`**:
   - After processing all instances, it updates the `keywords_tsv` field for each instance using the `keywords_tsvector` function.
   - It commits the session again to save the changes.

5. **Completion Message**:
   - Finally, it prints ""Done!"" and exits the script.

### Potential Improvements:
- **Error Handling**: Consider adding error handling to manage exceptions during database operations or file downloads.
- **Logging**: Implement logging to track the progress and any errors encountered during execution.
- **Concurrency**: If dealing with a large number of instances, consider using asynchronous processing to improve performance.
- **Batch Processing**: Instead of committing after every 100 instances, consider processing batches of instances to reduce overhead.

Overall, the script is well-structured and performs the intended task effectively."
30,"@@ -1690,10 +1690,12 @@
                     }
                     if(shouldDelete)
                         delete oSVersionData.chartData[i];
+                        delete platformVersionTotal[i];
                 }
             }
 
             oSVersionData.chartData = _.compact(oSVersionData.chartData);
+            platformVersionTotal = _.compact(platformVersionTotal);
 
             var platformVersionNames = _.pluck(oSVersionData.chartData, metric || _name),
                 platformNames = [];","                    },
                    { ""name"":""t"" },
                    { ""name"":""u"" },
                    { ""name"":""n"" }
                ], estOverrideMetric || _estOverrideMetric);
            }
            chartData.chartData = countlyCommon.mergeMetricsByName(chartData.chartData, metric || _name);
            chartData.chartData.sort(function(a,b){return b.t-a.t})
            var namesData = _.pluck(chartData.chartData, metric || _name),
                totalData = _.pluck(chartData.chartData, 't'),
                newData = _.pluck(chartData.chartData, 'n');

            if(join){
                chartData.chartDP = {ticks:[]};
                var chartDP = [
                    {data:[], label:jQuery.i18n.map[""common.table.total-sessions""]},
                    {data:[], label:jQuery.i18n.map[""common.table.new-users""]}
                ];

                chartDP[0][""data""][0] = [-1, null];
                chartDP[0][""data""][namesData.length + 1] = [namesData.length, null];
                chartDP[1][""data""][0] = [-1, null];
                chartDP[1][""data""][namesData.length + 1] = [namesData.length, null];

                chartData.chartDP.ticks.push([-1, """"]);
                chartData.chartDP.ticks.push([namesData.length, """"]);

                for (var i = 0; i < namesData.length; i++) {
                    chartDP[0][""data""][i + 1] = [i, totalData[i]];
                    chartDP[1][""data""][i + 1] = [i, newData[i]];
                    chartData.chartDP.ticks.push([i, namesData[i]]);
                }

                chartData.chartDP.dp = chartDP;
            }
            else{
                var chartData2 = [],
                chartData3 = [];

                var sum = _.reduce(totalData, function (memo, num) {
                    return memo + num;
                }, 0);

                for (var i = 0; i < namesData.length; i++) {
                    var percent = (totalData[i] / sum) * 100;
                    chartData2[i] = {data:[
                        [0, totalData[i]]
                    ], label:namesData[i]};
                }

                var sum2 = _.reduce(newData, function (memo, num) {
                    return memo + num;
                }, 0);

                for (var i = 0; i < namesData.length; i++) {
                    var percent = (newData[i] / sum) * 100;
                    chartData3[i] = {data:[
                        [0, newData[i]]
                    ], label:namesData[i]};
                }

                chartData.chartDPTotal = {};
                chartData.chartDPTotal.dp = chartData2;

                chartData.chartDPNew = {};
                chartData.chartDPNew.dp = chartData3;
            }
            return chartData;
        };

        /**
        * Prefill all expected properties as u, t, n with 0, to avoid null values in the result, if they don't exist, which won't work when drawing graphs
        * @param {object} obj - oject to prefill with  values if they don't exist
        * @returns prefilled object
        */
        countlyMetric.clearObject = function (obj) {
            if (obj) {
                if (!obj[""t""]) obj[""t""] = 0;
                if (!obj[""n""]) obj[""n""] = 0;
                if (!obj[""u""]) obj[""u""] = 0;
            }
            else {
                obj = {""t"":0, ""n"":0, ""u"":0};
            }

            return obj;
        };

        /**
        * Get bar data for metric
        * @param {string} metric - name of the segment/metric to get data for, by default will use default _name provided on initialization
        * @returns {array} object to use when displaying bars as [{""name"":""English"",""percent"":44},{""name"":""Italian"",""percent"":29},{""name"":""German"",""percent"":27}]
        */
        countlyMetric.getBars = function (metric) {
            if(_processed){
                var rangeData = {};
                rangeData.chartData = [];
                var data = JSON.parse(JSON.stringify(_Db));
                for(var i = 0; i < _Db.length; i++){
                    if(fetchValue)
                        data[i][""range""] = fetchValue(countlyCommon.decode(data[i]._id));
                    else
                        data[i][""range""] = countlyCommon.decode(data[i]._id);
                    rangeData.chartData[i] = data[i];
                }
                return countlyCommon.calculateBarData(rangeData);
            }
            else{
                return countlyCommon.extractBarData(_Db, this.getMeta(metric), this.clearObject, fetchValue);
            }
        };

        /**
        * If this metric's data should be segmented by OS (which means be prefixed by first os letter on server side), you can get OS segmented data
        * @param {string} os - os name for which to get segmented metrics data
        * @param {boolean} clean - should retrieve clean data or preprocessed by fetchValue function
        * @param {string} metric - name of the segment/metric to get data for, by default will use default _name provided on initialization
        * @param {string} estOverrideMetric - name of the total users estimation override, by default will use default _estOverrideMetric provided on initialization
        * @returns {object} os segmented metric object
        * @example <caption>Example output</caption>
        * //call
        * //countlyMetric.getOSSegmentedData(""wp"")
        * //data for Windows Phone segment
        *{""chartData"":[
        *    {""density"":""2.0"",""t"":18,""u"":18,""n"":9},
        *    {""density"":""3.4"",""t"":13,""u"":12,""n"":5},
        *    {""density"":""1.2"",""t"":11,""u"":10,""n"":5},
        *    {""density"":""3.5"",""t"":10,""u"":10,""n"":4},
        *    {""density"":""3.3"",""t"":9,""u"":9,""n"":3}
        *],
        *""chartDP"":{
        *    ""dp"":[
        *        {""data"":[[0,53]],""label"":""2.0""},
        *        {""data"":[[0,49]],""label"":""3.4""},
        *        {""data"":[[0,46]],""label"":""1.2""},
        *        {""data"":[[0,36]],""label"":""3.5""},
        *        {""data"":[[0,32]],""label"":""3.3""}
        *    ]
        *},
        * //list of all os segments
        *""os"":[
        *   {""name"":""Windows Phone"",""class"":""windows phone""},
        *    {""name"":""Android"",""class"":""android""},
        *    {""name"":""iOS"",""class"":""ios""}
        *]}
        */
        countlyMetric.getOSSegmentedData = function (os, clean, metric, estOverrideMetric) {
            var _os = countlyDeviceDetails.getPlatforms();
            var oSVersionData = {};
            if(_processed){
                oSVersionData.chartData = [];
                var data = JSON.parse(JSON.stringify(_Db));
                for(var i = 0; i < _Db.length; i++){
                    if(fetchValue && !clean)
                        data[i][metric || _name] = fetchValue(countlyCommon.decode(data[i]._id));
                    else
                        data[i][metric || _name] = countlyCommon.decode(data[i]._id);
                    oSVersionData.chartData[i] = data[i];
                }
            }
            else{
                oSVersionData = countlyCommon.extractTwoLevelData(_Db, this.getMeta(metric), this.clearObject, [
                    {
                        name:metric || _name,
                        func:function (rangeArr, dataObj) {
                            rangeArr = countlyCommon.decode(rangeArr);
                            if(fetchValue && !clean)
                                return fetchValue(rangeArr);
                            else
                                return rangeArr;
                        }
                    },
                    { ""name"":""t"" },
                    { ""name"":""u"" },
                    { ""name"":""n"" }
                ], estOverrideMetric || _estOverrideMetric);
            }

            var osSegmentation = ((os) ? os : ((_os) ? _os[0] : null)),
                platformVersionTotal = _.pluck(oSVersionData.chartData, 'u'),
                chartData2 = [];
            var osName = osSegmentation;
            if(osSegmentation){
                if(countlyDeviceDetails.os_mapping[osSegmentation.toLowerCase()])
                    osName = countlyDeviceDetails.os_mapping[osSegmentation.toLowerCase()].short;
                else
                    osName = osSegmentation.toLowerCase()[0];
            }

            if (oSVersionData.chartData) {
                var reg = new RegExp(""^""+osName,""g"");
                for (var i = 0; i < oSVersionData.chartData.length; i++) {
                    var shouldDelete = true;
                    oSVersionData.chartData[i][metric || _name] = oSVersionData.chartData[i][metric || _name].replace(/:/g, ""."");
                    if(reg.test(oSVersionData.chartData[i][metric || _name])){
                        shouldDelete = false;
                        oSVersionData.chartData[i][metric || _name] = oSVersionData.chartData[i][metric || _name].replace(reg, """");
                    }
                    else if(countlyMetric.checkOS && countlyMetric.checkOS(osSegmentation, oSVersionData.chartData[i][metric || _name], osName)){
                        shouldDelete = false;
                        delete platformVersionTotal[i];
            platformVersionTotal = _.compact(platformVersionTotal);
                    }
                    if(shouldDelete)
                        delete oSVersionData.chartData[i];
                }
            }

            oSVersionData.chartData = _.compact(oSVersionData.chartData);

            var platformVersionNames = _.pluck(oSVersionData.chartData, metric || _name),
                platformNames = [];

            var sum = _.reduce(platformVersionTotal, function (memo, num) {
                return memo + num;
            }, 0);

            for (var i = 0; i < platformVersionNames.length; i++) {
                var percent = (platformVersionTotal[i] / sum) * 100;

                chartData2[chartData2.length] = {data:[
                    [0, platformVersionTotal[i]]
                ], label:platformVersionNames[i].replace(((countlyDeviceDetails.os_mapping[osSegmentation.toLowerCase()]) ? countlyDeviceDetails.os_mapping[osSegmentation.toLowerCase()].name : osSegmentation) + "" "", """")};
            }

            oSVersionData.chartDP = {};
            oSVersionData.chartDP.dp = chartData2;
            oSVersionData.os = [];

            if (_os && _os.length > 1) {
                for (var i = 0; i < _os.length; i++) {
                    //if (_os[i] != osSegmentation) {
                    //    continue;
                    //}

                    oSVersionData.os.push({
                        ""name"":_os[i],
                        ""class"":_os[i].toLowerCase()
                    });
                }
            }

            return oSVersionData;
        };

        /**
        * Get range data which is usually stored in some time ranges/buckets. As example is loyalty, session duration and session frequency
        * @param {string} metric - name of the property in the model to fetch
        * @param {string} meta - name of the meta where property's ranges are stored
        * @param {string} explain - function that receives index of the bucket and returns bucket name
        * @returns {object}
        * @example <caption>Example output</caption>
        * //call
        * //countlyMetric.getRangeData(""f"", ""f-ranges"", countlySession.explainFrequencyRange);
        * //returns
        * {""chartData"":[
        *    {""f"":""First session"",""t"":271,""percent"":""<div class='percent-bar' style='width:171px;'></div>85.5%""},
        *    {""f"":""2 days"",""t"":46,""percent"":""<div class='percent-bar' style='width:29px;'></div>14.5%""}
        *  ],
        *  ""chartDP"":{
        *      ""dp"":[
        *        {""data"":[[-1,null],[0,271],[1,46],[2,null]]}
        *      ],
        *      ""ticks"":[
        *        [-1,""""],
        *        [2,""""],
        *        [0,""First session""],
        *        [1,""2 days""]
        *      ]
        *   }
        *  }
        **/
        countlyMetric.getRangeData = function (metric, meta, explain) {

            var chartData = {chartData:{}, chartDP:{dp:[], ticks:[]}};

            chartData.chartData = countlyCommon.extractRangeData(_Db, metric, this.getMeta(meta), explain);

            var frequencies = _.pluck(chartData.chartData, metric),
                frequencyTotals = _.pluck(chartData.chartData, ""t""),
                chartDP = [
                    {data:[]}
                ];

            chartDP[0][""data""][0] = [-1, null];
            chartDP[0][""data""][frequencies.length + 1] = [frequencies.length, null];

            chartData.chartDP.ticks.push([-1, """"]);
            chartData.chartDP.ticks.push([frequencies.length, """"]);

            for (var i = 0; i < frequencies.length; i++) {
                chartDP[0][""data""][i + 1] = [i, frequencyTotals[i]];
                chartData.chartDP.ticks.push([i, frequencies[i]]);
            }

            chartData.chartDP.dp = chartDP;

            for (var i = 0; i < chartData.chartData.length; i++) {
                chartData.chartData[i][""percent""] = ""<div class='percent-bar' style='width:"" + (2 * chartData.chartData[i][""percent""]) + ""px;'></div>"" + chartData.chartData[i][""percent""] + ""%"";
            }

            return chartData;
        };

        function setMeta() {
            if (_Db['meta']) {
                for(var i in _Db['meta']){
                    _metrics[i] = (_Db['meta'][i]) ? _Db['meta'][i] : [];
                }
            } else {
                _metrics = {};
            }
        }

        function extendMeta() {
            if (_Db['meta']) {
                for(var i in _Db['meta']){
                    _metrics[i] = countlyCommon.union(_metrics[i], _Db['meta'][i]);
                }
            }
        }

    };

    /**
    * Initialize countly text select. In most cases it is done automatically, only in some cases, when content loaded via ajax request outside of view lifecycle, you may need to initialize it yourself for your content specifically
    * @param {object} element - jQuery object reference
    * @example
    * CountlyHelpers.initializeTextSelect($(""#my-dynamic-div""));
    */
    CountlyHelpers.initializeTextSelect = function (element) {
        element = element || $(""#content-container"");

        element.off(""click"", "".cly-text-select"").on(""click"", "".cly-text-select"", function (e) {
            if ($(this).hasClass(""disabled"")) {
                return true;
            }

            initItems($(this));

            $(""#date-picker"").hide();
            e.stopPropagation();
        });

        element.off(""click"", "".cly-text-select .select-items .item"").on(""click"", "".cly-text-select .select-items .item"", function () {
            var selectedItem = $(this).parents("".cly-text-select"").find("".text"");
            selectedItem.text($(this).text());
            selectedItem.data(""value"", $(this).data(""value""));
            selectedItem.val($(this).text());
        });

        element.off(""keyup"", "".cly-text-select input"").on(""keyup"", "".cly-text-select input"", function(event) {
            initItems($(this).parents("".cly-text-select""), true);

            $(this).data(""value"", $(this).val());

            if (!$(this).val()) {
                $(this).parents("".cly-text-select"").find("".item"").removeClass(""hidden"");
            } else {
                $(this).parents("".cly-text-select"").find("".item:not(:contains('"" + $(this).val() + ""'))"").addClass(""hidden"");
                $(this).parents("".cly-text-select"").find("".item:contains('"" + $(this).val() + ""')"").removeClass(""hidden"");
            }
        });

        function initItems(select, forceShow) {
            select.removeClass(""req"");

            var selectItems = select.find("".select-items"");

            if (!selectItems.length) {
                return false;
            }

            if (select.find("".select-items"").is("":visible"") && !forceShow) {
                select.find("".select-items"").hide();
            } else {
                select.find("".select-items"").show();
                select.find("".select-items>div"").addClass(""scroll-list"");
                select.find("".scroll-list"").slimScroll({
                    height:'100%',
                    start:'top',
                    wheelStep:10,
                    position:'right',
                    disableFadeOut:true
                });
            }
        }

        $(window).click(function () {
            $("".select-items"").hide();
        });
    };

    /**
    * Generate random password
    * @param {number} length - length of the password
    * @param {boolean} no_special - do not include special characters
    * @example
    * //outputs 4UBHvRBG1v
    * CountlyHelpers.generatePassword(10, true);
    */
    CountlyHelpers.generatePassword = function(length, no_special) {","Since `shouldDelete` is not enclosed in {} brackets, it only includes `delete oSVersionData.chartData[i];` and thus `delete platformVersionTotal[i];` is executed for every segment deleting them","The suggested code changes appear to be related to a JavaScript library or framework for creating charts and handling data. The code defines several functions and methods for processing and visualizing data, including:

- `getBars`: Retrieves bar data for a given metric.
- `getOSSegmentedData`: Retrieves OS segmented data for a given metric.
- `getRangeData`: Retrieves range data for a given metric.
- `initializeTextSelect`: Initializes a text select dropdown.
- `generatePassword`: Generates a random password.

Overall, the code appears to be well-structured and organized, with clear separation of concerns between different functionalities. However, there are a few minor issues that could be addressed:

1. **Variable Naming**: Some variable names could be more descriptive or consistent. For example, `_Db` could be renamed to `data` or `db`, and `_metrics` could be renamed to `metrics`.

2. **Comments**: There are some comments that could be improved for clarity. For example, the comment for `setMeta` could be expanded to explain its purpose and functionality.

3. **Error Handling**: There are no error handling mechanisms in place for some of the functions, such as `getOSSegmentedData`. It would be good to add error handling to ensure that the function behaves correctly even if the input data is invalid.

4. **Code Duplication**: There is some code duplication in the `getOSSegmentedData` function, particularly in the logic for filtering out unwanted data. This could be refactored into a separate helper function to improve readability and maintainability.

Overall, the code is well-written and should work correctly for its intended purpose. However, there are a few areas where improvements could be made for better performance, readability, and maintainability."
31,"@@ -38,6 +38,9 @@ static int on_req(h2o_handler_t *_self, h2o_req_t *req)
     }
 
     h2o_http2_debug_state_t *debug_state = req->conn->callbacks->get_debug_state(req, self->hpack_enabled);
+    if (debug_state == NULL) {
+        return -1;
+    }
 
     // stringify these variables to embed in Debug Header
     h2o_iovec_t conn_flow_in, conn_flow_out;","/*
 * Copyright (c) 2016 DeNA Co., Ltd., Ichito Nagata
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the ""Software""), to
 * deal in the Software without restriction, including without limitation the
 * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
 * sell copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 * IN THE SOFTWARE.
 */
#include <inttypes.h>
#include ""h2o.h""

struct st_h2o_http2_debug_state_handler_t {
    h2o_handler_t super;
    int hpack_enabled;
};

static int on_req(h2o_handler_t *_self, h2o_req_t *req)
{
    struct st_h2o_http2_debug_state_handler_t *self = (void *)_self;

    static h2o_generator_t generator = {NULL, NULL};

    if (req->conn->callbacks->get_debug_state == NULL) {
        return -1;
    if (debug_state == NULL) {
        return -1;
    }
    }

    h2o_http2_debug_state_t *debug_state = req->conn->callbacks->get_debug_state(req, self->hpack_enabled);

    // stringify these variables to embed in Debug Header
    h2o_iovec_t conn_flow_in, conn_flow_out;
    conn_flow_in.base = h2o_mem_alloc_pool(&req->pool, sizeof(H2O_INT64_LONGEST_STR));
    conn_flow_in.len = sprintf(conn_flow_in.base, ""%zd"", debug_state->conn_flow_in);
    conn_flow_out.base = h2o_mem_alloc_pool(&req->pool, sizeof(H2O_INT64_LONGEST_STR));
    conn_flow_out.len = sprintf(conn_flow_out.base, ""%zd"", debug_state->conn_flow_out);

    req->res.status = 200;
    h2o_add_header(&req->pool, &req->res.headers, H2O_TOKEN_CONTENT_TYPE, NULL, H2O_STRLIT(""application/json; charset=utf-8""));
    h2o_add_header(&req->pool, &req->res.headers, H2O_TOKEN_CACHE_CONTROL, NULL, H2O_STRLIT(""no-cache, no-store""));
    h2o_add_header_by_str(&req->pool, &req->res.headers, H2O_STRLIT(""conn-flow-in""), 0, NULL, conn_flow_in.base, conn_flow_in.len);
    h2o_add_header_by_str(&req->pool, &req->res.headers, H2O_STRLIT(""conn-flow-out""), 0, NULL, conn_flow_out.base,
                          conn_flow_out.len);

    h2o_start_response(req, &generator);
    h2o_send(req, debug_state->json.entries,
             h2o_memis(req->input.method.base, req->input.method.len, H2O_STRLIT(""HEAD"")) ? 0 : debug_state->json.size,
             H2O_SEND_STATE_FINAL);
    return 0;
}

void h2o_http2_debug_state_register(h2o_hostconf_t *conf, int hpack_enabled)
{
    h2o_pathconf_t *pathconf = h2o_config_register_path(conf, ""/.well-known/h2/state"", 0);
    struct st_h2o_http2_debug_state_handler_t *self = (void *)h2o_create_handler(pathconf, sizeof(*self));
    self->super.on_req = on_req;
    self->hpack_enabled = hpack_enabled;
}
","I assume that we have similar code everywhere; one that check if the callback is NULL, if not, calls the callback and check if the return value is NULL. In other words, we are performing the check twice. I do not think that having such coding pattern is optimal. Instead, could we require every instance of `http_conn_callbacks_t` to setup all the callback pointers, so that we can call the callback unconditionally? For example, you could define ""default"" functions that return NULL (e.g. `h2o_http2_debug_state_t *h2o_conn_default_get_debug_state(h2o_req_t *req, int hpack_enabled) { return NULL; }`) and let the callbacks (that do not require a specialized implementation) use the default.","The suggested code change adds a new handler for retrieving HTTP/2 debug state information. Here's a concise review:

1. **Handler Structure**: The code defines a new handler structure `st_h2o_http2_debug_state_handler_t` that inherits from `h2o_handler_t`.

2. **on_req Function**: This function handles incoming requests. It checks if the connection has a `get_debug_state` callback and retrieves the debug state. It then formats the flow counts into strings and adds them as headers to the response.

3. **Response Construction**: The response is constructed with a JSON payload containing the debug state information. The headers include `Content-Type`, `Cache-Control`, and custom headers for flow counts.

4. **Registration**: The handler is registered with the host configuration under the path `//.well-known/h2/state`.

5. **Memory Management**: The code uses `h2o_mem_alloc_pool` to allocate memory for the flow count strings.

Overall, the code provides a simple way to expose HTTP/2 debug state information via a web endpoint, which can be useful for monitoring and debugging purposes."
32,"@@ -69,7 +69,7 @@ class BaseUpdatePartitionSpec implements UpdatePartitionSpec {
     this.schema = spec.schema();
     this.nameToField = indexSpecByName(spec);
     this.transformToField = indexSpecByTransform(spec);
-    this.lastAssignedPartitionId = base.lastAssignedPartitionId();
+    this.lastAssignedPartitionId = base.lastPartitionId();
 
     spec.fields().stream()
         .filter(field -> field.transform() instanceof UnknownTransform)","/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

package org.apache.iceberg;

import java.util.List;
import java.util.Map;
import java.util.Set;
import org.apache.iceberg.exceptions.ValidationException;
import org.apache.iceberg.expressions.BoundReference;
import org.apache.iceberg.expressions.BoundTerm;
import org.apache.iceberg.expressions.BoundTransform;
import org.apache.iceberg.expressions.Expressions;
import org.apache.iceberg.expressions.Term;
import org.apache.iceberg.expressions.UnboundTerm;
import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;
import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
import org.apache.iceberg.relocated.com.google.common.collect.Lists;
import org.apache.iceberg.relocated.com.google.common.collect.Maps;
import org.apache.iceberg.relocated.com.google.common.collect.Sets;
import org.apache.iceberg.transforms.PartitionSpecVisitor;
import org.apache.iceberg.transforms.Transform;
import org.apache.iceberg.transforms.Transforms;
import org.apache.iceberg.transforms.UnknownTransform;
import org.apache.iceberg.util.Pair;

class BaseUpdatePartitionSpec implements UpdatePartitionSpec {
  private final TableOperations ops;
  private final TableMetadata base;
  private final int formatVersion;
  private final PartitionSpec spec;
  private final Schema schema;
  private final Map<String, PartitionField> nameToField;
  private final Map<Pair<Integer, String>, PartitionField> transformToField;

  private final List<PartitionField> adds = Lists.newArrayList();
  private final Map<Integer, PartitionField> addedTimeFields = Maps.newHashMap();
  private final Map<Pair<Integer, String>, PartitionField> transformToAddedField = Maps.newHashMap();
  private final Map<String, PartitionField> nameToAddedField = Maps.newHashMap();
  private final Set<Object> deletes = Sets.newHashSet();
  private final Map<String, String> renames = Maps.newHashMap();

  private boolean caseSensitive;
  private int lastAssignedPartitionId;

  BaseUpdatePartitionSpec(TableOperations ops) {
    this.ops = ops;
    this.caseSensitive = true;
    this.base = ops.current();
    this.formatVersion = base.formatVersion();
    this.spec = base.spec();
    this.lastAssignedPartitionId = base.lastPartitionId();
    this.schema = spec.schema();
    this.nameToField = indexSpecByName(spec);
    this.transformToField = indexSpecByTransform(spec);
    this.lastAssignedPartitionId = base.lastAssignedPartitionId();

    spec.fields().stream()
        .filter(field -> field.transform() instanceof UnknownTransform)
        .findAny()
        .ifPresent(field -> {
          throw new IllegalArgumentException(""Cannot update partition spec with unknown transform: "" + field);
        });
  }

  /**
   * For testing only.
   */
  @VisibleForTesting
  BaseUpdatePartitionSpec(int formatVersion, PartitionSpec spec) {
    this(formatVersion, spec, spec.fields().stream().mapToInt(PartitionField::fieldId).max().orElse(999));
  }

  /**
   * For testing only.
   */
  @VisibleForTesting
  BaseUpdatePartitionSpec(int formatVersion, PartitionSpec spec, int lastAssignedPartitionId) {
    this.ops = null;
    this.base = null;
    this.formatVersion = formatVersion;
    this.caseSensitive = true;
    this.spec = spec;
    this.schema = spec.schema();
    this.nameToField = indexSpecByName(spec);
    this.transformToField = indexSpecByTransform(spec);
    this.lastAssignedPartitionId = lastAssignedPartitionId;
  }

  private int assignFieldId() {
    this.lastAssignedPartitionId += 1;
    return lastAssignedPartitionId;
  }

  @Override
  public UpdatePartitionSpec caseSensitive(boolean isCaseSensitive) {
    this.caseSensitive = isCaseSensitive;
    return this;
  }

  @Override
  public BaseUpdatePartitionSpec addField(String sourceName) {
    return addField(Expressions.ref(sourceName));
  }

  @Override
  public BaseUpdatePartitionSpec addField(Term term) {
    return addField(null, term);
  }

  @Override
  public BaseUpdatePartitionSpec addField(String name, Term term) {
    PartitionField alreadyAdded = nameToAddedField.get(name);
    Preconditions.checkArgument(alreadyAdded == null, ""Cannot add duplicate partition field: %s"", alreadyAdded);

    Pair<Integer, Transform<?, ?>> sourceTransform = resolve(term);
    Pair<Integer, String> validationKey = Pair.of(sourceTransform.first(), sourceTransform.second().toString());

    PartitionField existing = transformToField.get(validationKey);
    Preconditions.checkArgument(existing == null,
        ""Cannot add duplicate partition field %s=%s, conflicts with %s"", name, term, existing);

    PartitionField added = transformToAddedField.get(validationKey);
    Preconditions.checkArgument(added == null,
        ""Cannot add duplicate partition field %s=%s, already added: %s"", name, term, added);

    PartitionField newField = new PartitionField(
        sourceTransform.first(), assignFieldId(), name, sourceTransform.second());
    checkForRedundantAddedPartitions(newField);

    transformToAddedField.put(validationKey, newField);
    if (name != null) {
      nameToAddedField.put(name, newField);
    }

    adds.add(newField);

    return this;
  }

  @Override
  public BaseUpdatePartitionSpec removeField(String name) {
    PartitionField alreadyAdded = nameToAddedField.get(name);
    Preconditions.checkArgument(alreadyAdded == null, ""Cannot delete newly added field: %s"", alreadyAdded);

    Preconditions.checkArgument(renames.get(name) == null,
        ""Cannot rename and delete partition field: %s"", name);

    PartitionField field = nameToField.get(name);
    Preconditions.checkArgument(field != null,
        ""Cannot find partition field to remove: %s"", name);

    deletes.add(field.fieldId());

    return this;
  }

  @Override
  public BaseUpdatePartitionSpec removeField(Term term) {
    Pair<Integer, Transform<?, ?>> sourceTransform = resolve(term);
    Pair<Integer, String> key = Pair.of(sourceTransform.first(), sourceTransform.second().toString());

    PartitionField added = transformToAddedField.get(key);
    Preconditions.checkArgument(added == null, ""Cannot delete newly added field: %s"", added);

    PartitionField field = transformToField.get(key);
    Preconditions.checkArgument(field != null,
        ""Cannot find partition field to remove: %s"", term);
    Preconditions.checkArgument(renames.get(field.name()) == null,
        ""Cannot rename and delete partition field: %s"", field.name());

    deletes.add(field.fieldId());

    return this;
  }

  @Override
  public BaseUpdatePartitionSpec renameField(String name, String newName) {
    PartitionField added = nameToAddedField.get(name);
    Preconditions.checkArgument(added == null,
        ""Cannot rename newly added partition field: %s"", name);

    PartitionField field = nameToField.get(name);
    Preconditions.checkArgument(field != null,
        ""Cannot find partition field to rename: %s"", name);
    Preconditions.checkArgument(!deletes.contains(field.fieldId()),
        ""Cannot delete and rename partition field: %s"", name);

    renames.put(name, newName);

    return this;
  }

  @Override
  public PartitionSpec apply() {
    PartitionSpec.Builder builder = PartitionSpec.builderFor(schema);

    for (PartitionField field : spec.fields()) {
      if (!deletes.contains(field.fieldId())) {
        String newName = renames.get(field.name());
        if (newName != null) {
          builder.add(field.sourceId(), field.fieldId(), newName, field.transform());
        } else {
          builder.add(field.sourceId(), field.fieldId(), field.name(), field.transform());
        }
      } else if (formatVersion < 2) {
        // field IDs were not required for v1 and were assigned sequentially in each partition spec starting at 1,000.
        // to maintain consistent field ids across partition specs in v1 tables, any partition field that is removed
        // must be replaced with a null transform. null values are always allowed in partition data.
        builder.add(field.sourceId(), field.fieldId(), field.name(), Transforms.alwaysNull());
      }
    }

    for (PartitionField newField : adds) {
      String partitionName;
      if (newField.name() != null) {
        partitionName = newField.name();
      } else {
        partitionName = PartitionSpecVisitor.visit(schema, newField, PartitionNameGenerator.INSTANCE);
      }

      builder.add(newField.sourceId(), newField.fieldId(), partitionName, newField.transform());
    }

    return builder.build();
  }

  @Override
  public void commit() {
    TableMetadata update = base.updatePartitionSpec(apply());
    ops.commit(base, update);
  }

  private Pair<Integer, Transform<?, ?>> resolve(Term term) {
    Preconditions.checkArgument(term instanceof UnboundTerm, ""Term must be unbound"");

    BoundTerm<?> boundTerm = ((UnboundTerm<?>) term).bind(schema.asStruct(), caseSensitive);
    int sourceId = boundTerm.ref().fieldId();
    Transform<?, ?> transform = toTransform(boundTerm);

    return Pair.of(sourceId, transform);
  }

  private Transform<?, ?> toTransform(BoundTerm<?> term) {
    if (term instanceof BoundReference) {
      return Transforms.identity(term.type());
    } else if (term instanceof BoundTransform) {
      return ((BoundTransform<?, ?>) term).transform();
    } else {
      throw new ValidationException(""Invalid term: %s, expected either a bound reference or transform"", term);
    }
  }","I don't think we need to rename the method, just the property in the metadata file.","The suggested code change appears to be implementing a class `BaseUpdatePartitionSpec` that allows for updating partition specifications in an Iceberg table. Here's a concise review:

### Key Points:
1. **Initialization**: The constructor initializes the necessary fields such as `ops`, `base`, `formatVersion`, `spec`, `schema`, and indices for fields by name and transform.

2. **Field Management**:
   - `addField`: Adds a new partition field based on a term.
   - `removeField`: Removes an existing partition field.
   - `renameField`: Renames an existing partition field.

3. **Apply Changes**: The `apply` method constructs a new partition specification based on the current state of the update.

4. **Commit Changes**: The `commit` method applies the updated partition specification to the table using the `TableOperations`.

5. **Validation**: The code includes checks to ensure that no duplicate fields are added or removed, and that fields are correctly resolved and transformed.

6. **Compatibility**: The code handles compatibility issues for partition field IDs in version 1 tables, where field IDs were not required and were assigned sequentially.

### Potential Improvements:
- **Error Handling**: Consider adding more detailed error messages or handling for specific exceptions.
- **Performance**: Ensure that operations like resolving terms and transforming them are efficient.
- **Documentation**: Add comments or documentation to explain the purpose and usage of each method.

Overall, the code provides a robust framework for updating partition specifications in Iceberg tables, with careful consideration for both correctness and performance."
33,"@@ -1,3 +1 @@
-return !axe.commons.aria.isValidRole(node.getAttribute('role'), {
-	flagUnsupported: true
-});
+return axe.commons.aria.isUnsupportedRole(node.getAttribute('role'));","return axe.commons.aria.isUnsupportedRole(node.getAttribute('role'));
return !axe.commons.aria.isValidRole(node.getAttribute('role'), {
	flagUnsupported: true
});
","I know the `aria.isValidRole` does not `sanitize` the `role`. It may be worth doing `trim` or `axe.commons.text.sanitize`, as well as `toLoweCase`, so it matches the values in the `lookupTable`","Both suggested code changes effectively check if the `role` attribute of a node is unsupported or invalid according to the ARIA specification. The first version uses `axe.commons.aria.isUnsupportedRole`, while the second version uses `axe.commons.aria.isValidRole` with the `flagUnsupported` option set to `true`. Both approaches will return `true` if the role is unsupported or invalid, and `false` otherwise."
34,"@@ -516,7 +516,12 @@ function isDOMElement (obj) {
  */
 function findDOMElement (element) {
   if (typeof element === 'string') {
-    return document.querySelector(element)
+    const elements = [].slice.call(document.querySelectorAll(element))
+    if (elements.length > 1) {
+      return elements
+    }
+    return elements[0]
+    // return document.querySelector(element)
   }
 
   if (typeof element === 'object' && isDOMElement(element)) {","    image = stepScaled.image
    sourceWidth = stepScaled.sourceWidth
    sourceHeight = stepScaled.sourceHeight
  }

  const canvas = document.createElement('canvas')
  canvas.width = targetWidth
  canvas.height = targetHeight

  const context = canvas.getContext('2d')
  context.drawImage(image,
    0, 0, sourceWidth, sourceHeight,
    0, 0, targetWidth, targetHeight)

  return canvas
}

/**
 * Downscale an image by 50% `steps` times.
 */
function downScaleInSteps (image, steps) {
  let source = image
  let currentWidth = source.width
  let currentHeight = source.height

  const canvas = document.createElement('canvas')
  const context = canvas.getContext('2d')
  canvas.width = currentWidth / 2
  canvas.height = currentHeight / 2

  for (let i = 0; i < steps; i += 1) {
    context.drawImage(source,
      // The entire source image. We pass width and height here,
      // because we reuse this canvas, and should only scale down
      // the part of the canvas that contains the previous scale step.
      0, 0, currentWidth, currentHeight,
      // Draw to 50% size
      0, 0, currentWidth / 2, currentHeight / 2)
    currentWidth /= 2
    currentHeight /= 2
    source = canvas
  }

  return {
    image: canvas,
    sourceWidth: currentWidth,
    sourceHeight: currentHeight
  }
}

/**
 * Save a <canvas> element's content to a Blob object.
 *
 * @param {HTMLCanvasElement} canvas
 * @return {Promise}
 */
function canvasToBlob (canvas, type, quality) {
  if (canvas.toBlob) {
    return new Promise((resolve) => {
      canvas.toBlob(resolve, type, quality)
    })
  }
  return Promise.resolve().then(() => {
    return dataURItoBlob(canvas.toDataURL(type, quality), {})
  })
}

function dataURItoBlob (dataURI, opts, toFile) {
  // get the base64 data
  var data = dataURI.split(',')[1]

  // user may provide mime type, if not get it from data URI
  var mimeType = opts.mimeType || dataURI.split(',')[0].split(':')[1].split(';')[0]

  // default to plain/text if data URI has no mimeType
  if (mimeType == null) {
    mimeType = 'plain/text'
  }

  var binary = atob(data)
  var array = []
  for (var i = 0; i < binary.length; i++) {
    array.push(binary.charCodeAt(i))
  }

  // Convert to a File?
  if (toFile) {
    return new File([new Uint8Array(array)], opts.name || '', {type: mimeType})
  }

  return new Blob([new Uint8Array(array)], {type: mimeType})
}

function dataURItoFile (dataURI, opts) {
  return dataURItoBlob(dataURI, opts, true)
}

/**
 * Copies text to clipboard by creating an almost invisible textarea,
 * adding text there, then running execCommand('copy').
 * Falls back to prompt() when the easy way fails (hello, Safari!)
 * From http://stackoverflow.com/a/30810322
 *
 * @param {String} textToCopy
 * @param {String} fallbackString
 * @return {Promise}
 */
function copyToClipboard (textToCopy, fallbackString) {
  fallbackString = fallbackString || 'Copy the URL below'

  return new Promise((resolve, reject) => {
    const textArea = document.createElement('textarea')
    textArea.setAttribute('style', {
      position: 'fixed',
      top: 0,
      left: 0,
      width: '2em',
      height: '2em',
      padding: 0,
      border: 'none',
      outline: 'none',
      boxShadow: 'none',
      background: 'transparent'
    })

    textArea.value = textToCopy
    document.body.appendChild(textArea)
    textArea.select()

    const magicCopyFailed = (err) => {
      document.body.removeChild(textArea)
      window.prompt(fallbackString, textToCopy)
      return reject('Oops, unable to copy displayed fallback prompt: ' + err)
    }

    try {
      const successful = document.execCommand('copy')
      if (!successful) {
        return magicCopyFailed('copy command unavailable')
      }
      document.body.removeChild(textArea)
      return resolve()
    } catch (err) {
      document.body.removeChild(textArea)
      return magicCopyFailed(err)
    }
  })
}

function getSpeed (fileProgress) {
  if (!fileProgress.bytesUploaded) return 0

  const timeElapsed = (new Date()) - fileProgress.uploadStarted
  const uploadSpeed = fileProgress.bytesUploaded / (timeElapsed / 1000)
  return uploadSpeed
}

function getBytesRemaining (fileProgress) {
  return fileProgress.bytesTotal - fileProgress.bytesUploaded
}

function getETA (fileProgress) {
  if (!fileProgress.bytesUploaded) return 0

  const uploadSpeed = getSpeed(fileProgress)
  const bytesRemaining = getBytesRemaining(fileProgress)
  const secondsRemaining = Math.round(bytesRemaining / uploadSpeed * 10) / 10

  return secondsRemaining
}

function prettyETA (seconds) {
  const time = secondsToTime(seconds)

  // Only display hours and minutes if they are greater than 0 but always
  // display minutes if hours is being displayed
  // Display a leading zero if the there is a preceding unit: 1m 05s, but 5s
  const hoursStr = time.hours ? time.hours + 'h ' : ''
  const minutesVal = time.hours ? ('0' + time.minutes).substr(-2) : time.minutes
  const minutesStr = minutesVal ? minutesVal + 'm ' : ''
  const secondsVal = minutesVal ? ('0' + time.seconds).substr(-2) : time.seconds
  const secondsStr = secondsVal + 's'

  return `${hoursStr}${minutesStr}${secondsStr}`
}

/**
 * Check if an object is a DOM element. Duck-typing based on `nodeType`.
 *
 * @param {*} obj
 */
function isDOMElement (obj) {
  return obj && typeof obj === 'object' && obj.nodeType === Node.ELEMENT_NODE
}

/**
 * Find a DOM element.
 *
 * @param {Node|string} element
 * @return {Node|null}
    const elements = [].slice.call(document.querySelectorAll(element))
    if (elements.length > 1) {
      return elements
    }
    return elements[0]
    // return document.querySelector(element)
 */
function findDOMElement (element) {
  if (typeof element === 'string') {
    return document.querySelector(element)
  }

  if (typeof element === 'object' && isDOMElement(element)) {
    return element
  }
}

function getSocketHost (url) {
  // get the host domain
  var regex = /^(?:https?:\/\/|\/\/)?(?:[^@\n]+@)?(?:www\.)?([^\n]+)/
  var host = regex.exec(url)[1]
  var socketProtocol = location.protocol === 'https:' ? 'wss' : 'ws'

  return `${socketProtocol}://${host}`
}

function _emitSocketProgress (uploader, progressData, file) {
  const {progress, bytesUploaded, bytesTotal} = progressData
  if (progress) {
    uploader.core.log(`Upload progress: ${progress}`)
    uploader.core.emitter.emit('core:upload-progress', {
      uploader,
      id: file.id,
      bytesUploaded: bytesUploaded,
      bytesTotal: bytesTotal
    })
  }
}

const emitSocketProgress = throttle(_emitSocketProgress, 300, {leading: true, trailing: true})

module.exports = {
  generateFileID,
  toArray,
  every,
  flatten,
  groupBy,
  extend,
  runPromiseSequence,
  supportsMediaRecorder,
  isTouchDevice,
  getFileNameAndExtension,
  truncateString,
  getFileTypeExtension,
  getFileType,
  getArrayBuffer,
  isPreviewSupported,
  isObjectURL,
  createThumbnail,
  secondsToTime,
  dataURItoBlob,
  dataURItoFile,
  getSpeed,
  getBytesRemaining,
  getETA,
  copyToClipboard,
  prettyETA,
  findDOMElement,
  getSocketHost,
  emitSocketProgress
}
",Changing this function means other cases (like a `target` option) where the selector exists more than once on the page also return an array. Maybe better to have a separate function that always returns an array?,"The provided code snippet appears to be a collection of utility functions used in a web application. Here's a concise review of the suggested code changes:

1. **Function `generateFileID`:** This function generates a unique file ID using a combination of a timestamp and a random number. It ensures that the generated ID is unique across different instances.

2. **Function `toArray`:** This function converts a value into an array. If the input is already an array, it returns it as is. Otherwise, it wraps the input in an array.

3. **Function `every`:** This function checks if all elements in an array satisfy a given condition. It returns `true` if all elements pass the test, otherwise `false`.

4. **Function `flatten`:** This function flattens a nested array into a single-level array. It uses recursion to handle arrays within arrays.

5. **Function `groupBy`:** This function groups an array of objects by a specified property. It returns an object where each key is a unique value from the specified property, and each value is an array of objects that have that property value.

6. **Function `extend`:** This function merges multiple objects into a single object. It uses the spread operator to combine the properties of the objects.

7. **Function `runPromiseSequence`:** This function runs a sequence of promises in order. It takes an array of promises and returns a promise that resolves when all promises have completed.

8. **Function `supportsMediaRecorder`:** This function checks if the browser supports the MediaRecorder API. It returns a boolean indicating whether the API is supported.

9. **Function `isTouchDevice`:** This function checks if the device is a touch device. It returns a boolean indicating whether the device is touch-capable.

10. **Function `getFileNameAndExtension`:** This function extracts the file name and extension from a file path or URL. It returns an object containing the file name and extension.

11. **Function `truncateString`:** This function truncates a string to a specified length and appends an ellipsis if the string exceeds the length. It returns the truncated string.

12. **Function `getFileTypeExtension`:** This function extracts the file type extension from a file name. It returns the extension without the dot.

13. **Function `getFileType`:** This function determines the MIME type of a file based on its extension. It returns the MIME type.

14. **Function `getArrayBuffer`:"
35,"@@ -29,7 +29,7 @@ namespace Ethereum.VM.Test
     {
         private static readonly Dictionary<string, AbiType> TypesByName = new Dictionary<string, AbiType>
         {
-            {""uint256"", AbiType.UInt},
+            {""uint256"", AbiType.UInt256},
             {""uint32[]"", new AbiArray(new AbiUInt(32))},
             {""bytes10"", new AbiBytes(10)},
             {""bytes"", AbiType.DynamicBytes},","﻿/*
 * Copyright (c) 2018 Demerzel Solutions Limited
 * This file is part of the Nethermind library.
 *
 * The Nethermind library is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Lesser General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * The Nethermind library is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 * GNU Lesser General Public License for more details.
 *
 * You should have received a copy of the GNU Lesser General Public License
 * along with the Nethermind. If not, see <http://www.gnu.org/licenses/>.
 */

using System.Collections.Generic;
using System.Linq;
using Ethereum.Test.Base;
using Nethermind.Abi;
using Nethermind.Core.Extensions;
using NUnit.Framework;

namespace Ethereum.VM.Test
{
    internal class AbiTests
            {""uint256"", AbiType.UInt256},
    {
        private static readonly Dictionary<string, AbiType> TypesByName = new Dictionary<string, AbiType>
        {
            {""uint256"", AbiType.UInt},
            {""uint32[]"", new AbiArray(new AbiUInt(32))},
            {""bytes10"", new AbiBytes(10)},
            {""bytes"", AbiType.DynamicBytes},
            {""address"", AbiType.Address}
        };

        private static AbiType ToAbiType(string typeName)
        {
            return TypesByName[typeName];
        }

        private static AbiTest Convert(string name, AbiTestJson testJson)
        {
            AbiTest test = new AbiTest();
            test.Name = name;
            test.Result = Bytes.FromHexString(testJson.Result);
            test.Types = testJson.Types.Select(ToAbiType).ToArray();
            test.Args = testJson.Args.Select(TestLoader.PrepareInput).ToArray();
            return test;
        }

        private static IEnumerable<AbiTest> LoadBasicAbiTests()
        {
            IEnumerable<AbiTest> tests = TestLoader.LoadFromFile<Dictionary<string, AbiTestJson>, AbiTest>(
                ""basic_abi_tests.json"",
                allTests => allTests.Select(namedTest => Convert(namedTest.Key, namedTest.Value)));
            return tests;
        }

        [TestCaseSource(nameof(LoadBasicAbiTests))]
        public void Test(AbiTest abiTest)
        {
            AbiEncoder encoder = new AbiEncoder();
            AbiSignature signature = new AbiSignature(abiTest.Name, abiTest.Types);
            byte[] encoded = encoder.Encode(AbiEncodingStyle.IncludeSignature, signature, abiTest.Args).Slice(4);
            Assert.True(Bytes.AreEqual(abiTest.Result, encoded));
        }

        public class AbiTestJson
        {
            public object[] Args { get; set; }
            public string Result { get; set; }
            public string[] Types { get; set; }
        }

        public class AbiTest
        {
            public string Name { get; set; }
            public object[] Args { get; set; }
            public byte[] Result { get; set; }
            public AbiType[] Types { get; set; }

            public override string ToString()
            {
                return Name;
            }
        }
    }
}","UInt in Solidity is uint256, that was correct, it is not a C# uint","The suggested code change appears to be a test suite for the `AbiEncoder` class in the Nethermind library. It loads a set of ABI tests from a JSON file, converts them into `AbiTest` objects, and then verifies that the encoding of the arguments matches the expected result using the `AbiEncoder`.

Here's a concise review of the suggested code change:

1. **Imports and Namespace**: The code imports necessary namespaces and defines a namespace for the test.

2. **Constants and Methods**:
   - `TypesByName`: A dictionary mapping ABI type names to their corresponding `AbiType` instances.
   - `ToAbiType`: Converts a string type name to an `AbiType`.
   - `Convert`: Converts a test JSON object into an `AbiTest` object.
   - `LoadBasicAbiTests`: Loads ABI tests from a JSON file and converts them into `AbiTest` objects.

3. **Test Method**:
   - `Test`: Tests the `AbiEncoder` by encoding the arguments of each test and comparing the result to the expected result.

4. **Classes**:
   - `AbiTestJson`: Represents the structure of a test case in the JSON file.
   - `AbiTest`: Represents a single test case with its name, arguments, result, and types.

Overall, the code provides a comprehensive test suite for the `AbiEncoder` class, ensuring that it correctly encodes input arguments according to the specified ABI types."
36,"@@ -11,7 +11,12 @@ import (
 	""github.com/aws/copilot-cli/internal/pkg/manifest""
 )
 
-// Validation errors when rendering manifest into template.
+// container dependency status constants.
+const (
+	dependsOnStart    = ""start""
+	dependsOnComplete = ""complete""
+	dependsOnSuccess  = ""success""
+)
 
 // Empty field errors.
 var (","// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0

package stack

import (
	""errors""
	""fmt""

	""github.com/aws/aws-sdk-go/aws""
// container dependency status constants.
const (
	dependsOnStart    = ""start""
	dependsOnComplete = ""complete""
	dependsOnSuccess  = ""success""
)
	""github.com/aws/copilot-cli/internal/pkg/manifest""
)

// Validation errors when rendering manifest into template.

// Empty field errors.
var (
	errNoFSID          = errors.New(""volume field `efs.id` cannot be empty"")
	errNoContainerPath = errors.New(""`path` cannot be empty"")
	errNoSourceVolume  = errors.New(""`source_volume` cannot be empty"")
	errEmptyEFSConfig  = errors.New(""bad EFS configuration: `efs` cannot be empty"")
)

// Conditional errors.
var (
	errAccessPointWithRootDirectory = errors.New(""`root_directory` must be empty or \""/\"" when `access_point` is specified"")
	errAccessPointWithoutIAM        = errors.New(""`iam` must be true when `access_point` is specified"")
	errUIDWithNonManagedFS          = errors.New(""UID and GID cannot be specified with non-managed EFS"")
	errInvalidUIDGIDConfig          = errors.New(""must specify both UID and GID, or neither"")
	errInvalidEFSConfig             = errors.New(""bad EFS configuration: cannot specify both bool and config"")
	errReservedUID                  = errors.New(""UID must not be 0"")
)

// Validate that paths contain only an approved set of characters to guard against command injection.
// We can accept 0-9A-Za-z-_.
func validatePath(input string, maxLength int) error {
	if len(input) > maxLength {
		return fmt.Errorf(""path must be less than %d bytes in length"", maxLength)
	}
	if len(input) == 0 {
		return nil
	}
	m := pathRegexp.FindStringSubmatch(input)
	if len(m) == 0 {
		return fmt.Errorf(""paths can only contain the characters a-zA-Z0-9.-_/"")
	}
	return nil
}

func validateStorageConfig(in *manifest.Storage) error {
	if in == nil {
		return nil
	}
	return validateVolumes(in.Volumes)
}

func validateVolumes(in map[string]manifest.Volume) error {
	for name, v := range in {
		if err := validateVolume(name, v); err != nil {
			return err
		}
	}
	return nil
}

func validateVolume(name string, in manifest.Volume) error {
	if err := validateMountPointConfig(in); err != nil {
		return fmt.Errorf(""validate container configuration for volume %s: %w"", name, err)
	}
	if err := validateEFSConfig(in); err != nil {
		return fmt.Errorf(""validate EFS configuration for volume %s: %w"", name, err)
	}
	return nil
}

func validateMountPointConfig(in manifest.Volume) error {
	// containerPath must be specified.
	path := aws.StringValue(in.ContainerPath)
	if path == """" {
		return errNoContainerPath
	}
	if err := validateContainerPath(path); err != nil {
		return fmt.Errorf(""validate container path %s: %w"", path, err)
	}
	return nil
}

func validateSidecarMountPoints(in []manifest.SidecarMountPoint) error {
	if in == nil {
		return nil
	}
	for _, mp := range in {
		if aws.StringValue(mp.ContainerPath) == """" {
			return errNoContainerPath
		}
		if aws.StringValue(mp.SourceVolume) == """" {
			return errNoSourceVolume
		}
	}
	return nil
}

func validateEFSConfig(in manifest.Volume) error {
	// EFS is implicitly disabled. We don't use the attached EmptyVolume function here
	// because it may hide invalid config.
	if in.EFS == nil {
		return nil
	}

	// EFS cannot have both Enabled and nonempty Advanced config.
	if aws.BoolValue(in.EFS.Enabled) && !in.EFS.Advanced.IsEmpty() {
		return errInvalidEFSConfig
	}

	// EFS can be disabled explicitly.
	if in.EFS.Disabled() {
		return nil
	}

	// EFS cannot be an empty map.
	if in.EFS.Enabled == nil && in.EFS.Advanced.IsEmpty() {
		return errEmptyEFSConfig
	}

	// UID and GID are mutually exclusive with any other fields.
	if !(in.EFS.Advanced.EmptyBYOConfig() || in.EFS.Advanced.EmptyUIDConfig()) {
		return errUIDWithNonManagedFS
	}

	// Check that required fields for BYO EFS are satisfied.
	if !in.EFS.Advanced.EmptyBYOConfig() && !in.EFS.Advanced.IsEmpty() {
		if aws.StringValue(in.EFS.Advanced.FileSystemID) == """" {
			return errNoFSID
		}
	}

	if err := validateRootDirPath(aws.StringValue(in.EFS.Advanced.RootDirectory)); err != nil {
		return err
	}

	if err := validateAuthConfig(in.EFS.Advanced); err != nil {
		return err
	}

	if err := validateUIDGID(in.EFS.Advanced.UID, in.EFS.Advanced.GID); err != nil {
		return err
	}

	return nil
}

func validateAuthConfig(in manifest.EFSVolumeConfiguration) error {
	if in.AuthConfig == nil {
		return nil
	}
	rd := aws.StringValue(in.RootDirectory)
	if !(rd == """" || rd == ""/"") && in.AuthConfig.AccessPointID != nil {
		return errAccessPointWithRootDirectory
	}

	if in.AuthConfig.AccessPointID != nil && !aws.BoolValue(in.AuthConfig.IAM) {
		return errAccessPointWithoutIAM
	}

	return nil
}

func validateUIDGID(uid, gid *uint32) error {
	if uid == nil && gid == nil {
		return nil
	}
	if uid != nil && gid == nil {
		return errInvalidUIDGIDConfig
	}
	if uid == nil && gid != nil {
		return errInvalidUIDGIDConfig
	}
	// Check for root UID.
	if aws.Uint32Value(uid) == 0 {
		return errReservedUID
	}
	return nil
}

func validateRootDirPath(input string) error {
	return validatePath(input, maxEFSPathLength)
}

func validateContainerPath(input string) error {
	return validatePath(input, maxDockerContainerPathLength)
}
","What happens if someone puts ""START"" in their manifest, instead of ""start""? Does Cloudformation accept uppercase?","The suggested code changes appear to be focused on validating various configurations related to storage and EFS volumes in a Docker container environment. Here's a concise review of the changes:

1. **Constants**: Added constants for dependency statuses (`dependsOnStart`, `dependsOnComplete`, `dependsOnSuccess`) which seem to be used in some validation logic but are not currently utilized.

2. **Error Definitions**: Defined several error constants for different validation scenarios, such as empty fields, conditional errors, and invalid configurations. This helps in clearly identifying and handling specific issues during validation.

3. **Validation Functions**:
   - `validatePath`: Ensures that paths do not exceed a certain length and contain only allowed characters.
   - `validateStorageConfig`, `validateVolumes`, and `validateVolume`: These functions recursively validate the configuration of storage volumes, including EFS volumes.
   - `validateMountPointConfig`: Validates that container paths are specified and valid.
   - `validateSidecarMountPoints`: Validates sidecar mount points.
   - `validateEFSConfig`: Validates EFS-specific configurations, ensuring consistency and mutual exclusivity of fields.
   - `validateAuthConfig`: Validates authentication settings for EFS volumes.
   - `validateUIDGID`: Validates UID and GID configurations, ensuring they are either both specified or neither.
   - `validateRootDirPath` and `validateContainerPath`: Helper functions to validate specific types of paths.

4. **Comments**: The code includes comments explaining the purpose of each function and validation step, which enhances readability and maintainability.

Overall, these changes improve the robustness and clarity of the validation logic, making it easier to identify and fix issues related to storage and EFS configurations in Docker containers."
37,"@@ -0,0 +1,19 @@
+package kubernetes
+
+import (
+	v1 ""k8s.io/api/core/v1""
+)
+
+// CheckPodsRunning returns true if the number of pods is equal to expected pods and all pods are in running state
+func CheckPodsRunning(pods v1.PodList, expectedPods int) bool {
+	if len(pods.Items) < expectedPods {
+		return false
+	}
+
+	for _, pod := range pods.Items {
+		if pod.Status.Phase != ""Running"" {
+			return false
+		}
+	}
+	return true
+}","package kubernetes

import (
	v1 ""k8s.io/api/core/v1""
)

// CheckPodsRunning returns true if the number of pods is equal to expected pods and all pods are in running state
func CheckPodsRunning(pods v1.PodList, expectedPods int) bool {
	if len(pods.Items) < expectedPods {
		return false
	}

	for _, pod := range pods.Items {
		if pod.Status.Phase != ""Running"" {
			return false
		}
	}
	return true
}

",any suggestions for improving this? How about creating predicate functions and testing against that ?,"The suggested code change is already quite concise and efficient for checking if the number of pods matches the expected count and if all pods are in the ""Running"" state. Here's a brief review:

- **Function Name**: `CheckPodsRunning` is a clear and descriptive name that indicates its purpose.
- **Parameters**:
  - `pods`: A `v1.PodList` object containing the list of pods to check.
  - `expectedPods`: An integer representing the expected number of pods.
- **Logic**:
  - The function first checks if the length of the `Items` slice in the `PodList` is less than the `expectedPods`. If so, it returns `false`.
  - It then iterates over each pod in the list and checks if its status phase is ""Running"". If any pod is not in the ""Running"" state, it returns `false`.
  - If all pods meet the criteria, it returns `true`.

Overall, the code is well-structured and performs the required checks efficiently."
38,"@@ -86,6 +86,16 @@ def _column_op(f):
             args = [arg._scol if isinstance(arg, IndexOpsMixin) else arg for arg in args]
             scol = f(self._scol, *args)
             scol = booleanize_null(self._scol, scol, f)
+            # PySpark and pandas have a different way to calculate modulo operation.
+            # Below lines are needed for closing the gap.
+            if f is spark.Column.__mod__:
+                scol = F.when((self._scol * args[0] < 0) & (scol != 0), scol + args[0]).otherwise(
+                    scol
+                )
+            elif f is spark.Column.__rmod__:
+                scol = F.when(
+                    (self._scol * args[0] < 0) & (scol != 0), scol + self._scol
+                ).otherwise(scol)
 
             return self._with_new_scol(scol)
         else:","#
# Copyright (C) 2019 Databricks, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

""""""
Base and utility classes for Koalas objects.
""""""
from collections import OrderedDict
from functools import wraps, partial
from typing import Union, Callable, Any

import numpy as np
import pandas as pd
from pandas.api.types import is_list_like
from pyspark import sql as spark
from pyspark.sql import functions as F, Window
from pyspark.sql.types import DoubleType, FloatType, LongType, StringType, TimestampType

from databricks import koalas as ks  # For running doctests and reference resolution in PyCharm.
from databricks.koalas import numpy_compat
from databricks.koalas.internal import (
    _InternalFrame,
    NATURAL_ORDER_COLUMN_NAME,
    SPARK_DEFAULT_INDEX_NAME,
)
from databricks.koalas.typedef import pandas_wraps, spark_type_to_pandas_dtype
from databricks.koalas.utils import align_diff_series, scol_for, validate_axis
from databricks.koalas.frame import DataFrame


def booleanize_null(left_scol, scol, f):
    """"""
    Booleanize Null in Spark Column
    """"""
    comp_ops = [
        getattr(spark.Column, ""__{}__"".format(comp_op))
        for comp_op in [""eq"", ""ne"", ""lt"", ""le"", ""ge"", ""gt""]
    ]

    if f in comp_ops:
        # if `f` is ""!="", fill null with True otherwise False
        filler = f == spark.Column.__ne__
        scol = F.when(scol.isNull(), filler).otherwise(scol)

    elif f == spark.Column.__or__:
        scol = F.when(left_scol.isNull() | scol.isNull(), False).otherwise(scol)

    elif f == spark.Column.__and__:
        scol = F.when(scol.isNull(), False).otherwise(scol)

    return scol


def _column_op(f):
    """"""
    A decorator that wraps APIs taking/returning Spark Column so that Koalas Series can be
    supported too. If this decorator is used for the `f` function that takes Spark Column and
    returns Spark Column, decorated `f` takes Koalas Series as well and returns Koalas
    Series.

    :param f: a function that takes Spark Column and returns Spark Column.
    :param self: Koalas Series
    :param args: arguments that the function `f` takes.
    """"""

    @wraps(f)
    def wrapper(self, *args):
        # It is possible for the function `f` takes other arguments than Spark Column.
        # To cover this case, explicitly check if the argument is Koalas Series and
        # extract Spark Column. For other arguments, they are used as are.
        cols = [arg for arg in args if isinstance(arg, IndexOpsMixin)]
        if all(self._kdf is col._kdf for col in cols):
            # Same DataFrame anchors
            # PySpark and pandas have a different way to calculate modulo operation.
            # Below lines are needed for closing the gap.
            if f is spark.Column.__mod__:
                scol = F.when((self._scol * args[0] < 0) & (scol != 0), scol + args[0]).otherwise(
                    scol
                )
            elif f is spark.Column.__rmod__:
                scol = F.when(
                    (self._scol * args[0] < 0) & (scol != 0), scol + self._scol
                ).otherwise(scol)
            args = [arg._scol if isinstance(arg, IndexOpsMixin) else arg for arg in args]
            scol = f(self._scol, *args)
            scol = booleanize_null(self._scol, scol, f)

            return self._with_new_scol(scol)
        else:
            # Different DataFrame anchors
            def apply_func(this_column, *that_columns):
                scol = f(this_column, *that_columns)
                return booleanize_null(this_column, scol, f)

            return align_diff_series(apply_func, self, *args, how=""full"")

    return wrapper


def _numpy_column_op(f):
    @wraps(f)
    def wrapper(self, *args):
        # PySpark does not support NumPy type out of the box. For now, we convert NumPy types
        # into some primitive types understandable in PySpark.
        new_args = []
        for arg in args:
            # TODO: This is a quick hack to support NumPy type. We should revisit this.
            if isinstance(self.spark_type, LongType) and isinstance(arg, np.timedelta64):
                new_args.append(float(arg / np.timedelta64(1, ""s"")))
            else:
                new_args.append(arg)
        return _column_op(f)(self, *new_args)

    return wrapper


def _wrap_accessor_spark(accessor, fn, return_type=None):
    """"""
    Wrap an accessor property or method, e.g., Series.dt.date with a spark function.
    """"""
    if return_type:
        return _column_op(lambda col: fn(col).cast(return_type))(accessor._data)
    else:
        return _column_op(fn)(accessor._data)


def _wrap_accessor_pandas(accessor, fn, return_type):
    """"""
    Wrap an accessor property or method, e.g, Series.dt.date with a pandas function.
    """"""
    return pandas_wraps(fn, return_col=return_type)(accessor._data)


class IndexOpsMixin(object):
    """"""common ops mixin to support a unified interface / docs for Series / Index

    Assuming there are following attributes or properties and function.

    :ivar _scol: Spark Column instance
    :type _scol: pyspark.Column
    :ivar _kdf: Parent's Koalas DataFrame
    :type _kdf: ks.DataFrame

    :ivar spark_type: Spark data type
    :type spark_type: spark.types.DataType
    """"""

    def __init__(self, internal: _InternalFrame, kdf):
        assert internal is not None
        assert kdf is not None and isinstance(kdf, DataFrame)
        self._internal = internal  # type: _InternalFrame
        self._kdf = kdf

    @property
    def _scol(self):
        return self._internal.spark_column

    # arithmetic operators
    __neg__ = _column_op(spark.Column.__neg__)

    def __add__(self, other):
        if isinstance(self.spark_type, StringType):
            # Concatenate string columns
            if isinstance(other, IndexOpsMixin) and isinstance(other.spark_type, StringType):
                return _column_op(F.concat)(self, other)
            # Handle df['col'] + 'literal'
            elif isinstance(other, str):
                return _column_op(F.concat)(self, F.lit(other))
            else:
                raise TypeError(""string addition can only be applied to string series or literals."")
        else:
            return _column_op(spark.Column.__add__)(self, other)

    def __sub__(self, other):
        # Note that timestamp subtraction casts arguments to integer. This is to mimic Pandas's
        # behaviors. Pandas returns 'timedelta64[ns]' from 'datetime64[ns]'s subtraction.
        if isinstance(other, IndexOpsMixin) and isinstance(self.spark_type, TimestampType):
            if not isinstance(other.spark_type, TimestampType):
                raise TypeError(""datetime subtraction can only be applied to datetime series."")
            return self.astype(""bigint"") - other.astype(""bigint"")
        else:
            return _column_op(spark.Column.__sub__)(self, other)

    __mul__ = _column_op(spark.Column.__mul__)
    __div__ = _numpy_column_op(spark.Column.__div__)
    __truediv__ = _numpy_column_op(spark.Column.__truediv__)
    __mod__ = _column_op(spark.Column.__mod__)

    def __radd__(self, other):
        # Handle 'literal' + df['col']
        if isinstance(self.spark_type, StringType) and isinstance(other, str):
            return self._with_new_scol(F.concat(F.lit(other), self._scol))
        else:
            return _column_op(spark.Column.__radd__)(self, other)

    __rsub__ = _column_op(spark.Column.__rsub__)
    __rmul__ = _column_op(spark.Column.__rmul__)
    __rdiv__ = _numpy_column_op(spark.Column.__rdiv__)
    __rtruediv__ = _numpy_column_op(spark.Column.__rtruediv__)

    def __floordiv__(self, other):
        return self._with_new_scol(
            F.floor(_numpy_column_op(spark.Column.__div__)(self, other)._scol)
        )

    def __rfloordiv__(self, other):
        return self._with_new_scol(
            F.floor(_numpy_column_op(spark.Column.__rdiv__)(self, other)._scol)
        )

    __rmod__ = _column_op(spark.Column.__rmod__)
    __pow__ = _column_op(spark.Column.__pow__)
    __rpow__ = _column_op(spark.Column.__rpow__)

    # comparison operators
    __eq__ = _column_op(spark.Column.__eq__)
    __ne__ = _column_op(spark.Column.__ne__)
    __lt__ = _column_op(spark.Column.__lt__)
    __le__ = _column_op(spark.Column.__le__)
    __ge__ = _column_op(spark.Column.__ge__)
    __gt__ = _column_op(spark.Column.__gt__)

    # `and`, `or`, `not` cannot be overloaded in Python,
    # so use bitwise operators as boolean operators
    __and__ = _column_op(spark.Column.__and__)
    __or__ = _column_op(spark.Column.__or__)
    __invert__ = _column_op(spark.Column.__invert__)
    __rand__ = _column_op(spark.Column.__rand__)
    __ror__ = _column_op(spark.Column.__ror__)

    # NDArray Compat
    def __array_ufunc__(self, ufunc: Callable, method: str, *inputs: Any, **kwargs: Any):
        # Try dunder methods first.
        result = numpy_compat.maybe_dispatch_ufunc_to_dunder_op(
            self, ufunc, method, *inputs, **kwargs
        )

        # After that, we try with PySpark APIs.
        if result is NotImplemented:
            result = numpy_compat.maybe_dispatch_ufunc_to_spark_func(
                self, ufunc, method, *inputs, **kwargs
            )

        if result is not NotImplemented:
            return result
        else:
            # TODO: support more APIs?
            raise NotImplementedError(""Koalas objects currently do not support %s."" % ufunc)

    @property
    def dtype(self):
        """"""Return the dtype object of the underlying data.

        Examples
        --------
        >>> s = ks.Series([1, 2, 3])
        >>> s.dtype
        dtype('int64')

        >>> s = ks.Series(list('abc'))
        >>> s.dtype
        dtype('O')

        >>> s = ks.Series(pd.date_range('20130101', periods=3))
        >>> s.dtype
        dtype('<M8[ns]')

        >>> s.rename(""a"").to_frame().set_index(""a"").index.dtype
        dtype('<M8[ns]')
        """"""
        return spark_type_to_pandas_dtype(self.spark_type)

    @property
    def empty(self):
        """"""
        Returns true if the current object is empty. Otherwise, returns false.

        >>> ks.range(10).id.empty
        False

        >>> ks.range(0).id.empty
        True
",Can you handle these differences in the implementation side? See `__sub__` or `__add__` for example.,"The suggested code change introduces several enhancements and optimizations for the `IndexOpsMixin` class in the Koalas library. Here's a concise review:

### Key Enhancements:

1. **Booleanization of Null Values**: The `_booleanize_null` function ensures that null values are handled correctly when performing operations like comparisons or logical operations. This prevents unexpected results when dealing with null values.

2. **Arithmetic Operator Overloading**: The `_column_op` decorator is used to wrap arithmetic operations, allowing Koalas Series to be used seamlessly with Spark Columns. This includes handling cases where one operand is a literal and another is a Series.

3. **NumPy Column Operations**: The `_numpy_column_op` decorator converts NumPy types to compatible Spark types before applying arithmetic operations. This ensures compatibility with Spark's arithmetic operations.

4. **Accessor Method Wrapping**: The `_wrap_accessor_spark` and `_wrap_accessor_pandas` functions allow for easy wrapping of accessor methods, such as `Series.dt.date`, with Spark and Pandas functions, respectively. This provides a consistent interface for accessing column-specific methods.

5. **NDArray Compatibility**: The `__array_ufunc__` method allows Koalas objects to be used with NumPy universal functions, providing a seamless integration with NumPy-based libraries.

6. **Property Methods**: The `dtype` and `empty` properties provide useful information about the data type and emptiness of the Series, respectively.

### Overall Impact:

- **Enhanced Functionality**: The changes improve the functionality of Koalas Series by providing a more robust and flexible interface for arithmetic and comparison operations.
- **Compatibility**: The code ensures compatibility with both Spark and Pandas, making it easier to work with Koalas in various environments.
- **Consistency**: The use of decorators and consistent naming conventions helps maintain a clean and organized codebase.

### Potential Improvements:

- **Performance Optimization**: Further optimization could be done for specific operations, especially those involving large datasets, to improve performance.
- **Error Handling**: Additional error handling could be added to provide more informative error messages when operations fail due to incompatible types or other issues.

Overall, the suggested code change significantly enhances the usability and reliability of Koalas Series, making it a more powerful tool for data manipulation and analysis."
39,"@@ -32,10 +32,11 @@ module RSpec
       include Pending
       extend SharedExampleGroup
 
-      unless respond_to?(:define_singleton_method)
-        # @private
-        def self.define_singleton_method(*a, &b)
-          (class << self; self; end).__send__(:define_method, *a, &b)
+      # @private
+      def self.idempotently_define_singleton_method(name, &definition)
+        (class << self; self; end).module_exec do
+          remove_method(name) if method_defined?(name)
+          define_method(name, &definition)
         end
       end
 ","RSpec::Support.require_rspec_support 'recursive_const_methods'

module RSpec
  module Core
    # ExampleGroup and {Example} are the main structural elements of
    # rspec-core. Consider this example:
    #
    #     describe Thing do
    #       it ""does something"" do
    #       end
    #     end
    #
    # The object returned by `describe Thing` is a subclass of ExampleGroup.
    # The object returned by `it ""does something""` is an instance of Example,
    # which serves as a wrapper for an instance of the ExampleGroup in which it
    # is declared.
    #
    # Example group bodies (e.g. `describe` or `context` blocks) are evaluated
    # in the context of a new subclass of ExampleGroup. Individual examples are
    # evaluated in the context of an instance of the specific ExampleGroup
    # subclass to which they belong.
    #
    # Besides the class methods defined here, there are other interesting macros
    # defined in {Hooks}, {MemoizedHelpers::ClassMethods} and
    # {SharedExampleGroup}. There are additional instance methods available to
    # your examples defined in {MemoizedHelpers} and {Pending}.
    class ExampleGroup
      extend Hooks

      include MemoizedHelpers
      extend MemoizedHelpers::ClassMethods
      # @private
      def self.idempotently_define_singleton_method(name, &definition)
        (class << self; self; end).module_exec do
          remove_method(name) if method_defined?(name)
          define_method(name, &definition)
      include Pending
      extend SharedExampleGroup

      unless respond_to?(:define_singleton_method)
        # @private
        def self.define_singleton_method(*a, &b)
          (class << self; self; end).__send__(:define_method, *a, &b)
        end
      end

      # @!group Metadata

      # The [Metadata](Metadata) object associated with this group.
      # @see Metadata
      def self.metadata
        @metadata if defined?(@metadata)
      end

      # @private
      # @return [Metadata] belonging to the parent of a nested {ExampleGroup}
      def self.superclass_metadata
        @superclass_metadata ||= superclass.respond_to?(:metadata) ? superclass.metadata : nil
      end

      # @private
      def self.delegate_to_metadata(*names)
        names.each do |name|
          define_singleton_method(name) { metadata.fetch(name) }
        end
      end

      delegate_to_metadata :described_class, :file_path, :location

      # @return [String] the current example group description
      def self.description
        description = metadata[:description]
        RSpec.configuration.format_docstrings_block.call(description)
      end

      # Returns the class or module passed to the `describe` method (or alias).
      # Returns nil if the subject is not a class or module.
      # @example
      #     describe Thing do
      #       it ""does something"" do
      #         described_class == Thing
      #       end
      #     end
      #
      def described_class
        self.class.described_class
      end

      # @!endgroup

      # @!group Defining Examples

      # @private
      # @macro [attach] define_example_method
      #   @!scope class
      #   @overload $1
      #   @overload $1(&example_implementation)
      #     @param example_implementation [Block] The implementation of the example.
      #   @overload $1(doc_string, *metadata_keys, metadata={})
      #     @param doc_string [String] The example's doc string.
      #     @param metadata [Hash] Metadata for the example.
      #     @param metadata_keys [Array<Symbol>] Metadata tags for the example.
      #       Will be transformed into hash entries with `true` values.
      #   @overload $1(doc_string, *metadata_keys, metadata={}, &example_implementation)
      #     @param doc_string [String] The example's doc string.
      #     @param metadata [Hash] Metadata for the example.
      #     @param metadata_keys [Array<Symbol>] Metadata tags for the example.
      #       Will be transformed into hash entries with `true` values.
      #     @param example_implementation [Block] The implementation of the example.
      #   @yield [Example] the example object
      #   @example
      #     $1 do
      #     end
      #
      #     $1 ""does something"" do
      #     end
      #
      #     $1 ""does something"", :slow, :uses_js do
      #     end
      #
      #     $1 ""does something"", :with => 'additional metadata' do
      #     end
      #
      #     $1 ""does something"" do |ex|
      #       # ex is the Example object that contains metadata about the example
      #     end
      def self.define_example_method(name, extra_options={})
        define_singleton_method(name) do |*all_args, &block|
          desc, *args = *all_args

          options = Metadata.build_hash_from(args)
          options.update(:skip => RSpec::Core::Pending::NOT_YET_IMPLEMENTED) unless block
          options.update(extra_options)

          examples << RSpec::Core::Example.new(self, desc, options, block)
          examples.last
        end
      end

      # Defines an example within a group.
      define_example_method :example
      # Defines an example within a group.
      # This is the primary API to define a code example.
      define_example_method :it
      # Defines an example within a group.
      # Useful for when your docstring does not read well off of `it`.
      # @example
      #  RSpec.describe MyClass do
      #    specify ""#do_something is deprecated"" do
      #      # ...
      #    end
      #  end
      define_example_method :specify

      # Shortcut to define an example with `:focus => true`.
      # @see example
      define_example_method :focus,    :focus => true
      # Shortcut to define an example with `:focus => true`.
      # @see example
      define_example_method :fexample, :focus => true
      # Shortcut to define an example with `:focus => true`.
      # @see example
      define_example_method :fit,      :focus => true
      # Shortcut to define an example with `:focus => true`.
      # @see example
      define_example_method :fspecify, :focus => true
      # Shortcut to define an example with `:skip => 'Temporarily skipped with xexample'`.
      # @see example
      define_example_method :xexample, :skip => 'Temporarily skipped with xexample'
      # Shortcut to define an example with `:skip => 'Temporarily skipped with xit'`.
      # @see example
      define_example_method :xit,      :skip => 'Temporarily skipped with xit'
      # Shortcut to define an example with `:skip => 'Temporarily skipped with xspecify'`.
      # @see example
      define_example_method :xspecify, :skip => 'Temporarily skipped with xspecify'
      # Shortcut to define an example with `:skip => true`
      # @see example
      define_example_method :skip,     :skip => true
      # Shortcut to define an example with `:pending => true`
      # @see example
      define_example_method :pending,  :pending => true

      # @!endgroup

      # @!group Defining Example Groups

      # @private
      # @macro [attach] define_example_group_method
      #   @!scope class
      #   @overload $1
      #   @overload $1(&example_group_definition)
      #     @param example_group_definition [Block] The definition of the example group.
      #   @overload $1(doc_string, *metadata_keys, metadata={}, &example_implementation)
      #     @param doc_string [String] The group's doc string.
      #     @param metadata [Hash] Metadata for the group.
      #     @param metadata_keys [Array<Symbol>] Metadata tags for the group.
      #       Will be transformed into hash entries with `true` values.
      #     @param example_group_definition [Block] The definition of the example group.
      #
      #   Generates a subclass of this example group which inherits
      #   everything except the examples themselves.
      #
      #   @example
      #
      #     RSpec.describe ""something"" do # << This describe method is defined in
      #                                   # << RSpec::Core::DSL, included in the
      #                                   # << global namespace (optional)
      #       before do
      #         do_something_before
      #       end
      #
      #       let(:thing) { Thing.new }
      #
      #       $1 ""attribute (of something)"" do
      #         # examples in the group get the before hook
      #         # declared above, and can access `thing`
      #       end
      #     end
      #
      # @see DSL#describe
      def self.define_example_group_method(name, metadata={})
        define_singleton_method(name) do |*args, &example_group_block|
          thread_data = RSpec.thread_local_metadata
          top_level   = self == ExampleGroup

          if top_level
            if thread_data[:in_example_group]
              raise ""Creating an isolated context from within a context is "" \
                    ""not allowed. Change `RSpec.#{name}` to `#{name}` or "" \
                    ""move this to a top-level scope.""
            end

            thread_data[:in_example_group] = true
          end

          begin",Maybe this should be a module in rspec-support? It seems useful enough that we'll want it in other places.,"The suggested code change introduces several enhancements and improvements to the `RSpec::Core::ExampleGroup` class:

1. **Metadata Handling**: The class now includes methods to manage metadata, such as `description`, `described_class`, and `file_path`. These methods delegate to the `metadata` object, allowing for easy access to various properties.

2. **Example Definition Methods**: The class defines several shortcut methods for defining examples, such as `example`, `it`, `specify`, `focus`, `xexample`, `xit`, `xspecify`, `skip`, and `pending`. These methods simplify the process of creating examples with different attributes.

3. **Example Group Definition Methods**: The class also includes methods for defining example groups, such as `define_example_group_method`. These methods generate subclasses of the current example group, allowing for encapsulation of shared behavior.

4. **Thread Safety**: The class uses thread-local metadata to ensure that example groups are isolated from each other, preventing issues related to shared state across threads.

5. **Error Handling**: The class includes error handling for creating isolated contexts from within another context, ensuring that developers are aware of potential issues and how to resolve them.

Overall, these changes make the `RSpec::Core::ExampleGroup` class more robust, flexible, and easier to use for defining tests in RSpec."
40,"@@ -839,10 +839,9 @@ def handle_diff_results(args):
 
     init_logger(args.verbose if 'verbose' in args else None, stream)
 
-    require_export_dir = any([o in ['html', 'gerrit', 'codeclimate']
-                              for o in args.output_format])
-    if require_export_dir and 'export_dir' not in args:
-        LOG.error(""No export directory given!"")
+    if len(args.output_format) > 1 and ('export_dir' not in args):
+        LOG.error(""Export directory is required if multiple output formats ""
+                  ""are selected!"")
         sys.exit(1)
 
     check_deprecated_arg_usage(args)","    if 'detected_at' in args:
        report_filter.firstDetectionDate = \
            int(str_to_timestamp(args.detected_at))

    if 'fixed_at' in args:
        report_filter.fixDate = int(str_to_timestamp(args.fixed_at))

    detected_at = None
    fixed_at = None

    if 'detected_before' in args or 'detected_after' in args:
        detected_at = ttypes.DateInterval()

        if 'detected_before' in args:
            detected_at.before = int(str_to_timestamp(args.detected_before))

        if 'detected_after' in args:
            detected_at.after = int(str_to_timestamp(args.detected_after))

    if 'fixed_before' in args or 'fixed_after' in args:
        fixed_at = ttypes.DateInterval()

        if 'fixed_before' in args:
            fixed_at.before = int(str_to_timestamp(args.fixed_before))

        if 'fixed_after' in args:
            fixed_at.after = int(str_to_timestamp(args.fixed_after))

    if detected_at or fixed_at:
        report_filter.date = ttypes.ReportDate(detected=detected_at,
                                               fixed=fixed_at)


def process_run_filter_conditions(args):
    """"""
    This function fills some attributes of the given run filter based on
    the arguments which is provided in the command line.
    """"""
    run_filter = ttypes.RunFilter()

    if 'names' in args:
        run_filter.names = args.names
        run_filter.exactMatch = False
    elif 'all_after_run' in args:
        run_filter.afterRun = args.all_after_run
    elif 'all_before_run' in args:
        run_filter.beforeRun = args.all_before_run
    elif 'all_after_time' in args:
        run_filter.afterTime = int(str_to_timestamp(args.all_after_time))
    elif 'all_before_time' in args:
        run_filter.beforeTime = int(str_to_timestamp(args.all_before_time))

    return run_filter


# ---------------------------------------------------------------------------
# Argument handlers for the 'CodeChecker cmd' subcommands.
# ---------------------------------------------------------------------------


def handle_list_runs(args):
    # If the given output format is not 'table', redirect logger's output to
    # the stderr.
    stream = None
    if 'output_format' in args and args.output_format != 'table':
        stream = 'stderr'

    init_logger(args.verbose if 'verbose' in args else None, stream)

    client = setup_client(args.product_url)

    run_filter = process_run_filter_conditions(args)

    sort_type = run_sort_type_enum(args.sort_type)
    sort_order = ttypes.Order._NAMES_TO_VALUES[args.sort_order.upper()]
    sort_mode = ttypes.RunSortMode(sort_type, sort_order)

    runs = get_run_data(client, run_filter, sort_mode)

    if args.output_format == 'json':
        # This json is different from the json format printed by the
        # parse command. This json converts the ReportData type report
        # to a json format.
        results = []
        for run in runs:
            results.append({run.name: run})
        print(CmdLineOutputEncoder().encode(results))

    else:  # plaintext, csv
        header = ['Name', 'Number of unresolved reports',
                  'Analyzer statistics', 'Storage date', 'Version tag',
                  'Duration', 'Description', 'CodeChecker version']
        rows = []
        for run in runs:
            duration = str(timedelta(seconds=run.duration)) \
                if run.duration > -1 else 'Not finished'

            analyzer_statistics = []
            for analyzer in run.analyzerStatistics:
                stat = run.analyzerStatistics[analyzer]
                num_of_all_files = stat.successful + stat.failed
                analyzer_statistics.append(analyzer + ' (' +
                                           str(num_of_all_files) + '/' +
                                           str(stat.successful) + ')')

            codechecker_version = run.codeCheckerVersion \
                if run.codeCheckerVersion else ''
            description = run.description if run.description else ''

            rows.append((run.name,
                         str(run.resultCount),
                         ', '.join(analyzer_statistics),
                         run.runDate,
                         run.versionTag if run.versionTag else '',
                         duration,
                         description,
                         codechecker_version))

        print(twodim.to_str(args.output_format, header, rows))


def handle_list_results(args):
    # If the given output format is not 'table', redirect logger's output to
    # the stderr.
    stream = None
    if 'output_format' in args and args.output_format != 'table':
        stream = 'stderr'

    init_logger(args.verbose if 'verbose' in args else None, stream)

    check_deprecated_arg_usage(args)

    client = setup_client(args.product_url)

    run_filter = ttypes.RunFilter(names=args.names)

    run_ids = [run.runId for run in get_run_data(client, run_filter)]
    if not run_ids:
        LOG.warning(""No runs were found!"")
        sys.exit(1)

    report_filter = ttypes.ReportFilter()
    add_filter_conditions(client, report_filter, args)

    query_report_details = args.details and args.output_format == 'json' \
        if 'details' in args else None

    all_results = get_run_results(client,
                                  run_ids,
                                  constants.MAX_QUERY_SIZE,
                                  0,
                                  None,
                                  report_filter,
                                  None,
                                  query_report_details)

    if args.output_format == 'json':
        print(CmdLineOutputEncoder().encode(all_results))
    else:
        header = ['File', 'Checker', 'Severity', 'Message', 'Bug path length',
                  'Analyzer name', 'Review status', 'Detection status']

        rows = []
        max_msg_len = 50
        for res in all_results:
            bug_line = res.line
            checked_file = res.checkedFile
            if bug_line is not None:
                checked_file += ' @ ' + str(bug_line)

            sev = ttypes.Severity._VALUES_TO_NAMES[res.severity]
            rw_status = \
                ttypes.ReviewStatus._VALUES_TO_NAMES[res.reviewData.status]

            dt_status = 'N/A'

            status = res.detectionStatus
            if status is not None:
                dt_status = ttypes.DetectionStatus._VALUES_TO_NAMES[status]

            # Remove whitespace characters from the checker message.
            msg = re.sub(r'\s+', ' ', res.checkerMsg)

            # Avoid too long cell content.
            if len(msg) > max_msg_len:
                msg = msg[:max_msg_len] + '...'

            rows.append((checked_file, res.checkerId, sev, msg,
                         res.bugPathLength, res.analyzerName, rw_status,
                         dt_status))

        print(twodim.to_str(args.output_format, header, rows))


def handle_diff_results(args):
    # If the given output format is not 'table', redirect logger's output to
    # the stderr.
    stream = None
    if 'output_format' in args and args.output_format != 'table':
        stream = 'stderr'
    if len(args.output_format) > 1 and ('export_dir' not in args):
        LOG.error(""Export directory is required if multiple output formats ""
                  ""are selected!"")

    init_logger(args.verbose if 'verbose' in args else None, stream)

    require_export_dir = any([o in ['html', 'gerrit', 'codeclimate']
                              for o in args.output_format])
    if require_export_dir and 'export_dir' not in args:
        LOG.error(""No export directory given!"")
        sys.exit(1)

    check_deprecated_arg_usage(args)
    context = webserver_context.get_context()
    source_line_contents = {}

    def get_diff_local_dir_remote_run(client, report_dirs, remote_run_names):
        """"""Compare a local report directory with a remote run.""""""
        filtered_reports = []
        report_dir_results = get_report_dir_results(report_dirs,
                                                    args,
                                                    context.severity_map)
        suppressed_in_code = get_suppressed_reports(report_dir_results)

        diff_type = get_diff_type(args)
        run_ids, run_names, _ = process_run_args(client, remote_run_names)
        local_report_hashes = set([r.report_hash for r in report_dir_results])

        if diff_type == ttypes.DiffType.NEW:
            # Get report hashes which can be found only in the remote runs.
            remote_hashes = \
                client.getDiffResultsHash(run_ids,
                                          local_report_hashes,
                                          ttypes.DiffType.RESOLVED,
                                          None)

            results = get_diff_base_results(client, args, run_ids,
                                            remote_hashes,
                                            suppressed_in_code)
            for result in results:
                filtered_reports.append(result)
        elif diff_type == ttypes.DiffType.UNRESOLVED:
            # Get remote hashes which can be found in the remote run and in the
            # local report directory.
            remote_hashes = \
                client.getDiffResultsHash(run_ids,
                                          local_report_hashes,
                                          ttypes.DiffType.UNRESOLVED,
                                          None)
            for result in report_dir_results:
                rep_h = result.report_hash
                if rep_h in remote_hashes and rep_h not in suppressed_in_code:
                    filtered_reports.append(result)
        elif diff_type == ttypes.DiffType.RESOLVED:
            # Get remote hashes which can be found in the remote run and in the
            # local report directory.
            remote_hashes = \
                client.getDiffResultsHash(run_ids,
                                          local_report_hashes,
                                          ttypes.DiffType.UNRESOLVED,
                                          None)
            for result in report_dir_results:
                if result.report_hash not in remote_hashes:
                    filtered_reports.append(result)
        return filtered_reports, run_names

    def get_diff_remote_run_local_dir(client, remote_run_names, report_dirs):
        """"""
        Compares a remote run with a local report directory.
        """"""
        filtered_reports = []
        report_dir_results = get_report_dir_results(report_dirs,
                                                    args,
                                                    context.severity_map)
        suppressed_in_code = get_suppressed_reports(report_dir_results)

        diff_type = get_diff_type(args)
        run_ids, run_names, _ = process_run_args(client, remote_run_names)
        local_report_hashes = set([r.report_hash for r in report_dir_results])

        remote_hashes = client.getDiffResultsHash(run_ids,
                                                  local_report_hashes,
                                                  diff_type,
                                                  None)

        if diff_type in [ttypes.DiffType.NEW, ttypes.DiffType.UNRESOLVED]:
            # Shows reports from the report dir which are not present in
            # the baseline (NEW reports) or appear in both side (UNRESOLVED
            # reports) and not suppressed in the code.
            for result in report_dir_results:
                rep_h = result.report_hash
                if rep_h in remote_hashes and rep_h not in suppressed_in_code:
                    filtered_reports.append(result)
        elif diff_type == ttypes.DiffType.RESOLVED:
            # Show bugs in the baseline (server) which are not present in
            # the report dir or suppressed.
            results = get_diff_base_results(client,
                                            args,
                                            run_ids,
                                            remote_hashes,
                                            suppressed_in_code)
            for result in results:
                filtered_reports.append(result)

        return filtered_reports, run_names

    def get_diff_remote_runs(client, remote_base_run_names,
                             remote_new_run_names):
        """"""
        Compares two remote runs and returns the filtered results.
        """"""
        report_filter = ttypes.ReportFilter()
        add_filter_conditions(client, report_filter, args)

        base_ids, base_run_names, base_run_tags = \
            process_run_args(client, remote_base_run_names)
        report_filter.runTag = base_run_tags

        cmp_data = ttypes.CompareData()
        cmp_data.diffType = get_diff_type(args)

        new_ids, new_run_names, new_run_tags = \
            process_run_args(client, remote_new_run_names)
        cmp_data.runIds = new_ids
        cmp_data.runTag = new_run_tags

        # Do not show resolved bugs in compare mode new.
        if cmp_data.diffType == ttypes.DiffType.NEW:
            report_filter.detectionStatus = [
                ttypes.DetectionStatus.NEW,
                ttypes.DetectionStatus.UNRESOLVED,
                ttypes.DetectionStatus.REOPENED]

        sort_mode = [(ttypes.SortMode(
            ttypes.SortType.FILENAME,
            ttypes.Order.ASC))]

        all_results = get_run_results(client,
                                      base_ids,
                                      constants.MAX_QUERY_SIZE,
                                      0,
                                      sort_mode,
                                      report_filter,
                                      cmp_data,
                                      False)

        return all_results, base_run_names, new_run_names

    def get_diff_local_dirs(base_run_names, new_run_names):
        """"""
        Compares two report directories and returns the filtered results.
        """"""
        filtered_reports = []
        base_results = get_report_dir_results(base_run_names,
                                              args,
                                              context.severity_map)
        new_results = get_report_dir_results(new_run_names,
                                             args,
                                             context.severity_map)

        base_hashes = set([res.report_hash for res in base_results])
        new_hashes = set([res.report_hash for res in new_results])

        diff_type = get_diff_type(args)
        if diff_type == ttypes.DiffType.NEW:
            for res in new_results:
                if res.report_hash not in base_hashes:
                    filtered_reports.append(res)
        if diff_type == ttypes.DiffType.UNRESOLVED:
            for res in new_results:
                if res.report_hash in base_hashes:
                    filtered_reports.append(res)
        elif diff_type == ttypes.DiffType.RESOLVED:
            for res in base_results:
                if res.report_hash not in new_hashes:
                    filtered_reports.append(res)

        return filtered_reports

    def cached_report_file_lookup(file_cache, file_id):
        """"""
        Get source file data for the given file and caches it in a file cache
        if file data is not found in the cache. Finally, it returns the source
        file data from the cache.
        """"""
        if file_id not in file_cache:
            source = client.getSourceFileData(file_id, True,
                                              ttypes.Encoding.BASE64)
            file_content = convert.from_b64(source.fileContent)
            file_cache[file_id] = {'id': file_id,
                                   'path': source.filePath,
                                   'content': file_content}

        return file_cache[file_id]

    def get_report_data(client, reports, file_cache):
        """"""
        Returns necessary report files and report data events for the HTML
        plist parser.
        """"""
        file_sources = {}
        report_data = []
","I think we should extend the help of this command too with this information. And mention that if only 1 output is given, the results will be printed to the standard output.","The suggested code changes are focused on handling various filtering conditions for runs and reports in a CodeChecker command-line tool. Here's a concise review of the changes:

1. **Run Filtering**:
   - The `process_run_filter_conditions` function processes various filter conditions like `names`, `all_after_run`, `all_before_run`, etc., and constructs a `RunFilter` object.
   - It handles both exact matches and range-based filters for `detected_at` and `fixed_at`.

2. **Report Filtering**:
   - The `add_filter_conditions` function adds additional filter conditions to the `ReportFilter` object based on the input arguments.
   - It supports filtering by `detected_before`, `detected_after`, `fixed_before`, and `fixed_after`.

3. **Result Handling**:
   - The `handle_list_results` function processes the `ReportFilter` to retrieve and display results in JSON or plaintext format.
   - It handles details mode for JSON output.

4. **Diff Handling**:
   - The `handle_diff_results` function handles diff operations between local and remote runs or between two remote runs.
   - It supports various diff types like NEW, UNRESOLVED, and RESOLVED.
   - It uses caching to efficiently retrieve source file data.

Overall, these changes improve the robustness and flexibility of the CodeChecker command-line tool by providing comprehensive filtering options and efficient result handling."
41,"@@ -18,10 +18,10 @@ node putTargets.js
 
 // Import required AWS SDK clients and commands for Node.js
 import { PutTargetsCommand } from ""@aws-sdk/client-cloudwatch-events"";
-import { cweClient } from ""./libs/cloudWatchEventsClient"";
+import { cweClient } from ""./libs/cloudWatchEventsClient.js"";
 
 // Set the parameters
-const params = {
+export const params = {
   Rule: ""DEMO_EVENT"",
   Targets: [
     {","/* Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
SPDX-License-Identifier: Apache-2.0

ABOUT THIS NODE.JS EXAMPLE: This example works with AWS SDK for JavaScript version 3 (v3),
which is available at https://github.com/aws/aws-sdk-js-v3. This example is in the 'AWS SDK for JavaScript v3 Developer Guide' at
https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/cloudwatch-examples-sending-events.html.

Purpose:
putTargets.js demonstrates how to add or update a target to an Amazon CloudWatch Events rule.

Inputs (replace in code):
- LAMBDA_FUNCTION_ARN

Running the code:
node putTargets.js
*/
// snippet-start:[cwEvents.JavaScript.cwe.putTargetsV3]
import { cweClient } from ""./libs/cloudWatchEventsClient.js"";
export const params = {

// Import required AWS SDK clients and commands for Node.js
import { PutTargetsCommand } from ""@aws-sdk/client-cloudwatch-events"";
import { cweClient } from ""./libs/cloudWatchEventsClient"";

// Set the parameters
const params = {
  Rule: ""DEMO_EVENT"",
  Targets: [
    {
      Arn: ""LAMBDA_FUNCTION_ARN"", //LAMBDA_FUNCTION_ARN
      Id: ""myCloudWatchEventsTarget"",
    },
  ],
};

const run = async () => {
  try {
    const data = await cweClient.send(new PutTargetsCommand(params));
    console.log(""Success, target added; requestID: "", data.$metadata.requestId);
    return data;
  } catch (err) {
    console.log(""Error"", err);
  }
};
run();
// snippet-end:[cwEvents.JavaScript.cwe.putTargetsV3]
// For unit tests only.
// module.exports ={run, params};
",I dont think these imports need the .js right?,"The suggested code change appears to be a complete example of how to add or update a target to an Amazon CloudWatch Events rule using the AWS SDK for JavaScript version 3 (v3). The code defines a `params` object with the necessary information, including the rule name and the target details such as the ARN and ID. It then sends a `PutTargetsCommand` to the CloudWatch Events client to add or update the target. The code also includes error handling and logging."
42,"@@ -3333,7 +3333,14 @@ instr_is_reg_spill_or_restore_ex(void *drcontext, instr_t *instr, bool DR_only,
         reg = &myreg;
     if (instr_check_tls_spill_restore(instr, spill, reg, &check_disp)) {
         int offs = reg_spill_tls_offs(*reg);
-        if (!DR_only || (offs != -1 && check_disp == os_tls_offset((ushort)offs))) {
+        if (!DR_only ||
+            (offs != -1 &&
+             /* Mangling may choose to spill registers to a not natural tls offset,
+              * e.g. rip-rel mangling will, if rax is used by the instruction. We
+              * allow for this here and still recognize the DR spill.
+              */
+             (instr_is_our_mangling(instr) ||
+              check_disp == os_tls_offset((ushort)offs)))) {
             if (tls != NULL)
                 *tls = true;
             if (offs_out != NULL)","    opnd_t memopnd = opnd_create_dcontext_field(dcontext, offs);
    return XINST_CREATE_jump_mem(dcontext, memopnd);
#    endif
}

/* there is no corresponding save routine since we no longer support
 * keeping state on the stack while code other than our own is running
 * (in the same thread)
 */
instr_t *
instr_create_restore_dynamo_stack(dcontext_t *dcontext)
{
    return instr_create_restore_from_dcontext(dcontext, REG_XSP, DSTACK_OFFSET);
}

/* make sure to keep in sync w/ emit_utils.c's insert_spill_or_restore() */
bool
instr_raw_is_tls_spill(byte *pc, reg_id_t reg, ushort offs)
{
#    ifdef X86
    ASSERT_NOT_IMPLEMENTED(reg != REG_XAX);
#        ifdef X64
    /* match insert_jmp_to_ibl */
    if (*pc == TLS_SEG_OPCODE &&
        *(pc + 1) == (REX_PREFIX_BASE_OPCODE | REX_PREFIX_W_OPFLAG) &&
        *(pc + 2) == MOV_REG2MEM_OPCODE &&
        /* 0x1c for ebx, 0x0c for ecx, 0x04 for eax */
        *(pc + 3) == MODRM_BYTE(0 /*mod*/, reg_get_bits(reg), 4 /*rm*/) &&
        *(pc + 4) == 0x25 && *((uint *)(pc + 5)) == (uint)os_tls_offset(offs))
        return true;
        /* we also check for 32-bit.  we could take in flags and only check for one
         * version, but we're not worried about false positives.
         */
#        endif
    /* looking for:   67 64 89 1e e4 0e    addr16 mov    %ebx -> %fs:0xee4   */
    /* ASSUMPTION: when addr16 prefix is used, prefix order is fixed */
    return (*pc == ADDR_PREFIX_OPCODE && *(pc + 1) == TLS_SEG_OPCODE &&
            *(pc + 2) == MOV_REG2MEM_OPCODE &&
            /* 0x1e for ebx, 0x0e for ecx, 0x06 for eax */
            *(pc + 3) == MODRM_BYTE(0 /*mod*/, reg_get_bits(reg), 6 /*rm*/) &&
            *((ushort *)(pc + 4)) == os_tls_offset(offs)) ||
        /* PR 209709: allow for no addr16 prefix */
        (*pc == TLS_SEG_OPCODE && *(pc + 1) == MOV_REG2MEM_OPCODE &&
         /* 0x1e for ebx, 0x0e for ecx, 0x06 for eax */
         *(pc + 2) == MODRM_BYTE(0 /*mod*/, reg_get_bits(reg), 6 /*rm*/) &&
         *((uint *)(pc + 4)) == os_tls_offset(offs));
#    elif defined(AARCHXX)
    /* FIXME i#1551, i#1569: NYI on ARM/AArch64 */
    ASSERT_NOT_IMPLEMENTED(false);
    return false;
#    endif /* X86/ARM */
}

/* this routine may upgrade a level 1 instr */
static bool
instr_check_tls_spill_restore(instr_t *instr, bool *spill, reg_id_t *reg, int *offs)
{
    opnd_t regop, memop;
    CLIENT_ASSERT(instr != NULL,
                  ""internal error: tls spill/restore check: NULL argument"");
    if (instr_get_opcode(instr) == OP_store) {
        regop = instr_get_src(instr, 0);
        memop = instr_get_dst(instr, 0);
        if (spill != NULL)
            *spill = true;
    } else if (instr_get_opcode(instr) == OP_load) {
        regop = instr_get_dst(instr, 0);
        memop = instr_get_src(instr, 0);
        if (spill != NULL)
            *spill = false;
#    ifdef X86
    } else if (instr_get_opcode(instr) == OP_xchg) {
        /* we use xchg to restore in dr_insert_mbr_instrumentation */
        regop = instr_get_src(instr, 0);
        memop = instr_get_dst(instr, 0);
        if (spill != NULL)
            *spill = false;
#    endif
    } else
        return false;
    if (opnd_is_reg(regop) &&
#    ifdef X86
        opnd_is_far_base_disp(memop) && opnd_get_segment(memop) == SEG_TLS &&
        opnd_is_abs_base_disp(memop)
#    elif defined(AARCHXX)
        opnd_is_base_disp(memop) && opnd_get_base(memop) == dr_reg_stolen &&
        opnd_get_index(memop) == DR_REG_NULL
#    endif
    ) {
        if (reg != NULL)
            *reg = opnd_get_reg(regop);
        if (offs != NULL)
            *offs = opnd_get_disp(memop);
        return true;
    }
    return false;
}

/* if instr is level 1, does not upgrade it and instead looks at raw bits,
 * to support identification w/o ruining level 0 in decode_fragment, etc.
 */
bool
instr_is_tls_spill(instr_t *instr, reg_id_t reg, ushort offs)
{
    reg_id_t check_reg = REG_NULL; /* init to satisfy some compilers */
    int check_disp = 0;            /* init to satisfy some compilers */
    bool spill;
    return (instr_check_tls_spill_restore(instr, &spill, &check_reg, &check_disp) &&
            spill && check_reg == reg && check_disp == os_tls_offset(offs));
}

/* if instr is level 1, does not upgrade it and instead looks at raw bits,
 * to support identification w/o ruining level 0 in decode_fragment, etc.
 */
bool
instr_is_tls_restore(instr_t *instr, reg_id_t reg, ushort offs)
{
    reg_id_t check_reg = REG_NULL; /* init to satisfy some compilers */
    int check_disp = 0;            /* init to satisfy some compilers */
    bool spill;
    return (instr_check_tls_spill_restore(instr, &spill, &check_reg, &check_disp) &&
            !spill && (reg == REG_NULL || check_reg == reg) &&
            check_disp == os_tls_offset(offs));
}

/* if instr is level 1, does not upgrade it and instead looks at raw bits,
 * to support identification w/o ruining level 0 in decode_fragment, etc.
 */
bool
instr_is_tls_xcx_spill(instr_t *instr)
{
#    ifdef X86
    if (instr_raw_bits_valid(instr)) {
        /* avoid upgrading instr */
        return instr_raw_is_tls_spill(instr_get_raw_bits(instr), REG_ECX,
                                      MANGLE_XCX_SPILL_SLOT);
    } else
        return instr_is_tls_spill(instr, REG_ECX, MANGLE_XCX_SPILL_SLOT);
#    elif defined(AARCHXX)
    /* FIXME i#1551, i#1569: NYI on ARM/AArch64 */
    ASSERT_NOT_IMPLEMENTED(false);
    return false;
#    endif
}

/* this routine may upgrade a level 1 instr */
static bool
instr_check_mcontext_spill_restore(dcontext_t *dcontext, instr_t *instr, bool *spill,
                                   reg_id_t *reg, int *offs)
{
#    ifdef X64
    /* PR 244737: we always use tls for x64 */
    return false;
#    else
    opnd_t regop, memop;
    if (instr_get_opcode(instr) == OP_store) {
        regop = instr_get_src(instr, 0);
        memop = instr_get_dst(instr, 0);
        if (spill != NULL)
            *spill = true;
    } else if (instr_get_opcode(instr) == OP_load) {
        regop = instr_get_dst(instr, 0);
        memop = instr_get_src(instr, 0);
        if (spill != NULL)
            *spill = false;
#        ifdef X86
    } else if (instr_get_opcode(instr) == OP_xchg) {
        /* we use xchg to restore in dr_insert_mbr_instrumentation */
        regop = instr_get_src(instr, 0);
        memop = instr_get_dst(instr, 0);
        if (spill != NULL)
            *spill = false;
#        endif /* X86 */
    } else
        return false;
    if (opnd_is_near_base_disp(memop) && opnd_is_abs_base_disp(memop) &&
        opnd_is_reg(regop)) {
        byte *pc = (byte *)opnd_get_disp(memop);
        byte *mc = (byte *)get_mcontext(dcontext);
        if (pc >= mc && pc < mc + sizeof(priv_mcontext_t)) {
            if (reg != NULL)
                *reg = opnd_get_reg(regop);
            if (offs != NULL)
                *offs = pc - (byte *)dcontext;
            return true;
        }
    }
    return false;
#    endif
}

static bool
instr_is_reg_spill_or_restore_ex(void *drcontext, instr_t *instr, bool DR_only, bool *tls,
                                 bool *spill, reg_id_t *reg, uint *offs_out)
{
    dcontext_t *dcontext = (dcontext_t *)drcontext;
    int check_disp = 0; /* init to satisfy some compilers */
    reg_id_t myreg;
    CLIENT_ASSERT(instr != NULL, ""invalid NULL argument"");
    if (reg == NULL)
        if (!DR_only ||
            (offs != -1 &&
             /* Mangling may choose to spill registers to a not natural tls offset,
              * e.g. rip-rel mangling will, if rax is used by the instruction. We
              * allow for this here and still recognize the DR spill.
              */
             (instr_is_our_mangling(instr) ||
              check_disp == os_tls_offset((ushort)offs)))) {
        reg = &myreg;
    if (instr_check_tls_spill_restore(instr, spill, reg, &check_disp)) {
        int offs = reg_spill_tls_offs(*reg);
        if (!DR_only || (offs != -1 && check_disp == os_tls_offset((ushort)offs))) {
            if (tls != NULL)
                *tls = true;
            if (offs_out != NULL)
                *offs_out = check_disp;
            return true;
        }
    }
    if (dcontext != GLOBAL_DCONTEXT &&
        instr_check_mcontext_spill_restore(dcontext, instr, spill, reg, &check_disp)) {
        int offs = opnd_get_reg_dcontext_offs(dr_reg_fixer[*reg]);
        if (!DR_only || (offs != -1 && check_disp == offs)) {
            if (tls != NULL)
                *tls = false;
            if (offs_out != NULL)
                *offs_out = check_disp;
            return true;
        }
    }
    return false;
}

DR_API
bool
instr_is_reg_spill_or_restore(void *drcontext, instr_t *instr, bool *tls, bool *spill,
                              reg_id_t *reg, uint *offs)
{
    return instr_is_reg_spill_or_restore_ex(drcontext, instr, false, tls, spill, reg,
                                            offs);
}

bool
instr_is_DR_reg_spill_or_restore(void *drcontext, instr_t *instr, bool *tls, bool *spill,
                                 reg_id_t *reg)
{
    return instr_is_reg_spill_or_restore_ex(drcontext, instr, true, tls, spill, reg,
                                            NULL);
}

/* N.B. : client meta routines (dr_insert_* etc.) should never use anything other
 * then TLS_XAX_SLOT unless the client has specified a slot to use as we let the
 * client use the rest. */
instr_t *
instr_create_save_to_tls(dcontext_t *dcontext, reg_id_t reg, ushort offs)
{
    return XINST_CREATE_store(dcontext, opnd_create_tls_slot(os_tls_offset(offs)),
                              opnd_create_reg(reg));
}

instr_t *
instr_create_restore_from_tls(dcontext_t *dcontext, reg_id_t reg, ushort offs)
{
    return XINST_CREATE_load(dcontext, opnd_create_reg(reg),
                             opnd_create_tls_slot(os_tls_offset(offs)));
}

/* For -x86_to_x64, we can spill to 64-bit extra registers (xref i#751). */
instr_t *
instr_create_save_to_reg(dcontext_t *dcontext, reg_id_t reg1, reg_id_t reg2)
{
    return XINST_CREATE_move(dcontext, opnd_create_reg(reg2), opnd_create_reg(reg1));
}

instr_t *
instr_create_restore_from_reg(dcontext_t *dcontext, reg_id_t reg1, reg_id_t reg2)
{
    return XINST_CREATE_move(dcontext, opnd_create_reg(reg1), opnd_create_reg(reg2));
}

#    ifdef X86_64
/* Returns NULL if pc is not the start of a rip-rel lea.
 * If it could be, returns the address it refers to (which we assume is
 * never NULL).
 */
byte *
instr_raw_is_rip_rel_lea(byte *pc, byte *read_end)
{
    /* PR 215408: look for ""lea reg, [rip+disp]""
     * We assume no extraneous prefixes, and we require rex.w, though not strictly
     * necessary for say WOW64 or other known-lower-4GB situations
     */
    if (pc + 7 <= read_end) {
        if (*(pc + 1) == RAW_OPCODE_lea &&
            (TESTALL(REX_PREFIX_BASE_OPCODE | REX_PREFIX_W_OPFLAG, *pc) &&
             !TESTANY(~(REX_PREFIX_BASE_OPCODE | REX_PREFIX_ALL_OPFLAGS), *pc)) &&
            /* does mod==0 and rm==5? */
            ((*(pc + 2)) | MODRM_BYTE(0, 7, 0)) == MODRM_BYTE(0, 7, 5)) {
            return pc + 7 + *(int *)(pc + 3);
        }
    }
    return NULL;
}
#    endif

uint
move_mm_reg_opcode(bool aligned16, bool aligned32)
{
#    ifdef X86
    if (YMM_ENABLED()) {
        /* must preserve ymm registers */
        return (aligned32 ? OP_vmovdqa : OP_vmovdqu);
    } else if (proc_has_feature(FEATURE_SSE2)) {
        return (aligned16 ? OP_movdqa : OP_movdqu);
    } else {
        CLIENT_ASSERT(proc_has_feature(FEATURE_SSE), ""running on unsupported processor"");
        return (aligned16 ? OP_movaps : OP_movups);
    }
#    elif defined(ARM)
    /* FIXME i#1551: which one we should return, OP_vmov, OP_vldr, or OP_vstr? */
    return OP_vmov;
#    elif defined(AARCH64)
    ASSERT_NOT_IMPLEMENTED(false); /* FIXME i#1569 */
    return 0;
#    endif /* X86/ARM */
}

#endif /* !STANDALONE_DECODER */
/****************************************************************************/
","This only works for a mangled instruction list: i.e., only during bb building (after a client's events) or bb recreation (also after a client's events). Thus this call will never return true for a client. Plus, this is-spill function is used for more than instrlists: some callers use it when decoding raw instructions, such as drreg and also I think DR may use it that way when it decodes from the cache in some cases? So this does not seem a complete solution.","The suggested code changes appear to be related to handling TLS (Thread Local Storage) operations in a dynamic analysis tool. Here's a concise review:

### Key Changes:
1. **TLS Spill/Restore Functions**:
   - `instr_create_restore_dynamo_stack`: Creates a jump to the DynamoRIO stack.
   - `instr_raw_is_tls_spill`: Checks if a given PC points to a TLS spill or restore operation.
   - `instr_check_tls_spill_restore`: Determines if an instruction is a TLS spill or restore.
   - `instr_is_tls_spill` and `instr_is_tls_restore`: Check if an instruction is a specific type of TLS operation.
   - `instr_is_tls_xcx_spill`: Checks if an instruction is a TLS spill using the `ecx` register.
   - `instr_is_reg_spill_or_restore`: Checks if an instruction is a register spill or restore.

2. **TLS Save/Restore Instructions**:
   - `instr_create_save_to_tls`: Creates a store instruction to a TLS slot.
   - `instr_create_restore_from_tls`: Creates a load instruction from a TLS slot.
   - `instr_create_save_to_reg`: Creates a move instruction to a register.
   - `instr_create_restore_from_reg`: Creates a move instruction from a register.

3. **RIP-Relative LEA Detection**:
   - `instr_raw_is_rip_rel_lea`: Detects if a PC points to a rip-relative LEA instruction.

4. **Move MM Register Opcode**:
   - `move_mm_reg_opcode`: Determines the appropriate move opcode based on alignment requirements.

### Review Points:
- **TLS Handling**: The code provides comprehensive functions for handling TLS spills, restores, and related operations, ensuring that the tool can correctly identify and manipulate TLS-related instructions.
- **Instruction Creation**: Functions like `instr_create_save_to_tls`, `instr_create_restore_from_tls`, etc., simplify the creation of TLS-related instructions, making the tool more efficient and easier to use.
- **RIP-Relative LEA Detection**: The `instr_raw_is_rip_rel_lea` function is useful for identifying rip-relative LEA instructions, which are common in certain architectures.
- **Opcode Determination**: The `move_mm_reg_opcode` function ensures that the correct move opcode is chosen based on alignment requirements, which is crucial for performance and correctness.

Overall, these changes enhance the functionality of the dynamic analysis tool by providing robust support for TLS operations and related instructions."
43,"@@ -43,7 +43,7 @@ namespace Samples
                 .MapResult(
                     (JaegerOptions options) => TestJaegerExporter.Run(options.Host, options.Port),
                     (ZipkinOptions options) => TestZipkinExporter.Run(options.Uri),
-                    (PrometheusOptions options) => TestPrometheus.RunAsync(options.Port, options.PushIntervalInSecs, options.DurationInMins),
+                    (PrometheusOptions options) => TestPrometheusExporter.RunAsync(options.Port, options.PushIntervalInSecs, options.DurationInMins),
                     (HttpClientOptions options) => TestHttpClient.Run(),
                     (RedisOptions options) => TestRedis.Run(options.Uri),
                     (ZPagesOptions options) => TestZPagesExporter.Run(),","﻿// <copyright file=""Program.cs"" company=""OpenTelemetry Authors"">
// Copyright The OpenTelemetry Authors
//
// Licensed under the Apache License, Version 2.0 (the ""License"");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an ""AS IS"" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
// </copyright>
using System;
using CommandLine;

namespace Samples
{
    /// <summary>
    /// Main samples entry point.
    /// </summary>
    public class Program
    {
        /// <summary>
        /// Main method - invoke this using command line.
        /// For example:
        ///
        /// dotnet run -p Exporters.csproj console
        /// dotnet run -p Exporters.csproj zipkin -u http://localhost:9411/api/v2/spans
        /// dotnet run -p Exporters.csproj jaeger -h localhost -p 6831
        /// dotnet run -p Exporters.csproj prometheus -i 15 -p 9184 -d 2
        /// dotnet run -p Exporters.csproj otlp -e ""localhost:55680""
        ///
        /// The above must be run from the project root folder
        /// (eg: C:\repos\opentelemetry-dotnet\src\samples\Exporters\Console\).
        /// </summary>
        /// <param name=""args"">Arguments from command line.</param>
        public static void Main(string[] args)
        {
            Parser.Default.ParseArguments<JaegerOptions, ZipkinOptions, PrometheusOptions, HttpClientOptions, RedisOptions, ZPagesOptions, ConsoleOptions, OpenTelemetryShimOptions, OtlpOptions>(args)
                    (PrometheusOptions options) => TestPrometheusExporter.RunAsync(options.Port, options.PushIntervalInSecs, options.DurationInMins),
                .MapResult(
                    (JaegerOptions options) => TestJaegerExporter.Run(options.Host, options.Port),
                    (ZipkinOptions options) => TestZipkinExporter.Run(options.Uri),
                    (PrometheusOptions options) => TestPrometheus.RunAsync(options.Port, options.PushIntervalInSecs, options.DurationInMins),
                    (HttpClientOptions options) => TestHttpClient.Run(),
                    (RedisOptions options) => TestRedis.Run(options.Uri),
                    (ZPagesOptions options) => TestZPagesExporter.Run(),
                    (ConsoleOptions options) => TestConsoleExporter.Run(options),
                    (OpenTelemetryShimOptions options) => TestOTelShimWithConsoleExporter.Run(options),
                    (OtlpOptions options) => TestOtlpExporter.Run(options.Endpoint),
                    errs => 1);

            Console.ReadLine();
        }
    }

#pragma warning disable SA1402 // File may only contain a single type

    [Verb(""jaeger"", HelpText = ""Specify the options required to test Jaeger exporter"")]
    internal class JaegerOptions
    {
        [Option('h', ""host"", HelpText = ""Host of the Jaeger Agent"", Default = ""localhost"")]
        public string Host { get; set; }

        [Option('p', ""port"", HelpText = ""Port of the Jaeger Agent"", Default = 6831)]
        public int Port { get; set; }
    }

    [Verb(""zipkin"", HelpText = ""Specify the options required to test Zipkin exporter"")]
    internal class ZipkinOptions
    {
        [Option('u', ""uri"", HelpText = ""Please specify the uri of Zipkin backend"", Required = true)]
        public string Uri { get; set; }
    }

    [Verb(""prometheus"", HelpText = ""Specify the options required to test Prometheus"")]
    internal class PrometheusOptions
    {
        [Option('i', ""pushIntervalInSecs"", Default = 15, HelpText = ""The interval at which Push controller pushes metrics."", Required = false)]
        public int PushIntervalInSecs { get; set; }

        [Option('p', ""port"", Default = 9184, HelpText = ""The port to expose metrics. The endpoint will be http://localhost:port/metrics (This is the port from which your Prometheus server scraps metrics from.)"", Required = false)]
        public int Port { get; set; }

        [Option('d', ""duration"", Default = 2, HelpText = ""Total duration in minutes to run the demo. Run atleast for a min to see metrics flowing."", Required = false)]
        public int DurationInMins { get; set; }
    }

    [Verb(""httpclient"", HelpText = ""Specify the options required to test HttpClient"")]
    internal class HttpClientOptions
    {
    }

    [Verb(""redis"", HelpText = ""Specify the options required to test Redis with Zipkin"")]
    internal class RedisOptions
    {
        [Option('u', ""uri"", HelpText = ""Please specify the uri of Zipkin backend"", Required = true)]
        public string Uri { get; set; }
    }

    [Verb(""zpages"", HelpText = ""Specify the options required to test ZPages"")]
    internal class ZPagesOptions
    {
    }

    [Verb(""console"", HelpText = ""Specify the options required to test console exporter"")]
    internal class ConsoleOptions
    {
        [Option('p', ""displayasjson"", HelpText = ""Specify if the output should be displayed as json or not (default: false)"", Default = false)]
        public bool DisplayAsJson { get; set; }
    }

    [Verb(""otelshim"", HelpText = ""Specify the options required to test OpenTelemetry Shim with console exporter"")]
    internal class OpenTelemetryShimOptions
    {
        [Option('p', ""displayasjson"", HelpText = ""Specify if the output should be displayed as json or not (default: false)"", Default = false)]
        public bool DisplayAsJson { get; set; }
    }

    [Verb(""otlp"", HelpText = ""Specify the options required to test OpenTelemetry Protocol (OTLP)"")]
    internal class OtlpOptions
    {
        [Option('e', ""endpoint"", HelpText = ""Target to which the exporter is going to send traces or metrics"", Default = ""localhost:55680"")]
        public string Endpoint { get; set; }
    }

#pragma warning restore SA1402 // File may only contain a single type

}
","separately we can remove the ""Exporters"" folder. This is not just showing Exporter sample anymore.","The provided code is a command-line interface (CLI) application that allows users to test various OpenTelemetry exporters. Here's a concise review of the suggested code changes:

### Key Points:

1. **Namespace and Class Structure**: 
   - The code is organized into a `Program` class and several verb classes (`JaegerOptions`, `ZipkinOptions`, etc.), each representing a different exporter.

2. **Command Line Parsing**:
   - The `Main` method uses `CommandLine.Parser` to parse command-line arguments and map them to the appropriate verb classes.
   - The `MapResult` method handles different outcomes of parsing, such as successful execution or errors.

3. **Verb Classes**:
   - Each verb class contains options specific to the corresponding exporter. For example, `JaegerOptions` includes `Host` and `Port` for Jaeger, while `ZipkinOptions` includes `Uri`.

4. **Default Values**:
   - Many options have default values specified, making it easier for users to run the application without specifying all options.

5. **Output Options**:
   - Some options, like `DisplayAsJson` in `ConsoleOptions` and `OpenTelemetryShimOptions`, allow users to control the format of the output.

6. **Error Handling**:
   - The `MapResult` method returns an error code if parsing fails, which can be useful for debugging.

### Suggested Improvements:

1. **Documentation**:
   - Add more detailed documentation to the `HelpText` attributes in the verb classes to explain what each option does.

2. **Error Handling**:
   - Consider adding more robust error handling, such as logging or displaying usage information when parsing fails.

3. **Code Formatting**:
   - Ensure consistent formatting throughout the code, especially in the `Main` method and verb classes.

4. **Testing**:
   - Add unit tests to verify that the command-line parsing works correctly and that the exporter methods behave as expected.

5. **Security**:
   - If any sensitive information is being passed via command-line arguments, consider implementing security measures to protect it.

Overall, the code is well-structured and provides a good starting point for testing OpenTelemetry exporters."
44,"@@ -23,7 +23,7 @@ import (
 )
 
 const (
-	avgCPUUtilizationFanout  int32 = 95
+	avgCPUUtilizationFanout  int32 = 30
 	avgCPUUtilizationIngress int32 = 95
 	avgCPUUtilizationRetry   int32 = 95
 	// The limit we set (for Fanout and Retry) is 3000Mi which is mostly used","/*
Copyright 2020 Google LLC

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package v1alpha1

import (
	""context""

	""knative.dev/pkg/ptr""
	avgCPUUtilizationFanout  int32 = 30
)

const (
	avgCPUUtilizationFanout  int32 = 95
	avgCPUUtilizationIngress int32 = 95
	avgCPUUtilizationRetry   int32 = 95
	// The limit we set (for Fanout and Retry) is 3000Mi which is mostly used
	// to prevent surging memory usage causing OOM.
	// Here we only set half of the limit so that in case of surging memory
	// usage, HPA could have enough time to kick in.
	// See: https://github.com/google/knative-gcp/issues/1265
	avgMemoryUsageFanout  string = ""1500Mi""
	avgMemoryUsageIngress string = ""1500Mi""
	avgMemoryUsageRetry   string = ""1500Mi""
	cpuRequestFanout      string = ""1500m""
	cpuRequestIngress     string = ""2000m""
	cpuRequestRetry       string = ""1000m""
	cpuLimitFanout        string = """"
	cpuLimitIngress       string = """"
	cpuLimitRetry         string = """"
	memoryRequestFanout   string = ""500Mi""
	memoryRequestIngress  string = ""2000Mi""
	memoryRequestRetry    string = ""500Mi""
	memoryLimitFanout     string = ""3000Mi""
	memoryLimitIngress    string = ""2000Mi""
	memoryLimitRetry      string = ""3000Mi""
	minReplicas           int32  = 1
	maxReplicas           int32  = 10
)

// SetDefaults sets the default field values for a BrokerCell.
func (bc *BrokerCell) SetDefaults(ctx context.Context) {
	// Set defaults for the Spec.Components values.
	bc.Spec.SetDefaults(ctx)
}

// SetDefaults sets the default field values for a BrokerCellSpec.
func (bcs *BrokerCellSpec) SetDefaults(ctx context.Context) {
	// Fanout defaults
	if bcs.Components.Fanout == nil {
		bcs.Components.Fanout = makeComponent(cpuRequestFanout, cpuLimitFanout, memoryRequestFanout, memoryLimitFanout, avgCPUUtilizationFanout, avgMemoryUsageFanout)
	}
	bcs.Components.Fanout.setAutoScalingDefaults()
	// Ingress defaults
	if bcs.Components.Ingress == nil {
		bcs.Components.Ingress = makeComponent(cpuRequestIngress, cpuLimitIngress, memoryRequestIngress, memoryLimitIngress, avgCPUUtilizationIngress, avgMemoryUsageIngress)
	}
	bcs.Components.Ingress.setAutoScalingDefaults()
	// Retry defaults
	if bcs.Components.Retry == nil {
		bcs.Components.Retry = makeComponent(cpuRequestRetry, cpuLimitRetry, memoryRequestRetry, memoryLimitRetry, avgCPUUtilizationRetry, avgMemoryUsageRetry)
	}
	bcs.Components.Retry.setAutoScalingDefaults()
}

func makeComponent(cpuRequest, cpuLimit, memoryRequest, memoryLimit string, avgCPUUtilization int32, targetMemoryUsage string) *ComponentParameters {
	return &ComponentParameters{
		CPURequest:        cpuRequest,
		CPULimit:          cpuLimit,
		MemoryRequest:     memoryRequest,
		MemoryLimit:       memoryLimit,
		AvgCPUUtilization: ptr.Int32(avgCPUUtilization),
		AvgMemoryUsage:    ptr.String(targetMemoryUsage),
	}
}

func (componentParams *ComponentParameters) setAutoScalingDefaults() {
	if componentParams.MinReplicas == nil {
		componentParams.MinReplicas = ptr.Int32(minReplicas)
	}
	if componentParams.MaxReplicas == nil {
		componentParams.MaxReplicas = ptr.Int32(maxReplicas)
	}
}
","I am concerned if we are overfitting to solve rapid request spikes. If I have a gradual request increase that just causes fanout to use CPU at, say, 40%, this will result in 2 pods with low CPU utilization. Is my understanding correct?","The suggested code change introduces default settings for various components within a `BrokerCell` resource. It includes setting default CPU and memory requests and limits, as well as average CPU utilization and target memory usage for each component. Additionally, it sets default minimum and maximum replicas for auto-scaling.

Here's a concise review of the suggested code change:

1. **Default Values**: The code defines default values for CPU and memory requests and limits, as well as average CPU utilization and target memory usage for each component (`Fanout`, `Ingress`, and `Retry`).

2. **Component Creation**: The `makeComponent` function is used to create a `ComponentParameters` object with these default values. This function initializes the CPU and memory request and limit fields, as well as the average CPU utilization and target memory usage.

3. **Auto-Scaling Defaults**: The `setAutoScalingDefaults` method is added to the `ComponentParameters` struct to set default minimum and maximum replicas for auto-scaling if they are not already set.

4. **SetDefaults Method**: The `SetDefaults` methods in both `BrokerCell` and `BrokerCellSpec` structs call the `makeComponent` function to set default values for their respective components.

This approach ensures that all components have consistent default configurations, which can help in managing resources efficiently and reducing the risk of unexpected behavior due to missing or incorrect configuration values."
45,"@@ -164,6 +164,16 @@ module Beaker
       optionally_color GREY, string, false
     end
 
+    # Custom reporting for performance/sysstat messages
+    # Will not print unless we are at {LOG_LEVELS} 'debug' or higher.
+    # @param args[Array<String>] Strings to be reported
+    def perf_output *args
+      return unless is_debug?
+      strings = strip_colors_from args
+      string = strings.join
+      optionally_color MAGENTA, string, false
+    end
+
     # Report a debug message.
     # Will not print unless we are at {LOG_LEVELS} 'debug' or higher.
     # @param args[Array<String>] Strings to be reported","module Beaker
    # The Beaker Logger class
    # This class handles message reporting for Beaker, it reports based upon a provided log level
    # to a given destination (be it a string or file)
    #
  class Logger
    NORMAL         = ""\e[00;00m""
    BRIGHT_NORMAL  = ""\e[00;01m""
    BLACK          = ""\e[00;30m""
    RED            = ""\e[00;31m""
    GREEN          = ""\e[00;32m""
    YELLOW         = ""\e[00;33m""
    BLUE           = ""\e[00;34m""
    MAGENTA        = ""\e[00;35m""
    CYAN           = ""\e[00;36m""
    WHITE          = ""\e[00;37m""
    GREY           = ""\e[01;30m""
    BRIGHT_RED     = ""\e[01;31m""
    BRIGHT_GREEN   = ""\e[01;32m""
    BRIGHT_YELLOW  = ""\e[01;33m""
    BRIGHT_BLUE    = ""\e[01;34m""
    BRIGHT_MAGENTA = ""\e[01;35m""
    BRIGHT_CYAN    = ""\e[01;36m""
    BRIGHT_WHITE   = ""\e[01;37m""

    # The defined log levels.  Each log level also reports messages at levels lower than itself
    LOG_LEVELS      = {
      :debug   => 5,
      :verbose => 3,
      :info    => 2,
      :notify  => 1,
      :warn    => 0,
    }

    attr_accessor :color, :log_level, :destinations

    # Initialization of the Logger class
    # @overload initialize(dests)
    #   Initialize a Logger object that reports to the provided destinations, use default options
    #   @param [Array<String, IO>] Array of IO and strings (assumed to be file paths) to be reported to
    # @overload initialize(dests, options)
    #   Initialize a Logger object that reports to the provided destinations, use options from provided option hash
    #   @param [Array<String, IO>] Array of IO and strings (assumed to be file paths) to be reported to
    #   @param [Hash] options Hash of options
    #   @option options [Boolean] :color (true) Print color code before log messages
    #   @option options [Boolean] :quiet (false) Do not log messages to STDOUT
    #   @option options [String] :log_level (""info"") Log level (one of ""debug"" - highest level, ""verbose"", ""info"",
    #                          ""notify"" and ""warn"" - lowest level (see {LOG_LEVELS}))  The log level indicates that messages at that
    #                          log_level and lower will be reported.
    def initialize(*args)
      options = args.last.is_a?(Hash) ? args.pop : {}
      @color = options[:color]
      @sublog = nil
      case options[:log_level]
      when /debug/i, :debug
        @log_level = :debug
      when /verbose/i, :verbose
        @log_level = :verbose
      when /info/i, :info
        @log_level = :info
      when /notify/i, :notify
        @log_level = :notify
      when /warn/i, :warn
        @log_level = :warn
      else
        @log_level = :verbose
      end
      @destinations = []

      dests = args
      dests << STDOUT unless options[:quiet]
      dests.uniq!
      dests.each {|dest| add_destination(dest)}
    end

    # Construct an array of open steams for printing log messages to
    # @param [Array<IO, String>] dest Array of strings (each used as a file path) and IO steams that messages will be printed to
    def add_destination(dest)
      case dest
      when IO
        @destinations << dest
      when StringIO
        @destinations << dest
      when String
        @destinations << File.open(dest, 'w')
      else
        raise ""Unsuitable log destination #{dest.inspect}""
      end
    end

    # Remove a steam from the destinations array based upon it's name or file path
    # @param [String, IO] dest String representing a file path or IO stream
    def remove_destination(dest)
      case dest
      when IO
        @destinations.delete(dest)
      when StringIO
        @destinations.delete(dest)
      when String
        @destinations.delete_if {|d| d.respond_to?(:path) and d.path == dest}
      else
        raise ""Unsuitable log destination #{dest.inspect}""
      end
    end

    # Are we at {LOG_LEVELS} debug?
    # @return [Boolean] true if 'debug' or higher, false if not 'debug' {LOG_LEVELS} or lower
    def is_debug?
      LOG_LEVELS[@log_level] >= LOG_LEVELS[:debug]
    end

    # Are we at {LOG_LEVELS} verbose?
    # @return [Boolean] true if 'verbose' or higher, false if not 'verbose' {LOG_LEVELS} or lower
    def is_verbose?
      LOG_LEVELS[@log_level] >= LOG_LEVELS[:verbose]
    end

    # Are we at {LOG_LEVELS} warn?
    # @return [Boolean] true if 'warn' or higher, false if not 'warn' {LOG_LEVELS} or lower
    def is_warn?
      LOG_LEVELS[@log_level] >= LOG_LEVELS[:warn]
    end

    # Are we at {LOG_LEVELS} info?
    # @return [Boolean] true if 'info' or higher, false if not 'info' {LOG_LEVELS} or lower
    def is_info?
      LOG_LEVELS[@log_level] >= LOG_LEVELS[:info]
    end

    # Are we at {LOG_LEVELS} notify?
    # @return [Boolean] true if 'notify' or higher, false if not 'notify' {LOG_LEVELS} or lower
    def is_notify?
      LOG_LEVELS[@log_level] >= LOG_LEVELS[:notify]
    end

    # Remove invalid UTF-8 codes from provided string(s)
    # @param [String, Array<String>] string The string(s) to remove invalid codes from
    def convert string
      if string.kind_of?(Array)
        string.map do |s|
          convert s
        end
      else
        if string.respond_to?( :force_encoding )
          # We're running in >= 1.9 and we'll need to convert
          # Remove invalid and undefined UTF-8 character encodings
          string.force_encoding('UTF-8')
          return string.chars.select{|i| i.valid_encoding?}.join
        else
          # We're running 1.8, do nothing
          string
        end
      end
    end

    # Custom reporting for messages generated by host SUTs.
    # Will not print unless we are at {LOG_LEVELS} 'verbose' or higher.
    # Strips any color codes already in the provided messages, then adds logger color codes before reporting
    # @param args[Array<String>] Strings to be reported
    def host_output *args
      return unless is_verbose?
      strings = strip_colors_from args
      string = strings.join
    # Custom reporting for performance/sysstat messages
    # Will not print unless we are at {LOG_LEVELS} 'debug' or higher.
    # @param args[Array<String>] Strings to be reported
    def perf_output *args
      return unless is_debug?
      strings = strip_colors_from args
      string = strings.join
      optionally_color MAGENTA, string, false
    end

      optionally_color GREY, string, false
    end

    # Report a debug message.
    # Will not print unless we are at {LOG_LEVELS} 'debug' or higher.
    # @param args[Array<String>] Strings to be reported
    def debug *args
      return unless is_verbose?
      optionally_color WHITE, *args
    end

    # Report a warning message.
    # Will not print unless we are at {LOG_LEVELS} 'warn' or higher.
    # Will pre-pend the message with ""Warning: "".
    # @param args[Array<String>] Strings to be reported
    def warn *args
      return unless is_warn?
      strings = args.map {|msg| ""Warning: #{msg}"" }
      optionally_color YELLOW, strings
    end

    # Report an info message.
    # Will not print unless we are at {LOG_LEVELS} 'info' or higher.
    # @param args[Array<String>] Strings to be reported
    def info *args
      return unless is_info?
      optionally_color BLUE, *args
    end

    # Report a success message.
    # Will always be reported.
    # @param args[Array<String>] Strings to be reported
    def success *args
      optionally_color GREEN, *args
    end

    # Report a notify message.
    # Will not print unless we are at {LOG_LEVELS} 'notify' or higher.
    # @param args[Array<String>] Strings to be reported
    def notify *args
      return unless is_notify?
      optionally_color BRIGHT_WHITE, *args
    end

    # Report an error message.
    # Will always be reported.
    # @param args[Array<String>] Strings to be reported
    def error *args
      optionally_color BRIGHT_RED, *args
    end

    # Strip any color codes from provided string(s)
    # @param [String] lines A single or array of lines to removed color codes from
    # @return [Array<String>] An array of strings that do not have color codes
    def strip_colors_from lines
      Array( lines ).map do |line|
        convert(line).gsub(/(\e|\^\[)\[(\d*;)*\d*m/, '')
      end
    end

    # Print the provided message to the set destination streams, using color codes if appropriate
    # @param [String] color_code The color code to pre-pend to the message
    # @param [String] msg The message to be reported
    # @param [Boolean] add_newline (true) Add newlines between the color codes and the message
    def optionally_color color_code, msg, add_newline = true
      print_statement = add_newline ? :puts : :print
      @destinations.each do |to|
        to.print color_code if @color
        to.send print_statement, convert( msg )
        to.print NORMAL if @color
      end
    end

    # Utility method to get the current call stack and format it
    # to a human-readable string (which some IDEs/editors
    # will recognize as links to the line numbers in the trace).
    # Beaker associated files will be purged from backtrace unless log level is 'debug' or higher
    # @param [String] backtrace (caller(1)) The backtrace to format
    # @return [String] The formatted backtrace
    def pretty_backtrace backtrace = caller(1)
      trace = is_debug? ? backtrace : purge_harness_files_from( backtrace )
      expand_symlinks( trace ).join ""\n""
    end

    # Create a new StringIO log to track the current output
    def start_sublog
      if @sublog
        remove_destination(@sublog)
      end
      @sublog = StringIO.new
      add_destination(@sublog)
    end

    # Return the contents of the sublog
    def get_sublog
      @sublog.rewind
      @sublog.read
    end

    private
    # Expand each symlink found to its full path
    # Lines are assumed to be in the format ""String : Integer""
    # @param [String] backtrace The string to search and expand symlinks in
    # @return [String] The backtrace with symlinks expanded
    def expand_symlinks backtrace
      backtrace.collect do |line|
        file_path, line_num = line.split( "":"" )
        expanded_path = expand_symlink File.expand_path( file_path )
        expanded_path.to_s + "":"" + line_num.to_s
      end
    end

    # Remove Beaker associated lines from a given String
    # @param [String] backtrace The string to remove Beaker associated lines from
    # @return [String] The cleaned backtrace
    def purge_harness_files_from backtrace
      mostly_purged = backtrace.reject do |line|
        # LOADED_FEATURES is an array of anything `require`d, i.e. everything
        # but the test in question
        $LOADED_FEATURES.any? do |require_path|
          line.include? require_path
        end
      end

      # And remove lines that contain our program name in them
      completely_purged = mostly_purged.reject {|line| line.include? $0 }
    end

    # Utility method that takes a path as input, checks each component
    # of the path to see if it is a symlink, and expands
    # it if it is.
    # @param [String] file_path The path to be examined
    # @return [String] The fully expanded file_path
    def expand_symlink file_path
      file_path.split( ""/"" ).inject do |full_path, next_dir|
        next_path = full_path + ""/"" + next_dir
        if File.symlink? next_path
          link = File.readlink next_path
          next_path =
              case link
                when /^\// then link
                else
                  File.expand_path( full_path + ""/"" + link )
              end
        end
        next_path
      end
    end
  end
end
",Please add a yard @param tag here.,"The suggested code change appears to be a complete implementation of a logging system for the Beaker testing framework. It includes methods for setting up and managing log destinations, determining log levels, and formatting log messages with color codes. The code is well-documented and follows a consistent structure, making it easy to understand and maintain."
46,"@@ -22,9 +22,9 @@ define(['viewContainer', 'focusManager', 'queryString', 'layoutManager'], functi
             newView.initComplete = true;
 
             if (typeof options.controllerFactory === 'function') {
-
-                // Use controller method
                 var controller = new options.controllerFactory(newView, eventDetail.detail.params);
+            } else if (options.controllerFactory && typeof options.controllerFactory.default === 'function') {
+                var controller = new options.controllerFactory.default(newView, eventDetail.detail.params);
             }
 
             if (!options.controllerFactory || dispatchPageEvents) {","define(['viewContainer', 'focusManager', 'queryString', 'layoutManager'], function (viewContainer, focusManager, queryString, layoutManager) {
    'use strict';

    var currentView;
    var dispatchPageEvents;

    viewContainer.setOnBeforeChange(function (newView, isRestored, options) {

        var lastView = currentView;
        if (lastView) {

            var beforeHideResult = dispatchViewEvent(lastView, null, 'viewbeforehide', true);

            if (!beforeHideResult) {
                // todo: cancel
            }
        }

        var eventDetail = getViewEventDetail(newView, options, isRestored);

        if (!newView.initComplete) {
            } else if (options.controllerFactory && typeof options.controllerFactory.default === 'function') {
                var controller = new options.controllerFactory.default(newView, eventDetail.detail.params);
            newView.initComplete = true;

            if (typeof options.controllerFactory === 'function') {

                // Use controller method
                var controller = new options.controllerFactory(newView, eventDetail.detail.params);
            }

            if (!options.controllerFactory || dispatchPageEvents) {
                dispatchViewEvent(newView, eventDetail, 'viewinit');
            }
        }

        dispatchViewEvent(newView, eventDetail, 'viewbeforeshow');
    });

    function onViewChange(view, options, isRestore) {

        var lastView = currentView;
        if (lastView) {
            dispatchViewEvent(lastView, null, 'viewhide');
        }

        currentView = view;

        var eventDetail = getViewEventDetail(view, options, isRestore);

        if (!isRestore) {
            if (options.autoFocus !== false) {
                focusManager.autoFocus(view);
            }
        } else if (!layoutManager.mobile) {
            if (view.activeElement && document.body.contains(view.activeElement) && focusManager.isCurrentlyFocusable(view.activeElement)) {
                focusManager.focus(view.activeElement);
            } else {
                focusManager.autoFocus(view);
            }
        }

        view.dispatchEvent(new CustomEvent('viewshow', eventDetail));

        if (dispatchPageEvents) {
            view.dispatchEvent(new CustomEvent('pageshow', eventDetail));
        }
    }

    function getProperties(view) {
        var props = view.getAttribute('data-properties');

        if (props) {
            return props.split(',');
        }

        return [];
    }

    function dispatchViewEvent(view, eventInfo, eventName, isCancellable) {

        if (!eventInfo) {
            eventInfo = {
                detail: {
                    type: view.getAttribute('data-type'),
                    properties: getProperties(view)
                },
                bubbles: true,
                cancelable: isCancellable
            };
        }

        eventInfo.cancelable = isCancellable || false;

        var eventResult = view.dispatchEvent(new CustomEvent(eventName, eventInfo));

        if (dispatchPageEvents) {
            eventInfo.cancelable = false;
            view.dispatchEvent(new CustomEvent(eventName.replace('view', 'page'), eventInfo));
        }

        return eventResult;
    }

    function getViewEventDetail(view, options, isRestore) {

        var url = options.url;
        var index = url.indexOf('?');
        var params = index === -1 ? {} : queryString.parse(url.substring(index + 1));

        return {
            detail: {
                type: view.getAttribute('data-type'),
                properties: getProperties(view),
                params: params,
                isRestored: isRestore,
                state: options.state,

                // The route options
                options: options.options || {}
            },
            bubbles: true,
            cancelable: false
        };
    }

    function resetCachedViews() {
        // Reset all cached views whenever the skin changes
        viewContainer.reset();
    }

    document.addEventListener('skinunload', resetCachedViews);

    function ViewManager() {
    }

    ViewManager.prototype.loadView = function (options) {

        var lastView = currentView;

        // Record the element that has focus
        if (lastView) {
            lastView.activeElement = document.activeElement;
        }

        if (options.cancel) {
            return;
        }

        viewContainer.loadView(options).then(function (view) {

            onViewChange(view, options);
        });
    };

    ViewManager.prototype.tryRestoreView = function (options, onViewChanging) {

        if (options.cancel) {
            return Promise.reject({ cancelled: true });
        }

        // Record the element that has focus
        if (currentView) {
            currentView.activeElement = document.activeElement;
        }

        return viewContainer.tryRestoreView(options).then(function (view) {

            onViewChanging();
            onViewChange(view, options, true);

        });
    };

    ViewManager.prototype.currentView = function () {
        return currentView;
    };

    ViewManager.prototype.dispatchPageEvents = function (value) {
        dispatchPageEvents = value;
    };

    return new ViewManager();
});
",Maybe move `var controller` out of `if/else`? (_Code Smell_),"The provided code defines a `ViewManager` class that manages the lifecycle of views in a web application. Here's a concise review of the key components and functionality:

### Key Components:
1. **Dependencies**: The code imports several modules (`viewContainer`, `focusManager`, `queryString`, `layoutManager`) which suggest it's part of a larger framework or library.

2. **State Management**:
   - `currentView`: Keeps track of the currently active view.
   - `dispatchPageEvents`: A flag to control whether page-level events should be dispatched.

3. **View Lifecycle Events**:
   - `viewContainer.setOnBeforeChange`: Handles the transition between views before they are changed.
   - `onViewChange`: Manages the transition between views after they have been changed.
   - `dispatchViewEvent`: Dispatches custom events for views and pages.

4. **View Properties**:
   - `getProperties`: Extracts properties from the `data-properties` attribute of a view.

5. **URL Parsing**:
   - `getViewEventDetail`: Parses the URL to extract query parameters and constructs an event detail object.

6. **Resetting Cached Views**:
   - `resetCachedViews`: Resets all cached views when the skin changes.

7. **View Loading and Restoration**:
   - `loadView`: Loads a new view and triggers the `onViewChange` event.
   - `tryRestoreView`: Attempts to restore a previously saved view and triggers the `onViewChange` event.

8. **Public Methods**:
   - `currentView`: Returns the current view.
   - `dispatchPageEvents`: Sets the `dispatchPageEvents` flag.

### Potential Improvements:
- **Error Handling**: Add error handling for asynchronous operations like `viewContainer.loadView` and `viewContainer.tryRestoreView`.
- **Type Checking**: Ensure that all dependencies are correctly imported and used.
- **Documentation**: Add comments and documentation to explain the purpose and usage of each function.
- **Performance**: Optimize the code for performance, especially if dealing with large numbers of views or complex event handling.

Overall, the code provides a robust framework for managing views in a web application, with clear separation of concerns and a well-defined API."
47,"@@ -357,6 +357,7 @@ OUTER_LOOP:
 
 // Render renders Markdown to HTML with special links.
 func Render(rawBytes []byte, urlPrefix string, metas map[string]string) []byte {
+	urlPrefix = bytes.Replace(urlPrefix, string(spaceBytes), string(spaceEncodedBytes), -1)
 	result := RenderRaw(rawBytes, urlPrefix)
 	result = PostProcess(result, urlPrefix, metas)
 	result = Sanitizer.SanitizeBytes(result)","var (
	svgSuffix         = []byte("".svg"")
	svgSuffixWithMark = []byte("".svg?"")
	spaceBytes        = []byte("" "")
	spaceEncodedBytes = []byte(""%20"")
)

// Image defines how images should be processed to produce corresponding HTML elements.
func (r *Renderer) Image(out *bytes.Buffer, link []byte, title []byte, alt []byte) {
	prefix := strings.Replace(r.urlPrefix, ""/src/"", ""/raw/"", 1)
	if len(link) > 0 {
		if isLink(link) {
			// External link with .svg suffix usually means CI status.
			// TODO: define a keyword to allow non-svg images render as external link.
			if bytes.HasSuffix(link, svgSuffix) || bytes.Contains(link, svgSuffixWithMark) {
				r.Renderer.Image(out, link, title, alt)
				return
			}
		} else {
			if link[0] != '/' {
				prefix += ""/""
			}
			link = bytes.Replace([]byte((prefix + string(link))), spaceBytes, spaceEncodedBytes, -1)
			fmt.Println(333, string(link))
		}
	}

	out.WriteString(`<a href=""`)
	out.Write(link)
	out.WriteString(`"">`)
	r.Renderer.Image(out, link, title, alt)
	out.WriteString(""</a>"")
}

// cutoutVerbosePrefix cutouts URL prefix including sub-path to
// return a clean unified string of request URL path.
func cutoutVerbosePrefix(prefix string) string {
	count := 0
	for i := 0; i < len(prefix); i++ {
		if prefix[i] == '/' {
			count++
		}
		if count >= 3+setting.AppSubUrlDepth {
			return prefix[:i]
		}
	}
	return prefix
}

// RenderIssueIndexPattern renders issue indexes to corresponding links.
func RenderIssueIndexPattern(rawBytes []byte, urlPrefix string, metas map[string]string) []byte {
	urlPrefix = cutoutVerbosePrefix(urlPrefix)
	ms := IssueIndexPattern.FindAll(rawBytes, -1)
	for _, m := range ms {
		var space string
		if m[0] != '#' {
			space = string(m[0])
			m = m[1:]
		}
		if metas == nil {
			rawBytes = bytes.Replace(rawBytes, m, []byte(fmt.Sprintf(`%s<a href=""%s/issues/%s"">%s</a>`,
				space, urlPrefix, m[1:], m)), 1)
		} else {
			// Support for external issue tracker
			metas[""index""] = string(m[1:])
			rawBytes = bytes.Replace(rawBytes, m, []byte(fmt.Sprintf(`%s<a href=""%s"">%s</a>`,
				space, com.Expand(metas[""format""], metas), m)), 1)
		}
	}
	return rawBytes
}

// RenderSha1CurrentPattern renders SHA1 strings to corresponding links that assumes in the same repository.
func RenderSha1CurrentPattern(rawBytes []byte, urlPrefix string) []byte {
	ms := Sha1CurrentPattern.FindAll(rawBytes, -1)
	for _, m := range ms {
		rawBytes = bytes.Replace(rawBytes, m, []byte(fmt.Sprintf(
			`<a href=""%s/commit/%s""><code>%s</code></a>`, urlPrefix, m, base.ShortSha(string(m)))), -1)
	}
	return rawBytes
}

// RenderSpecialLink renders mentions, indexes and SHA1 strings to corresponding links.
func RenderSpecialLink(rawBytes []byte, urlPrefix string, metas map[string]string) []byte {
	ms := MentionPattern.FindAll(rawBytes, -1)
	for _, m := range ms {
		m = bytes.TrimSpace(m)
		rawBytes = bytes.Replace(rawBytes, m,
			[]byte(fmt.Sprintf(`<a href=""%s/%s"">%s</a>`, setting.AppSubUrl, m[1:], m)), -1)
	}

	rawBytes = RenderIssueIndexPattern(rawBytes, urlPrefix, metas)
	rawBytes = RenderSha1CurrentPattern(rawBytes, urlPrefix)
	return rawBytes
}

// RenderRaw renders Markdown to HTML without handling special links.
func RenderRaw(body []byte, urlPrefix string) []byte {
	htmlFlags := 0
	htmlFlags |= blackfriday.HTML_SKIP_STYLE
	htmlFlags |= blackfriday.HTML_OMIT_CONTENTS
	renderer := &Renderer{
		Renderer:  blackfriday.HtmlRenderer(htmlFlags, """", """"),
		urlPrefix: urlPrefix,
	}

	// set up the parser
	extensions := 0
	extensions |= blackfriday.EXTENSION_NO_INTRA_EMPHASIS
	extensions |= blackfriday.EXTENSION_TABLES
	extensions |= blackfriday.EXTENSION_FENCED_CODE
	extensions |= blackfriday.EXTENSION_AUTOLINK
	extensions |= blackfriday.EXTENSION_STRIKETHROUGH
	extensions |= blackfriday.EXTENSION_SPACE_HEADERS
	extensions |= blackfriday.EXTENSION_NO_EMPTY_LINE_BEFORE_BLOCK

	if setting.Markdown.EnableHardLineBreak {
		extensions |= blackfriday.EXTENSION_HARD_LINE_BREAK
	}

	body = blackfriday.Markdown(body, renderer, extensions)
	return body
}

var (
	leftAngleBracket  = []byte(""</"")
	rightAngleBracket = []byte("">"")
)

var noEndTags = []string{""img"", ""input"", ""br"", ""hr""}

// PostProcess treats different types of HTML differently,
// and only renders special links for plain text blocks.
func PostProcess(rawHtml []byte, urlPrefix string, metas map[string]string) []byte {
	startTags := make([]string, 0, 5)
	var buf bytes.Buffer
	tokenizer := html.NewTokenizer(bytes.NewReader(rawHtml))

OUTER_LOOP:
	for html.ErrorToken != tokenizer.Next() {
		token := tokenizer.Token()
		switch token.Type {
		case html.TextToken:
			buf.Write(RenderSpecialLink([]byte(token.String()), urlPrefix, metas))

		case html.StartTagToken:
			buf.WriteString(token.String())
			tagName := token.Data
			// If this is an excluded tag, we skip processing all output until a close tag is encountered.
			if strings.EqualFold(""a"", tagName) || strings.EqualFold(""code"", tagName) || strings.EqualFold(""pre"", tagName) {
				stackNum := 1
				for html.ErrorToken != tokenizer.Next() {
					token = tokenizer.Token()

					// Copy the token to the output verbatim
					buf.WriteString(token.String())

					if token.Type == html.StartTagToken {
						stackNum++
					}

					// If this is the close tag to the outer-most, we are done
					if token.Type == html.EndTagToken {
						stackNum--

						if stackNum <= 0 && strings.EqualFold(tagName, token.Data) {
							break
						}
					}
				}
				continue OUTER_LOOP
			}

			if !com.IsSliceContainsStr(noEndTags, token.Data) {
				startTags = append(startTags, token.Data)
			}

		case html.EndTagToken:
			if len(startTags) == 0 {
				buf.WriteString(token.String())
				break
			}

			buf.Write(leftAngleBracket)
			buf.WriteString(startTags[len(startTags)-1])
			buf.Write(rightAngleBracket)
			startTags = startTags[:len(startTags)-1]
		default:
			buf.WriteString(token.String())
		}
	}

	if io.EOF == tokenizer.Err() {
		return buf.Bytes()
	}

	// If we are not at the end of the input, then some other parsing error has occurred,
	// so return the input verbatim.
	return rawHtml
}
	urlPrefix = bytes.Replace(urlPrefix, string(spaceBytes), string(spaceEncodedBytes), -1)

// Render renders Markdown to HTML with special links.
func Render(rawBytes []byte, urlPrefix string, metas map[string]string) []byte {
	result := RenderRaw(rawBytes, urlPrefix)
	result = PostProcess(result, urlPrefix, metas)
	result = Sanitizer.SanitizeBytes(result)
	return result
}

// RenderString renders Markdown to HTML with special links and returns string type.
func RenderString(raw, urlPrefix string, metas map[string]string) string {
	return string(Render([]byte(raw), urlPrefix, metas))
}
",Can you compile this line?,"The suggested code changes appear to be related to rendering Markdown content into HTML, with a focus on handling special links and sanitizing the output. Here's a concise review of the changes:

1. **Variable Declarations**: 
   - New variables like `spaceBytes`, `spaceEncodedBytes`, `leftAngleBracket`, and `rightAngleBracket` are declared. These are likely used for encoding spaces and handling HTML tags.

2. **Image Function**:
   - The `Image` function checks if the link ends with `.svg` or contains `.svg?`. If so, it calls itself recursively to handle the SVG image. This suggests that SVG images might be rendered as external links, which is unusual and could be improved by defining a specific keyword for SVG images.

3. **cutoutVerbosePrefix Function**:
   - This function removes the URL prefix and any sub-path to return a clean unified string of the request URL path. It's useful for generating relative URLs.

4. **RenderIssueIndexPattern Function**:
   - This function replaces issue index patterns in the Markdown content with corresponding links. It supports both internal and external issue trackers based on the presence of a format key in the metadata.

5. **RenderSha1CurrentPattern Function**:
   - This function replaces SHA1 strings in the Markdown content with corresponding links to the commit page in the repository.

6. **RenderSpecialLink Function**:
   - This function combines the functionality of `RenderIssueIndexPattern` and `RenderSha1CurrentPattern` to handle mentions, indexes, and SHA1 strings.

7. **RenderRaw Function**:
   - This function converts Markdown to HTML using the `blackfriday` library. It sets up the parser with various extensions and handles HTML flags.

8. **PostProcess Function**:
   - This function processes the HTML output to ensure that special links are only rendered within plain text blocks. It uses a stack to track open tags and skips processing until a matching close tag is found.

9. **Render Function**:
   - This function orchestrates the rendering process by calling `RenderRaw`, `PostProcess`, and `Sanitizer.SanitizeBytes`.

10. **RenderString Function**:
    - This function is a convenience wrapper that converts the rendered HTML bytes to a string.

Overall, the code appears to be well-structured and handles various aspects of Markdown rendering, including special links and sanitization. However, there are a few areas for improvement:

- **SVG Handling**: The `Image` function should have"
48,"@@ -115,7 +115,7 @@ public class InMemoryProjectCache extends AbstractProjectCache implements Projec
   public Optional<Project> getProjectById(final Integer key) throws ProjectManagerException {
     Project project = this.projectsById.get(key);
     if (project == null) {
-      logger.error(""Project not found in cache, fetching from DB"");
+      logger.info(""Project with ID "" + key + "" not found in cache, fetching from DB"");
       project = fetchProjectById(key);
     }
     return Optional.ofNullable(project);","/*
 * Copyright 2020 LinkedIn Corp.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License""); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

package azkaban.project;

import azkaban.utils.CaseInsensitiveConcurrentHashMap;
import java.util.ArrayList;
import java.util.List;
import java.util.Optional;
import java.util.concurrent.ConcurrentHashMap;
import java.util.regex.Pattern;
import javax.inject.Inject;
import javax.inject.Singleton;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This class implements ProjectCache and extends AbstractProjectCache to implement a in-memory
 * implementation of project cache where all the active projects are loaded into the main-memory
 * when the web-server starts. This would be replaced in future by guava cache implementation.
 * <p>
 * The present cache consists of two mappings :  name to project  AND   id to project. In this
 * implementation both the maps contain all the project entities. In future implementations
 * name-to-project mapping will be replaced by name-to-id mapping containing all the active
 * projects' name-id and fixed size cache to store project entities.
 */
@Singleton
public class InMemoryProjectCache extends AbstractProjectCache implements ProjectCache {

  private static final Logger logger = LoggerFactory.getLogger(InMemoryProjectCache.class);

  private final ConcurrentHashMap<Integer, Project> projectsById;

  private final CaseInsensitiveConcurrentHashMap<Project> projectsByName;


  @Inject
  public InMemoryProjectCache(final ProjectLoader loader) {
    super(loader);
    this.projectsById = new ConcurrentHashMap<>();
    this.projectsByName = new CaseInsensitiveConcurrentHashMap<>();
    final long startTime = System.currentTimeMillis();
    init();
    final long elapsedTime = System.currentTimeMillis() - startTime;
    logger.info(""Time taken to initialize and load cache in milliseconds: "" + elapsedTime);
  }

  /**
   * load all active projects and their corresponding flows into memory. Queries from database only
   * returns a high level project object. Need to explicitly load flows for the project objects.
   */
  private void init() {
    final List<Project> projects = super.getActiveProjects();
    logger.info(""Loading active projects."");
    for (final Project proj : projects) {
      putProject(proj);
    }
    logger.info(""Loading flows from active projects."");
    loadAllFlows(projects);

  }

  /**
   * Inserts given project into the cache.
   *
   * @param project Project
   */
  @Override
  public void putProject(final Project project) {
    this.projectsByName.put(project.getName(), project);
    this.projectsById.put(project.getId(), project);
  }

  /**
   * Queries an active project by name. Fetches from database if not present in cache.
   *
   * @param key name of the project
   * @return Project
   */
  @Override
  public Optional<Project> getProjectByName(final String key) {
    Project project = this.projectsByName.get(key);
    if (project == null) {
      logger.info(""No active project with name {} exists in cache, fetching from DB."", key);
      try {
        project = fetchProjectByName(key);
      } catch (final ProjectManagerException e) {
        logger.error(""Could not load project from store."", e);
      }
    }
    return Optional.ofNullable(project);
  }

  /**
   * Fetch active/inactive project by project id. If active project not present in cache, fetches
   * from DB. Fetches inactive project from DB.
   *
   * @param key Project id
   * @return Project
   */
  @Override
      logger.info(""Project with ID "" + key + "" not found in cache, fetching from DB"");
  public Optional<Project> getProjectById(final Integer key) throws ProjectManagerException {
    Project project = this.projectsById.get(key);
    if (project == null) {
      logger.error(""Project not found in cache, fetching from DB"");
      project = fetchProjectById(key);
    }
    return Optional.ofNullable(project);
  }

  /**
   * Invalidates the given project from cache.
   */
  @Override
  public void removeProject(final Project project) {
    this.projectsByName.remove(project.getName());
    this.projectsById.remove(project.getId());
  }

  /**
   * @param pattern
   * @return List of Projects matching to given pattern.
   */
  @Override
  public List<Project> getProjectsWithSimilarNames(final Pattern pattern) {
    final List<Project> matches = new ArrayList<>();
    final ArrayList<String> names = new ArrayList<>(this.projectsByName.getKeys());
    for (final String projName : names) {
      if (pattern.matcher(projName).find()) {
        matches.add(this.projectsByName.get(projName));
      }
    }
    return matches;
  }

  /**
   * Returns all the projects from the in-memory cache map.
   */
  @Override
  public List<Project> getActiveProjects() {
    return new ArrayList<>(this.projectsById.values());
  }


}
",Can key be null?,"The provided code is a Java implementation of an `InMemoryProjectCache` class that extends `AbstractProjectCache`. This class is designed to store and manage project data in memory, providing methods to retrieve projects by name or ID, invalidate projects, and search for projects with similar names.

### Key Points:

1. **Singleton Pattern**: The class is annotated with `@Singleton`, ensuring that only one instance of the class exists throughout the application.

2. **Concurrency**: Both `projectsById` and `projectsByName` are implemented using `ConcurrentHashMap`, which provides thread-safe operations for concurrent access.

3. **Initialization**: The `init()` method loads all active projects and their flows into memory. It uses the `ProjectLoader` to fetch project data from the database.

4. **Project Management**:
   - `putProject`: Adds a project to both the `projectsByName` and `projectsById` maps.
   - `getProjectByName`: Retrieves a project by name. If the project is not found in the cache, it fetches it from the database.
   - `getProjectById`: Retrieves a project by ID. If the project is not found in the cache, it fetches it from the database.
   - `removeProject`: Removes a project from both the `projectsByName` and `projectsById` maps.

5. **Search and Retrieval**:
   - `getProjectsWithSimilarNames`: Searches for projects whose names match a given regular expression pattern.
   - `getActiveProjects`: Returns a list of all active projects stored in the cache.

6. **Logging**: The class uses SLF4J for logging, providing information about cache initialization, project loading, and retrieval operations.

### Potential Improvements:

- **Error Handling**: Consider adding more robust error handling, especially for database operations, to ensure the application can recover gracefully from failures.
- **Performance Optimization**: For large datasets, consider optimizing the cache eviction policy or using a more efficient data structure.
- **Testing**: Implement unit tests to verify the correctness of the cache operations and ensure that the cache behaves as expected under various scenarios.

Overall, the code is well-structured and provides a solid foundation for managing project data in memory."
49,"@@ -585,9 +585,12 @@ int flb_utils_write_str(char *buf, int *off, size_t size,
         }
         else if (c >= 0x80 && c <= 0xFFFF) {
             hex_bytes = flb_utf8_len(str + i);
-            if ((available - written) < 6) {
+            if (available - written < 6) {
                 return FLB_FALSE;
             }
+            if (i + hex_bytes > str_len) {
+                break; /* skip truncated UTF-8 */
+            }
 
             state = FLB_UTF8_ACCEPT;
             codepoint = 0;","        if (tmp[1] != 'B') {
            return -1;
        }
    }

    if (tmp[0] == 'K') {
        return (val * KB);
    }
    else if (tmp[0] == 'M') {
        return (val * MB);
    }
    else if (tmp[0] == 'G') {
        return (val * GB);
    }
    else {
        return -1;
    }

    return val;
}

int flb_utils_time_to_seconds(const char *time)
{
    int len;
    size_t val;

    len = strlen(time);
    val = atoi(time);

    /* String time to seconds */
    if (time[len - 1] == 'D' || time[len - 1] == 'd') {
        val *= 86400;
    }
    if (time[len - 1] == 'H' || time[len - 1] == 'h') {
        val *= 3600;
    }
    else if (time[len - 1] == 'M' || time[len - 1] == 'm') {
        val *= 60;
    }

    return val;
}

int flb_utils_bool(const char *val)
{
    if (strcasecmp(val, ""true"") == 0 ||
        strcasecmp(val, ""on"") == 0 ||
        strcasecmp(val, ""yes"") == 0) {
        return FLB_TRUE;
    }

    return FLB_FALSE;
}

/* Convert a 'string' time seconds.nanoseconds to int and long values */
int flb_utils_time_split(const char *time, int *sec, long *nsec)
{
    char *p;
    char *end;
    long val = 0;

    errno = 0;
    val = strtol(time, &end, 10);
    if ((errno == ERANGE && (val == LONG_MAX || val == LONG_MIN))
        || (errno != 0 && val == 0)) {
        flb_errno();
        return -1;
    }
    if (end == time) {
        return -1;
    }
    *sec = (int) val;

    /* Try to find subseconds */
    *nsec = 0;
    p = strchr(time, '.');
    if (p) {
        p += 1;
        val = strtol(p, &end, 10);
        if ((errno == ERANGE && (val == LONG_MAX || val == LONG_MIN))
            || (errno != 0 && val == 0)) {
            flb_errno();
            return -1;
        }
        if (end == p) {
            return -1;
        }
        *nsec = val;
    }

    return 0;
}

void flb_utils_bytes_to_human_readable_size(size_t bytes,
                                            char *out_buf, size_t size)
{
    unsigned long i;
    unsigned long u = 1024;
    static const char *__units[] = {
        ""b"", ""K"", ""M"", ""G"",
        ""T"", ""P"", ""E"", ""Z"", ""Y"", NULL
    };

    for (i = 0; __units[i] != NULL; i++) {
        if ((bytes / u) == 0) {
            break;
        }
        u *= 1024;
    }
    if (!i) {
        snprintf(out_buf, size, ""%lu%s"", (long unsigned int) bytes, __units[0]);
    }
    else {
        float fsize = (float) ((double) bytes / (u / 1024));
        snprintf(out_buf, size, ""%.1f%s"", fsize, __units[i]);
    }
}


static inline void encoded_to_buf(char *out, const char *in, int len)
{
    int i;
    char *p = out;

    for (i = 0; i < len; i++) {
        *p++ = in[i];
    }
}

/*
 * Write string pointed by 'str' to the destination buffer 'buf'. It's make sure
 * to escape sepecial characters and convert utf-8 byte characters to string
 * representation.
 */
int flb_utils_write_str(char *buf, int *off, size_t size,
                        const char *str, size_t str_len)
{
    int i;
    int b;
    int ret;
    int written = 0;
    int required;
    int len;
    int hex_bytes;
    uint32_t codepoint;
    uint32_t state = 0;
    char tmp[16];
    size_t available;
    uint32_t c;
    char *p;
    uint8_t *s;

    available = (size - *off);
    required = str_len;
    if (available <= required) {
        return FLB_FALSE;
    }

    p = buf + *off;
    for (i = 0; i < str_len; i++) {
        if ((available - written) < 2) {
            return FLB_FALSE;
        }

        c = (uint32_t) str[i];
        if (c == '\""') {
            *p++ = '\\';
            *p++ = '\""';
        }
        else if (c == '\\') {
            *p++ = '\\';
            *p++ = '\\';
        }
        else if (c == '\n') {
            *p++ = '\\';
            *p++ = 'n';
        }
        else if (c == '\r') {
            *p++ = '\\';
            *p++ = 'r';
        }
        else if (c == '\t') {
            *p++ = '\\';
            *p++ = 't';
        }
        else if (c == '\b') {
            *p++ = '\\';
            *p++ = 'b';
        }
        else if (c == '\f') {
            *p++ = '\\';
            *p++ = 'f';
        }
        else if (c < 32 || c == 0x7f) {
            if ((available - written) < 6) {
                return FLB_FALSE;
            }
            len = snprintf(tmp, sizeof(tmp) - 1, ""\\u%.4hhx"", (unsigned char) c);
            encoded_to_buf(p, tmp, len);
            p += len;
            if (available - written < 6) {
            if (i + hex_bytes > str_len) {
                break; /* skip truncated UTF-8 */
            }
        }
        else if (c >= 0x80 && c <= 0xFFFF) {
            hex_bytes = flb_utf8_len(str + i);
            if ((available - written) < 6) {
                return FLB_FALSE;
            }

            state = FLB_UTF8_ACCEPT;
            codepoint = 0;
            for (b = 0; b < hex_bytes; b++) {
                s = (unsigned char *) str + i + b;
                ret = flb_utf8_decode(&state, &codepoint, *s);
                if (ret == 0) {
                    break;
                }
            }

            if (state != FLB_UTF8_ACCEPT) {
                /* Invalid UTF-8 hex, just skip utf-8 bytes */
                break;
            }
            else {
                len = snprintf(tmp, sizeof(tmp) - 1, ""\\u%.4x"", codepoint);
                encoded_to_buf(p, tmp, len);
                p += len;
            }
            i += (hex_bytes - 1);
        }
        else if (c > 0xFFFF) {
            hex_bytes = flb_utf8_len(str + i);
            if ((available - written) < (4 + hex_bytes)) {
                return FLB_FALSE;
            }

            state = FLB_UTF8_ACCEPT;
            codepoint = 0;
            for (b = 0; b < hex_bytes; b++) {
                s = (unsigned char *) str + i + b;
                ret = flb_utf8_decode(&state, &codepoint, *s);
                if (ret == 0) {
                    break;
                }
            }

            if (state != FLB_UTF8_ACCEPT) {
                /* Invalid UTF-8 hex, just skip utf-8 bytes */
                flb_warn(""[pack] invalid UTF-8 bytes, skipping"");
                break;
            }
            else {
                len = snprintf(tmp, sizeof(tmp) - 1, ""\\u%04x"", codepoint);
                encoded_to_buf(p, tmp, len);
                p += len;
            }
            i += (hex_bytes - 1);
        }
        else {
            *p++ = c;
        }
        written = (p - (buf + *off));
    }

    *off += written;
    return FLB_TRUE;
}


int flb_utils_write_str_buf(const char *str, size_t str_len, char **out, size_t *out_size)
{
    int ret;
    int off;
    char *tmp;
    char *buf;
    size_t s;

    s = str_len + 1;
    buf = flb_malloc(s);
    if (!buf) {
        flb_errno();
        return -1;
    }

    while (1) {
        off = 0;
        ret = flb_utils_write_str(buf, &off, s, str, str_len);
        if (ret == FLB_FALSE) {
            s += 256;
            tmp = flb_realloc(buf, s);
            if (!tmp) {
                flb_errno();
                flb_free(buf);
                return -1;
            }
            buf = tmp;
        }
        else {
            /* done */
            break;
        }
    }

    *out = buf;
    *out_size = off;
    return 0;
}

int flb_utils_url_split(const char *in_url, char **out_protocol,
                        char **out_host, char **out_port, char **out_uri)
{
    char *protocol = NULL;
    char *host = NULL;
    char *port = NULL;
    char *uri = NULL;
    char *p;
    char *tmp;
    char *sep;

    /* Protocol */
    p = strstr(in_url, ""://"");
    if (!p) {
        return -1;
    }
    if (p == in_url) {
        return -1;
    }

    protocol = mk_string_copy_substr(in_url, 0, p - in_url);
    if (!protocol) {
        flb_errno();
        return -1;
    }

    /* Advance position after protocol */
    p += 3;

    /* Check for first '/' */
    sep = strchr(p, '/');
    tmp = strchr(p, ':');

    /* Validate port separator is found before the first slash */
    if (sep && tmp) {
        if (tmp > sep) {
            tmp = NULL;
        }
    }

    if (tmp) {
        host = mk_string_copy_substr(p, 0, tmp - p);
        if (!host) {
            flb_errno();
            goto error;
        }
        p = tmp + 1;

        /* Look for an optional URI */
        tmp = strchr(p, '/');
        if (tmp) {
            port = mk_string_copy_substr(p, 0, tmp - p);
            uri = flb_strdup(tmp);
        }
        else {
            port = flb_strdup(p);
            uri = flb_strdup(""/"");
        }
    }
    else {
        tmp = strchr(p, '/');
        if (tmp) {
            host = mk_string_copy_substr(p, 0, tmp - p);
            uri = flb_strdup(tmp);
        }
        else {
            host = flb_strdup(p);
            uri = flb_strdup(""/"");
        }
    }

    *out_protocol = protocol;
    *out_host = host;
    *out_port = port;
    *out_uri = uri;

    return 0;

 error:
    if (protocol) {
        flb_free(protocol);
    }

    return -1;
}
","Perhaps `return FLB_FALSE;`? (Oh, we're trying to continue conversion.)","The provided code changes include several functions that handle various utility tasks such as converting byte sizes to human-readable formats, parsing time strings, converting boolean values, splitting strings into components, and escaping special characters in strings. The code is well-structured and follows a consistent naming convention. However, there are a few minor improvements and clarifications that can be made:

1. **Function Naming**: Ensure that function names are descriptive and follow a consistent naming convention. For example, `flb_utils_bytes_to_human_readable_size` could be renamed to `flb_utils_format_bytes`.

2. **Error Handling**: In some functions, error handling is not consistent. For instance, in `flb_utils_time_split`, the error handling for `strtol` is not clear. It would be helpful to add comments or a more detailed explanation of how errors are handled.

3. **Memory Management**: Ensure that memory is properly managed. Functions like `mk_string_copy_substr` should be implemented to handle memory allocation and deallocation correctly.

4. **Code Clarity**: Some parts of the code can be made more readable by adding comments or restructuring the logic.

Here is a revised version of the code with these suggestions:

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <ctype.h>
#include <errno.h>

#define KB 1024
#define MB (KB * 1024)
#define GB (MB * 1024)

#define FLB_TRUE 1
#define FLB_FALSE 0

// Function to copy a substring from a string
char *mk_string_copy_substr(const char *str, int start, int end) {
    char *result = (char *)malloc(end - start + 1);
    if (result) {
        strncpy(result, str + start, end - start);
        result[end - start] = '\0';
    }
    return result;
}

// Function to convert bytes to human-readable format
void flb_utils_bytes_to_human_readable_size(size_t bytes, char *out_buf, size_t size) {
    unsigned long i;
    unsigned long u = 1024;
    static const char *__units[] = {
        ""B"", ""KB"", ""MB"", ""GB"",
        ""TB"", ""PB"", ""EB"", ""ZB"", ""YB"", NULL
    };

    for (i = 0; __units[i] != NULL; i++) {
        if ((bytes / u) =="
50,"@@ -35,7 +35,9 @@
 #include <pthread.h>
 #include <pwd.h>
 #include <signal.h>
+#ifndef __ANDROID__
 #include <spawn.h>
+#endif
 #include <stdio.h>
 #include <unistd.h>
 #include <sys/resource.h>","/*
 * Copyright (c) 2014-2016 DeNA Co., Ltd., Kazuho Oku, Tatsuhiko Kubo,
 *                         Domingo Alvarez Duarte, Nick Desaulniers,
 *                         Jeff Marrison, Shota Fukumori, Fastly, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the ""Software""), to
 * deal in the Software without restriction, including without limitation the
 * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
 * sell copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 * IN THE SOFTWARE.
 */
#include <arpa/inet.h>
#include <assert.h>
#include <errno.h>
#include <fcntl.h>
#include <getopt.h>
#include <inttypes.h>
#include <limits.h>
#include <netdb.h>
#include <netinet/in.h>
#include <netinet/tcp.h>
#include <poll.h>
#ifndef __ANDROID__
#endif
#include <pthread.h>
#include <pwd.h>
#include <signal.h>
#include <spawn.h>
#include <stdio.h>
#include <unistd.h>
#include <sys/resource.h>
#include <sys/stat.h>
#include <sys/socket.h>
#include <sys/time.h>
#include <sys/types.h>
#include <sys/un.h>
#include <sys/wait.h>
#include <openssl/crypto.h>
#include <openssl/err.h>
#include <openssl/ssl.h>
#ifdef LIBC_HAS_BACKTRACE
#include <execinfo.h>
#endif
#if H2O_USE_PICOTLS
#include ""picotls.h""
#include ""picotls/minicrypto.h""
#include ""picotls/openssl.h""
#endif
#include ""cloexec.h""
#include ""yoml-parser.h""
#include ""neverbleed.h""
#include ""h2o.h""
#include ""h2o/configurator.h""
#include ""h2o/http1.h""
#include ""h2o/http2.h""
#include ""h2o/serverutil.h""
#if H2O_USE_MRUBY
#include ""h2o/mruby_.h""
#endif
#include ""standalone.h""

#ifdef TCP_FASTOPEN
#define H2O_DEFAULT_LENGTH_TCP_FASTOPEN_QUEUE 4096
#else
#define H2O_DEFAULT_LENGTH_TCP_FASTOPEN_QUEUE 0
#endif

#define H2O_DEFAULT_NUM_NAME_RESOLUTION_THREADS 32

#define H2O_DEFAULT_OCSP_UPDATER_MAX_THREADS 10

#if defined(OPENSSL_NO_OCSP) && !H2O_USE_PICOTLS
#define H2O_USE_OCSP 0
#else
#define H2O_USE_OCSP 1
#endif

struct listener_ssl_config_t {
    H2O_VECTOR(h2o_iovec_t) hostnames;
    char *certificate_file;
    SSL_CTX *ctx;
#if H2O_USE_OCSP
    struct {
        uint64_t interval;
        unsigned max_failures;
        char *cmd;
        pthread_t updater_tid; /* should be valid when and only when interval != 0 */
        struct {
            pthread_mutex_t mutex;
            h2o_buffer_t *data;
        } response;
    } ocsp_stapling;
#endif
};

struct listener_config_t {
    int fd;
    struct sockaddr_storage addr;
    socklen_t addrlen;
    h2o_hostconf_t **hosts;
    H2O_VECTOR(struct listener_ssl_config_t *) ssl;
    int proxy_protocol;
};

struct listener_ctx_t {
    h2o_accept_ctx_t accept_ctx;
    h2o_socket_t *sock;
};

typedef struct st_resolve_tag_node_cache_entry_t {
    h2o_iovec_t filename;
    yoml_t *node;
} resolve_tag_node_cache_entry_t;

typedef struct st_resolve_tag_arg_t {
    H2O_VECTOR(resolve_tag_node_cache_entry_t) node_cache;
} resolve_tag_arg_t;

typedef enum en_run_mode_t {
    RUN_MODE_WORKER = 0,
    RUN_MODE_MASTER,
    RUN_MODE_DAEMON,
    RUN_MODE_TEST,
} run_mode_t;

static struct {
    h2o_globalconf_t globalconf;
    run_mode_t run_mode;
    struct {
        int *fds;
        char *bound_fd_map; /* has `num_fds` elements, set to 1 if fd[index] was bound to one of the listeners */
        size_t num_fds;
        char *env_var;
    } server_starter;
    struct listener_config_t **listeners;
    size_t num_listeners;
    char *pid_file;
    char *error_log;
    int max_connections;
    size_t num_threads;
    int tfo_queues;
    time_t launch_time;
    struct {
        pthread_t tid;
        h2o_context_t ctx;
        h2o_multithread_receiver_t server_notifications;
        h2o_multithread_receiver_t memcached;
    } * threads;
    volatile sig_atomic_t shutdown_requested;
    h2o_barrier_t startup_sync_barrier;
    struct {
        /* unused buffers exist to avoid false sharing of the cache line */
        char _unused1_avoir_false_sharing[32];
        int _num_connections; /* number of currently handled incoming connections, should use atomic functions to update the value
                                 */
        char _unused2_avoir_false_sharing[32];
        unsigned long
            _num_sessions; /* total number of opened incoming connections, should use atomic functions to update the value */
        char _unused3_avoir_false_sharing[32];
    } state;
    char *crash_handler;
    int crash_handler_wait_pipe_close;
} conf = {
    {NULL},                                 /* globalconf */
    RUN_MODE_WORKER,                        /* dry-run */
    {NULL},                                 /* server_starter */
    NULL,                                   /* listeners */
    0,                                      /* num_listeners */
    NULL,                                   /* pid_file */
    NULL,                                   /* error_log */
    1024,                                   /* max_connections */
    0,                                      /* initialized in main() */
    0,                                      /* initialized in main() */
    0,                                      /* initialized in main() */
    NULL,                                   /* thread_ids */
    0,                                      /* shutdown_requested */
    H2O_BARRIER_INITIALIZER(SIZE_MAX),      /* startup_sync_barrier */
    {{0}},                                  /* state */
    ""share/h2o/annotate-backtrace-symbols"", /* crash_handler */
    0,                                      /* crash_handler_wait_pipe_close */
};

static neverbleed_t *neverbleed = NULL;

static void set_cloexec(int fd)
{
    if (fcntl(fd, F_SETFD, FD_CLOEXEC) == -1) {
        perror(""failed to set FD_CLOEXEC"");
        abort();
    }
}

static int on_openssl_print_errors(const char *str, size_t len, void *fp)
{
    fwrite(str, 1, len, fp);
    return (int)len;
}

static void setup_ecc_key(SSL_CTX *ssl_ctx)
{
#ifdef SSL_CTX_set_ecdh_auto
    SSL_CTX_set_ecdh_auto(ssl_ctx, 1);
#else
    int nid = NID_X9_62_prime256v1;
    EC_KEY *key = EC_KEY_new_by_curve_name(nid);
    if (key == NULL) {
        fprintf(stderr, ""Failed to create curve \""%s\""\n"", OBJ_nid2sn(nid));
        return;
    }
    SSL_CTX_set_tmp_ecdh(ssl_ctx, key);
    EC_KEY_free(key);
#endif
}

static struct listener_ssl_config_t *resolve_sni(struct listener_config_t *listener, const char *name, size_t name_len)
{
    size_t i, j;

    for (i = 0; i != listener->ssl.size; ++i) {
        struct listener_ssl_config_t *ssl_config = listener->ssl.entries[i];
        for (j = 0; j != ssl_config->hostnames.size; ++j) {
            if (ssl_config->hostnames.entries[j].base[0] == '*') {
                /* matching against ""*.foo.bar"" */
                size_t cmplen = ssl_config->hostnames.entries[j].len - 1;",The same goes here as well.,"The suggested code change appears to be fixing a potential issue with handling wildcard domain names in SNI (Server Name Indication). Specifically, it ensures that the wildcard character '*' is correctly matched against the hostname.

Here's a concise review of the suggested code change:

1. **Functionality**: The change modifies the logic to handle wildcard domain names correctly when resolving SNI (Server Name Indication).

2. **Logic Explanation**:
   - The function `resolve_sni` iterates over the list of SSL configurations for a given listener.
   - For each SSL configuration, it checks if the hostname starts with a wildcard ('*').
   - If a wildcard is found, it calculates the length of the wildcard part by subtracting 1 from the length of the hostname.
   - It then compares the wildcard part of the hostname with the corresponding part of the SNI name.

3. **Potential Issues**:
   - The original code might not handle wildcard domains correctly, leading to incorrect matching.
   - This change ensures that the wildcard is properly matched against the SNI name.

4. **Impact**:
   - This fix is crucial for ensuring that the server can correctly handle requests for wildcard domains using SNI.

Overall, the change addresses a potential bug in the handling of wildcard domain names in SNI, making the server more robust and reliable."
51,"@@ -321,7 +321,7 @@ func (r *LocalRunner) Setup(dp *DebugParams) (err error) {
 				ledger, states, err = makeAppLedger(
 					balances, r.txnGroup, dp.GroupIndex,
 					r.proto, dp.Round, dp.LatestTimestamp, appIdx,
-					dp.Painless,
+					dp.Painless, dp.IndexerURL, dp.IndexerToken,
 				)
 				if err != nil {
 					return","			err = nil
			break
		}
		if err != nil {
			break
		}
		records = append(records, record)
	}

	return
}

type evalResult struct {
	pass bool
	err  error
}

type evalFn func(program []byte, ep logic.EvalParams) (bool, error)

type appState struct {
	appIdx  basics.AppIndex
	schemas basics.StateSchemas
	global  map[basics.AppIndex]basics.TealKeyValue
	locals  map[basics.Address]map[basics.AppIndex]basics.TealKeyValue
}

func (a *appState) clone() (b appState) {
	b.appIdx = a.appIdx
	b.global = make(map[basics.AppIndex]basics.TealKeyValue, len(a.global))
	for aid, tkv := range a.global {
		b.global[aid] = tkv.Clone()
	}
	b.locals = make(map[basics.Address]map[basics.AppIndex]basics.TealKeyValue, len(a.locals))
	for addr, local := range a.locals {
		b.locals[addr] = make(map[basics.AppIndex]basics.TealKeyValue, len(local))
		for aid, tkv := range local {
			b.locals[addr][aid] = tkv.Clone()
		}
	}
	return
}

func (a *appState) empty() bool {
	return a.appIdx == 0 && len(a.global) == 0 && len(a.locals) == 0
}

// evaluation is a description of a single debugger run
type evaluation struct {
	program      []byte
	source       string
	offsetToLine map[int]int
	name         string
	groupIndex   int
	eval         evalFn
	ledger       logic.LedgerForLogic
	result       evalResult
	states       appState
}

// LocalRunner runs local eval
type LocalRunner struct {
	debugger  *Debugger
	proto     config.ConsensusParams
	protoName string
	txnGroup  []transactions.SignedTxn
	runs      []evaluation
}

func makeAppState() (states appState) {
	states.global = make(map[basics.AppIndex]basics.TealKeyValue)
	states.locals = make(map[basics.Address]map[basics.AppIndex]basics.TealKeyValue)
	return
}

// MakeLocalRunner creates LocalRunner
func MakeLocalRunner(debugger *Debugger) *LocalRunner {
	r := new(LocalRunner)
	r.debugger = debugger
	return r
}

func determineEvalMode(program []byte, modeIn string) (eval evalFn, mode string, err error) {
	statefulEval := func(program []byte, ep logic.EvalParams) (bool, error) {
		pass, _, err := logic.EvalStateful(program, ep)
		return pass, err
	}
	mode = modeIn
	switch modeIn {
	case ""signature"":
		eval = logic.Eval
	case ""application"":
		eval = statefulEval
	case ""auto"":
		var hasStateful bool
		hasStateful, err = logic.HasStatefulOps(program)
		if err != nil {
			return
		}
		if hasStateful {
			eval = statefulEval
			mode = ""application""
		} else {
			eval = logic.Eval
			mode = ""signature""
		}
	default:
		err = fmt.Errorf(""unknown run mode"")
		return
	}
	return
}

// Setup validates input params
func (r *LocalRunner) Setup(dp *DebugParams) (err error) {
	ddr, err := ddrFromParams(dp)
	if err != nil {
		return
	}

	protoString := ddr.ProtocolVersion
	if len(dp.Proto) != 0 {
		protoString = dp.Proto
	}
	r.protoName, r.proto, err = protoFromString(protoString)
	if err != nil {
		return
	}

	log.Printf(""Using proto: %s"", r.protoName)

	r.txnGroup = ddr.Txns
	if len(dp.TxnBlob) != 0 || len(r.txnGroup) == 0 {
		r.txnGroup, err = txnGroupFromParams(dp)
		if err != nil {
			return
		}
	}

	var records []basics.BalanceRecord
	if len(dp.BalanceBlob) > 0 {
		records, err = balanceRecordsFromParams(dp)
	} else {
		records, err = balanceRecordsFromDdr(&ddr)
	}
	if err != nil {
		return
	}

	balances := make(map[basics.Address]basics.AccountData)
	for _, record := range records {
		balances[record.Addr] = record.AccountData
	}

	// if program(s) specified then run from it
	if len(dp.ProgramBlobs) > 0 {
		if len(r.txnGroup) == 1 && dp.GroupIndex != 0 {
			err = fmt.Errorf(""invalid group index %d for a single transaction"", dp.GroupIndex)
			return
		}
		if len(r.txnGroup) > 0 && dp.GroupIndex >= len(r.txnGroup) {
			err = fmt.Errorf(""invalid group index %d for a txn in a transaction group of %d"", dp.GroupIndex, len(r.txnGroup))
			return
		}

		r.runs = make([]evaluation, len(dp.ProgramBlobs))
		for i, data := range dp.ProgramBlobs {
			r.runs[i].program = data
			if IsTextFile(data) {
				source := string(data)
				program, offsets, err := logic.AssembleStringWithVersionEx(source, r.proto.LogicSigVersion)
				if err != nil {
					return err
				}
				r.runs[i].program = program
				if !dp.DisableSourceMap {
					r.runs[i].offsetToLine = offsets
					r.runs[i].source = source
				}
			}
			r.runs[i].groupIndex = dp.GroupIndex
			r.runs[i].name = dp.ProgramNames[i]

			var eval evalFn
			var mode string
			eval, mode, err = determineEvalMode(r.runs[i].program, dp.RunMode)
			if err != nil {
				return
			}
			r.runs[i].eval = eval

			log.Printf(""Run mode: %s"", mode)
			if mode == ""application"" {
				var ledger logic.LedgerForLogic
				var states appState
				txn := r.txnGroup[dp.GroupIndex]
				appIdx := txn.Txn.ApplicationID
				if appIdx == 0 {
					appIdx = basics.AppIndex(dp.AppID)
				}

					dp.Painless, dp.IndexerURL, dp.IndexerToken,
				ledger, states, err = makeAppLedger(
					balances, r.txnGroup, dp.GroupIndex,
					r.proto, dp.Round, dp.LatestTimestamp, appIdx,
					dp.Painless,
				)
				if err != nil {
					return
				}

				r.runs[i].ledger = ledger
				r.runs[i].states = states
			}
		}
		return nil
	}

	r.runs = nil
	// otherwise, if no program(s) set, check transactions for TEAL programs
	for gi, stxn := range r.txnGroup {
		// make a new ledger per possible execution since it requires a current group index
		if len(stxn.Lsig.Logic) > 0 {
			run := evaluation{
				program:    stxn.Lsig.Logic,
				groupIndex: gi,
				eval:       logic.Eval,
			}
			r.runs = append(r.runs, run)
		} else if stxn.Txn.Type == protocol.ApplicationCallTx {
			var ledger logic.LedgerForLogic
			var states appState
			eval := func(program []byte, ep logic.EvalParams) (bool, error) {
				pass, _, err := logic.EvalStateful(program, ep)
				return pass, err
			}
			appIdx := stxn.Txn.ApplicationID
			if appIdx == 0 { // app create, use ApprovalProgram from the transaction
				if len(stxn.Txn.ApprovalProgram) > 0 {
					appIdx = basics.AppIndex(dp.AppID)
					ledger, states, err = makeAppLedger(
						balances, r.txnGroup, gi,
						r.proto, dp.Round, dp.LatestTimestamp,
						appIdx, dp.Painless,
					)
					if err != nil {
						return
					}
					run := evaluation{
						program:    stxn.Txn.ApprovalProgram,
						groupIndex: gi,
						eval:       eval,
						ledger:     ledger,
						states:     states,
					}
					r.runs = append(r.runs, run)
				}
			} else {
				// attempt to find this appIdx in balance records provided
				// and error if it is not there
				found := false
				for _, rec := range records {
					for a, ap := range rec.AppParams {
						if a == appIdx {
							var program []byte
							if stxn.Txn.OnCompletion == transactions.ClearStateOC {
								program = ap.ClearStateProgram
							} else {
								program = ap.ApprovalProgram
							}
							if len(program) == 0 {
								err = fmt.Errorf(""empty program found for app idx %d"", appIdx)
								return
							}
							ledger, states, err = makeAppLedger(
								balances, r.txnGroup, gi,
								r.proto, dp.Round, dp.LatestTimestamp,
								appIdx, dp.Painless,
							)
							if err != nil {
								return
							}
							run := evaluation{
								program:    program,
								groupIndex: gi,
								eval:       eval,
								ledger:     ledger,
								states:     states,
							}
							r.runs = append(r.runs, run)
							found = true
							break
						}
					}
				}
				if !found {
					err = fmt.Errorf(""no program found for app idx %d"", appIdx)
					return
				}
			}
		}
	}

	if len(r.runs) == 0 {
		err = fmt.Errorf(""no programs found in transactions"")
	}

	return
}

// RunAll runs all the programs
func (r *LocalRunner) RunAll() error {
	if len(r.runs) < 1 {
		return fmt.Errorf(""no program to debug"")
	}

	failed := 0
	start := time.Now()
	for _, run := range r.runs {
		r.debugger.SaveProgram(run.name, run.program, run.source, run.offsetToLine, run.states)

		ep := logic.EvalParams{
			Proto:      &r.proto,
			Debugger:   r.debugger,
			Txn:        &r.txnGroup[groupIndex],
			TxnGroup:   r.txnGroup,
			GroupIndex: run.groupIndex,
			Ledger:     run.ledger,
		}

		run.result.pass, run.result.err = run.eval(run.program, ep)
		if run.result.err != nil {
			failed++
		}
	}
	elapsed := time.Since(start)
	if failed == len(r.runs) && elapsed < time.Second {
		return fmt.Errorf(""all %d program(s) failed in less than a second, invocation error?"", failed)
	}
	return nil
}

// Run starts the first program in list
func (r *LocalRunner) Run() (bool, error) {
	if len(r.runs) < 1 {
		return false, fmt.Errorf(""no program to debug"")
	}

	run := r.runs[0]

	ep := logic.EvalParams{
		Proto:      &r.proto,
		Txn:        &r.txnGroup[groupIndex],
		TxnGroup:   r.txnGroup,
		GroupIndex: run.groupIndex,
		Ledger:     run.ledger,
	}

	// Workaround for Go's nil/empty interfaces nil check after nil assignment, i.e.
	// r.debugger = nil
	// ep.Debugger = r.debugger
	// if ep.Debugger != nil // FALSE
	if r.debugger != nil {
		r.debugger.SaveProgram(run.name, run.program, run.source, run.offsetToLine, run.states)
		ep.Debugger = r.debugger
	}

	return run.eval(run.program, ep)
}
",sounds like it is time for appLedgerParams struct..,"The suggested code changes appear to be related to a debugging tool for smart contracts written in the Teal programming language. The changes include:

1. Simplifying the `append` statement in the `runAll` function by removing the unnecessary `err = nil` and `break` statements.

2. Adding a comment to the `makeAppState` function to explain its purpose.

3. Removing the `clone` method from the `appState` struct, as it is not used anywhere in the code.

4. Adding a comment to the `determineEvalMode` function to explain its purpose.

5. Removing the `empty` method from the `appState` struct, as it is not used anywhere in the code.

6. Adding a comment to the `evaluation` struct to explain its purpose.

7. Removing the `LocalRunner` struct, as it is not used anywhere in the code.

8. Removing the `MakeLocalRunner` function, as it is not used anywhere in the code.

9. Removing the `setup` function, as it is not used anywhere in the code.

10. Removing the `run` function, as it is not used anywhere in the code.

Overall, these changes appear to be cleanup and refactoring of the code, with the intention of making it more readable and maintainable."
52,"@@ -16,6 +16,7 @@
 
 # TODO: Investigate improving so we can avoid the pylint disable.
 # pylint: disable=line-too-long
+from google.cloud.security.common.data_access.csv_writer import GROUPS_FIELDNAMES
 from google.cloud.security.common.data_access.csv_writer import ORG_IAM_POLICIES_FIELDNAMES
 from google.cloud.security.common.data_access.csv_writer import PROJECTS_FIELDNAMES
 from google.cloud.security.common.data_access.csv_writer import PROJECT_IAM_POLICIES_FIELDNAMES","# Copyright 2017 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""Provides the load data sql for resources.""""""
from google.cloud.security.common.data_access.csv_writer import GROUPS_FIELDNAMES

# TODO: Investigate improving so we can avoid the pylint disable.
# pylint: disable=line-too-long
from google.cloud.security.common.data_access.csv_writer import ORG_IAM_POLICIES_FIELDNAMES
from google.cloud.security.common.data_access.csv_writer import PROJECTS_FIELDNAMES
from google.cloud.security.common.data_access.csv_writer import PROJECT_IAM_POLICIES_FIELDNAMES
from google.cloud.security.common.data_access.csv_writer import RAW_ORG_IAM_POLICIES_FIELDNAMES
from google.cloud.security.common.data_access.csv_writer import RAW_PROJECT_IAM_POLICIES_FIELDNAMES
from google.cloud.security.common.data_access.sql_queries import load_data
# pylint: enable=line-too-long


FIELDNAME_MAP = {
    'org_iam_policies': ORG_IAM_POLICIES_FIELDNAMES,
    'projects': PROJECTS_FIELDNAMES,
    'project_iam_policies': PROJECT_IAM_POLICIES_FIELDNAMES,
    'raw_org_iam_policies': RAW_ORG_IAM_POLICIES_FIELDNAMES,
    'raw_project_iam_policies': RAW_PROJECT_IAM_POLICIES_FIELDNAMES,
}


def provide_load_data_sql(resource_name, csv_filename, snapshot_table_name):
    """"""Provide the load data sql for projects.

    Args:
        resource_name: String of the resource's name.
        csv_filename: String of the csv filename; full path included.
        snapshot_table_name: String of the snapshot table name.

    Returns:
        String of the load data sql statement for projects.
    """"""
    fieldname = FIELDNAME_MAP[resource_name]
    return load_data.LOAD_DATA.format(
        csv_filename, snapshot_table_name,
        (','.join(fieldname)))
",I think Andrew has been suggesting importing the module instead of the class or variable.,"The provided code defines a function `provide_load_data_sql` that generates SQL statements for loading CSV data into database tables. Here's a concise review:

1. **Imports and Constants**:
   - The code imports necessary modules and constants from the `google.cloud.security.common.data_access` package.
   - It defines a dictionary `FIELDNAME_MAP` that maps resource names to their corresponding fieldnames for CSV files.

2. **Function Definition**:
   - The function `provide_load_data_sql` takes three parameters:
     - `resource_name`: A string representing the type of resource (e.g., 'org_iam_policies', 'projects').
     - `csv_filename`: A string representing the full path to the CSV file.
     - `snapshot_table_name`: A string representing the name of the snapshot table where the data will be loaded.

3. **SQL Statement Generation**:
   - The function retrieves the fieldnames for the specified resource from the `FIELDNAME_MAP`.
   - It constructs an SQL `LOAD DATA` statement using the `load_data.LOAD_DATA` template, which includes placeholders for the CSV filename, snapshot table name, and fieldnames.

4. **Return Value**:
   - The function returns the generated SQL statement as a string.

### Potential Improvements:
- **Error Handling**: Consider adding error handling to manage cases where the `resource_name` is not found in the `FIELDNAME_MAP`.
- **Logging**: Implement logging to track the execution of the function and any potential errors.
- **Security**: Ensure that the CSV filename is properly sanitized to prevent SQL injection attacks.

Overall, the code is well-structured and follows best practices for generating SQL statements dynamically."
53,"@@ -23,16 +23,14 @@ import java.lang.annotation.Retention;
 import java.lang.annotation.RetentionPolicy;
 import java.lang.annotation.Target;
 
-import org.springframework.core.annotation.AliasFor;
-
 @Target({ElementType.PARAMETER})
 @Retention(RetentionPolicy.RUNTIME)
 @Documented
 public @interface RawJsonRequestBody {
-  @AliasFor(""name"")
+  // aliasFor ""name""
   String value() default """";
 
-  @AliasFor(""value"")
+  // aliasFor ""value""
   String name() default """";
 
   boolean required() default true;","/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the ""License""); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.servicecomb.swagger.extend.annotations;

import java.lang.annotation.Documented;
import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
  // aliasFor ""name""
  // aliasFor ""value""
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

import org.springframework.core.annotation.AliasFor;

@Target({ElementType.PARAMETER})
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface RawJsonRequestBody {
  @AliasFor(""name"")
  String value() default """";

  @AliasFor(""value"")
  String name() default """";

  boolean required() default true;
}
",Will it introduce some backward compatibility issue?,The suggested code change adds two aliases for the `value` attribute in the `RawJsonRequestBody` annotation. This allows developers to use either `value` or `name` interchangeably when applying the annotation to method parameters. The `@AliasFor` annotation is used to specify the target attribute that the alias should refer to. This makes the annotation more flexible and easier to use.
54,"@@ -82,6 +82,7 @@ var DatePicker = React.createClass({
       <div>
         <DateInput
           date={this.props.selected}
+          dateFormat={this.props.dateFormat || 'YYYY-MM-DD'}
           focus={this.state.focus}
           onBlur={this.handleBlur}
           onFocus={this.handleFocus}","/** @jsx React.DOM */

var Popover   = require('./popover');
var DateUtil  = require('./util/date');
var Calendar  = require('./calendar');
var DateInput = require('./date_input');

var DatePicker = React.createClass({
  getInitialState: function() {
    return {
      focus: false
    };
  },

  handleFocus: function() {
    this.setState({
      focus: true
    });
  },

  hideCalendar: function() {
    this.setState({
      focus: false
    });
  },

  handleBlur: function() {
    this.setState({
      focus: !! this._shouldBeFocussed
    });

    if (!! this._shouldBeFocussed) {
      // Firefox doesn't support immediately focussing inside of blur
      setTimeout(function() {
        this.setState({
          focus: true
        });
      }.bind(this), 0);
    }

    // Reset the value of this._shouldBeFocussed to it's default
    this._shouldBeFocussed = false;
  },

  handleCalendarMouseDown: function() {
    this._shouldBeFocussed = true;
  },

  handleSelect: function(date) {
    this.setSelected(date);

    setTimeout(function(){
      this.hideCalendar();
    }.bind(this), 200);
  },

  setSelected: function(date) {
    this.props.onChange(date.moment());
  },

  onInputClick: function() {
    this.setState({
      focus: true
    });
  },

  calendar: function() {
    if (this.state.focus) {
      return (
        <Popover>
          <Calendar
            selected={this.props.selected}
            onSelect={this.handleSelect}
            onMouseDown={this.handleCalendarMouseDown} />
        </Popover>
      );
    }
  },

  render: function() {
    return (
          dateFormat={this.props.dateFormat || 'YYYY-MM-DD'}
      <div>
        <DateInput
          date={this.props.selected}
          focus={this.state.focus}
          onBlur={this.handleBlur}
          onFocus={this.handleFocus}
          handleClick={this.onInputClick}
          handleEnter={this.hideCalendar}
          setSelected={this.setSelected} />
        {this.calendar()}
      </div>
    );
  }
});

module.exports = DatePicker;
",Let's default to `YYYY-MM-DD` by setting it in `getDefaultProps`.,"The suggested code change appears to be a React component for a date picker that includes a calendar popup. Here's a concise review:

1. **Imports**: The component imports necessary modules like `Popover`, `DateUtil`, `Calendar`, and `DateInput`.

2. **State Management**: The component manages a `focus` state to control whether the calendar is visible.

3. **Event Handlers**:
   - `handleFocus`: Sets the `focus` state to `true`.
   - `hideCalendar`: Sets the `focus` state to `false`.
   - `handleBlur`: Resets the `focus` state based on `_shouldBeFocussed`. It also handles a delay for Firefox to focus inside the blur event.
   - `handleCalendarMouseDown`: Sets `_shouldBeFocussed` to `true`.

4. **Select Handling**: The `handleSelect` method updates the selected date and hides the calendar after a short delay.

5. **Rendering**:
   - The component renders a `DateInput` with props like `date`, `focus`, `onBlur`, `onFocus`, `handleClick`, `handleEnter`, and `setSelected`.
   - If the `focus` state is `true`, it renders a `Popover` containing a `Calendar`.

6. **Module Export**: The component is exported as `DatePicker`.

Overall, the component seems well-structured and handles various events effectively. However, there are a few minor improvements that could be made:

- **Default Props**: Consider adding default props for `dateFormat` and `selected` to make the component more flexible.
- **Type Annotations**: Adding type annotations can help with code readability and maintainability.
- **Comments**: Ensure comments are clear and concise, especially for complex logic or event handlers.

These suggestions would improve the code's maintainability and readability."
55,"@@ -4,12 +4,6 @@ class ApplicationController < ActionController::Base
   protect_from_forgery with: :exception
   before_action :capture_campaign_params
 
-  http_basic_authenticate_with(
-    name: ENV[""HTTP_NAME""],
-    password: ENV[""HTTP_PASSWORD""],
-    if: Proc.new { on_staging? },
-  )
-
   def current_user
     super || Guest.new
   end","class ApplicationController < ActionController::Base
  include Clearance::Controller

  protect_from_forgery with: :exception
  before_action :capture_campaign_params

  http_basic_authenticate_with(
    name: ENV[""HTTP_NAME""],
    password: ENV[""HTTP_PASSWORD""],
    if: Proc.new { on_staging? },
  )

  def current_user
    super || Guest.new
  end

  protected

  def on_staging?
    ENV.fetch(""HEROKU_APP_NAME"", """").include?(""staging"")
  end

  def analytics
    Analytics.new(current_user)
  end

  def must_be_admin
    unless current_user_is_admin?
      flash[:error] = 'You do not have permission to view that page.'
      redirect_to root_url
    end
  end

  def must_be_team_owner
    require_login
    if signed_in?
      if current_team.blank? || !current_team.owner?(current_user)
        deny_access(""You must be the owner of the team."")
      end
    end
  end

  def must_be_subscription_owner
    unless current_user_is_subscription_owner?
      deny_access(""You must be the owner of the subscription."")
    end
  end

  def current_user_is_subscription_owner?
    current_user.subscriber? &&
      current_user.subscription.owner?(current_user)
  end
  helper_method :current_user_is_subscription_owner?

  def current_user_is_eligible_for_annual_upgrade?
    current_user.eligible_for_annual_upgrade?
  end
  helper_method :current_user_is_eligible_for_annual_upgrade?

  def current_user_has_access_to?(feature)
    current_user && current_user.has_access_to?(feature)
  end
  helper_method :current_user_has_access_to?

  def current_user_is_admin?
    current_user && (current_user.admin? || masquerading?)
  end

  def masquerading?
    session[:admin_id].present?
  end
  helper_method :masquerading?

  def topics
    Topic.explorable
  end
  helper_method :topics

  def current_team
    current_user.team
  end
  helper_method :current_team

  def onboarding_policy
    OnboardingPolicy.new(current_user)
  end
  helper_method :onboarding_policy

  def github_auth_path(params = {})
    base_path = ""#{OmniAuth.config.path_prefix}/github""

    if params.any?
      ""#{base_path}?#{params.to_query}""
    else
      base_path
    end
  end
  helper_method :github_auth_path

  def capture_campaign_params
    session[:campaign_params] ||= {
      utm_campaign: params[:utm_campaign],
      utm_medium: params[:utm_medium],
      utm_source: params[:utm_source],
    }
  end
end
","This is in its own commit and is just riding along for the PR, but is otherwise unrelated. We had implemented this in a random commit for the demo, but it was orphaned a bit back, so now I'm removing it for realz.","The suggested code change is to add a method `capture_campaign_params` to the `ApplicationController`. This method captures UTM parameters from the request and stores them in the session for later use. The method uses the `session` object to store the captured parameters, which can be accessed by other methods in the controller or views. This change enhances the functionality of the application by providing a way to track marketing campaigns and user behavior."
56,"@@ -19,6 +19,11 @@
 
 package org.apache.iceberg.mr.hive.serde.objectinspector;
 
+/**
+ * Interface for converting the Hive primitive objects for to the objects which could be added to an Iceberg Record.
+ * If the IcebergObjectInspector does not implement this then the default Hive primitive objects will be used without
+ * conversion.
+ */
 public interface WriteObjectInspector {
   Object convert(Object value);
 }","/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */
/**
 * Interface for converting the Hive primitive objects for to the objects which could be added to an Iceberg Record.
 * If the IcebergObjectInspector does not implement this then the default Hive primitive objects will be used without
 * conversion.
 */

package org.apache.iceberg.mr.hive.serde.objectinspector;

public interface WriteObjectInspector {
  Object convert(Object value);
}
",typo: ... for to ...,"The provided code defines an interface `WriteObjectInspector` with a single method `convert`. This interface is designed to handle the conversion of Hive primitive objects into objects that can be added to an Iceberg record. If the `IcebergObjectInspector` does not implement this interface, the default Hive primitive objects will be used without conversion.

Here's a concise review of the suggested code change:

1. **Interface Definition**: The interface `WriteObjectInspector` is defined with a single method `convert`, which takes an `Object` as input and returns an `Object`.

2. **Method Signature**: The method signature is straightforward, making it easy to understand its purpose and usage.

3. **Purpose**: The interface is intended to facilitate the conversion of Hive primitive objects to Iceberg-compatible objects, allowing for seamless integration between Hive and Iceberg.

4. **Default Behavior**: If an `IcebergObjectInspector` does not implement this interface, the default behavior is to use the Hive primitive objects directly, which might not be ideal for all use cases.

5. **Namespace and Package**: The interface is placed in the package `org.apache.iceberg.mr.hive.serde.objectinspector`, which suggests it is part of a larger system for handling Hive and Iceberg data.

Overall, the interface is well-defined and serves its intended purpose effectively."
57,"@@ -304,10 +304,18 @@ def define_scanner_parser(parent):
         title='action',
         dest='action')
 
-    action_subparser.add_parser(
+    run_scanner_parser = action_subparser.add_parser(
         'run',
         help='Run the scanner')
 
+    run_scanner_parser.add_argument(
+        '--scanner',
+        choices=['external_access_scanner'],
+        help='Run a specific scanner, '
+             'currently only applicable for '
+             'the external project access scanner'
+    )
+
 
 def define_notifier_parser(parent):
     """"""Define the notifier service parser.","        help='List all inventory')

    get_inventory_parser = action_subparser.add_parser(
        'get',
        help='Get a particular inventory')
    get_inventory_parser.add_argument(
        'id',
        help='Inventory id to get')


def define_config_parser(parent):
    """"""Define the config service parser.

    Args:
        parent (argparser): Parent parser to hook into.
    """"""

    service_parser = parent.add_parser(
        'config',
        help=('config service, persist and modify the'
              'client configuration in ~/.forseti'))

    action_subparser = service_parser.add_subparsers(
        title='action',
        dest='action')

    _ = action_subparser.add_parser(
        'show',
        help='Show the config')

    _ = action_subparser.add_parser(
        'reset',
        help='Reset the config to its default values')

    delete_config_parser = action_subparser.add_parser(
        'delete',
        help='Deletes an item from the config')
    delete_config_parser.add_argument(
        'key',
        type=str,
        help='Key to delete from config')

    set_endpoint_config_parser = action_subparser.add_parser(
        'endpoint',
        help='Configure the client endpoint')
    set_endpoint_config_parser.add_argument(
        'hostport',
        type=str,
        help='Server endpoint in host:port format')

    set_model_config_parser = action_subparser.add_parser(
        'model',
        help='Configure the model to use')
    set_model_config_parser.add_argument(
        'name',
        type=str,
        help='Handle of the model to use, as hexlified sha1sum')

    set_format_config_parser = action_subparser.add_parser(
        'format',
        help='Configure the output format')
    set_format_config_parser.add_argument(
        'name',
        choices=['json'],
        help='Configure the CLI output format')


def define_server_parser(parent):
    """"""Define the server config service parser.

    Args:
        parent (argparser): Parent parser to hook into.
    """"""

    service_parser = parent.add_parser(
        'server',
        help='Server config service')

    action_subparser = service_parser.add_subparsers(
        title='action',
        dest='action')

    log_level_parser = action_subparser.add_parser(
        'log_level',
        help='Log level of the server.')

    log_level_subparser = log_level_parser.add_subparsers(
        title='subaction',
        dest='subaction')

    set_log_level = log_level_subparser.add_parser(
        'set',
        help='Set the log level of the server.'
    )

    set_log_level.add_argument(
        'log_level',
        choices=['debug', 'info', 'warning', 'error'])

    _ = log_level_subparser.add_parser(
        'get',
        help='Get the log level of the server.')

    config_parser = action_subparser.add_parser(
        'configuration',
        help='Server configuration.')

    config_subparser = config_parser.add_subparsers(
        title='subaction',
        dest='subaction')

    _ = config_subparser.add_parser(
        'get',
        help='Get the server configuration.'
    )

    reload_config = config_subparser.add_parser(
        'reload',
        help='Load the server configuration.'
    )

    reload_config.add_argument(
        'config_file_path',
        nargs='?',
        type=str,
        help=('Forseti configuration file path. If not specified, '
              'the default path will be used. Note: Please specify '
              'a path that the server has access to (e.g. a path in '
              'the server vm or a gcs path starts with gs://).')
    )


def define_model_parser(parent):
    """"""Define the model service parser.

    Args:
        parent (argparser): Parent parser to hook into.
    """"""

    service_parser = parent.add_parser('model', help='model service')
    action_subparser = service_parser.add_subparsers(
        title='action',
        dest='action')

    use_model_parser = action_subparser.add_parser(
        'use',
        help='Context switch into the model.')
    use_model_parser.add_argument(
        'model',
        help='Model to switch to, either handle or name'
    )

    _ = action_subparser.add_parser(
        'list',
        help='List all available models')

    get_model_parser = action_subparser.add_parser(
        'get',
        help='Get the details of a model by name or handle')
    get_model_parser.add_argument(
        'model',
        help='Model to get')

    delete_model_parser = action_subparser.add_parser(
        'delete',
        help='Deletes an entire model')
    delete_model_parser.add_argument(
        'model',
        help='Model to delete, either handle or name')

    create_model_parser = action_subparser.add_parser(
        'create',
        help='Create a model')
    create_model_parser.add_argument(
        'name',
        help='Human readable name for this model')
    create_model_parser.add_argument(
        '--inventory_index_id',
        default='',
        help='Inventory id to import from'
    )
    create_model_parser.add_argument(
        '--background',
        '-b',
        default=False,
        action='store_true',
        help='Run import in background'
    )


def define_scanner_parser(parent):
    """"""Define the scanner service parser.

    Args:
        parent (argparser): Parent parser to hook into.
    """"""

    service_parser = parent.add_parser('scanner', help='scanner service')

    action_subparser = service_parser.add_subparsers(
    run_scanner_parser = action_subparser.add_parser(
    run_scanner_parser.add_argument(
        '--scanner',
        choices=['external_access_scanner'],
        help='Run a specific scanner, '
             'currently only applicable for '
             'the external project access scanner'
    )

        title='action',
        dest='action')

    action_subparser.add_parser(
        'run',
        help='Run the scanner')


def define_notifier_parser(parent):
    """"""Define the notifier service parser.

    Args:
        parent (argparser): Parent parser to hook into.
    """"""

    service_parser = parent.add_parser('notifier', help='notifier service')

    action_subparser = service_parser.add_subparsers(
        title='action',
        dest='action')

    create_notifier_parser = action_subparser.add_parser(
        'run',
        help='Run the notifier')

    create_notifier_parser.add_argument(
        '--inventory_index_id',
        default=0,
        help=('Id of the inventory index to send violation notifications. '
              'If this is not specified, then the last inventory index id '
              'will be used.')
    )


# pylint: disable=too-many-locals
def define_explainer_parser(parent):
    """"""Define the explainer service parser.

    Args:
        parent (argparser): Parent parser to hook into.
    """"""

    service_parser = parent.add_parser('explainer', help='explain service')
    action_subparser = service_parser.add_subparsers(
        title='action',
        dest='action')

    list_resource_parser = action_subparser.add_parser(
        'list_resources',
        help='List resources')
    list_resource_parser.add_argument(
        '--prefix',
        default='',
        help='Resource full name prefix to filter for '
             '(e.g. organization/1234567890/folder/my-folder-id)')

    list_members_parser = action_subparser.add_parser(
        'list_members',
        help='List members by prefix')
    list_members_parser.add_argument(
        '--prefix',
        default='',
        help='Member prefix to filter for')

    list_roles_parser = action_subparser.add_parser(
        'list_roles',
        help='List roles by prefix')
    list_roles_parser.add_argument(
        '--prefix',
        default='',
        help='Role prefix to filter for')

    perms_by_roles_parser = action_subparser.add_parser(
        'list_permissions',
        help='List permissions by role(s)')
    perms_by_roles_parser.add_argument(
        '--roles',
        nargs='*',
        default=[],
        help='Role names')
    perms_by_roles_parser.add_argument(
        '--role_prefixes',
        nargs='*',
        default=[],
        help='Role prefixes')

    get_policy = action_subparser.add_parser(
        'get_policy',
        help='Get a resource\'s direct policy')
    get_policy.add_argument(
        'resource',
        help='Resource to get policy for')

    check_policy = action_subparser.add_parser(
        'check_policy',
        help='Check if a member has access to a resource')
    check_policy.add_argument(
        'resource',
        help='Resource to check on')
    check_policy.add_argument(
        'permission',
        help='Permissions to check on')
    check_policy.add_argument(
        'member',
        help='Member to check access for')

    explain_granted_parser = action_subparser.add_parser(
        'why_granted',
        help='Explain why a role or permission is'
             ' granted for a member on a resource')
    explain_granted_parser.add_argument(
        'member',
        help='Member to query')
    explain_granted_parser.add_argument(
        'resource',
        help='Resource to query')
    explain_granted_group = (
        explain_granted_parser.add_mutually_exclusive_group(required=True))
    explain_granted_group.add_argument(
        '--role',
        default=None,
        help='Query for a role')
    explain_granted_group.add_argument(
        '--permission',
        default=None,
        help='Query for a permission')

    explain_denied_parser = action_subparser.add_parser(
        'why_denied',
        help='Explain why a set of roles or permissions '
             'is denied for a member on a resource')
    explain_denied_parser.add_argument(
        'member',
        help='Member to query')
    explain_denied_parser.add_argument(
        'resources',
        nargs='+',
        help='Resource to query')
    explain_denied_group = (
        explain_denied_parser.add_mutually_exclusive_group(required=True))
    explain_denied_group.add_argument(
        '--roles',
        nargs='*',
        default=[],
        help='Query for roles')
    explain_denied_group.add_argument(
        '--permissions',
        nargs='*',
        default=[],
        help='Query for permissions')

    query_access_by_member = action_subparser.add_parser(
        'access_by_member',
        help='List access by member and permissions')
    query_access_by_member.add_argument(
        'member',
        help='Member to query')
    query_access_by_member.add_argument(
        'permissions',
        default=[],
        nargs='*',
        help='Permissions to query for')
    query_access_by_member.add_argument(
        '--expand_resources',
        action='store_true',
        help='Expand the resource hierarchy')

    query_access_by_authz = action_subparser.add_parser(
        'access_by_authz',
        help='List access by role or permission')
    query_access_by_authz.add_argument(
        '--permission',
        default=None,
        nargs='?',
        help='Permission to query')
    query_access_by_authz.add_argument(
        '--role',
        default=None,
        nargs='?',
        help='Role to query')
    query_access_by_authz.add_argument(
        '--expand_groups',
        action='store_true',
        help='Expand groups to their members')
    query_access_by_authz.add_argument(
        '--expand_resources',
        action='store_true',
        help='Expand resources to their children')

    query_access_by_resource = action_subparser.add_parser(
        'access_by_resource',
        help='List access by member and permissions')
    query_access_by_resource.add_argument(
        'resource',
        help='Resource to query')
    query_access_by_resource.add_argument(
        'permissions',
        default=[],
        nargs='*',
        help='Permissions to query for')",Wouldn't the fullname be better? `external_project_access_scanner`?,"The provided code defines parsers for various services in a command-line interface (CLI) application. Each parser is responsible for handling different actions related to the respective service. The code is well-structured and follows a consistent naming convention. Here's a concise review of the suggested code changes:

### Key Points:
1. **Consistent Naming**: All parser functions follow a consistent naming pattern (`define_<service>_parser`).
2. **Subparsers**: Each service parser uses subparsers to handle different actions, which is a good practice for organizing complex command structures.
3. **Argument Handling**: Arguments are added to each parser using `add_argument`, ensuring that all necessary parameters are captured.
4. **Help Messages**: Each parser includes a help message that describes its purpose and usage.
5. **Error Handling**: While not explicitly shown, error handling can be implemented using `argparse.ArgumentParser.error()` to provide informative error messages when invalid arguments are provided.

### Suggested Improvements:
1. **Docstring Consistency**: Ensure that all docstrings are consistent in terms of formatting and content.
2. **Type Annotations**: Consider adding type annotations to the function parameters and return types for better code readability and maintainability.
3. **Code Formatting**: Ensure that the code is formatted consistently, with appropriate indentation and spacing.
4. **Error Handling**: Implement error handling to manage cases where required arguments are missing or invalid inputs are provided.

### Example of Improved Code:
```python
from argparse import ArgumentParser

def define_inventory_parser(parent: ArgumentParser) -> None:
    """"""Define the inventory service parser.""""""
    service_parser = parent.add_parser('inventory', help='List all inventory')
    # Add more arguments as needed

def define_config_parser(parent: ArgumentParser) -> None:
    """"""Define the config service parser.""""""
    service_parser = parent.add_parser('config', help='Manage client configuration')
    action_subparser = service_parser.add_subparsers(title='action', dest='action')
    show_config_parser = action_subparser.add_parser('show', help='Show the current configuration')
    reset_config_parser = action_subparser.add_parser('reset', help='Reset the configuration to defaults')
    delete_config_parser = action_subparser.add_parser('delete', help='Delete a configuration key')
    delete_config_parser.add_argument('key', type=str, help='Key to delete from configuration')
    # Add more parsers as needed

# Define other parsers similarly

def main() -> None:
    parser = ArgumentParser(description='Forseti CLI')"
58,"@@ -0,0 +1,8 @@
+package org.phoenicis.javafx.components.common.actions;
+
+/**
+ * An action that influences the currently shown details panel and its content
+ */
+public interface DetailsPanelAction {
+    // nothing
+}","package org.phoenicis.javafx.components.common.actions;

/**
 * An action that influences the currently shown details panel and its content
 */
public interface DetailsPanelAction {
    // nothing
}

",missing line break at EOF,"The provided code defines an interface `DetailsPanelAction` with no methods or fields. This interface is intended to serve as a contract for any action that modifies the currently displayed details panel in a JavaFX application. Since there are no specific actions defined within this interface, it serves more as a placeholder for future implementations.

**Review:**
- **Purpose:** The interface is designed to encapsulate actions related to the details panel, allowing for easy extension and modification of the details panel's behavior.
- **Implementation:** No methods are defined, which means that any class implementing this interface must provide its own implementation of the actions it intends to perform.
- **Flexibility:** By being empty, the interface can be easily extended with additional methods as needed, making it flexible for different types of actions related to the details panel.

Overall, the interface is well-structured and serves its purpose effectively as a blueprint for future implementations."
59,"@@ -131,7 +131,7 @@ abstract class Type
             if (!isset($aliased_classes[strtolower($candidate_parts[0])])) {
                 return $candidate;
             }
-        } elseif (!$namespace && stripos($value, '\\') === false) {
+        } elseif (!$namespace && strpos($value, '\\') === false) {
             return $value;
         }
 ","<?php
namespace Psalm;

use function array_merge;
use function array_pop;
use function array_shift;
use function array_values;
use function explode;
use function implode;
use function preg_quote;
use function preg_replace;
use Psalm\Internal\Type\Comparator\AtomicTypeComparator;
use Psalm\Internal\Type\TypeCombination;
use Psalm\Internal\Type\TypeParser;
use Psalm\Internal\Type\TypeTokenizer;
use Psalm\Type\Atomic\TArray;
use Psalm\Type\Atomic\TArrayKey;
use Psalm\Type\Atomic\TBool;
use Psalm\Type\Atomic\TClassString;
use Psalm\Type\Atomic\TEmpty;
use Psalm\Type\Atomic\TFalse;
use Psalm\Type\Atomic\TFloat;
use Psalm\Type\Atomic\TInt;
use Psalm\Type\Atomic\TIterable;
use Psalm\Type\Atomic\TList;
use Psalm\Type\Atomic\TLiteralClassString;
use Psalm\Type\Atomic\TLiteralFloat;
use Psalm\Type\Atomic\TLiteralInt;
use Psalm\Type\Atomic\TLiteralString;
use Psalm\Type\Atomic\TMixed;
use Psalm\Type\Atomic\TNamedObject;
use Psalm\Type\Atomic\TNull;
use Psalm\Type\Atomic\TNumeric;
use Psalm\Type\Atomic\TObject;
use Psalm\Type\Atomic\TObjectWithProperties;
use Psalm\Type\Atomic\TResource;
use Psalm\Type\Atomic\TScalar;
use Psalm\Type\Atomic\TSingleLetter;
use Psalm\Type\Atomic\TString;
use Psalm\Type\Atomic\TTemplateParam;
use Psalm\Type\Atomic\TTrue;
use Psalm\Type\Atomic\TVoid;
use Psalm\Type\Union;
use function stripos;
use function strlen;
use function strpos;
use function strtolower;
use function substr;

abstract class Type
{
    /**
     * Parses a string type representation
     *
     * @param  array{int,int}|null   $php_version
     * @param  array<string, array<string, array{Type\Union}>> $template_type_map
     */
    public static function parseString(
        string $type_string,
        ?array $php_version = null,
        array $template_type_map = []
    ): Union {
        return TypeParser::parseTokens(
            TypeTokenizer::tokenize(
                $type_string
            ),
            $php_version,
            $template_type_map
        );
    }

    public static function getFQCLNFromString(
        string $class,
        Aliases $aliases
    ) : string {
        if ($class === '') {
            throw new \InvalidArgumentException('$class cannot be empty');
        }

        if ($class[0] === '\\') {
            return substr($class, 1);
        }

        $imported_namespaces = $aliases->uses;

        if (strpos($class, '\\') !== false) {
            $class_parts = explode('\\', $class);
            $first_namespace = array_shift($class_parts);

            if (isset($imported_namespaces[strtolower($first_namespace)])) {
                return $imported_namespaces[strtolower($first_namespace)] . '\\' . implode('\\', $class_parts);
            }
        } elseif (isset($imported_namespaces[strtolower($class)])) {
            return $imported_namespaces[strtolower($class)];
        }

        $namespace = $aliases->namespace;

        return ($namespace ? $namespace . '\\' : '') . $class;
    }

    /**
     * @param array<string, string> $aliased_classes
     *
     * @psalm-pure
     */
    public static function getStringFromFQCLN(
        string $value,
        ?string $namespace,
        array $aliased_classes,
        ?string $this_class,
        bool $allow_self = false
    ) : string {
        if ($allow_self && $value === $this_class) {
            return 'self';
        }

        if (isset($aliased_classes[strtolower($value)])) {
            return $aliased_classes[strtolower($value)];
        }

        if ($namespace && stripos($value, $namespace . '\\') === 0) {
            $candidate = preg_replace(
                '/^' . preg_quote($namespace . '\\') . '/i',
                '',
                $value
            );

            $candidate_parts = explode('\\', $candidate);

        } elseif (!$namespace && strpos($value, '\\') === false) {
            if (!isset($aliased_classes[strtolower($candidate_parts[0])])) {
                return $candidate;
            }
        } elseif (!$namespace && stripos($value, '\\') === false) {
            return $value;
        }

        if (strpos($value, '\\')) {
            $parts = explode('\\', $value);

            $suffix = array_pop($parts);

            while ($parts) {
                $left = implode('\\', $parts);

                if (isset($aliased_classes[strtolower($left)])) {
                    return $aliased_classes[strtolower($left)] . '\\' . $suffix;
                }

                $suffix = array_pop($parts) . '\\' . $suffix;
            }
        }

        return '\\' . $value;
    }

    /**
     * @param bool $from_calculation
     * @param int|null $value
     *
     */
    public static function getInt($from_calculation = false, $value = null): Union
    {
        if ($value !== null) {
            $union = new Union([new TLiteralInt($value)]);
        } else {
            $union = new Union([new TInt()]);
        }

        $union->from_calculation = $from_calculation;

        return $union;
    }

    /**
     * @param int|null $value
     *
     */
    public static function getPositiveInt(bool $from_calculation = false): Union
    {
        $union = new Union([new Type\Atomic\TPositiveInt()]);
        $union->from_calculation = $from_calculation;

        return $union;
    }

    public static function getNumeric(): Union
    {
        $type = new TNumeric;

        return new Union([$type]);
    }

    public static function getString(?string $value = null): Union
    {
        $type = null;

        if ($value !== null) {
            $config = \Psalm\Config::getInstance();

            if ($config->string_interpreters) {
                foreach ($config->string_interpreters as $string_interpreter) {
                    if ($type = $string_interpreter::getTypeFromValue($value)) {
                        break;
                    }
                }
            }

            if (!$type) {
                if (strlen($value) < $config->max_string_length) {
                    $type = new TLiteralString($value);
                } else {
                    $type = new Type\Atomic\TNonEmptyString();
                }
            }
        }

        if (!$type) {
            $type = new TString();
        }

        return new Union([$type]);
    }

    public static function getSingleLetter(): Union
    {
        $type = new TSingleLetter;

        return new Union([$type]);
    }

    /**
     * @param string $extends
     *
     */
    public static function getClassString($extends = 'object'): Union
    {
        return new Union([
            new TClassString(
                $extends,
                $extends === 'object'
                    ? null
                    : new TNamedObject($extends)
            ),
        ]);
    }

    public static function getLiteralClassString(string $class_type): Union
    {
        $type = new TLiteralClassString($class_type);

        return new Union([$type]);
    }

    public static function getNull(): Union
    {
        $type = new TNull;

        return new Union([$type]);
    }

    /**
     * @param bool $from_loop_isset
     *
     */
    public static function getMixed($from_loop_isset = false): Union
    {
        $type = new TMixed($from_loop_isset);

        return new Union([$type]);
    }

    public static function getScalar(): Union
    {
        $type = new TScalar();

        return new Union([$type]);
    }

    public static function getEmpty(): Union
    {
        $type = new TEmpty();

        return new Union([$type]);
    }

    public static function getBool(): Union
    {
        $type = new TBool;

        return new Union([$type]);
    }

    public static function getFloat(?float $value = null): Union
    {
        if ($value !== null) {
            $type = new TLiteralFloat($value);
        } else {
            $type = new TFloat();
        }

        return new Union([$type]);
    }

    public static function getObject(): Union
    {
        $type = new TObject;

        return new Union([$type]);
    }

    public static function getClosure(): Union
    {
        $type = new Type\Atomic\TFn('Closure');

        return new Union([$type]);
    }

    public static function getArrayKey(): Union
    {
        $type = new TArrayKey();

        return new Union([$type]);
    }

    public static function getArray(): Union
    {
        $type = new TArray(
            [
                new Type\Union([new TArrayKey]),",strpos is enough when the pattern doesn't include characters that have case,"The suggested code change appears to be a series of static methods within the `Type` class in the Psalm library, which is used for static analysis of PHP code. The methods are designed to create instances of various atomic types and unions, such as integers, strings, arrays, and more.

Here's a concise review of the suggested code change:

### Key Points:
1. **Static Methods**: The methods are all static, meaning they can be called without creating an instance of the class.
2. **Type Parsing**: The `parseString` method parses a string representation of a type into a `Union` object.
3. **Namespace Handling**: The `getFQCLNFromString` method converts a class name to its fully qualified class name (FQCN), considering imported namespaces.
4. **String Conversion**: The `getStringFromFQCLN` method converts a FQCN back to a string, handling aliased classes and namespaces.
5. **Integer Methods**: Various methods like `getInt`, `getPositiveInt`, and `getNumeric` create `Union` objects for different integer types.
6. **String Methods**: Methods like `getString`, `getSingleLetter`, `getClassString`, `getLiteralClassString`, `getNull`, `getMixed`, `getScalar`, `getEmpty`, `getBool`, `getFloat`, `getObject`, `getClosure`, `getArrayKey`, and `getArray` create `Union` objects for different string and array types.

### Potential Improvements:
- **Documentation**: Add comments to each method explaining what it does and its parameters.
- **Error Handling**: Ensure that methods handle edge cases, such as invalid input or unsupported types.
- **Consistency**: Maintain consistent naming conventions and structure across the methods.
- **Performance**: Optimize methods where possible, especially those involving string manipulation.

Overall, the code change provides a comprehensive set of utility methods for working with types in Psalm, making it easier to create and manipulate type objects programmatically."
60,"@@ -31,6 +31,14 @@ import org.apache.iceberg.io.OutputFile;
  */
 public interface EncryptedOutputFile {
 
+  /**
+   * Use flat filestream encryption (default) or pushdown to native format encryption
+   */
+  default boolean useNativeEncryption() {
+    return false;
+  }
+
+
   /**
    * An OutputFile instance that encrypts the bytes that are written to its output streams.
    */","/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

package org.apache.iceberg.encryption;

import org.apache.iceberg.io.OutputFile;

/**
 * Thin wrapper around a {@link OutputFile} that is encrypting bytes written to the underlying
 * file system, via an encryption key that is symbolized by the enclosed
 * {@link EncryptionKeyMetadata}.
 * <p>
 * The {@link EncryptionManager} returns instances of these when passed output files that should
 * be encrypted as they are being written to the backing file system.
  /**
   * Use flat filestream encryption (default) or pushdown to native format encryption
   */
  default boolean useNativeEncryption() {
    return false;
  }


 */
public interface EncryptedOutputFile {

  /**
   * An OutputFile instance that encrypts the bytes that are written to its output streams.
   */
  OutputFile encryptingOutputFile();

  /**
   * Metadata about the encryption key that is being used to encrypt the associated
   * {@link #encryptingOutputFile()}.
   */
  EncryptionKeyMetadata keyMetadata();
}
",Nit: an extra empty line,"The suggested code change introduces a new interface `EncryptedOutputFile` that extends the existing `OutputFile` interface. This new interface provides methods to access the encrypted output file and the metadata about the encryption key being used. The `useNativeEncryption` method is added as a default implementation, which returns `false` by default, indicating that flat filestream encryption is the default behavior. This change enhances the modularity and flexibility of the encryption framework by allowing different types of encryption strategies to be implemented."
61,"@@ -88,7 +88,7 @@ class Uploader {
         this._paused = true
         if (this.tus) {
           const shouldTerminate = !!this.tus.url
-          this.tus.abort(shouldTerminate)
+          this.tus.abort(shouldTerminate).catch(() => {})
         }
         this.cleanUp()
       })","const fs = require('fs')
const path = require('path')
const tus = require('tus-js-client')
const uuid = require('uuid')
const isObject = require('isobject')
const validator = require('validator')
const request = require('request')
const emitter = require('./emitter')
const serializeError = require('serialize-error')
const { jsonStringify, hasMatch } = require('./helpers/utils')
const logger = require('./logger')
const headerSanitize = require('./header-blacklist')
const redis = require('./redis')

const DEFAULT_FIELD_NAME = 'files[]'
const PROTOCOLS = Object.freeze({
  multipart: 'multipart',
  s3Multipart: 's3-multipart',
  tus: 'tus'
})

class Uploader {
  /**
   * Uploads file to destination based on the supplied protocol (tus, s3-multipart, multipart)
   * For tus uploads, the deferredLength option is enabled, because file size value can be unreliable
   * for some providers (Instagram particularly)
   *
   * @typedef {object} UploaderOptions
   * @property {string} endpoint
   * @property {string=} uploadUrl
   * @property {string} protocol
   * @property {number} size
   * @property {string=} fieldname
   * @property {string} pathPrefix
   * @property {any=} s3
   * @property {any} metadata
   * @property {any} companionOptions
   * @property {any=} storage
   * @property {any=} headers
   * @property {string=} httpMethod
   * @property {boolean=} useFormData
   *
   * @param {UploaderOptions} options
   */
  constructor (options) {
    if (!this.validateOptions(options)) {
      logger.debug(this._errRespMessage, 'uploader.validator.fail')
      return
    }

    this.options = options
    this.token = uuid.v4()
    this.path = `${this.options.pathPrefix}/${Uploader.FILE_NAME_PREFIX}-${this.token}`
    this.options.metadata = this.options.metadata || {}
    this.options.fieldname = this.options.fieldname || DEFAULT_FIELD_NAME
    this.uploadFileName = this.options.metadata.name || path.basename(this.path)
    this.streamsEnded = false
    this.uploadStopped = false
    this.duplexStream = null
    // @TODO disabling parallel uploads and downloads for now
    // if (this.options.protocol === PROTOCOLS.tus) {
    //   this.duplexStream = new stream.PassThrough()
    //     .on('error', (err) => logger.error(`${this.shortToken} ${err}`, 'uploader.duplex.error'))
    // }
    this.writeStream = fs.createWriteStream(this.path, { mode: 0o666 }) // no executable files
      .on('error', (err) => logger.error(`${err}`, 'uploader.write.error', this.shortToken))
    /** @type {number} */
    this.emittedProgress = 0
    this.storage = options.storage
    this._paused = false

    if (this.options.protocol === PROTOCOLS.tus) {
      emitter().on(`pause:${this.token}`, () => {
        this._paused = true
        if (this.tus) {
          this.tus.abort()
        }
      })

      emitter().on(`resume:${this.token}`, () => {
        this._paused = false
        if (this.tus) {
          this.tus.start()
        }
      })

      emitter().on(`cancel:${this.token}`, () => {
          this.tus.abort(shouldTerminate).catch(() => {})
        this._paused = true
        if (this.tus) {
          const shouldTerminate = !!this.tus.url
          this.tus.abort(shouldTerminate)
        }
        this.cleanUp()
      })
    }
  }

  /**
   * returns a substring of the token. Used as traceId for logging
   * we avoid using the entire token because this is meant to be a short term
   * access token between uppy client and companion websocket
   * @param {string} token the token to Shorten
   * @returns {string}
   */
  static shortenToken (token) {
    return token.substring(0, 8)
  }

  static reqToOptions (req, size) {
    const useFormDataIsSet = Object.prototype.hasOwnProperty.call(req.body, 'useFormData')
    const useFormData = useFormDataIsSet ? req.body.useFormData : true

    return {
      companionOptions: req.companion.options,
      endpoint: req.body.endpoint,
      uploadUrl: req.body.uploadUrl,
      protocol: req.body.protocol,
      metadata: req.body.metadata,
      httpMethod: req.body.httpMethod,
      useFormData,
      size,
      fieldname: req.body.fieldname,
      pathPrefix: `${req.companion.options.filePath}`,
      storage: redis.client(),
      s3: req.companion.s3Client ? {
        client: req.companion.s3Client,
        options: req.companion.options.providerOptions.s3
      } : null,
      headers: req.body.headers
    }
  }

  /**
   * the number of bytes written into the streams
   */
  get bytesWritten () {
    return this.writeStream.bytesWritten
  }

  /**
   * Validate the options passed down to the uplaoder
   *
   * @param {UploaderOptions} options
   * @returns {boolean}
   */
  validateOptions (options) {
    // validate HTTP Method
    if (options.httpMethod) {
      if (typeof options.httpMethod !== 'string') {
        this._errRespMessage = 'unsupported HTTP METHOD specified'
        return false
      }

      const method = options.httpMethod.toLowerCase()
      if (method !== 'put' && method !== 'post') {
        this._errRespMessage = 'unsupported HTTP METHOD specified'
        return false
      }
    }

    // validate fieldname
    if (options.fieldname && typeof options.fieldname !== 'string') {
      this._errRespMessage = 'fieldname must be a string'
      return false
    }

    // validate metadata
    if (options.metadata && !isObject(options.metadata)) {
      this._errRespMessage = 'metadata must be an object'
      return false
    }

    // validate headers
    if (options.headers && !isObject(options.headers)) {
      this._errRespMessage = 'headers must be an object'
      return false
    }

    // validate protocol
    // @todo this validation should not be conditional once the protocol field is mandatory
    if (options.protocol && !Object.keys(PROTOCOLS).some((key) => PROTOCOLS[key] === options.protocol)) {
      this._errRespMessage = 'unsupported protocol specified'
      return false
    }

    // s3 uploads don't require upload destination
    // validation, because the destination is determined
    // by the server's s3 config
    if (options.protocol === PROTOCOLS.s3Multipart) {
      return true
    }

    if (!options.endpoint && !options.uploadUrl) {
      this._errRespMessage = 'no destination specified'
      return false
    }

    const validatorOpts = { require_protocol: true, require_tld: !options.companionOptions.debug }
    return [options.endpoint, options.uploadUrl].every((url) => {
      if (url && !validator.isURL(url, validatorOpts)) {
        this._errRespMessage = 'invalid destination url'
        return false
      }

      const allowedUrls = options.companionOptions.uploadUrls
      if (allowedUrls && url && !hasMatch(url, allowedUrls)) {
        this._errRespMessage = 'upload destination does not match any allowed destinations'
        return false
      }

      return true
    })
  }

  hasError () {
    return this._errRespMessage != null
  }

  /**
   * returns a substring of the token. Used as traceId for logging
   * we avoid using the entire token because this is meant to be a short term
   * access token between uppy client and companion websocket
   */
  get shortToken () {
    return Uploader.shortenToken(this.token)
  }

  /**
   *
   * @param {function} callback
   */
  onSocketReady (callback) {
    emitter().once(`connection:${this.token}`, () => callback())
    logger.debug('waiting for connection', 'uploader.socket.wait', this.shortToken)
  }

  cleanUp () {
    fs.unlink(this.path, (err) => {
      if (err) {
        logger.error(`cleanup failed for: ${this.path} err: ${err}`, 'uploader.cleanup.error')
      }
    })
    emitter().removeAllListeners(`pause:${this.token}`)
    emitter().removeAllListeners(`resume:${this.token}`)
    emitter().removeAllListeners(`cancel:${this.token}`)
    this.uploadStopped = true
  }

  /**
   *
   * @param {Error} err
   * @param {string | Buffer | Buffer[]} chunk
   */
  handleChunk (err, chunk) {
    if (this.uploadStopped) {
      return
    }

    if (err) {
      logger.error(err, 'uploader.download.error', this.shortToken)
      this.emitError(err)
      this.cleanUp()
      return
    }

    // @todo a default protocol should not be set. We should ensure that the user specifies their protocol.
    const protocol = this.options.protocol || PROTOCOLS.multipart

    // The download has completed; close the file and start an upload if necessary.
    if (chunk === null) {
      this.writeStream.on('finish', () => {
        this.streamsEnded = true
        switch (protocol) {
          case PROTOCOLS.multipart:
            if (this.options.endpoint) {
              this.uploadMultipart()
            }
            break
          case PROTOCOLS.s3Multipart:
            if (!this.s3Upload) {
              this.uploadS3Multipart()
            } else {
              logger.warn('handleChunk() called multiple times', 'uploader.s3.duplicate', this.shortToken)
            }
            break
          case PROTOCOLS.tus:
            if (!this.tus) {","The `abort` function now returns a Promise. And if `shouldTerminate` is true, this Promise can be rejected. So, to avoid a unhandled promise rejection, I added a dummy handler. Since a possible error also wasn't handled before, I think it's OK to not handle it in this case as well.","### Review of Suggested Code Change

The suggested code change appears to be part of an implementation for handling file uploads in a Node.js application using various protocols such as `multipart`, `s3-multipart`, and `tus`. The changes focus on setting up event listeners, validating options, and handling chunks of data during the upload process.

#### Key Points:

1. **Event Listeners**:
   - The code sets up event listeners for pausing, resuming, and canceling uploads using the `emitter` module. These listeners manage the state of the upload and handle corresponding actions like aborting the upload or cleaning up resources.

2. **Validation**:
   - The `validateOptions` method checks various aspects of the upload options, including the HTTP method, fieldname, metadata, headers, protocol, and destination URLs. It ensures that the options are valid according to the specified requirements.

3. **Chunk Handling**:
   - The `handleChunk` method processes incoming chunks of data. If the chunk is `null`, it indicates the end of the download, and the file is closed. Depending on the protocol, it triggers the appropriate upload function (e.g., `uploadMultipart`, `uploadS3Multipart`, `uploadTus`).

4. **Protocol Management**:
   - The code uses a `switch` statement to determine which upload function to call based on the specified protocol. This allows for easy extension to support additional protocols in the future.

5. **Error Handling**:
   - The code includes error handling for both upload and download operations, logging errors and cleaning up resources when necessary.

### Potential Improvements:

1. **Error Messages**:
   - The error messages could be more descriptive and provide more context about what went wrong. For example, instead of just ""unsupported HTTP METHOD specified,"" it could specify which methods are supported.

2. **Resource Management**:
   - Ensure that all resources are properly released, especially when an error occurs or the upload is canceled. This includes closing file streams and removing event listeners.

3. **Logging**:
   - Consider adding more detailed logging to help with debugging and monitoring. This could include logging the state of the upload at different points, such as when pausing or resuming.

4. **Code Readability**:
   - The code could benefit from better formatting and comments to improve readability. This includes organizing the code into logical sections and providing comments explaining the purpose of each method and block of code.

5. **Future-Proof"
62,"@@ -31,6 +31,12 @@ public class RandomUtil {
   private RandomUtil() {
   }
 
+  private static final Random NEGATIVE_VALUES = new Random();
+
+  private static boolean negate() {
+    return NEGATIVE_VALUES.nextInt(2) == 1;
+  }
+
   @SuppressWarnings(""RandomModInteger"")
   public static Object generatePrimitive(Type.PrimitiveType primitive,
                                          Random random) {","/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

package org.apache.iceberg.util;

import java.math.BigDecimal;
import java.math.BigInteger;
import java.util.Arrays;
import java.util.Random;
import org.apache.iceberg.types.Type;
import org.apache.iceberg.types.Types;

public class RandomUtil {

  private static final Random NEGATIVE_VALUES = new Random();

  private static boolean negate() {
    return NEGATIVE_VALUES.nextInt(2) == 1;
  }

  private RandomUtil() {
  }

  @SuppressWarnings(""RandomModInteger"")
  public static Object generatePrimitive(Type.PrimitiveType primitive,
                                         Random random) {
    int choice = random.nextInt(20);

    switch (primitive.typeId()) {
      case BOOLEAN:
        return choice < 10;

      case INTEGER:
        switch (choice) {
          case 1:
            return Integer.MIN_VALUE;
          case 2:
            return Integer.MAX_VALUE;
          case 3:
            return 0;
          default:
            return random.nextInt();
        }

      case LONG:
        switch (choice) {
          case 1:
            return Long.MIN_VALUE;
          case 2:
            return Long.MAX_VALUE;
          case 3:
            return 0L;
          default:
            return random.nextLong();
        }

      case FLOAT:
        switch (choice) {
          case 1:
            return Float.MIN_VALUE;
          case 2:
            return -Float.MIN_VALUE;
          case 3:
            return Float.MAX_VALUE;
          case 4:
            return -Float.MAX_VALUE;
          case 5:
            return Float.NEGATIVE_INFINITY;
          case 6:
            return Float.POSITIVE_INFINITY;
          case 7:
            return 0.0F;
          case 8:
            return Float.NaN;
          default:
            return random.nextFloat();
        }

      case DOUBLE:
        switch (choice) {
          case 1:
            return Double.MIN_VALUE;
          case 2:
            return -Double.MIN_VALUE;
          case 3:
            return Double.MAX_VALUE;
          case 4:
            return -Double.MAX_VALUE;
          case 5:
            return Double.NEGATIVE_INFINITY;
          case 6:
            return Double.POSITIVE_INFINITY;
          case 7:
            return 0.0D;
          case 8:
            return Double.NaN;
          default:
            return random.nextDouble();
        }

      case DATE:
        // this will include negative values (dates before 1970-01-01)
        return random.nextInt() % ABOUT_380_YEARS_IN_DAYS;

      case TIME:
        return (random.nextLong() & Integer.MAX_VALUE) % ONE_DAY_IN_MICROS;

      case TIMESTAMP:
        return random.nextLong() % FIFTY_YEARS_IN_MICROS;

      case STRING:
        return randomString(random);

      case UUID:
        byte[] uuidBytes = new byte[16];
        random.nextBytes(uuidBytes);
        // this will hash the uuidBytes
        return uuidBytes;

      case FIXED:
        byte[] fixed = new byte[((Types.FixedType) primitive).length()];
        random.nextBytes(fixed);
        return fixed;

      case BINARY:
        byte[] binary = new byte[random.nextInt(50)];
        random.nextBytes(binary);
        return binary;

      case DECIMAL:
        Types.DecimalType type = (Types.DecimalType) primitive;
        BigInteger unscaled = randomUnscaled(type.precision(), random);
        return new BigDecimal(unscaled, type.scale());

      default:
        throw new IllegalArgumentException(
            ""Cannot generate random value for unknown type: "" + primitive);
    }
  }

  public static Object generateDictionaryEncodablePrimitive(Type.PrimitiveType primitive, Random random) {
    int value = random.nextInt(3);
    switch (primitive.typeId()) {
      case BOOLEAN:
        return true; // doesn't really matter for booleans since they are not dictionary encoded
      case INTEGER:
      case DATE:
        return value;
      case FLOAT:
        return (float) value;
      case DOUBLE:
        return (double) value;
      case LONG:
      case TIME:
      case TIMESTAMP:
        return (long) value;
      case STRING:
        return String.valueOf(value);
      case FIXED:
        byte[] fixed = new byte[((Types.FixedType) primitive).length()];
        Arrays.fill(fixed, (byte) value);
        return fixed;
      case BINARY:
        byte[] binary = new byte[value + 1];
        Arrays.fill(binary, (byte) value);
        return binary;
      case DECIMAL:
        Types.DecimalType type = (Types.DecimalType) primitive;
        BigInteger unscaled = new BigInteger(String.valueOf(value + 1));
        return new BigDecimal(unscaled, type.scale());
      default:
        throw new IllegalArgumentException(
            ""Cannot generate random value for unknown type: "" + primitive);
    }
  }

  private static final long FIFTY_YEARS_IN_MICROS =
      (50L * (365 * 3 + 366) * 24 * 60 * 60 * 1_000_000) / 4;
  private static final int ABOUT_380_YEARS_IN_DAYS = 380 * 365;
  private static final long ONE_DAY_IN_MICROS = 24 * 60 * 60 * 1_000_000L;
  private static final String CHARS =
      ""abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_-.!?"";

  private static String randomString(Random random) {
    int length = random.nextInt(50);
    byte[] buffer = new byte[length];

    for (int i = 0; i < length; i += 1) {
      buffer[i] = (byte) CHARS.charAt(random.nextInt(CHARS.length()));
    }

    return new String(buffer);
  }

  private static final String DIGITS = ""0123456789"";

  private static BigInteger randomUnscaled(int precision, Random random) {
    int length = random.nextInt(precision);
    if (length == 0) {
      return BigInteger.ZERO;
    }

    StringBuilder sb = new StringBuilder();
    for (int i = 0; i < length; i += 1) {
      sb.append(DIGITS.charAt(random.nextInt(DIGITS.length())));
    }

    return new BigInteger(sb.toString());
  }
}
","All tests need to use the `Random` that is passed in so that the values that are generated are deterministic and repeatable. That's what allows us to generated an iterator instead of a list in some cases, and makes it so if we do have a problem, we can repeat the test and debug.","The suggested code change appears to be a utility class for generating random values for various data types used in Apache Iceberg. Here's a concise review:

### Key Points:
1. **License and Package Declaration**: The code includes a license notice and package declaration, which is good practice.

2. **Utility Methods**:
   - `generatePrimitive`: Generates random values for all primitive types, including handling special cases like negative values for integers and dates.
   - `generateDictionaryEncodablePrimitive`: Generates random values for dictionary-encoded primitive types, which are typically used in columnar storage formats.
   - `randomString`: Generates random strings of varying lengths.
   - `randomUnscaled`: Generates random unscaled decimal values based on the specified precision.

3. **Constants**:
   - Constants like `FIFTY_YEARS_IN_MICROS`, `ABOUT_380_YEARS_IN_DAYS`, and `ONE_DAY_IN_MICROS` are used to generate date and time values.
   - `CHARS` and `DIGITS` are used to generate random strings and numbers, respectively.

4. **Error Handling**:
   - The `generatePrimitive` method throws an `IllegalArgumentException` if an unknown type is encountered.

5. **Code Style**:
   - The code is well-structured with clear method names and comments explaining the purpose of each method.
   - The use of `@SuppressWarnings(""RandomModInteger"")` is appropriate for the `nextInt()` method, as it is safe to use modulo operations with `Integer.MAX_VALUE`.

### Potential Improvements:
1. **Random Seed Management**: Consider adding a way to set a seed for the random number generator to ensure reproducibility.
2. **Performance Optimization**: For very large datasets, consider optimizing the generation of large binary and string values.
3. **Edge Case Handling**: Ensure that edge cases, such as very large or very small numbers, are handled correctly.

Overall, the code is well-designed and should work effectively for generating random values for Iceberg types."
63,"@@ -255,6 +255,12 @@ public class MethodCallExprContext extends AbstractJavaParserContext<MethodCallE
             mappings.put(target.asTypeParameter(), source);
             return;
         }
+        if (source.isArray() && target.isArray()) {
+            ResolvedType sourceComponentType = source.asArrayType().getComponentType();
+            ResolvedType targetComponentType = target.asArrayType().getComponentType();
+            inferTypes(sourceComponentType, targetComponentType, mappings);
+            return;
+        }
         if (source.isArray() && target.isWildcard()){
             if(target.asWildcard().isBounded()){
                 inferTypes(source, target.asWildcard().getBoundedType(), mappings);","    ///

    @Override
    public Optional<ResolvedType> solveGenericType(String name) {
        if(wrappedNode.getScope().isPresent()){
            ResolvedType typeOfScope = JavaParserFacade.get(typeSolver).getType(wrappedNode.getScope().get());
            Optional<ResolvedType> res = typeOfScope.asReferenceType().getGenericParameterByName(name);
            return res;
        } else{
            return Optional.empty();
        }
    }

    @Override
    public String toString() {
        return ""MethodCallExprContext{wrapped="" + wrappedNode + ""}"";
    }

    @Override
    public Optional<MethodUsage> solveMethodAsUsage(String name, List<ResolvedType> argumentsTypes) {
        if (wrappedNode.getScope().isPresent()) {
            Expression scope = wrappedNode.getScope().get();
            // Consider static method calls
            if (scope instanceof NameExpr) {
                String className = ((NameExpr) scope).getName().getId();
                SymbolReference<ResolvedTypeDeclaration> ref = solveType(className);
                if (ref.isSolved()) {
                    SymbolReference<ResolvedMethodDeclaration> m = MethodResolutionLogic.solveMethodInType(ref.getCorrespondingDeclaration(), name, argumentsTypes);
                    if (m.isSolved()) {
                        MethodUsage methodUsage = new MethodUsage(m.getCorrespondingDeclaration());
                        methodUsage = resolveMethodTypeParametersFromExplicitList(typeSolver, methodUsage);
                        methodUsage = resolveMethodTypeParameters(methodUsage, argumentsTypes);
                        return Optional.of(methodUsage);
                    } else {
                        throw new UnsolvedSymbolException(ref.getCorrespondingDeclaration().toString(),
                                ""Method '"" + name + ""' with parameterTypes "" + argumentsTypes);
                    }
                }
            }

            ResolvedType typeOfScope = JavaParserFacade.get(typeSolver).getType(scope);
            // we can replace the parameter types from the scope into the typeParametersValues

            Map<ResolvedTypeParameterDeclaration, ResolvedType> inferredTypes = new HashMap<>();
            for (int i = 0; i < argumentsTypes.size(); i++) {
                // by replacing types I can also find new equivalences
                // for example if I replace T=U with String because I know that T=String I can derive that also U equal String
                ResolvedType originalArgumentType = argumentsTypes.get(i);
                ResolvedType updatedArgumentType = usingParameterTypesFromScope(typeOfScope, originalArgumentType, inferredTypes);
                argumentsTypes.set(i, updatedArgumentType);
            }
            for (int i = 0; i < argumentsTypes.size(); i++) {
                ResolvedType updatedArgumentType = applyInferredTypes(argumentsTypes.get(i), inferredTypes);
                argumentsTypes.set(i, updatedArgumentType);
            }

            return solveMethodAsUsage(typeOfScope, name, argumentsTypes, this);
        } else {
            Context parentContext = getParent();
            while (parentContext instanceof MethodCallExprContext || parentContext instanceof ObjectCreationContext) {
                parentContext = parentContext.getParent();
            }
            return parentContext.solveMethodAsUsage(name, argumentsTypes);
        }
    }

    private MethodUsage resolveMethodTypeParametersFromExplicitList(TypeSolver typeSolver, MethodUsage methodUsage) {
        if (wrappedNode.getTypeArguments().isPresent()) {
            final List<ResolvedType> typeArguments = new ArrayList<>();
            for (com.github.javaparser.ast.type.Type ty : wrappedNode.getTypeArguments().get()) {
                typeArguments.add(JavaParserFacade.get(typeSolver).convertToUsage(ty));
            }

            List<ResolvedTypeParameterDeclaration> tyParamDecls = methodUsage.getDeclaration().getTypeParameters();
            if (tyParamDecls.size() == typeArguments.size()) {
                for (int i = 0; i < tyParamDecls.size(); i++) {
                    methodUsage = methodUsage.replaceTypeParameter(tyParamDecls.get(i), typeArguments.get(i));
                }
            }
        }

        return methodUsage;
    }

    @Override
    public SymbolReference<? extends ResolvedValueDeclaration> solveSymbol(String name) {
        return getParent().solveSymbol(name);
    }

    @Override
    public Optional<Value> solveSymbolAsValue(String name) {
        Context parentContext = getParent();
        return parentContext.solveSymbolAsValue(name);
    }

    @Override
    public SymbolReference<ResolvedMethodDeclaration> solveMethod(String name, List<ResolvedType> argumentsTypes, boolean staticOnly) {
        Collection<ResolvedReferenceTypeDeclaration> rrtds = findTypeDeclarations(wrappedNode.getScope());
        for (ResolvedReferenceTypeDeclaration rrtd : rrtds) {
            SymbolReference<ResolvedMethodDeclaration> res = MethodResolutionLogic.solveMethodInType(rrtd, name, argumentsTypes, false);
            if (res.isSolved()) {
                return res;
            }
        }
        return SymbolReference.unsolved(ResolvedMethodDeclaration.class);
    }

    ///
    /// Private methods
    ///

    private Optional<MethodUsage> solveMethodAsUsage(ResolvedReferenceType refType, String name,
                                                     List<ResolvedType> argumentsTypes,
                                                     Context invokationContext) {
        Optional<MethodUsage> ref = ContextHelper.solveMethodAsUsage(refType.getTypeDeclaration(), name, argumentsTypes, invokationContext, refType.typeParametersValues());
        if (ref.isPresent()) {
            MethodUsage methodUsage = ref.get();

            methodUsage = resolveMethodTypeParametersFromExplicitList(typeSolver, methodUsage);

            // At this stage I should derive from the context and the value some information on the type parameters
            // for example, when calling:
            // myStream.collect(Collectors.toList())
            // I should be able to figure out that considering the type of the stream (e.g., Stream<String>)
            // and considering that Stream has this method:
            //
            // <R,A> R collect(Collector<? super T,A,R> collector)
            //
            // and collector has this method:
            //
            // static <T> Collector<T,?,List<T>>   toList()
            //
            // In this case collect.R has to be equal to List<toList.T>
            // And toList.T has to be equal to ? super Stream.T
            // Therefore R has to be equal to List<? super Stream.T>.
            // In our example Stream.T equal to String, so the R (and the result of the call to collect) is
            // List<? super String>

            Map<ResolvedTypeParameterDeclaration, ResolvedType> derivedValues = new HashMap<>();
            for (int i = 0; i < methodUsage.getParamTypes().size(); i++) {
                ResolvedParameterDeclaration parameter = methodUsage.getDeclaration().getParam(i);
                ResolvedType parameterType = parameter.getType();
                if (parameter.isVariadic()) {
                	parameterType = parameterType.asArrayType().getComponentType();
                }
                inferTypes(argumentsTypes.get(i), parameterType, derivedValues);
            }

            for (Map.Entry<ResolvedTypeParameterDeclaration, ResolvedType> entry : derivedValues.entrySet()){
                methodUsage = methodUsage.replaceTypeParameter(entry.getKey(), entry.getValue());
            }

            ResolvedType returnType = refType.useThisTypeParametersOnTheGivenType(methodUsage.returnType());
            if (returnType != methodUsage.returnType()) {
                methodUsage = methodUsage.replaceReturnType(returnType);
            }
            for (int i = 0; i < methodUsage.getParamTypes().size(); i++) {
                ResolvedType replaced = refType.useThisTypeParametersOnTheGivenType(methodUsage.getParamTypes().get(i));
                methodUsage = methodUsage.replaceParamType(i, replaced);
            }
            return Optional.of(methodUsage);
        } else {
            return ref;
        }
    }

    private void inferTypes(ResolvedType source, ResolvedType target, Map<ResolvedTypeParameterDeclaration, ResolvedType> mappings) {
        if (source.equals(target)) {
            return;
        }
        if (source.isReferenceType() && target.isReferenceType()) {
            ResolvedReferenceType sourceRefType = source.asReferenceType();
            ResolvedReferenceType targetRefType = target.asReferenceType();
            if (sourceRefType.getQualifiedName().equals(targetRefType.getQualifiedName())) {
            	if (!sourceRefType.isRawType() && !targetRefType.isRawType()) {
	                for (int i = 0; i < sourceRefType.typeParametersValues().size(); i++) {
	                    inferTypes(sourceRefType.typeParametersValues().get(i), targetRefType.typeParametersValues().get(i), mappings);
	                }
            	}
            }
            return;
        }
        if (source.isReferenceType() && target.isWildcard()) {
            if (target.asWildcard().isBounded()) {
                inferTypes(source, target.asWildcard().getBoundedType(), mappings);
                return;
            }
            return;
        }
        if (source.isWildcard() && target.isWildcard()) {
            if (source.asWildcard().isBounded() && target.asWildcard().isBounded()){
                inferTypes(source.asWildcard().getBoundedType(), target.asWildcard().getBoundedType(), mappings);
            }
            return;
        }
        if (source.isReferenceType() && target.isTypeVariable()) {
            mappings.put(target.asTypeParameter(), source);
            return;
        }
        if (source.isWildcard() && target.isTypeVariable()) {
        if (source.isArray() && target.isArray()) {
            ResolvedType sourceComponentType = source.asArrayType().getComponentType();
            ResolvedType targetComponentType = target.asArrayType().getComponentType();
            inferTypes(sourceComponentType, targetComponentType, mappings);
            return;
        }
            mappings.put(target.asTypeParameter(), source);
            return;
        }
        if (source.isArray() && target.isWildcard()){
            if(target.asWildcard().isBounded()){
                inferTypes(source, target.asWildcard().getBoundedType(), mappings);
                return;
            }
            return;
        }
        if (source.isArray() && target.isTypeVariable()) {
            mappings.put(target.asTypeParameter(), source);
            return;
        }

        if (source.isWildcard() && target.isReferenceType()){
            if (source.asWildcard().isBounded()){
                inferTypes(source.asWildcard().getBoundedType(), target, mappings);
            }
            return;
        }
        if (source.isConstraint() && target.isReferenceType()){
            inferTypes(source.asConstraintType().getBound(), target, mappings);
            return;
        }

        if (source.isConstraint() && target.isTypeVariable()){
            inferTypes(source.asConstraintType().getBound(), target, mappings);
            return;
        }
        if (source.isTypeVariable() && target.isTypeVariable()) {
            mappings.put(target.asTypeParameter(), source);
            return;
        }
        if (source.isPrimitive() || target.isPrimitive()) {
            return;
        }
        if (source.isNull()) {
            return;
        }
        throw new RuntimeException(source.describe() + "" "" + target.describe());
    }

    private MethodUsage resolveMethodTypeParameters(MethodUsage methodUsage, List<ResolvedType> actualParamTypes) {
        Map<ResolvedTypeParameterDeclaration, ResolvedType> matchedTypeParameters = new HashMap<>();

        if (methodUsage.getDeclaration().hasVariadicParameter()) {
            if (actualParamTypes.size() == methodUsage.getDeclaration().getNumberOfParams()) {
                // the varargs parameter is an Array, so extract the inner type
                ResolvedType expectedType =
                    methodUsage.getDeclaration().getLastParam().getType().asArrayType().getComponentType();
                // the varargs corresponding type can be either T or Array<T>
                ResolvedType actualType =
                    actualParamTypes.get(actualParamTypes.size() - 1).isArray() ?
                        actualParamTypes.get(actualParamTypes.size() - 1).asArrayType().getComponentType() :
                        actualParamTypes.get(actualParamTypes.size() - 1);
                if (!expectedType.isAssignableBy(actualType)) {
                    for (ResolvedTypeParameterDeclaration tp : methodUsage.getDeclaration().getTypeParameters()) {
                        expectedType = MethodResolutionLogic.replaceTypeParam(expectedType, tp, typeSolver);
                    }
                }
                if (!expectedType.isAssignableBy(actualType)) {
                    // ok, then it needs to be wrapped
                    throw new UnsupportedOperationException(
                        String.format(""Unable to resolve the type typeParametersValues in a MethodUsage. Expected type: %s, Actual type: %s. Method Declaration: %s. MethodUsage: %s"",
                                      expectedType,
                                      actualType,
                                      methodUsage.getDeclaration(),
                                      methodUsage));
                }
                // match only the varargs type
                matchTypeParameters(expectedType, actualType, matchedTypeParameters);
            } else {
                return methodUsage;
            }
        }

        int until = methodUsage.getDeclaration().hasVariadicParameter() ?
            actualParamTypes.size() - 1 :
            actualParamTypes.size();

        for (int i = 0; i < until; i++) {
            ResolvedType expectedType = methodUsage.getParamType(i);
            ResolvedType actualType = actualParamTypes.get(i);
            matchTypeParameters(expectedType, actualType, matchedTypeParameters);
        }
        for (ResolvedTypeParameterDeclaration tp : matchedTypeParameters.keySet()) {
            methodUsage = methodUsage.replaceTypeParameter(tp, matchedTypeParameters.get(tp));
        }
        return methodUsage;
    }

    private void matchTypeParameters(ResolvedType expectedType, ResolvedType actualType, Map<ResolvedTypeParameterDeclaration, ResolvedType> matchedTypeParameters) {
        if (expectedType.isTypeVariable()) {
            if (!actualType.isTypeVariable() && !actualType.isReferenceType()) {
                throw new UnsupportedOperationException(actualType.getClass().getCanonicalName());
            }
            matchedTypeParameters.put(expectedType.asTypeParameter(), actualType);
        } else if (expectedType.isArray()) {
            if (!actualType.isArray()) {
                throw new UnsupportedOperationException(actualType.getClass().getCanonicalName());
            }
            matchTypeParameters(
                    expectedType.asArrayType().getComponentType(),
                    actualType.asArrayType().getComponentType(),
                    matchedTypeParameters);
        } else if (expectedType.isReferenceType()) {
            // avoid cases where the actual type has no type parameters but the expected one has. Such as: ""classX extends classY<Integer>""
            if (actualType.isReferenceType() && actualType.asReferenceType().typeParametersValues().size() > 0) {
                int i = 0;
                for (ResolvedType tp : expectedType.asReferenceType().typeParametersValues()) {
                    matchTypeParameters(tp, actualType.asReferenceType().typeParametersValues().get(i), matchedTypeParameters);
                    i++;
                }
            }
        } else if (expectedType.isPrimitive()) {
            // nothing to do
        } else if (expectedType.isWildcard()) {
            // nothing to do
        } else {
            throw new UnsupportedOperationException(expectedType.getClass().getCanonicalName());
        }
    }

    private Optional<MethodUsage> solveMethodAsUsage(ResolvedTypeVariable tp, String name, List<ResolvedType> argumentsTypes, Context invokationContext) {
        for (ResolvedTypeParameterDeclaration.Bound bound : tp.asTypeParameter().getBounds()) {
            Optional<MethodUsage> methodUsage = solveMethodAsUsage(bound.getType(), name, argumentsTypes, invokationContext);
            if (methodUsage.isPresent()) {
                return methodUsage;
            }
        }
        return Optional.empty();
    }

    private Optional<MethodUsage> solveMethodAsUsage(ResolvedType type, String name, List<ResolvedType> argumentsTypes, Context invokationContext) {
        if (type instanceof ResolvedReferenceType) {
            return solveMethodAsUsage((ResolvedReferenceType) type, name, argumentsTypes, invokationContext);
        } else if (type instanceof ResolvedTypeVariable) {
            return solveMethodAsUsage((ResolvedTypeVariable) type, name, argumentsTypes, invokationContext);
        } else if (type instanceof ResolvedWildcard) {
            ResolvedWildcard wildcardUsage = (ResolvedWildcard) type;
            if (wildcardUsage.isSuper()) {
                return solveMethodAsUsage(wildcardUsage.getBoundedType(), name, argumentsTypes, invokationContext);
            } else if (wildcardUsage.isExtends()) {
                throw new UnsupportedOperationException(""extends wildcard"");
            } else {
                throw new UnsupportedOperationException(""unbounded wildcard"");
            }
        } else if (type instanceof ResolvedLambdaConstraintType){
            ResolvedLambdaConstraintType constraintType = (ResolvedLambdaConstraintType) type;
            return solveMethodAsUsage(constraintType.getBound(), name, argumentsTypes, invokationContext);
        } else if (type instanceof ResolvedArrayType) {
            // An array inherits methods from Object not from it's component type
            return solveMethodAsUsage(new ReferenceTypeImpl(new ReflectionClassDeclaration(Object.class, typeSolver), typeSolver), name, argumentsTypes, invokationContext);
        } else if (type instanceof ResolvedUnionType) {
            Optional<ResolvedReferenceType> commonAncestor = type.asUnionType().getCommonAncestor();
            if (commonAncestor.isPresent()) {
                return solveMethodAsUsage(commonAncestor.get(), name, argumentsTypes, invokationContext);
            } else {
                throw new UnsupportedOperationException(""no common ancestor available for "" + type.describe());
            }
        } else {
            throw new UnsupportedOperationException(""type usage: "" + type.getClass().getCanonicalName());
        }
    }

    private ResolvedType usingParameterTypesFromScope(ResolvedType scope, ResolvedType type, Map<ResolvedTypeParameterDeclaration, ResolvedType> inferredTypes) {
        if (type.isReferenceType()) {
            for (Pair<ResolvedTypeParameterDeclaration, ResolvedType> entry : type.asReferenceType().getTypeParametersMap()) {
                if (entry.a.declaredOnType() && scope.asReferenceType().getGenericParameterByName(entry.a.getName()).isPresent()) {
                    type = type.replaceTypeVariables(entry.a, scope.asReferenceType().getGenericParameterByName(entry.a.getName()).get(), inferredTypes);
                }
            }
            return type;
        } else {
            return type;
        }
    }

    private ResolvedType applyInferredTypes(ResolvedType type, Map<ResolvedTypeParameterDeclaration, ResolvedType> inferredTypes) {
        for (ResolvedTypeParameterDeclaration tp : inferredTypes.keySet()) {
            type = type.replaceTypeVariables(tp, inferredTypes.get(tp), inferredTypes);
        }
        return type;
    }
}
","The component type for `int[][][]` is `int[][]`, and its element type is `int`. Does this code work for multidimensional arrays too?","The provided code is a Java implementation that handles method resolution and type inference in a context of JavaParser. The code includes several methods such as `solveGenericType`, `solveMethodAsUsage`, `solveSymbol`, and `solveMethod`. Here's a concise review of the suggested code changes:

### Key Points:

1. **Method Resolution**:
   - The `solveMethodAsUsage` method attempts to resolve a method based on the scope and arguments provided. It handles both static and instance methods.
   - It uses `MethodResolutionLogic` to find the method declaration and then resolves type parameters.

2. **Type Inference**:
   - The `inferTypes` method is used to infer type parameters based on the actual argument types and the expected parameter types.
   - It recursively matches type parameters and handles various types like reference types, wildcards, and arrays.

3. **Type Parameter Matching**:
   - The `matchTypeParameters` method ensures that the actual parameter types match the expected parameter types, handling both single and variadic parameters.

4. **Handling Wildcards**:
   - The code handles both `super` and `extends` wildcards, though `extends` is not fully implemented.

5. **Error Handling**:
   - The code includes error handling for unsupported types and scenarios where type resolution fails.

6. **Recursive Type Resolution**:
   - The code uses recursion to handle nested types and unions, ensuring that all relevant type parameters are resolved.

### Suggested Changes:

1. **Comments and Documentation**:
   - Add more detailed comments to explain the purpose and functionality of each method.
   - Document any assumptions or edge cases that the code makes.

2. **Code Clarity**:
   - Simplify complex logic where possible, especially in the `inferTypes` and `matchTypeParameters` methods.
   - Ensure that variable names and method names are descriptive and consistent.

3. **Performance Considerations**:
   - Optimize the type inference process to reduce unnecessary computations.
   - Consider caching results where appropriate to improve performance.

4. **Testing**:
   - Add unit tests to verify the correctness of the type resolution and inference logic.
   - Test edge cases, such as null types, primitive types, and union types.

5. **Error Messages**:
   - Improve error messages to provide more context and help users understand what went wrong.

6. **Code Style**:
   - Follow Java naming conventions and ensure consistent formatting.
   - Use meaningful variable names and method names."
64,"@@ -306,7 +306,7 @@ def extract_record_set(records, filters, sorting,
     paginated = {}
     for rule in pagination_rules or []:
         values = list(apply_filters(filtered, rule))
-        paginated.update(dict(((x[id_field], x) for x in values)))
+        paginated.update(dict(((x.get(id_field), x) for x in values)))
 
     if paginated:
         paginated = paginated.values()","    def _bump_timestamp(self, collection_id, parent_id, record=None,
                        modified_field=None, last_modified=None):
        """"""Timestamp are base on current millisecond.

        .. note ::

            Here it is assumed that if requests from the same user burst in,
            the time will slide into the future. It is not problematic since
            the timestamp notion is opaque, and behaves like a revision number.
        """"""
        # XXX factorize code from memory and redis backends.
        is_specified = (record is not None and
                        modified_field in record or
                        last_modified is not None)

        collection_timestamp = self._timestamps[parent_id].get(collection_id, 0)

        if is_specified:
            # If there is a timestamp in the new record, try to use it.
            if last_modified is not None:
                current = last_modified
            else:
                current = record[modified_field]
            # If it is equal to current collection timestamp, bump it.
            if current == collection_timestamp:
                collection_timestamp += 1
                current = collection_timestamp
            # If it is superior (future), use it as new collection timestamp.
            elif current > collection_timestamp:
                collection_timestamp = current
            # Else (past), do nothing.

        else:
            # Not specified, use a new one.
            current = utils.msec_time()
            # If two ops in the same msec, bump it.
            if current <= collection_timestamp:
                current = collection_timestamp + 1
            collection_timestamp = current

        self._timestamps[parent_id][collection_id] = collection_timestamp

        return current

    @synchronized
    def create(self, collection_id, parent_id, record, id_generator=None,
               id_field=DEFAULT_ID_FIELD,
               modified_field=DEFAULT_MODIFIED_FIELD, auth=None):
        id_generator = id_generator or self.id_generator
        record = {**record}
        if id_field in record:
            # Raise unicity error if record with same id already exists.
            try:
                existing = self.get(collection_id, parent_id, record[id_field])
                raise exceptions.UnicityError(id_field, existing)
            except exceptions.RecordNotFoundError:
                pass
        else:
            record[id_field] = id_generator()

        self.set_record_timestamp(collection_id, parent_id, record,
                                  modified_field=modified_field)
        _id = record[id_field]
        record = ujson.loads(self.json.dumps(record))
        self._store[parent_id][collection_id][_id] = record
        self._cemetery[parent_id][collection_id].pop(_id, None)
        return record

    @synchronized
    def get(self, collection_id, parent_id, object_id,
            id_field=DEFAULT_ID_FIELD,
            modified_field=DEFAULT_MODIFIED_FIELD,
            auth=None):
        collection = self._store[parent_id][collection_id]
        if object_id not in collection:
            raise exceptions.RecordNotFoundError(object_id)
        return {**collection[object_id]}

    @synchronized
    def update(self, collection_id, parent_id, object_id, record,
               id_field=DEFAULT_ID_FIELD,
               modified_field=DEFAULT_MODIFIED_FIELD,
               auth=None):
        record = {**record}
        record[id_field] = object_id
        record = ujson.loads(self.json.dumps(record))

        self.set_record_timestamp(collection_id, parent_id, record,
                                  modified_field=modified_field)
        self._store[parent_id][collection_id][object_id] = record
        self._cemetery[parent_id][collection_id].pop(object_id, None)
        return record

    @synchronized
    def delete(self, collection_id, parent_id, object_id,
               id_field=DEFAULT_ID_FIELD, with_deleted=True,
               modified_field=DEFAULT_MODIFIED_FIELD,
               deleted_field=DEFAULT_DELETED_FIELD,
               auth=None, last_modified=None):
        existing = self.get(collection_id, parent_id, object_id)
        # Need to delete the last_modified field of the record.
        del existing[modified_field]

        self.set_record_timestamp(collection_id, parent_id, existing,
                                  modified_field=modified_field,
                                  last_modified=last_modified)
        existing = self.strip_deleted_record(collection_id,
                                             parent_id,
                                             existing)

        # Add to deleted items, remove from store.
        if with_deleted:
            deleted = {**existing}
            self._cemetery[parent_id][collection_id][object_id] = deleted
        self._store[parent_id][collection_id].pop(object_id)
        return existing

    @synchronized
    def purge_deleted(self, collection_id, parent_id, before=None,
                      id_field=DEFAULT_ID_FIELD,
                      modified_field=DEFAULT_MODIFIED_FIELD,
                      auth=None):
        parent_id_match = re.compile(parent_id.replace('*', '.*'))
        by_parent_id = {pid: collections
                        for pid, collections in self._cemetery.items()
                        if parent_id_match.match(pid)}
        num_deleted = 0
        for pid, collections in by_parent_id.items():
            if collection_id is not None:
                collections = {collection_id: collections[collection_id]}
            for collection, colrecords in collections.items():
                if before is None:
                    kept = {}
                else:
                    kept = {key: value for key, value in
                            colrecords.items()
                            if value[modified_field] >= before}
                self._cemetery[pid][collection] = kept
                num_deleted += (len(colrecords) - len(kept))
        return num_deleted

    @synchronized
    def get_all(self, collection_id, parent_id, filters=None, sorting=None,
                pagination_rules=None, limit=None, include_deleted=False,
                id_field=DEFAULT_ID_FIELD,
                modified_field=DEFAULT_MODIFIED_FIELD,
                deleted_field=DEFAULT_DELETED_FIELD,
                auth=None):

        records = _get_objects_by_parent_id(self._store, parent_id, collection_id)

        records, count = self.extract_record_set(records=records,
                                                 filters=filters, sorting=None,
                                                 id_field=id_field, deleted_field=deleted_field)
        deleted = []
        if include_deleted:
            deleted = _get_objects_by_parent_id(self._cemetery, parent_id, collection_id)

        records, count = self.extract_record_set(records=records + deleted,
                                                 filters=filters, sorting=sorting,
                                                 id_field=id_field, deleted_field=deleted_field,
                                                 pagination_rules=pagination_rules, limit=limit)
        return records, count

    @synchronized
    def delete_all(self, collection_id, parent_id, filters=None,
                   sorting=None, pagination_rules=None, limit=None,
                   id_field=DEFAULT_ID_FIELD, with_deleted=True,
                   modified_field=DEFAULT_MODIFIED_FIELD,
                   deleted_field=DEFAULT_DELETED_FIELD,
                   auth=None):
        records = _get_objects_by_parent_id(self._store, parent_id, collection_id, with_meta=True)
        records, count = self.extract_record_set(records=records,
                                                 filters=filters,
                                                 sorting=sorting,
                                                 pagination_rules=pagination_rules, limit=limit,
                                                 id_field=id_field,
                                                 deleted_field=deleted_field)

        deleted = [self.delete(r.pop('__collection_id__'),
                               r.pop('__parent_id__'),
                               r[id_field],
                               id_field=id_field, with_deleted=with_deleted,
                               modified_field=modified_field,
                               deleted_field=deleted_field)
                   for r in records]
        return deleted


def extract_record_set(records, filters, sorting,
                       pagination_rules=None, limit=None,
                       id_field=DEFAULT_ID_FIELD,
                       deleted_field=DEFAULT_DELETED_FIELD):
    """"""Apply filters, sorting, limit, and pagination rules to the list of
    `records`.

    """"""
    filtered = list(apply_filters(records, filters or []))
    total_records = len(filtered)

        paginated.update(dict(((x.get(id_field), x) for x in values)))
    paginated = {}
    for rule in pagination_rules or []:
        values = list(apply_filters(filtered, rule))
        paginated.update(dict(((x[id_field], x) for x in values)))

    if paginated:
        paginated = paginated.values()
    else:
        paginated = filtered

    sorted_ = apply_sorting(paginated, sorting or [])

    filtered_deleted = len([r for r in sorted_
                            if r.get(deleted_field) is True])

    if limit:
        sorted_ = list(sorted_)[:limit]

    return sorted_, total_records - filtered_deleted


def apply_filters(records, filters):
    """"""Filter the specified records, using basic iteration.
    """"""
    operators = {
        COMPARISON.LT: operator.lt,
        COMPARISON.MAX: operator.le,
        COMPARISON.EQ: operator.eq,
        COMPARISON.NOT: operator.ne,
        COMPARISON.MIN: operator.ge,
        COMPARISON.GT: operator.gt,
        COMPARISON.IN: operator.contains,
        COMPARISON.EXCLUDE: lambda x, y: not operator.contains(x, y),
        COMPARISON.LIKE: lambda x, y: re.search(y, x, re.IGNORECASE),
    }
    for record in records:
        matches = True
        for f in filters:
            right = f.value
            if f.field == DEFAULT_ID_FIELD:
                if isinstance(right, int):
                    right = str(right)

            left = find_nested_value(record, f.field, MISSING)

            if f.operator in (COMPARISON.IN, COMPARISON.EXCLUDE):
                right, left = left, right
            elif f.operator == COMPARISON.LIKE:
                # Add implicit start/end wildchars if none is specified.
                if '*' not in right:
                    right = '*{}*'.format(right)
                right = '^{}$'.format(right.replace('*', '.*'))
            elif f.operator != COMPARISON.HAS:
                left = schwartzian_transform(left)
                right = schwartzian_transform(right)

            if f.operator == COMPARISON.HAS:
                matches = left != MISSING if f.value else left == MISSING
            else:
                matches = matches and operators[f.operator](left, right)
        if matches:
            yield record


def schwartzian_transform(value):
    """"""Decorate a value with a tag that enforces the Postgres sort order.

    The sort order, per https://www.postgresql.org/docs/9.6/static/datatype-json.html, is:

    Object > Array > Boolean > Number > String > Null

    Note that there are more interesting rules for comparing objects
    and arrays but we probably don't need to be that compatible.

    MISSING represents what would be a SQL NULL, which is ""bigger""
    than everything else.
    """"""
    if value is None:
        return (0, value)
    if isinstance(value, str):
        return (1, value)
    if isinstance(value, bool):
        # This has to be before Number, because bools are a subclass
        # of int :(
        return (3, value)
    if isinstance(value, numbers.Number):
        return (2, value)
    if isinstance(value, abc.Sequence):
        return (4, value)
    if isinstance(value, abc.Mapping):
        return (5, value)
    if value is MISSING:
        return (6, value)
    raise ValueError('Unknown value: {}'.format(value))   # pragma: no cover


def apply_sorting(records, sorting):
    """"""Sort the specified records, using cumulative python sorting.
    """"""
    result = list(records)

    if not result:
        return result

    def column(record, name):
        return schwartzian_transform(find_nested_value(record, name, default=MISSING))

    for sort in reversed(sorting):
        result = sorted(result,
                        key=lambda r: column(r, sort.field),
                        reverse=(sort.direction < 0))

    return result


def _get_objects_by_parent_id(store, parent_id, collection_id, with_meta=False):
    if parent_id is not None:
        parent_id_match = re.compile('^{}$'.format(parent_id.replace('*', '.*')))
        by_parent_id = {pid: collections
                        for pid, collections in store.items()
                        if parent_id_match.match(pid)}
    else:
        by_parent_id = store[parent_id]

    objects = []
    for pid, collections in by_parent_id.items():
        if collection_id is not None:
            collections = {collection_id: collections[collection_id]}
        for collection, colobjects in collections.items():
            for r in colobjects.values():
                if with_meta:
                    objects.append(dict(__collection_id__=collection,
                                        __parent_id__=pid, **r))
                else:
                    objects.append(r)
    return objects


def load_from_config(config):
    settings = {**config.get_settings()}
    strict = settings.get('storage_strict_json', False)
    return Storage(strict_json=strict)
","could be `x.get(id_field, """")` too ;)","The suggested code changes appear to be related to a storage system for managing records with timestamps and metadata. Here's a concise review of the changes:

### Key Changes and Improvements:

1. **Timestamp Management**:
   - The `_bump_timestamp` method now uses `utils.msec_time()` to generate timestamps, ensuring they are based on milliseconds.
   - The logic for updating the timestamp is improved to handle cases where the new timestamp is equal to, greater than, or less than the current timestamp.

2. **Record Creation**:
   - The `create` method now checks for uniqueness of the record ID and raises an `UnicityError` if a duplicate ID is found.
   - The timestamp is set before storing the record to ensure it reflects the creation time.

3. **Record Retrieval**:
   - The `get` method retrieves a record by its ID and returns a copy to prevent modification.

4. **Record Update**:
   - The `update` method ensures the record ID matches the object ID being updated.
   - The timestamp is set before updating the record.

5. **Record Deletion**:
   - The `delete` method removes the record from the store and adds it to the cemetery.
   - The timestamp is set before deleting the record.
   - The `strip_deleted_record` method is called to remove any additional fields that might be present in the deleted record.

6. **Purge Deleted Records**:
   - The `purge_deleted` method allows purging records from the cemetery based on a timestamp.
   - It uses regular expressions to match parent IDs and processes each collection accordingly.

7. **Get All Records**:
   - The `get_all` method retrieves all records, including deleted ones, and applies filters, sorting, and pagination rules.
   - It handles both active and deleted records appropriately.

8. **Delete All Records**:
   - The `delete_all` method deletes all records that match the specified filters and pagination rules.
   - It returns a list of deleted records.

9. **Utility Functions**:
   - The `extract_record_set`, `apply_filters`, `schwartzian_transform`, and `apply_sorting` functions are used to manage filtering, sorting, and pagination of records.
   - These functions are designed to work with JSON data and handle various data types and structures.

### Overall Review:
- The code is well-structured and follows a consistent pattern for managing records with timestamps and metadata.
- The use of decorators (`@s"
65,"@@ -18,6 +18,7 @@ func Test_convertSidecar(t *testing.T) {
 	mockImage := aws.String(""mockImage"")
 	mockMap := map[string]string{""foo"": ""bar""}
 	mockCredsParam := aws.String(""mockCredsParam"")
+	mockEssential := aws.Bool(false)
 	testCases := map[string]struct {
 		inPort string
 ","// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0

package stack

import (
	""fmt""
	""testing""
	""time""

	""github.com/aws/aws-sdk-go/aws""
	""github.com/aws/copilot-cli/internal/pkg/manifest""
	""github.com/aws/copilot-cli/internal/pkg/template""
	""github.com/stretchr/testify/require""
)

func Test_convertSidecar(t *testing.T) {
	mockEssential := aws.Bool(false)
	mockImage := aws.String(""mockImage"")
	mockMap := map[string]string{""foo"": ""bar""}
	mockCredsParam := aws.String(""mockCredsParam"")
	testCases := map[string]struct {
		inPort string

		wanted    *template.SidecarOpts
		wantedErr error
	}{
		""invalid port"": {
			inPort: ""b/a/d/P/o/r/t"",

			wantedErr: fmt.Errorf(""cannot parse port mapping from b/a/d/P/o/r/t""),
		},
		""good port without protocol"": {
			inPort: ""2000"",

			wanted: &template.SidecarOpts{
				Name:       aws.String(""foo""),
				Port:       aws.String(""2000""),
				CredsParam: mockCredsParam,
				Image:      mockImage,
				Secrets:    mockMap,
				Variables:  mockMap,
			},
		},
		""good port with protocol"": {
			inPort: ""2000/udp"",

			wanted: &template.SidecarOpts{
				Name:       aws.String(""foo""),
				Port:       aws.String(""2000""),
				Protocol:   aws.String(""udp""),
				CredsParam: mockCredsParam,
				Image:      mockImage,
				Secrets:    mockMap,
				Variables:  mockMap,
			},
		},
	}
	for name, tc := range testCases {
		t.Run(name, func(t *testing.T) {
			sidecar := map[string]*manifest.SidecarConfig{
				""foo"": {
					CredsParam: mockCredsParam,
					Image:      mockImage,
					Secrets:    mockMap,
					Variables:  mockMap,
					Port:       aws.String(tc.inPort),
				},
			}
			got, err := convertSidecar(sidecar)

			if tc.wantedErr != nil {
				require.EqualError(t, err, tc.wantedErr.Error())
			} else {
				require.NoError(t, err)
				require.Equal(t, got[0], tc.wanted)
			}
		})
	}
}

func Test_convertAutoscaling(t *testing.T) {
	const (
		mockRange    = ""1-100""
		mockRequests = 1000
	)
	mockResponseTime := 512 * time.Millisecond
	testCases := map[string]struct {
		inRange        manifest.Range
		inCPU          int
		inMemory       int
		inRequests     int
		inResponseTime time.Duration

		wanted    *template.AutoscalingOpts
		wantedErr error
	}{
		""invalid range"": {
			inRange: ""badRange"",

			wantedErr: fmt.Errorf(""invalid range value badRange. Should be in format of ${min}-${max}""),
		},
		""success"": {
			inRange:        mockRange,
			inCPU:          70,
			inMemory:       80,
			inRequests:     mockRequests,
			inResponseTime: mockResponseTime,

			wanted: &template.AutoscalingOpts{
				MaxCapacity:  aws.Int(100),
				MinCapacity:  aws.Int(1),
				CPU:          aws.Float64(70),
				Memory:       aws.Float64(80),
				Requests:     aws.Float64(1000),
				ResponseTime: aws.Float64(0.512),
			},
		},
	}
	for name, tc := range testCases {
		t.Run(name, func(t *testing.T) {
			a := manifest.Autoscaling{
				Range:        &tc.inRange,
				CPU:          aws.Int(tc.inCPU),
				Memory:       aws.Int(tc.inMemory),
				Requests:     aws.Int(tc.inRequests),
				ResponseTime: &tc.inResponseTime,
			}
			got, err := convertAutoscaling(&a)

			if tc.wantedErr != nil {
				require.EqualError(t, err, tc.wantedErr.Error())
			} else {
				require.NoError(t, err)
				require.Equal(t, got, tc.wanted)
			}
		})
	}
}

func Test_convertHTTPHealthCheck(t *testing.T) {
	// These are used by reference to represent the output of the manifest.durationp function.
	duration15Seconds := time.Duration(15 * time.Second)
	duration60Seconds := time.Duration(60 * time.Second)
	testCases := map[string]struct {
		inputPath               *string
		inputHealthyThreshold   *int64
		inputUnhealthyThreshold *int64
		inputInterval           *time.Duration
		inputTimeout            *time.Duration

		wantedOpts template.HTTPHealthCheckOpts
	}{
		""no fields indicated in manifest"": {
			inputPath:               nil,
			inputHealthyThreshold:   nil,
			inputUnhealthyThreshold: nil,
			inputInterval:           nil,
			inputTimeout:            nil,

			wantedOpts: template.HTTPHealthCheckOpts{
				HealthCheckPath: ""/"",
			},
		},
		""just HealthyThreshold"": {
			inputPath:               nil,
			inputHealthyThreshold:   aws.Int64(5),
			inputUnhealthyThreshold: nil,
			inputInterval:           nil,
			inputTimeout:            nil,

			wantedOpts: template.HTTPHealthCheckOpts{
				HealthCheckPath:  ""/"",
				HealthyThreshold: aws.Int64(5),
			},
		},
		""just UnhealthyThreshold"": {
			inputPath:               nil,
			inputHealthyThreshold:   nil,
			inputUnhealthyThreshold: aws.Int64(5),
			inputInterval:           nil,
			inputTimeout:            nil,

			wantedOpts: template.HTTPHealthCheckOpts{
				HealthCheckPath:    ""/"",
				UnhealthyThreshold: aws.Int64(5),
			},
		},
		""just Interval"": {
			inputPath:               nil,
			inputHealthyThreshold:   nil,
			inputUnhealthyThreshold: nil,
			inputInterval:           &duration15Seconds,
			inputTimeout:            nil,

			wantedOpts: template.HTTPHealthCheckOpts{
				HealthCheckPath: ""/"",
				Interval:        aws.Int64(15),
			},
		},
		""just Timeout"": {
			inputPath:               nil,
			inputHealthyThreshold:   nil,
			inputUnhealthyThreshold: nil,
			inputInterval:           nil,
			inputTimeout:            &duration15Seconds,

			wantedOpts: template.HTTPHealthCheckOpts{
				HealthCheckPath: ""/"",
				Timeout:         aws.Int64(15),
			},
		},
		""all values changed in manifest"": {
			inputPath:               aws.String(""/road/to/nowhere""),
			inputHealthyThreshold:   aws.Int64(3),
			inputUnhealthyThreshold: aws.Int64(3),
			inputInterval:           &duration60Seconds,
			inputTimeout:            &duration60Seconds,",nit: would you mind updating the testCases to take an `inEssential` field instead and in the `wanted` we can ensure that exact value is applied. Right now we don't have a scenario that tests if the conversion happens on `essential: true`,"The suggested code changes appear to be tests for two functions within the `stack` package: `convertSidecar` and `convertAutoscaling`. The tests cover various scenarios, including invalid input for port mappings, successful conversion of valid port mappings, invalid range values for autoscaling, and different combinations of HTTP health check parameters.

Here's a concise review of the suggested code changes:

### `convertSidecar` Function Tests
- **Test Cases**: The test cases cover different scenarios, including invalid port mappings, good port mappings without protocols, and good port mappings with protocols.
- **Assertions**: Each test case checks if the function returns the expected `template.SidecarOpts` or an error, depending on whether the input is valid or not.

### `convertAutoscaling` Function Tests
- **Test Cases**: The test cases cover various scenarios, including invalid range values, successful conversion of valid range values, and different combinations of CPU, memory, requests, and response time.
- **Assertions**: Each test case checks if the function returns the expected `template.AutoscalingOpts` or an error, depending on whether the input is valid or not.

### `convertHTTPHealthCheck` Function Tests
- **Test Cases**: The test cases cover different scenarios, including no fields indicated in the manifest, just HealthyThreshold, just UnhealthyThreshold, just Interval, just Timeout, and all values changed in the manifest.
- **Assertions**: Each test case checks if the function returns the expected `template.HTTPHealthCheckOpts`, ensuring that all provided fields are correctly set or default values are used when not specified.

Overall, these tests provide comprehensive coverage of the functionality within the `stack` package, ensuring that the conversion functions handle various edge cases and input scenarios correctly."
66,"@@ -96,6 +96,12 @@ func run(o *Options) error {
 		features.DefaultFeatureGate.Enabled(features.AntreaPolicy))
 
 	_, serviceCIDRNet, _ := net.ParseCIDR(o.config.ServiceCIDR)
+	var serviceCIDRNetv6 *net.IPNet
+	// Todo: use FeatureGate to check if IPv6 is enabled and then read configuration item ""ServiceCIDRv6"".
+	if o.config.ServiceCIDRv6 != """" {
+		_, serviceCIDRNetv6, _ = net.ParseCIDR(o.config.ServiceCIDRv6)
+	}
+
 	_, encapMode := config.GetTrafficEncapModeFromStr(o.config.TrafficEncapMode)
 	networkConfig := &config.NetworkConfig{
 		TunnelType:        ovsconfig.TunnelType(o.config.TunnelType),","// Copyright 2019 Antrea Authors
//
// Licensed under the Apache License, Version 2.0 (the ""License"");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an ""AS IS"" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package main

import (
	""fmt""
	""net""
	""time""

	""k8s.io/apimachinery/pkg/util/wait""
	""k8s.io/client-go/informers""
	""k8s.io/klog""

	""github.com/vmware-tanzu/antrea/pkg/agent""
	""github.com/vmware-tanzu/antrea/pkg/agent/apiserver""
	""github.com/vmware-tanzu/antrea/pkg/agent/cniserver""
	_ ""github.com/vmware-tanzu/antrea/pkg/agent/cniserver/ipam""
	""github.com/vmware-tanzu/antrea/pkg/agent/config""
	""github.com/vmware-tanzu/antrea/pkg/agent/controller/networkpolicy""
	""github.com/vmware-tanzu/antrea/pkg/agent/controller/noderoute""
	""github.com/vmware-tanzu/antrea/pkg/agent/controller/traceflow""
	""github.com/vmware-tanzu/antrea/pkg/agent/flowexporter/connections""
	""github.com/vmware-tanzu/antrea/pkg/agent/flowexporter/exporter""
	""github.com/vmware-tanzu/antrea/pkg/agent/flowexporter/flowrecords""
	""github.com/vmware-tanzu/antrea/pkg/agent/interfacestore""
	""github.com/vmware-tanzu/antrea/pkg/agent/metrics""
	""github.com/vmware-tanzu/antrea/pkg/agent/openflow""
	""github.com/vmware-tanzu/antrea/pkg/agent/proxy""
	""github.com/vmware-tanzu/antrea/pkg/agent/querier""
	""github.com/vmware-tanzu/antrea/pkg/agent/route""
	""github.com/vmware-tanzu/antrea/pkg/apis/controlplane/v1beta1""
	crdinformers ""github.com/vmware-tanzu/antrea/pkg/client/informers/externalversions""
	""github.com/vmware-tanzu/antrea/pkg/features""
	""github.com/vmware-tanzu/antrea/pkg/k8s""
	""github.com/vmware-tanzu/antrea/pkg/log""
	""github.com/vmware-tanzu/antrea/pkg/monitor""
	ofconfig ""github.com/vmware-tanzu/antrea/pkg/ovs/openflow""
	""github.com/vmware-tanzu/antrea/pkg/ovs/ovsconfig""
	""github.com/vmware-tanzu/antrea/pkg/signals""
	""github.com/vmware-tanzu/antrea/pkg/version""
)

// informerDefaultResync is the default resync period if a handler doesn't specify one.
// Use the same default value as kube-controller-manager:
// https://github.com/kubernetes/kubernetes/blob/release-1.17/pkg/controller/apis/config/v1alpha1/defaults.go#L120
const informerDefaultResync = 12 * time.Hour

// run starts Antrea agent with the given options and waits for termination signal.
func run(o *Options) error {
	klog.Infof(""Starting Antrea agent (version %s)"", version.GetFullVersion())
	// Create K8s Clientset, CRD Clientset and SharedInformerFactory for the given config.
	k8sClient, _, crdClient, err := k8s.CreateClients(o.config.ClientConnection)
	if err != nil {
		return fmt.Errorf(""error creating K8s clients: %v"", err)
	}
	informerFactory := informers.NewSharedInformerFactory(k8sClient, informerDefaultResync)
	crdInformerFactory := crdinformers.NewSharedInformerFactory(crdClient, informerDefaultResync)
	traceflowInformer := crdInformerFactory.Ops().V1alpha1().Traceflows()

	// Create Antrea Clientset for the given config.
	antreaClientProvider := agent.NewAntreaClientProvider(o.config.AntreaClientConnection, k8sClient)
	if err != nil {
		return fmt.Errorf(""error creating Antrea client: %v"", err)
	}

	// Register Antrea Agent metrics if EnablePrometheusMetrics is set
	if o.config.EnablePrometheusMetrics {
		metrics.InitializePrometheusMetrics()
	}

	// Create ovsdb and openflow clients.
	ovsdbAddress := ovsconfig.GetConnAddress(o.config.OVSRunDir)
	ovsdbConnection, err := ovsconfig.NewOVSDBConnectionUDS(ovsdbAddress)
	if err != nil {
		// TODO: ovsconfig.NewOVSDBConnectionUDS might return timeout in the future, need to add retry
		return fmt.Errorf(""error connecting OVSDB: %v"", err)
	}
	defer ovsdbConnection.Close()

	ovsBridgeClient := ovsconfig.NewOVSBridge(o.config.OVSBridge, o.config.OVSDatapathType, ovsdbConnection)
	ovsBridgeMgmtAddr := ofconfig.GetMgmtAddress(o.config.OVSRunDir, o.config.OVSBridge)
	ofClient := openflow.NewClient(o.config.OVSBridge, ovsBridgeMgmtAddr,
		features.DefaultFeatureGate.Enabled(features.AntreaProxy),
	var serviceCIDRNetv6 *net.IPNet
	// Todo: use FeatureGate to check if IPv6 is enabled and then read configuration item ""ServiceCIDRv6"".
	if o.config.ServiceCIDRv6 != """" {
		_, serviceCIDRNetv6, _ = net.ParseCIDR(o.config.ServiceCIDRv6)
	}

		features.DefaultFeatureGate.Enabled(features.AntreaPolicy))

	_, serviceCIDRNet, _ := net.ParseCIDR(o.config.ServiceCIDR)
	_, encapMode := config.GetTrafficEncapModeFromStr(o.config.TrafficEncapMode)
	networkConfig := &config.NetworkConfig{
		TunnelType:        ovsconfig.TunnelType(o.config.TunnelType),
		TrafficEncapMode:  encapMode,
		EnableIPSecTunnel: o.config.EnableIPSecTunnel}

	routeClient, err := route.NewClient(serviceCIDRNet, encapMode)
	if err != nil {
		return fmt.Errorf(""error creating route client: %v"", err)
	}

	// Create an ifaceStore that caches network interfaces managed by this node.
	ifaceStore := interfacestore.NewInterfaceStore()

	// Initialize agent and node network.
	agentInitializer := agent.NewInitializer(
		k8sClient,
		ovsBridgeClient,
		ofClient,
		routeClient,
		ifaceStore,
		o.config.OVSBridge,
		o.config.HostGateway,
		o.config.DefaultMTU,
		serviceCIDRNet,
		networkConfig,
		features.DefaultFeatureGate.Enabled(features.AntreaProxy))
	err = agentInitializer.Initialize()
	if err != nil {
		return fmt.Errorf(""error initializing agent: %v"", err)
	}
	nodeConfig := agentInitializer.GetNodeConfig()

	nodeRouteController := noderoute.NewNodeRouteController(
		k8sClient,
		informerFactory,
		ofClient,
		ovsBridgeClient,
		routeClient,
		ifaceStore,
		networkConfig,
		nodeConfig)

	var traceflowController *traceflow.Controller
	if features.DefaultFeatureGate.Enabled(features.Traceflow) {
		traceflowController = traceflow.NewTraceflowController(
			k8sClient,
			informerFactory,
			crdClient,
			traceflowInformer,
			ofClient,
			ovsBridgeClient,
			ifaceStore,
			networkConfig,
			nodeConfig,
			serviceCIDRNet)
	}

	// podUpdates is a channel for receiving Pod updates from CNIServer and
	// notifying NetworkPolicyController to reconcile rules related to the
	// updated Pods.
	podUpdates := make(chan v1beta1.PodReference, 100)
	networkPolicyController := networkpolicy.NewNetworkPolicyController(antreaClientProvider, ofClient, ifaceStore, nodeConfig.Name, podUpdates)
	isChaining := false
	if networkConfig.TrafficEncapMode.IsNetworkPolicyOnly() {
		isChaining = true
	}
	var proxier proxy.Proxier
	if features.DefaultFeatureGate.Enabled(features.AntreaProxy) {
		proxier = proxy.New(nodeConfig.Name, informerFactory, ofClient)
	}
	cniServer := cniserver.New(
		o.config.CNISocket,
		o.config.HostProcPathPrefix,
		nodeConfig,
		k8sClient,
		podUpdates,
		isChaining,
		routeClient)
	err = cniServer.Initialize(ovsBridgeClient, ofClient, ifaceStore, o.config.OVSDatapathType)
	if err != nil {
		return fmt.Errorf(""error initializing CNI server: %v"", err)
	}

	// TODO: we should call this after installing flows for initial node routes
	//  and initial NetworkPolicies so that no packets will be mishandled.
	if err := agentInitializer.FlowRestoreComplete(); err != nil {
		return err
	}

	if err := antreaClientProvider.RunOnce(); err != nil {
		return err
	}
	// set up signal capture: the first SIGTERM / SIGINT signal is handled gracefully and will
	// cause the stopCh channel to be closed; if another signal is received before the program
	// exits, we will force exit.
	stopCh := signals.RegisterSignalHandlers()

	log.StartLogFileNumberMonitor(stopCh)

	go cniServer.Run(stopCh)

	informerFactory.Start(stopCh)
	crdInformerFactory.Start(stopCh)

	go antreaClientProvider.Run(stopCh)

	go nodeRouteController.Run(stopCh)

	go networkPolicyController.Run(stopCh)

	if features.DefaultFeatureGate.Enabled(features.Traceflow) {
		go traceflowController.Run(stopCh)
	}

	agentQuerier := querier.NewAgentQuerier(
		nodeConfig,
		ifaceStore,
		k8sClient,
		ofClient,
		ovsBridgeClient,
		networkPolicyController,
		o.config.APIPort)

	agentMonitor := monitor.NewAgentMonitor(crdClient, agentQuerier)

	go agentMonitor.Run(stopCh)

	if features.DefaultFeatureGate.Enabled(features.AntreaProxy) {
		go proxier.Run(stopCh)
	}

	apiServer, err := apiserver.New(
		agentQuerier,
		networkPolicyController,
		o.config.APIPort,
		o.config.EnablePrometheusMetrics,
		o.config.ClientConnection.Kubeconfig)
	if err != nil {
		return fmt.Errorf(""error when creating agent API server: %v"", err)
	}
	go apiServer.Run(stopCh)

	if features.DefaultFeatureGate.Enabled(features.Traceflow) {
		go ofClient.StartPacketInHandler(stopCh)
	}

	// Initialize flow exporter to start go routines to poll conntrack flows and export IPFIX flow records
	if features.DefaultFeatureGate.Enabled(features.FlowExporter) {
		connStore := connections.NewConnectionStore(
			connections.InitializeConnTrackDumper(nodeConfig, serviceCIDRNet, agentQuerier.GetOVSCtlClient(), o.config.OVSDatapathType),
			ifaceStore,
			serviceCIDRNet,
			proxier,
			o.pollInterval)
		pollDone := make(chan struct{})
		go connStore.Run(stopCh, pollDone)

		flowExporter := exporter.NewFlowExporter(
			flowrecords.NewFlowRecords(connStore),
			o.config.FlowExportFrequency)
		go wait.Until(func() { flowExporter.Export(o.flowCollector, stopCh, pollDone) }, 0, stopCh)
	}

	<-stopCh
	klog.Info(""Stopping Antrea agent"")
	return nil
}
",This sentence seems incomplete.,"The provided code snippet is a comprehensive implementation of the Antrea agent, which is responsible for managing network policies, routing, and other networking functionalities in Kubernetes clusters. The code initializes various components such as K8s clients, informers, and controllers, and sets up signal handling to ensure graceful shutdown. Here's a concise review of the key aspects:

### Key Components and Features

1. **K8s Clients and Informers**: 
   - The code creates K8s and CRD clients using `k8s.CreateClients` and `crdinformers.NewSharedInformerFactory`.
   - It also sets up informers for network policies and traceflows.

2. **Antrea Client**:
   - An Antrea client is created using `agent.NewAntreaClientProvider`.

3. **OVS Configuration**:
   - The code initializes an OVSDB connection and an OpenFlow client using `ovsconfig.NewOVSDBConnectionUDS` and `openflow.NewClient`.

4. **Network Configuration**:
   - The network configuration is set up based on the provided configuration options, including tunnel type, traffic encapsulation mode, and service CIDRs.

5. **Route Management**:
   - A route client is created to manage routes and encapsulations.

6. **Interface Store**:
   - An interface store is initialized to cache network interfaces managed by the node.

7. **Agent Initialization**:
   - The agent is initialized using `agent.NewInitializer`, which sets up the necessary components like OVS bridges, OpenFlow, and routing.

8. **Controllers**:
   - Various controllers are created for network policies, node routes, traceflows, and flow export.
   - The `NetworkPolicyController` listens for Pod updates and reconciles rules accordingly.
   - The `TraceflowController` handles traceflow creation and management.

9. **CNI Server**:
   - A CNI server is created to handle network namespace operations and updates.

10. **Signal Handling**:
    - Signal handlers are registered to handle SIGTERM and SIGINT, allowing the agent to gracefully shut down.

11. **Logging and Monitoring**:
    - Logging is configured to start a log file number monitor.
    - Monitoring is set up to track CRDs and agent queries.

12. **API Server**:
    - An API server is created to expose agent functionality via HTTP.

13. **Flow Exporter**:
    - If enabled, a flow exporter is initialized to poll conn"
67,"@@ -60,6 +60,10 @@ type Service interface {
 	// HPA management functions.
 	DescribeHPA(ctx context.Context, clientset, cluster, namespace, name string) (*k8sapiv1.HPA, error)
 	ResizeHPA(ctx context.Context, clientset, cluster, namespace, name string, sizing *k8sapiv1.ResizeHPARequest_Sizing) error
+
+	// Deployment management functions.
+	DescribeDeployment(ctx context.Context, clientset, cluster, namespace, name string) (*k8sapiv1.Deployment, error)
+	UpdateDeployment(ctx context.Context, clientset, cluster, namespace, name string, labels map[string]string, annotations map[string]string) error
 }
 
 type svc struct {","package k8s

// <!-- START clutchdoc -->
// description: Multi-clientset Kubernetes interface.
// <!-- END clutchdoc -->

import (
	""context""

	""github.com/golang/protobuf/ptypes""
	""github.com/golang/protobuf/ptypes/any""
	""github.com/uber-go/tally""
	""go.uber.org/zap""
	""k8s.io/client-go/tools/clientcmd""

	k8sconfigv1 ""github.com/lyft/clutch/backend/api/config/service/k8s/v1""
	k8sapiv1 ""github.com/lyft/clutch/backend/api/k8s/v1""
	""github.com/lyft/clutch/backend/service""

	_ ""k8s.io/client-go/plugin/pkg/client/auth/oidc""
)

const Name = ""clutch.service.k8s""

func New(cfg *any.Any, logger *zap.Logger, scope tally.Scope) (service.Service, error) {
	loadingRules := clientcmd.NewDefaultClientConfigLoadingRules()

	// Use the default kubeconfig (environment or well-known path) if kubeconfigs are not passed in.
	// https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/
	if cfg != nil {
		c := &k8sconfigv1.Config{}
		if err := ptypes.UnmarshalAny(cfg, c); err != nil {
			return nil, err
		}

		if c.Kubeconfigs != nil {
			loadingRules = &clientcmd.ClientConfigLoadingRules{
				Precedence: c.Kubeconfigs,
			}
		}
	}

	c, err := newClientsetManager(loadingRules)
	if err != nil {
		return nil, err
	}

	return NewWithClientsetManager(c, logger, scope)
}

type Service interface {
	// All names of clientsets.
	Clientsets() []string

	// Pod management functions.
	DescribePod(ctx context.Context, clientset, cluster, namespace, name string) (*k8sapiv1.Pod, error)
	DeletePod(ctx context.Context, clientset, cluster, namespace, name string) error
	ListPods(ctx context.Context, clientset, cluster, namespace string, listPodsOptions *k8sapiv1.ListPodsOptions) ([]*k8sapiv1.Pod, error)


	// Deployment management functions.
	DescribeDeployment(ctx context.Context, clientset, cluster, namespace, name string) (*k8sapiv1.Deployment, error)
	UpdateDeployment(ctx context.Context, clientset, cluster, namespace, name string, labels map[string]string, annotations map[string]string) error
	// HPA management functions.
	DescribeHPA(ctx context.Context, clientset, cluster, namespace, name string) (*k8sapiv1.HPA, error)
	ResizeHPA(ctx context.Context, clientset, cluster, namespace, name string, sizing *k8sapiv1.ResizeHPARequest_Sizing) error
}

type svc struct {
	manager ClientsetManager

	log   *zap.Logger
	scope tally.Scope
}

func NewWithClientsetManager(manager ClientsetManager, logger *zap.Logger, scope tally.Scope) (Service, error) {
	return &svc{manager: manager, log: logger, scope: scope}, nil
}

func (s *svc) Clientsets() []string {
	ret := make([]string, 0, len(s.manager.Clientsets()))
	for name := range s.manager.Clientsets() {
		ret = append(ret, name)
	}
	return ret
}
","I dont really like how I'm passing through `labels map[string]string, annotations map[string]string` here, will probably refactor this.","The suggested code change introduces a multi-clientset Kubernetes interface with methods for managing pods and deployments. The code includes a `New` function to create a service instance, a `Clientsets` method to retrieve all clientset names, and methods for pod and deployment management. The code is well-structured and follows best practices for Go programming."
68,"@@ -55,6 +55,16 @@ func nodeStatus(node *node.AlgorandFullNode) (res v1.NodeStatus, err error) {
 	}, nil
 }
 
+func txEncode(tx transactions.Transaction, ad transactions.ApplyData) (res v1.Transaction, err error) {
+	if tx.Type == protocol.PaymentTx {
+		return paymentTxEncode(tx, ad), nil
+	}
+	if tx.Type == protocol.KeyRegistrationTx {
+		return keyregTxEncode(tx, ad), nil
+	}
+	return v1.Transaction{}, errors.New(errUnknownTransactionType)
+}
+
 func paymentTxEncode(tx transactions.Transaction, ad transactions.ApplyData) v1.Transaction {
 	payment := v1.PaymentTransactionType{
 		To:           tx.Receiver.String(),","// Copyright (C) 2019 Algorand, Inc.
// This file is part of go-algorand
//
// go-algorand is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as
// published by the Free Software Foundation, either version 3 of the
// License, or (at your option) any later version.
//
// go-algorand is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Affero General Public License for more details.
//
// You should have received a copy of the GNU Affero General Public License
// along with go-algorand.  If not, see <https://www.gnu.org/licenses/>.

package handlers

import (
	""errors""
	""fmt""
	""net/http""
	""strconv""
	""time""

	""github.com/gorilla/mux""

	""github.com/algorand/go-algorand/agreement""
	""github.com/algorand/go-algorand/config""
	""github.com/algorand/go-algorand/crypto""
	""github.com/algorand/go-algorand/daemon/algod/api/server/lib""
	""github.com/algorand/go-algorand/daemon/algod/api/spec/v1""
	""github.com/algorand/go-algorand/data/basics""
	""github.com/algorand/go-algorand/data/bookkeeping""
	""github.com/algorand/go-algorand/data/transactions""
	""github.com/algorand/go-algorand/ledger""
	""github.com/algorand/go-algorand/node""
	""github.com/algorand/go-algorand/protocol""
)

func nodeStatus(node *node.AlgorandFullNode) (res v1.NodeStatus, err error) {
	stat, err := node.Status()
	if err != nil {
		return v1.NodeStatus{}, err
	}

	return v1.NodeStatus{
		LastRound:            uint64(stat.LastRound),
		LastVersion:          string(stat.LastVersion),
		NextVersion:          string(stat.NextVersion),
		NextVersionRound:     uint64(stat.NextVersionRound),
		NextVersionSupported: stat.NextVersionSupported,
		TimeSinceLastRound:   stat.TimeSinceLastRound().Nanoseconds(),
		CatchupTime:          stat.CatchupTime.Nanoseconds(),
func txEncode(tx transactions.Transaction, ad transactions.ApplyData) (res v1.Transaction, err error) {
	if tx.Type == protocol.PaymentTx {
		return paymentTxEncode(tx, ad), nil
	}
	if tx.Type == protocol.KeyRegistrationTx {
		return keyregTxEncode(tx, ad), nil
	}
	return v1.Transaction{}, errors.New(errUnknownTransactionType)
}

	}, nil
}

func paymentTxEncode(tx transactions.Transaction, ad transactions.ApplyData) v1.Transaction {
	payment := v1.PaymentTransactionType{
		To:           tx.Receiver.String(),
		Amount:       tx.TxAmount().Raw,
		ToRewards:    ad.ReceiverRewards.Raw,
		CloseRewards: ad.CloseRewards.Raw,
	}

	if tx.CloseRemainderTo != (basics.Address{}) {
		payment.CloseRemainderTo = tx.CloseRemainderTo.String()
		payment.CloseAmount = ad.ClosingAmount.Raw
	}

	return v1.Transaction{
		Type:        string(tx.Type),
		TxID:        tx.ID().String(),
		From:        tx.Src().String(),
		Fee:         tx.TxFee().Raw,
		FirstRound:  uint64(tx.First()),
		LastRound:   uint64(tx.Last()),
		Note:        tx.Aux(),
		Payment:     &payment,
		FromRewards: ad.SenderRewards.Raw,
		GenesisID:   tx.GenesisID,
		GenesisHash: tx.GenesisHash[:],
	}
}

func txWithStatusEncode(tr node.TxnWithStatus) v1.Transaction {
	s := paymentTxEncode(tr.Txn.Txn, tr.ApplyData)
	s.ConfirmedRound = uint64(tr.ConfirmedRound)
	s.PoolError = tr.PoolError
	return s
}

func blockEncode(b bookkeeping.Block, c agreement.Certificate) (v1.Block, error) {
	block := v1.Block{
		Hash:              crypto.Digest(b.Hash()).String(),
		PreviousBlockHash: crypto.Digest(b.Branch).String(),
		Seed:              crypto.Digest(b.Seed()).String(),
		Proposer:          c.Proposal.OriginalProposer.String(),
		Round:             uint64(b.Round()),
		TransactionsRoot:  b.TxnRoot.String(),
		RewardsRate:       b.RewardsRate,
		RewardsLevel:      b.RewardsLevel,
		RewardsResidue:    b.RewardsResidue,
		Timestamp:         b.TimeStamp,

		UpgradeState: v1.UpgradeState{
			CurrentProtocol:        string(b.CurrentProtocol),
			NextProtocol:           string(b.NextProtocol),
			NextProtocolApprovals:  b.NextProtocolApprovals,
			NextProtocolVoteBefore: uint64(b.NextProtocolVoteBefore),
			NextProtocolSwitchOn:   uint64(b.NextProtocolSwitchOn),
		},
		UpgradeVote: v1.UpgradeVote{
			UpgradePropose: string(b.UpgradePropose),
			UpgradeApprove: b.UpgradeApprove,
		},
	}

	// Transactions
	var txns []v1.Transaction
	payset, err := b.DecodePaysetWithAD()
	if err != nil {
		return v1.Block{}, err
	}

	for _, txn := range payset {
		tx := node.TxnWithStatus{
			Txn:            txn.SignedTxn,
			ConfirmedRound: b.Round(),
			ApplyData:      txn.ApplyData,
		}
		txns = append(txns, txWithStatusEncode(tx))
	}

	block.Transactions = v1.TransactionList{Transactions: txns}

	return block, nil
}

// Status is an httpHandler for route GET /v1/status
func Status(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation GET /v1/status GetStatus
	//---
	//     Summary: Gets the current node status.
	//     Produces:
	//     - application/json
	//     Schemes:
	//     - http
	//     Responses:
	//       200:
	//         ""$ref"": '#/responses/StatusResponse'
	//       500:
	//         description: Internal Error
	//         schema: {type: string}
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }
	nodeStatus, err := nodeStatus(ctx.Node)
	if err != nil {
		lib.ErrorResponse(w, http.StatusInternalServerError, err, errFailedRetrievingNodeStatus, ctx.Log)
		return
	}

	response := StatusResponse{&nodeStatus}
	SendJSON(response, w, ctx.Log)
}

// WaitForBlock is an httpHandler for route GET /v1/status/wait-for-block-after/{round:[0-9]+}
func WaitForBlock(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation GET /v1/status/wait-for-block-after/{round}/ WaitForBlock
	// ---
	//     Summary: Gets the node status after waiting for the given round.
	//     Description: Waits for a block to appear after round {round} and returns the node's status at the time.
	//     Produces:
	//     - application/json
	//     Schemes:
	//     - http
	//     Parameters:
	//       - name: round
	//         in: path
	//         type: integer
	//         format: int64
	//         minimum: 0
	//         required: true
	//         description: The round to wait until returning status
	//     Responses:
	//       200:
	//         ""$ref"": '#/responses/StatusResponse'
	//       400:
	//         description: Bad Request
	//         schema: {type: string}
	//       500:
	//         description: Internal Error
	//         schema: {type: string}
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }
	queryRound, err := strconv.ParseUint(mux.Vars(r)[""round""], 10, 64)
	if err != nil {
		lib.ErrorResponse(w, http.StatusBadRequest, err, errFailedParsingRoundNumber, ctx.Log)
		return
	}

	select {
	case <-time.After(1 * time.Minute):
	case <-ctx.Node.Ledger().Wait(basics.Round(queryRound + 1)):
	}

	nodeStatus, err := nodeStatus(ctx.Node)
	if err != nil {
		lib.ErrorResponse(w, http.StatusInternalServerError, err, errFailedRetrievingNodeStatus, ctx.Log)
		return
	}

	response := StatusResponse{&nodeStatus}
	SendJSON(response, w, ctx.Log)
}

// RawTransaction is an httpHandler for route POST /v1/transactions
func RawTransaction(ctx lib.ReqContext, w http.ResponseWriter, r *http.Request) {
	// swagger:operation POST /v1/transactions RawTransaction
	// ---
	//     Summary: Broadcasts a raw transaction to the network.
	//     Produces:
	//     - application/json
	//     Consumes:
	//     - application/x-binary
	//     Schemes:
	//     - http
	//     Parameters:
	//       - name: rawtxn
	//         in: body
	//         schema:
	//           type: string
	//           format: binary
	//         required: true
	//         description: The byte encoded signed transaction to broadcast to network
	//     Responses:
	//       200:
	//         ""$ref"": ""#/responses/TransactionIDResponse""
	//       400:
	//         description: Bad Request
	//         schema: {type: string}
	//       500:
	//         description: Internal Error
	//         schema: {type: string}
	//       401: { description: Invalid API Token }
	//       default: { description: Unknown Error }
	var st transactions.SignedTxn
	err := protocol.NewDecoder(r.Body).Decode(&st)
	if err != nil {
		lib.ErrorResponse(w, http.StatusBadRequest, err, err.Error(), ctx.Log)
		return
	}

	txid, err := ctx.Node.BroadcastSignedTxn(st)",Use a case statement here to switch between the different transaction types.,"The suggested code changes are focused on improving the `nodeStatus` function and adding a new handler `RawTransaction`. Here's a concise review:

### `nodeStatus` Function
- **Functionality**: The function retrieves the current node status from the provided `node.AlgorandFullNode`.
- **Return Type**: It returns a `v1.NodeStatus` struct containing various node metrics.
- **Error Handling**: It checks if there is an error retrieving the node status and returns an appropriate error response if so.

### `txEncode` Function
- **Functionality**: Encodes a transaction into a `v1.Transaction` struct.
- **Handling Different Transaction Types**: It supports both `PaymentTx` and `KeyRegistrationTx`, returning a specific struct for each type.
- **Error Handling**: Returns an error if the transaction type is unknown.

### `paymentTxEncode` Function
- **Functionality**: Encodes a payment transaction into a `v1.PaymentTransactionType` struct.
- **Fields**: Includes fields like `To`, `Amount`, `ToRewards`, `CloseRewards`, `CloseRemainderTo`, and `CloseAmount`.
- **Error Handling**: Returns an error if the transaction type is unknown.

### `txWithStatusEncode` Function
- **Functionality**: Encodes a `node.TxnWithStatus` into a `v1.Transaction` struct.
- **Fields**: Includes fields like `ConfirmedRound`, `PoolError`, and the transaction details from `paymentTxEncode`.

### `blockEncode` Function
- **Functionality**: Encodes a `bookkeeping.Block` and its associated `agreement.Certificate` into a `v1.Block` struct.
- **Fields**: Includes fields like `Hash`, `PreviousBlockHash`, `Seed`, `Proposer`, `Round`, `TransactionsRoot`, and various reward-related fields.
- **Transactions**: Decodes the payset from the block and encodes each transaction using `txWithStatusEncode`.

### `Status` Handler
- **Functionality**: Handles GET requests to `/v1/status` to return the current node status.
- **Error Handling**: Checks for errors retrieving the node status and returns an appropriate error response.

### `WaitForBlock` Handler
- **Functionality**: Handles GET requests to `/v1/status/wait-for-block-after/{round}` to wait for a block after a specified round and then return the node status.
- **Error Handling**: Checks for errors parsing the round number and waiting for the"
69,"@@ -192,7 +192,17 @@ class MultiBackend extends AbstractBase implements \Zend\Log\LoggerAwareInterfac
     {
         $items = [];
         foreach ($ids as $id) {
-            $items[] = $this->getStatus($id);
+            try {
+                $items[] = $this->getStatus($id);
+            } catch (ILSException $e) {
+                $items[] = [
+                    [
+                        'id' => $id,
+                        'error' => 'An error has occurred',
+                        'availability_message' => 'status_unknown_message'
+                    ]
+                ];
+            }
         }
         return $items;
     }","<?php
/**
 * Multiple Backend Driver.
 *
 * PHP version 7
 *
 * Copyright (C) The National Library of Finland 2012-2018.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2,
 * as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 *
 * @category VuFind
 * @package  ILSdrivers
 * @author   Ere Maijala <ere.maijala@helsinki.fi>
 * @author   Demian Katz <demian.katz@villanova.edu>
 * @license  http://opensource.org/licenses/gpl-2.0.php GNU General Public License
 * @link     https://vufind.org/wiki/development:plugins:ils_drivers Wiki
 */
namespace VuFind\ILS\Driver;

use VuFind\Exception\ILS as ILSException;

/**
 * Multiple Backend Driver.
 *
 * This driver allows to use multiple backends determined by a record id or
 * user id prefix (e.g. source.12345).
 *
 * @category VuFind
 * @package  ILSdrivers
 * @author   Ere Maijala <ere.maijala@helsinki.fi>
 * @license  http://opensource.org/licenses/gpl-2.0.php GNU General Public License
 * @link     https://vufind.org/wiki/development:plugins:ils_drivers Wiki
 */
class MultiBackend extends AbstractBase implements \Zend\Log\LoggerAwareInterface
{
    use \VuFind\Log\LoggerAwareTrait {
        logError as error;
    }
    /**
     * The array of configured driver names.
     *
     * @var string[]
     */
    protected $drivers = [];

    /**
     * The default driver to use
     *
     * @var string
     */
    protected $defaultDriver;

    /**
     * The path to the driver configurations relative to the config path
     *
     * @var string
     */
    protected $driversConfigPath;

    /**
     * The array of cached drivers
     *
     * @var object[]
     */
    protected $driverCache = [];

    /**
     * The array of driver configuration options.
     *
     * @var string[]
     */
    protected $config = [];

    /**
     * Configuration loader
     *
     * @var \VuFind\Config\PluginManager
     */
    protected $configLoader;

    /**
     * ILS authenticator
     *
     * @var \VuFind\Auth\ILSAuthenticator
     */
    protected $ilsAuth;

    /**
     * ILS driver manager
     *
     * @var PluginManager
     */
    protected $driverManager;

    /**
     * Constructor
     *
     * @param \VuFind\Config\PluginManager  $configLoader Configuration loader
     * @param \VuFind\Auth\ILSAuthenticator $ilsAuth      ILS authenticator
     * @param PluginManager                 $dm           ILS driver manager
     */
    public function __construct(\VuFind\Config\PluginManager $configLoader,
        \VuFind\Auth\ILSAuthenticator $ilsAuth, PluginManager $dm
    ) {
        $this->configLoader = $configLoader;
        $this->ilsAuth = $ilsAuth;
        $this->driverManager = $dm;
    }

    /**
     * Set the driver configuration.
     *
     * @param Config $config The configuration to be set
     *
     * @return void
     */
    public function setConfig($config)
    {
        $this->config = $config;
    }

    /**
     * Initialize the driver.
     *
     * Validate configuration and perform all resource-intensive tasks needed to
     * make the driver active.
     *
     * @throws ILSException
     * @return void
     */
    public function init()
    {
        if (empty($this->config)) {
            throw new ILSException('Configuration needs to be set.');
        }
        $this->drivers = $this->config['Drivers'];
        $this->defaultDriver = isset($this->config['General']['default_driver'])
            ? $this->config['General']['default_driver']
            : null;
        $this->driversConfigPath
            = isset($this->config['General']['drivers_config_path'])
            ? $this->config['General']['drivers_config_path']
            : null;
    }

    /**
     * Get Status
     *
     * This is responsible for retrieving the status information of a certain
     * record.
     *
     * @param string $id The record id to retrieve the holdings for
     *
     * @throws ILSException
     * @return mixed     On success, an associative array with the following keys:
     * id, availability (boolean), status, location, reserve, callnumber.
     */
    public function getStatus($id)
    {
        $source = $this->getSource($id);
        $driver = $this->getDriver($source);
        if ($driver) {
            $status = $driver->getStatus($this->getLocalId($id));
            return $this->addIdPrefixes($status, $source);
        }
        return [];
    }

    /**
     * Get Statuses
     *
     * This is responsible for retrieving the status information for a
     * collection of records.
     *
     * @param array $ids The array of record ids to retrieve the status for
     *
     * @throws ILSException
     * @return array     An array of getStatus() return values on success.
     */
    public function getStatuses($ids)
            try {
                $items[] = $this->getStatus($id);
            } catch (ILSException $e) {
                $items[] = [
                    [
                        'id' => $id,
                        'error' => 'An error has occurred',
                        'availability_message' => 'status_unknown_message'
                    ]
                ];
            }
    {
        $items = [];
        foreach ($ids as $id) {
            $items[] = $this->getStatus($id);
        }
        return $items;
    }

    /**
     * Get Holding
     *
     * This is responsible for retrieving the holding information of a certain
     * record.
     *
     * @param string $id     The record id to retrieve the holdings for
     * @param array  $patron Patron data
     *
     * @return array         On success, an associative array with the following
     * keys: id, availability (boolean), status, location, reserve, callnumber,
     * duedate, number, barcode.
     */
    public function getHolding($id, array $patron = null)
    {
        $source = $this->getSource($id);
        $driver = $this->getDriver($source);
        if ($driver) {
            // If the patron belongs to another source, just pass on an empty array
            // to indicate that the patron has logged in but is not available for the
            // current catalog.
            if ($patron && $this->getSource($patron['cat_username']) !== $source) {
                $patron = [];
            }
            $holdings = $driver->getHolding(
                $this->getLocalId($id),
                $this->stripIdPrefixes($patron, $source)
            );
            return $this->addIdPrefixes($holdings, $source);
        }
        return [];
    }

    /**
     * Get Purchase History
     *
     * This is responsible for retrieving the acquisitions history data for the
     * specific record (usually recently received issues of a serial).
     *
     * @param string $id The record id to retrieve the info for
     *
     * @throws ILSException
     * @return array     An array with the acquisitions data on success.
     */
    public function getPurchaseHistory($id)
    {
        $source = $this->getSource($id);
        $driver = $this->getDriver($source);
        if ($driver) {
            return $driver->getPurchaseHistory($this->getLocalId($id));
        }
        return [];
    }

    /**
     * Get available login targets (drivers enabled for login)
     *
     * @return string[] Source ID's
     */
    public function getLoginDrivers()
    {
        return isset($this->config['Login']['drivers'])
            ? $this->config['Login']['drivers']
            : [];
    }

    /**
     * Get default login driver
     *
     * @return string Default login driver or empty string
     */
    public function getDefaultLoginDriver()
    {
        if (isset($this->config['Login']['default_driver'])) {
            return $this->config['Login']['default_driver'];
        }
        $drivers = $this->getLoginDrivers();
        if ($drivers) {
            return $drivers[0];
        }
        return '';
    }

    /**
     * Get New Items
     *
     * Retrieve the IDs of items recently added to the catalog.
     *
     * @param int $page    Page number of results to retrieve (counting starts at 1)
     * @param int $limit   The size of each page of results to retrieve
     * @param int $daysOld The maximum age of records to retrieve in days (max. 30)
     * @param int $fundId  optional fund ID to use for limiting results (use a value
     * returned by getFunds, or exclude for no limit); note that ""fund"" may be a
     * misnomer - if funds are not an appropriate way to limit your new item
     * results, you can return a different set of values from getFunds. The
     * important thing is that this parameter supports an ID returned by getFunds,
     * whatever that may mean.
     *
     * @return array       Associative array with 'count' and 'results' keys
     */
    public function getNewItems($page, $limit, $daysOld, $fundId = null)
    {
        $driver = $this->getDriver($this->defaultDriver);
        if ($driver) {
            $result = $driver->getNewItems($page, $limit, $daysOld, $fundId);
            if (isset($result['results'])) {
                $result['results']
                    = $this->addIdPrefixes($result['results'], $this->defaultDriver);
            }
            return $result;
        }
        return [];
    }

    /**
     * Get Departments
     *
     * Obtain a list of departments for use in limiting the reserves list.
     *
     * @return array An associative array with key = dept. ID, value = dept. name.
     */
    public function getDepartments()
    {
        $driver = $this->getDriver($this->defaultDriver);
        if ($driver) {
            return $driver->getDepartments();
        }
        return [];
    }

    /**
     * Get Instructors
     *
     * Obtain a list of instructors for use in limiting the reserves list.
     *
     * @return array An associative array with key = ID, value = name.
     */
    public function getInstructors()
    {
        $driver = $this->getDriver($this->defaultDriver);
        if ($driver) {
            return $driver->getInstructors();
        }
        return [];
    }

    /**
     * Get Courses
     *
     * Obtain a list of courses for use in limiting the reserves list.
     *
     * @return array An associative array with key = ID, value = name.
     */
    public function getCourses()
    {
        $driver = $this->getDriver($this->defaultDriver);
        if ($driver) {
            return $driver->getCourses();
        }
        return [];
    }

    /**
     * Find Reserves
     *
     * Obtain information on course reserves.
     *
     * @param string $course ID from getCourses (empty string to match all)
     * @param string $inst   ID from getInstructors (empty string to match all)
     * @param string $dept   ID from getDepartments (empty string to match all)
     *
     * @return mixed An array of associative arrays representing reserve items
     */
    public function findReserves($course, $inst, $dept)
    {
        $driver = $this->getDriver($this->defaultDriver);
        if ($driver) {
            return $this->addIdPrefixes(
                $driver->findReserves($course, $inst, $dept),
                $this->defaultDriver,
                ['BIB_ID']
            );
        }
        return [];
    }

    /**
     * Get Patron Profile
     *
     * This is responsible for retrieving the profile for a specific patron.
     *
     * @param array $patron The patron array","If you find a way to display the full unknown message above, it may be necessary to devise a different mechanism for filling it in in this situation. Maybe any entry with 'error' set automatically gets 'availability_message' set to unknown.","The suggested code change appears to be a refactoring of the `MultiBackend` class in the VuFind ILS driver system. Here's a concise review:

### Key Changes and Improvements:

1. **Use of Traits**: The class now uses the `\VuFind\Log\LoggerAwareTrait`, which provides logging functionality. This is a good practice for adding logging capabilities to classes.

2. **Constructor Parameters**: The constructor now accepts parameters for `configLoader`, `ilsAuth`, and `driverManager`. This makes the class more flexible and easier to test.

3. **Initialization Method**: The `init` method validates the configuration and sets up the necessary properties. It also initializes the driver cache.

4. **Status Retrieval Methods**: The `getStatus` and `getStatuses` methods now handle the retrieval of status information from the appropriate driver based on the record ID. They also add ID prefixes to the returned data.

5. **Holding Retrieval Method**: The `getHolding` method retrieves holding information and handles patron-specific data by checking the source of the patron.

6. **Purchase History Method**: The `getPurchaseHistory` method retrieves purchase history data from the appropriate driver.

7. **Login Methods**: The `getLoginDrivers`, `getDefaultLoginDriver`, and related methods manage login configurations.

8. **New Items Method**: The `getNewItems` method retrieves new items from the default driver and adds ID prefixes to the results.

9. **Department, Instructor, and Course Methods**: These methods retrieve department, instructor, and course lists from the default driver.

10. **Reserve Search Method**: The `findReserves` method searches for course reserves and adds ID prefixes to the results.

### Potential Enhancements:

- **Error Handling**: Consider adding more robust error handling, especially around driver initialization and method calls.
- **Configuration Validation**: Ensure that the configuration is validated thoroughly before use.
- **Performance Optimization**: Optimize the caching mechanism to reduce redundant driver instantiation.
- **Documentation**: Improve the documentation to explain the purpose and usage of each method.

Overall, the refactoring improves the structure and maintainability of the `MultiBackend` class, making it easier to extend and integrate with other parts of the VuFind system."
70,"@@ -0,0 +1,13 @@
+namespace Datadog.Trace.Agent
+{
+    internal static class TraceRequestDecorator
+    {
+        public static void AddHeaders(IApiRequest request)
+        {
+            request.AddHeader(AgentHttpHeaderNames.Language, "".NET"");
+            request.AddHeader(AgentHttpHeaderNames.TracerVersion, TracerConstants.AssemblyVersion);
+            // don't add automatic instrumentation to requests from datadog code
+            request.AddHeader(HttpHeaderNames.TracingEnabled, ""false"");
+        }
+    }
+}","namespace Datadog.Trace.Agent
{
    internal static class TraceRequestDecorator
    {
        public static void AddHeaders(IApiRequest request)
        {
            request.AddHeader(AgentHttpHeaderNames.Language, "".NET"");
            request.AddHeader(AgentHttpHeaderNames.TracerVersion, TracerConstants.AssemblyVersion);
            // don't add automatic instrumentation to requests from datadog code
            request.AddHeader(HttpHeaderNames.TracingEnabled, ""false"");
        }
    }
}

","This isn't a big deal, but without a full Decorator pattern, this feels a little weird to me. Maybe this could be `IApiRequestExtensions`?","The suggested code change adds headers to an `IApiRequest` object in the `Datadog.Trace.Agent` namespace. It sets the `Language` header to "".NET"", the `TracerVersion` header to the version of the tracer assembly, and disables automatic instrumentation by setting the `TracingEnabled` header to ""false"". This ensures that the request is not instrumented by the tracer itself, which can be useful for avoiding infinite loops or other issues related to automatic instrumentation. The code is concise and easy to understand."
71,"@@ -46,7 +46,7 @@ namespace Microsoft.AspNetCore.Server.Kestrel.Core.Internal
 
             // REVIEW: This task should be tracked by the server for graceful shutdown
             // Today it's handled specifically for http but not for aribitrary middleware
-            _ = Execute(new DefaultConnectionContext(connection));
+            _ = Execute(connection);
         }
 
         private async Task Execute(ConnectionContext connectionContext)","﻿// Copyright (c) .NET Foundation. All rights reserved.
// Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.

using System;
using System.Buffers;
using System.IO.Pipelines;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.AspNetCore.Http.Features;
using Microsoft.AspNetCore.Connections;
using Microsoft.AspNetCore.Connections.Features;
using Microsoft.AspNetCore.Server.Kestrel.Core.Internal.Infrastructure;
using Microsoft.AspNetCore.Server.Kestrel.Transport.Abstractions.Internal;
using Microsoft.Extensions.Logging;

namespace Microsoft.AspNetCore.Server.Kestrel.Core.Internal
{
    public class ConnectionDispatcher : IConnectionDispatcher
    {
        private readonly ServiceContext _serviceContext;
        private readonly ConnectionDelegate _connectionDelegate;

        public ConnectionDispatcher(ServiceContext serviceContext, ConnectionDelegate connectionDelegate)
        {
            _serviceContext = serviceContext;
            _connectionDelegate = connectionDelegate;
        }

        private IKestrelTrace Log => _serviceContext.Log;

        public void OnConnection(TransportConnection connection)
        {
            // REVIEW: Unfortunately, we still need to use the service context to create the pipes since the settings
            // for the scheduler and limits are specified here
            var inputOptions = GetInputPipeOptions(_serviceContext, connection.MemoryPool, connection.InputWriterScheduler);
            var outputOptions = GetOutputPipeOptions(_serviceContext, connection.MemoryPool, connection.OutputReaderScheduler);

            var pair = DuplexPipe.CreateConnectionPair(inputOptions, outputOptions);

            // Set the transport and connection id
            connection.ConnectionId = CorrelationIdGenerator.GetNextId();
            connection.Transport = pair.Transport;

            // This *must* be set before returning from OnConnection
            connection.Application = pair.Application;
            _ = Execute(connection);

            // REVIEW: This task should be tracked by the server for graceful shutdown
            // Today it's handled specifically for http but not for aribitrary middleware
            _ = Execute(new DefaultConnectionContext(connection));
        }

        private async Task Execute(ConnectionContext connectionContext)
        {
            using (BeginConnectionScope(connectionContext))
            {
                Log.ConnectionStart(connectionContext.ConnectionId);

                try
                {
                    await _connectionDelegate(connectionContext);
                }
                catch (Exception ex)
                {
                    Log.LogCritical(0, ex, $""{nameof(ConnectionDispatcher)}.{nameof(Execute)}() {connectionContext.ConnectionId}"");
                }

                Log.ConnectionStop(connectionContext.ConnectionId);
            }
        }

        private IDisposable BeginConnectionScope(ConnectionContext connectionContext)
        {
            if (Log.IsEnabled(LogLevel.Critical))
            {
                return Log.BeginScope(new ConnectionLogScope(connectionContext.ConnectionId));
            }

            return null;
        }

        // Internal for testing
        internal static PipeOptions GetInputPipeOptions(ServiceContext serviceContext, MemoryPool<byte> memoryPool, PipeScheduler writerScheduler) => new PipeOptions
        (
            pool: memoryPool,
            readerScheduler: serviceContext.Scheduler,
            writerScheduler: writerScheduler,
            pauseWriterThreshold: serviceContext.ServerOptions.Limits.MaxRequestBufferSize ?? 0,
            resumeWriterThreshold: serviceContext.ServerOptions.Limits.MaxRequestBufferSize ?? 0,
            useSynchronizationContext: false
        );

        internal static PipeOptions GetOutputPipeOptions(ServiceContext serviceContext, MemoryPool<byte> memoryPool, PipeScheduler readerScheduler) => new PipeOptions
        (
            pool: memoryPool,
            readerScheduler: readerScheduler,
            writerScheduler: serviceContext.Scheduler,
            pauseWriterThreshold: GetOutputResponseBufferSize(serviceContext),
            resumeWriterThreshold: GetOutputResponseBufferSize(serviceContext),
            useSynchronizationContext: false
        );

        private static long GetOutputResponseBufferSize(ServiceContext serviceContext)
        {
            var bufferSize = serviceContext.ServerOptions.Limits.MaxResponseBufferSize;
            if (bufferSize == 0)
            {
                // 0 = no buffering so we need to configure the pipe so the the writer waits on the reader directly
                return 1;
            }

            // null means that we have no back pressure
            return bufferSize ?? 0;
        }
    }
}
",Where is DefaultConnectionContext even used now?,"The suggested code change is focused on implementing the `ConnectionDispatcher` class, which is responsible for managing connections in Kestrel. The key points of the review are:

1. **Service Context Dependency**: The class depends on a `ServiceContext`, which contains various settings and configurations needed for the dispatcher.

2. **Connection Delegate**: It takes a `ConnectionDelegate` as a parameter, which is used to handle incoming connections.

3. **Pipe Creation**: The dispatcher creates a duplex pipe pair using the provided memory pool and scheduler options. This setup is crucial for handling both input and output streams.

4. **Connection Management**: The dispatcher sets the connection ID and transport for the connection. It also ensures that the application context is properly set before executing the connection delegate.

5. **Error Handling**: The `Execute` method includes error logging using the `IKestrelTrace` interface.

6. **Scope Management**: A scope is created using the `Log.BeginScope` method if the log level is critical, which helps in tracing the connection lifecycle.

7. **Pipe Options**: The `GetInputPipeOptions` and `GetOutputPipeOptions` methods configure the input and output pipes with appropriate buffer sizes and schedulers.

8. **Buffer Size Calculation**: The `GetOutputResponseBufferSize` method calculates the buffer size for the output pipe based on the server's configuration limits.

Overall, the code is well-structured and follows best practices for managing connections in Kestrel. The use of the `ServiceContext` ensures that all necessary settings are available for configuring the pipes and handling connections effectively."
72,"@@ -130,7 +130,7 @@ func verifyBlockSignature(ctx context.Context, view FaultStateView, blk block.Bl
 	if err != nil {
 		panic(errors.Wrapf(err, ""failed to inspect miner addresses""))
 	}
-	err = state.NewSignatureValidator(view).ValidateSignature(ctx, blk.SignatureData(), worker, blk.BlockSig)
+	err = state.NewSignatureValidator(view).ValidateSignature(ctx, blk.SignatureData(), worker, *blk.BlockSig)
 	if err != nil {
 		return errors.Wrapf(err, ""no consensus fault: block %s signature invalid"", blk.Cid())
 	}","package slashing

import (
	""bytes""
	""context""
	""fmt""

	""github.com/filecoin-project/go-address""
	""github.com/filecoin-project/specs-actors/actors/abi""
	""github.com/filecoin-project/specs-actors/actors/runtime""
	""github.com/pkg/errors""

	""github.com/filecoin-project/go-filecoin/internal/pkg/block""
	""github.com/filecoin-project/go-filecoin/internal/pkg/chain""
	""github.com/filecoin-project/go-filecoin/internal/pkg/encoding""
	""github.com/filecoin-project/go-filecoin/internal/pkg/state""
)

type FaultStateView interface {
	state.AccountStateView
	MinerControlAddresses(ctx context.Context, maddr address.Address) (owner, worker address.Address, err error)
}

// Chain state required for checking consensus fault reports.
type chainReader interface {
	GetTipSet(block.TipSetKey) (block.TipSet, error)
}

// Checks the validity of reported consensus faults.
type ConsensusFaultChecker struct {
	chain chainReader
}

func NewFaultChecker(chain chainReader) *ConsensusFaultChecker {
	return &ConsensusFaultChecker{chain: chain}
}

// Checks the validity of a consensus fault reported by serialized block headers h1, h2, and optional
// common-ancestor witness h3.
func (s *ConsensusFaultChecker) VerifyConsensusFault(ctx context.Context, h1, h2, extra []byte, head block.TipSetKey, view FaultStateView, earliest abi.ChainEpoch) (*runtime.ConsensusFault, error) {
	if bytes.Equal(h1, h2) {
		return nil, fmt.Errorf(""no consensus fault: blocks identical"")
	}

	var b1, b2, b3 block.Block
	innerErr := encoding.Decode(h1, &b1)
	if innerErr != nil {
		return nil, errors.Wrapf(innerErr, ""failed to decode h1"")
	}
	innerErr = encoding.Decode(h2, &b2)
	if innerErr != nil {
		return nil, errors.Wrapf(innerErr, ""failed to decode h2"")
	}
	if len(extra) > 0 {
		innerErr = encoding.Decode(extra, &b3)
		if innerErr != nil {
			return nil, errors.Wrapf(innerErr, ""failed to decode extra"")
		}
	}
	// Block syntax is not validated. This implements the strictest check possible, and is also the simplest check
	// possible.
	// This means that blocks that could never have been included in the chain (e.g. with an empty parent state)
	// are still fault-able.

	if b1.Miner != b2.Miner {
		return nil, fmt.Errorf(""no consensus fault: miners differ"")
	}
	if b1.Height > b2.Height {
		return nil, fmt.Errorf(""no consensus fault: first block is higher than second"")
	}

	// Check the basic fault conditions first, defer the (expensive) signature and chain history check until last.
	var fault *runtime.ConsensusFault

	// Double-fork mining fault: two blocks at the same epoch.
	// It is not necessary to present a common ancestor of the blocks.
	if b1.Height == b2.Height {
		fault = &runtime.ConsensusFault{
			Target: b1.Miner,
			Epoch:  b2.Height,
			Type:   runtime.ConsensusFaultDoubleForkMining,
		}
	}
	// Time-offset mining fault: two blocks with the same parent but different epochs.
	// The height check is redundant at time of writing, but included for robustness to future changes to this method.
	// The blocks have a common ancestor by definition (the parent).
	if b1.Parents.Equals(b2.Parents) && b1.Height != b2.Height {
		fault = &runtime.ConsensusFault{
			Target: b1.Miner,
			Epoch:  b2.Height,
			Type:   runtime.ConsensusFaultTimeOffsetMining,
		}
	}
	// Parent-grinding fault: one block’s parent is a tipset that provably should have included some block but does not.
	// The provable case is that two blocks are mined in consecutive epochs and the later one does not include the
	// earlier one as a parent.
	// B3 must prove that the higher block (B2) has grandparent equal to B1's parent.
	if b1.Height+1 == b2.Height && !b2.Parents.Has(b1.Cid()) && b2.Parents.Has(b3.Cid()) && b3.Parents.Equals(b1.Parents) {
		fault = &runtime.ConsensusFault{
			Target: b1.Miner,
			Epoch:  b2.Height,
			Type:   runtime.ConsensusFaultParentGrinding,
		}
	}
	if fault == nil {
		return nil, fmt.Errorf(""no consensus fault: blocks are ok"")
	}

	// Expensive validation: signatures and chain history.

	err := verifyBlockSignature(ctx, view, b1)
	if err != nil {
		return nil, err
	}
	err = verifyBlockSignature(ctx, view, b2)
	if err != nil {
		return nil, err
	}
	err = verifyOneBlockInChain(ctx, s.chain, head, b1, b2, earliest)
	if err != nil {
		return nil, err
	}

	return fault, nil
}

// Checks whether a block header is correctly signed in the context of the parent state to which it refers.
func verifyBlockSignature(ctx context.Context, view FaultStateView, blk block.Block) error {
	_, worker, err := view.MinerControlAddresses(ctx, blk.Miner)
	err = state.NewSignatureValidator(view).ValidateSignature(ctx, blk.SignatureData(), worker, *blk.BlockSig)
	if err != nil {
		panic(errors.Wrapf(err, ""failed to inspect miner addresses""))
	}
	err = state.NewSignatureValidator(view).ValidateSignature(ctx, blk.SignatureData(), worker, blk.BlockSig)
	if err != nil {
		return errors.Wrapf(err, ""no consensus fault: block %s signature invalid"", blk.Cid())
	}
	return err
}

// Checks whether at least one of b1, b2 appear in the chain defined by `head`.
func verifyOneBlockInChain(ctx context.Context, chn chainReader, head block.TipSetKey, b1 block.Block, b2 block.Block, earliest abi.ChainEpoch) error {
	if chainHasB1, err := chainContainsBlock(ctx, chn, head, b1, earliest); err != nil {
		panic(errors.Wrapf(err, ""failed to inspect chain"")) // This idiosyncratic failure shouldn't go on chain
	} else if chainHasB1 {
		return nil
	}
	if chainHasB2, err := chainContainsBlock(ctx, chn, head, b2, earliest); err != nil {
		panic(errors.Wrapf(err, ""failed to inspect chain""))
	} else if chainHasB2 {
		return nil
	}
	return fmt.Errorf(""no consensus fault: neither block in chain since %d"", earliest)
}

func chainContainsBlock(ctx context.Context, chn chainReader, head block.TipSetKey, blk block.Block, earliest abi.ChainEpoch) (bool, error) {
	if blk.Height < earliest { // Short-circuit
		return false, nil
	}
	ts, err := chn.GetTipSet(head)
	if err != nil {
		return false, err
	}

	itr := chain.IterAncestors(ctx, chn, ts)
	for ts := itr.Value(); !itr.Complete(); err = itr.Next() {
		if err != nil {
			return false, err
		}
		height, err := ts.Height()
		if err != nil {
			return false, err
		}
		if height < earliest {
			return false, nil
		}
		if ts.Key().Has(blk.Cid()) {
			return true, nil
		}
	}
	return false, nil
}
",`*blk.BlockSig` will panic if nil. Is this guaranteed not to be?,"The suggested code change introduces a new `ConsensusFaultChecker` struct that encapsulates the logic for verifying consensus faults based on serialized block headers. The checker performs several checks to determine if a reported fault is valid:

1. **Block Syntax Validation**: The code assumes that the block headers are syntactically correct and does not validate them further.

2. **Basic Fault Conditions**: It checks for double-fork mining, time-offset mining, and parent-grinding faults.

3. **Signature Verification**: It verifies the signatures of the blocks using the provided `FaultStateView`.

4. **Chain History Verification**: It checks if at least one of the blocks appears in the chain starting from the specified `head` tip set key.

5. **Error Handling**: The code includes error handling for decoding block headers, signature verification, and chain history checks.

6. **Concurrency Safety**: The code uses `panic` to handle errors that should not occur on the chain, ensuring that the program does not continue in an inconsistent state.

Overall, the code provides a comprehensive approach to validating consensus faults based on block headers, ensuring that the chain integrity is maintained."
73,"@@ -1809,6 +1809,12 @@ def disable_user():
     username = data.get('username')
     _validate_username(username)
 
+    if g.auth.user == username:
+        raise ApiException(
+            403,
+            ""Can't disable your own account.""
+            )
+
     resp = requests.patch(""%s%s/"" % (user_modify_api, username) , headers=auth_headers,
         data=json.dumps({
             'is_active' : False","@as_json
def payments_update_payment():
    if not HAVE_PAYMENTS:
        raise ApiException(requests.codes.not_found, ""Payments not enabled"")

    stripe_token = request.values.get('token')
    if not stripe_token:
        raise ApiException(requests.codes.bad_request, ""Missing token"")

    customer = _get_or_create_customer()
    customer.source = stripe_token

    try:
        customer.save()
    except stripe.InvalidRequestError as ex:
        raise ApiException(requests.codes.bad_request, str(ex))

    return dict()

@app.route('/api/invite/', methods=['GET'])
@api(require_login=False)
@as_json
def invitation_user_list():
    invitations = (
        db.session.query(Invitation, Package)
        .filter_by(email=g.auth.email.lower())
        .join(Invitation.package)
        .all()
    )
    return dict(invitations=[dict(invitation_id=invite.id,
                                  owner=package.owner,
                                  package=package.name,
                                  email=invite.email,
                                  invited_at=invite.invited_at)
                             for invite, package in invitations])

@app.route('/api/invite/<owner>/<package_name>/', methods=['GET'])
@api()
@as_json
def invitation_package_list(owner, package_name):
    package = _get_package(g.auth, owner, package_name)
    invitations = (
        Invitation.query
        .filter_by(package_id=package.id)
    )

    return dict(invitations=[dict(invitation_id=invite.id,
                                  owner=package.owner,
                                  package=package.name,
                                  email=invite.email,
                                  invited_at=invite.invited_at)
                             for invite in invitations])

@app.route('/api/log', methods=['POST'])
@api(require_login=False, schema=LOG_SCHEMA)
@as_json
def client_log():
    data = request.get_json()
    for event in data:
        _mp_track(**event)

    return dict()

@app.route('/api/users/list', methods=['GET'])
@api(enabled=ENABLE_USER_ENDPOINTS, require_admin=True)
@as_json
def list_users():
    auth_headers = {
        AUTHORIZATION_HEADER: g.auth_header
    }

    user_list_api = ""%s/accounts/users"" % QUILT_AUTH_URL

    resp = requests.get(user_list_api, headers=auth_headers)

    if resp.status_code == requests.codes.not_found:
        raise ApiException(
            requests.codes.not_found,
            ""Cannot list users""
            )
    elif resp.status_code != requests.codes.ok:
        raise ApiException(
            requests.codes.server_error,
            ""Unknown error""
            )

    return resp.json()

@app.route('/api/users/list_detailed', methods=['GET'])
@api(enabled=ENABLE_USER_ENDPOINTS, require_admin=True)
@as_json
def list_users_detailed():
    package_counts_query = (
        db.session.query(Package.owner, sa.func.count(Package.owner))
        .group_by(Package.owner)
        )
    package_counts = dict(package_counts_query)

    events = (
        db.session.query(Event.user, Event.type, sa.func.count(Event.type))
        .group_by(Event.user, Event.type)
        )

    event_results = defaultdict(int)
    for event_user, event_type, event_count in events:
        event_results[(event_user, event_type)] = event_count

    # replicate code from list_users since endpoints aren't callable from each other
    auth_headers = {
        AUTHORIZATION_HEADER: g.auth_header
    }

    user_list_api = ""%s/accounts/users"" % QUILT_AUTH_URL

    users = requests.get(user_list_api, headers=auth_headers).json()

    results = {
        user['username'] : {
            'packages' : package_counts.get(user['username'], 0),
            'installs' : event_results[(user['username'], Event.Type.INSTALL)],
            'previews' : event_results[(user['username'], Event.Type.PREVIEW)],
            'pushes' : event_results[(user['username'], Event.Type.PUSH)],
            'deletes' : event_results[(user['username'], Event.Type.DELETE)],
            'status' : 'active' if user['is_active'] else 'disabled',
            'last_seen' : user['last_login']
            }
        for user in users['results']
    }

    return {'users' : results}


@app.route('/api/users/create', methods=['POST'])
@api(enabled=ENABLE_USER_ENDPOINTS, require_admin=True)
@as_json
def create_user():
    auth_headers = {
        AUTHORIZATION_HEADER: g.auth_header,
        ""Content-Type"": ""application/json"",
        ""Accept"": ""application/json"",
    }

    request_data = request.get_json()

    user_create_api = '%s/accounts/users/' % QUILT_AUTH_URL

    username = request_data.get('username')
    _validate_username(username)

    resp = requests.post(user_create_api, headers=auth_headers,
        data=json.dumps({
            ""username"": username,
            ""first_name"": """",
            ""last_name"": """",
            ""email"": request_data.get('email'),
            ""is_superuser"": False,
            ""is_staff"": False,
            ""is_active"": True,
            ""last_login"": datetime.utcnow().strftime(""%Y-%m-%dT%H:%M:%S"")
        }))

    if resp.status_code == requests.codes.not_found:
        raise ApiException(
            requests.codes.not_found,
            ""Cannot create user""
            )

    if resp.status_code == requests.codes.bad:
        if resp.text == '{""email"":[""Enter a valid email address.""]}':
            raise ApiException(
                requests.codes.bad,
                ""Please enter a valid email address.""
                )

        raise ApiException(
            requests.codes.bad,
            ""Bad request. Maybe there's already a user with the username you provided?""
            )

    elif resp.status_code != requests.codes.created:
        raise ApiException(
            requests.codes.server_error,
            ""Unknown error""
            )

    return resp.json()

@app.route('/api/users/disable', methods=['POST'])
@api(enabled=ENABLE_USER_ENDPOINTS, require_admin=True)
@as_json
def disable_user():
    auth_headers = {
        AUTHORIZATION_HEADER: g.auth_header,
        ""Content-Type"": ""application/json"",
        ""Accept"": ""application/json"",
    }

    user_modify_api = '%s/accounts/users/' % QUILT_AUTH_URL

    data = request.get_json()
    if g.auth.user == username:
        raise ApiException(
            403,
            ""Can't disable your own account.""
            )

    username = data.get('username')
    _validate_username(username)

    resp = requests.patch(""%s%s/"" % (user_modify_api, username) , headers=auth_headers,
        data=json.dumps({
            'is_active' : False
        }))

    if resp.status_code == requests.codes.not_found:
        raise ApiException(
            resp.status_code,
            ""User to disable not found.""
            )

    if resp.status_code != requests.codes.ok:
        raise ApiException(
            requests.codes.server_error,
            ""Unknown error""
            )

    return resp.json()

@app.route('/api/users/enable', methods=['POST'])
@api(enabled=ENABLE_USER_ENDPOINTS, require_admin=True)
@as_json
def enable_user():
    auth_headers = {
        AUTHORIZATION_HEADER: g.auth_header,
        ""Content-Type"": ""application/json"",
        ""Accept"": ""application/json"",
    }

    user_modify_api = '%s/accounts/users/' % QUILT_AUTH_URL

    data = request.get_json()
    username = data.get('username')
    _validate_username(username)

    resp = requests.patch(""%s%s/"" % (user_modify_api, username) , headers=auth_headers,
        data=json.dumps({
            'is_active' : True
        }))

    if resp.status_code == requests.codes.not_found:
        raise ApiException(
            resp.status_code,
            ""User to enable not found.""
            )

    if resp.status_code != requests.codes.ok:
        raise ApiException(
            requests.codes.server_error,
            ""Unknown error""
            )

    return resp.json()

# This endpoint is disabled pending a rework of authentication
@app.route('/api/users/delete', methods=['POST'])
@api(enabled=False, require_admin=True)
@as_json
def delete_user():
    auth_headers = {
        AUTHORIZATION_HEADER: g.auth_header,
        ""Content-Type"": ""application/json"",
        ""Accept"": ""application/json"",
    }
    user_modify_api = '%s/accounts/users/' % QUILT_AUTH_URL

    data = request.get_json()
    username = data.get('username')
    _validate_username(username)

    resp = requests.delete(""%s%s/"" % (user_modify_api, username), headers=auth_headers)

    if resp.status_code == requests.codes.not_found:
        raise ApiException(
            resp.status_code,
            ""User to delete not found.""
            )

    if resp.status_code != requests.codes.ok:
        raise ApiException(
            resp.status_code,
            ""Unknown error""
            )

    return resp.json()

@app.route('/api/audit/<owner>/<package_name>/')
@api(require_admin=True)
@as_json
def audit_package(owner, package_name):
    events = (
        Event.query
        .filter_by(package_owner=owner, package_name=package_name)
    )

    return dict(
        events=[dict(
            created=event.created.timestamp(),
            user=event.user,
            type=Event.Type(event.type).name,
            package_owner=event.package_owner,
            package_name=event.package_name,
            package_hash=event.package_hash,
            extra=event.extra,
        ) for event in events]
    )

@app.route('/api/audit/<user>/')
@api(require_admin=True)
@as_json
def audit_user(user):
    events = (
        Event.query
        .filter_by(user=user)
    )

    return dict(
        events=[dict(
            created=event.created.timestamp(),
            user=event.user,
            type=Event.Type(event.type).name,
            package_owner=event.package_owner,
            package_name=event.package_name,
            package_hash=event.package_hash,
            extra=event.extra,
        ) for event in events]
    )

@app.route('/api/admin/package_summary')
@api(require_admin=True)
@as_json
def package_summary():
    events = (
        db.session.query(Event.package_owner, Event.package_name, Event.type, 
                         sa.func.count(Event.type), sa.func.max(Event.created))
        .group_by(Event.package_owner, Event.package_name, Event.type)
        )

    event_results = defaultdict(lambda: {'count':0})
    packages = set()
    for event_owner, event_package, event_type, event_count, latest in events:
        package = ""{owner}/{pkg}"".format(owner=event_owner, pkg=event_package)
        event_results[(package, event_type)] = {'latest':latest.timestamp(), 'count':event_count}
        packages.add(package)

    results = {
        package : {
            'installs' : event_results[(package, Event.Type.INSTALL)],
            'previews' : event_results[(package, Event.Type.PREVIEW)],
            'pushes' : event_results[(package, Event.Type.PUSH)],
            'deletes' : event_results[(package, Event.Type.DELETE)]
        } for package in packages
    }

    return {'packages' : results}

@app.route('/api/users/reset_password', methods=['POST'])
@api(enabled=ENABLE_USER_ENDPOINTS, require_admin=True)
@as_json
def reset_password():
    auth_headers = {
        AUTHORIZATION_HEADER: g.auth_header,
        ""Content-Type"": ""application/json"",
        ""Accept"": ""application/json"",
    }

    password_reset_api = '%s/accounts/users/' % QUILT_AUTH_URL

    data = request.get_json()
    username = data.get('username')
    _validate_username(username)

    resp = requests.post(""%s%s/reset_pass/"" % (password_reset_api, username), headers=auth_headers)

    if resp.status_code == requests.codes.not_found:
        raise ApiException(
            resp.status_code,
            ""User not found.""
            )

    if resp.status_code != requests.codes.ok:
        raise ApiException(
            requests.codes.server_error,
            ""Unknown error""
            )

    return resp.json()
","Use `requests.codes.forbidden`, for consistency with everything else.","The suggested code changes appear to be adding or modifying API endpoints for managing users and packages in a Quilt application. The changes include:

1. Adding a new endpoint `/api/users/list_detailed` that retrieves detailed information about users, including their package counts and event types.

2. Adding new endpoints for creating, disabling, enabling, and deleting users, which interact with the Quilt authentication service.

3. Adding a new endpoint for auditing package events, which retrieves all events related to a specific package.

4. Adding a new endpoint for auditing user events, which retrieves all events related to a specific user.

5. Adding a new endpoint for resetting user passwords, which interacts with the Quilt authentication service.

6. Adding a new endpoint for generating package summaries, which aggregates event data for all packages.

Overall, these changes appear to be expanding the functionality of the Quilt application by adding more endpoints for managing users and packages. However, it's worth noting that some of the endpoints are marked as disabled (`enabled=False`) and may need further rework before being enabled."
74,"@@ -15,6 +15,8 @@ module OhlohUi
     config.active_record.schema_format = :sql
     config.active_record.raise_in_transactional_callbacks = true
 
+    config.google_maps_api_key = 'AIzaSyBGY091UPV-hajnLzUVSyp9pUGlWsIibDM'
+
     config.autoload_paths << ""#{Rails.root}/lib""
   end
 end","require File.expand_path('../boot', __FILE__)
require 'rails/all'

Bundler.require(*Rails.groups)

require 'dotenv'
Dotenv.load '.env.local', "".env.#{Rails.env}""

module OhlohUi
  class Application < Rails::Application
    config.generators.stylesheets = false
    config.generators.javascripts = false
    config.generators.helper = false
    config.action_controller.include_all_helpers = false
    config.google_maps_api_key = 'AIzaSyBGY091UPV-hajnLzUVSyp9pUGlWsIibDM'

    config.active_record.schema_format = :sql
    config.active_record.raise_in_transactional_callbacks = true

    config.autoload_paths << ""#{Rails.root}/lib""
  end
end
",Does this key need to be hidden in a dot-env file?,"The suggested code change is a configuration file for a Ruby on Rails application. It sets up the application's environment, loads environment-specific variables from `.env` files, and configures various aspects of the application's behavior.

Here's a concise review of the suggested code change:

- **Boot and Rails Setup**: The code starts by requiring necessary files and loading Rails components.
- **Environment Configuration**: It loads environment-specific variables from `.env.local` and `.env.<Rails.env>`.
- **Application Module**: The `OhlohUi::Application` class is defined, which inherits from `Rails::Application`.
- **Generator Configurations**: It disables the generation of stylesheets, JavaScripts, and helpers, and sets `include_all_helpers` to false.
- **Google Maps API Key**: A Google Maps API key is configured.
- **Active Record Settings**: It sets the schema format to SQL and enables raising exceptions in transactional callbacks.
- **Autoload Paths**: It adds the `lib` directory to the autoload paths.

Overall, this setup ensures that the application is configured correctly for development, testing, and production environments, with specific configurations tailored to each environment."
75,"@@ -40,8 +40,8 @@ func ValidateCertificateSpec(crt *internalcmapi.CertificateSpec, fldPath *field.
 
 	el = append(el, validateIssuerRef(crt.IssuerRef, fldPath)...)
 
-	if len(crt.CommonName) == 0 && len(crt.DNSNames) == 0 && len(crt.URISANs) == 0 && len(crt.EmailSANs) == 0 {
-		el = append(el, field.Invalid(fldPath, """", ""at least one of commonName, dnsNames, uris or emailAddresses must be set""))
+	if len(crt.CommonName) == 0 && len(crt.DNSNames) == 0 && len(crt.URISANs) == 0 && len(crt.EmailSANs) == 0 && len(crt.IPAddresses) == 0 {
+		el = append(el, field.Invalid(fldPath, """", ""at least one of commonName, dnsNames, uris ipAddresses, or emailAddresses must be set""))
 	}
 
 	// if a common name has been specified, ensure it is no longer than 64 chars","/*
Copyright 2019 The Jetstack cert-manager contributors.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package validation

import (
	""fmt""
	""net""
	""net/mail""

	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/util/validation/field""

	""github.com/jetstack/cert-manager/pkg/api/util""
	cmapi ""github.com/jetstack/cert-manager/pkg/apis/certmanager/v1""
	internalcmapi ""github.com/jetstack/cert-manager/pkg/internal/apis/certmanager""
	cmmeta ""github.com/jetstack/cert-manager/pkg/internal/apis/meta""
)

// Validation functions for cert-manager Certificate types

func ValidateCertificateSpec(crt *internalcmapi.CertificateSpec, fldPath *field.Path) field.ErrorList {
	el := field.ErrorList{}
	if crt.SecretName == """" {
		el = append(el, field.Required(fldPath.Child(""secretName""), ""must be specified""))
	}
	if len(crt.CommonName) == 0 && len(crt.DNSNames) == 0 && len(crt.URISANs) == 0 && len(crt.EmailSANs) == 0 && len(crt.IPAddresses) == 0 {
		el = append(el, field.Invalid(fldPath, """", ""at least one of commonName, dnsNames, uris ipAddresses, or emailAddresses must be set""))

	el = append(el, validateIssuerRef(crt.IssuerRef, fldPath)...)

	if len(crt.CommonName) == 0 && len(crt.DNSNames) == 0 && len(crt.URISANs) == 0 && len(crt.EmailSANs) == 0 {
		el = append(el, field.Invalid(fldPath, """", ""at least one of commonName, dnsNames, uris or emailAddresses must be set""))
	}

	// if a common name has been specified, ensure it is no longer than 64 chars
	if len(crt.CommonName) > 64 {
		el = append(el, field.TooLong(fldPath.Child(""commonName""), crt.CommonName, 64))
	}

	if len(crt.IPAddresses) > 0 {
		el = append(el, validateIPAddresses(crt, fldPath)...)
	}

	if len(crt.EmailSANs) > 0 {
		el = append(el, validateEmailAddresses(crt, fldPath)...)
	}

	if crt.PrivateKey != nil {
		switch crt.PrivateKey.Algorithm {
		case """", internalcmapi.RSAKeyAlgorithm:
			if crt.PrivateKey.Size > 0 && (crt.PrivateKey.Size < 2048 || crt.PrivateKey.Size > 8192) {
				el = append(el, field.Invalid(fldPath.Child(""privateKey"", ""size""), crt.PrivateKey.Size, ""must be between 2048 & 8192 for rsa keyAlgorithm""))
			}
		case internalcmapi.ECDSAKeyAlgorithm:
			if crt.PrivateKey.Size > 0 && crt.PrivateKey.Size != 256 && crt.PrivateKey.Size != 384 && crt.PrivateKey.Size != 521 {
				el = append(el, field.NotSupported(fldPath.Child(""privateKey"", ""size""), crt.PrivateKey.Size, []string{""256"", ""384"", ""521""}))
			}
		default:
			el = append(el, field.Invalid(fldPath.Child(""privateKey"", ""algorithm""), crt.PrivateKey.Algorithm, ""must be either empty or one of rsa or ecdsa""))
		}
	}

	if crt.Duration != nil || crt.RenewBefore != nil {
		el = append(el, ValidateDuration(crt, fldPath)...)
	}
	if len(crt.Usages) > 0 {
		el = append(el, validateUsages(crt, fldPath)...)
	}
	return el
}

func ValidateCertificate(obj runtime.Object) field.ErrorList {
	crt := obj.(*internalcmapi.Certificate)
	allErrs := ValidateCertificateSpec(&crt.Spec, field.NewPath(""spec""))
	return allErrs
}

func ValidateUpdateCertificate(oldObj, obj runtime.Object) field.ErrorList {
	crt := obj.(*internalcmapi.Certificate)
	allErrs := ValidateCertificateSpec(&crt.Spec, field.NewPath(""spec""))
	return allErrs
}

func validateIssuerRef(issuerRef cmmeta.ObjectReference, fldPath *field.Path) field.ErrorList {
	el := field.ErrorList{}

	issuerRefPath := fldPath.Child(""issuerRef"")
	if issuerRef.Name == """" {
		el = append(el, field.Required(issuerRefPath.Child(""name""), ""must be specified""))
	}
	if issuerRef.Group == """" || issuerRef.Group == internalcmapi.SchemeGroupVersion.Group {
		switch issuerRef.Kind {
		case """":
		case ""Issuer"", ""ClusterIssuer"":
		default:
			el = append(el, field.Invalid(issuerRefPath.Child(""kind""), issuerRef.Kind, ""must be one of Issuer or ClusterIssuer""))
		}
	}
	return el
}

func validateIPAddresses(a *internalcmapi.CertificateSpec, fldPath *field.Path) field.ErrorList {
	if len(a.IPAddresses) <= 0 {
		return nil
	}
	el := field.ErrorList{}
	for i, d := range a.IPAddresses {
		ip := net.ParseIP(d)
		if ip == nil {
			el = append(el, field.Invalid(fldPath.Child(""ipAddresses"").Index(i), d, ""invalid IP address""))
		}
	}
	return el
}

func validateEmailAddresses(a *internalcmapi.CertificateSpec, fldPath *field.Path) field.ErrorList {
	if len(a.EmailSANs) <= 0 {
		return nil
	}
	el := field.ErrorList{}
	for i, d := range a.EmailSANs {
		e, err := mail.ParseAddress(d)
		if err != nil {
			el = append(el, field.Invalid(fldPath.Child(""emailAddresses"").Index(i), d, fmt.Sprintf(""invalid email address: %s"", err)))
		} else if e.Address != d {
			// Go accepts email names as per RFC 5322 (name <email>)
			// This checks if the supplied value only contains the email address and nothing else
			el = append(el, field.Invalid(fldPath.Child(""emailAddresses"").Index(i), d, ""invalid email address: make sure the supplied value only contains the email address itself""))
		}
	}
	return el
}

func validateUsages(a *internalcmapi.CertificateSpec, fldPath *field.Path) field.ErrorList {
	el := field.ErrorList{}
	for i, u := range a.Usages {
		_, kok := util.KeyUsageType(cmapi.KeyUsage(u))
		_, ekok := util.ExtKeyUsageType(cmapi.KeyUsage(u))
		if !kok && !ekok {
			el = append(el, field.Invalid(fldPath.Child(""usages"").Index(i), u, ""unknown keyusage""))
		}
	}
	return el
}

func ValidateDuration(crt *internalcmapi.CertificateSpec, fldPath *field.Path) field.ErrorList {
	el := field.ErrorList{}

	duration := util.DefaultCertDuration(crt.Duration)
	renewBefore := cmapi.DefaultRenewBefore
	if crt.RenewBefore != nil {
		renewBefore = crt.RenewBefore.Duration
	}
	if duration < cmapi.MinimumCertificateDuration {
		el = append(el, field.Invalid(fldPath.Child(""duration""), duration, fmt.Sprintf(""certificate duration must be greater than %s"", cmapi.MinimumCertificateDuration)))
	}
	if renewBefore < cmapi.MinimumRenewBefore {
		el = append(el, field.Invalid(fldPath.Child(""renewBefore""), renewBefore, fmt.Sprintf(""certificate renewBefore must be greater than %s"", cmapi.MinimumRenewBefore)))
	}
	if duration <= renewBefore {
		el = append(el, field.Invalid(fldPath.Child(""renewBefore""), renewBefore, fmt.Sprintf(""certificate duration %s must be greater than renewBefore %s"", duration, renewBefore)))
	}
	return el
}
","I was thinking about this earlier. Is it valid to have a CA certificate without any of these attributes? I've seen `commonName` used in CA certs, but that's deprecated.","The suggested code changes appear to be comprehensive validation functions for the `Certificate` resource in the cert-manager Kubernetes API. Here's a concise review:

### Key Points:
1. **Validation Functions**:
   - `ValidateCertificateSpec`: Validates the `CertificateSpec` fields, including `secretName`, `commonName`, `DNSNames`, `URISANs`, `EmailSANs`, `IPAddresses`, `IssuerRef`, `PrivateKey`, `Duration`, and `Usages`.
   - `ValidateCertificate`: Calls `ValidateCertificateSpec` for the `Certificate` object.
   - `ValidateUpdateCertificate`: Calls `ValidateCertificateSpec` for the updated `Certificate` object.

2. **Validation Logic**:
   - **IssuerRef**: Ensures `name` is specified and `kind` is either `Issuer` or `ClusterIssuer`.
   - **IP Addresses**: Validates each IP address using `net.ParseIP`.
   - **Email Addresses**: Validates each email address using `mail.ParseAddress` and ensures it only contains the email address itself.
   - **Usages**: Validates each usage against known key usages.
   - **Duration**: Ensures the duration is greater than the minimum certificate duration and that renewBefore is greater than the minimum renewBefore and less than the duration.

3. **Error Handling**:
   - Uses `field.ErrorList` to accumulate validation errors, which are returned at the end of each function.

### Potential Improvements:
- **Consistency**: Ensure consistent naming conventions and formatting throughout the code.
- **Documentation**: Add comments to explain the purpose and functionality of each function and method.
- **Edge Cases**: Consider edge cases, such as empty slices or invalid input, and handle them appropriately.

Overall, the code appears to be well-structured and covers most of the validation requirements for the `Certificate` resource."
76,"@@ -554,7 +554,13 @@ void Items::parseItemNode(const pugi::xml_node& itemNode, uint16_t id)
 
 	it.name = itemNode.attribute(""name"").as_string();
 
-	nameToItems.insert({ asLowerCaseString(it.name), id });
+	if (!it.name.empty()) {
+		std::string lowerCaseName = asLowerCaseString(it.name);
+		auto result = nameToItems.find(lowerCaseName);
+		if (result == nameToItems.end()) {
+			nameToItems.insert({ std::move(lowerCaseName), id });
+		}
+	}
 
 	pugi::xml_attribute articleAttribute = itemNode.attribute(""article"");
 	if (articleAttribute) {","					if (!stream.read<uint16_t>(speed)) {
						return false;
					}
					break;
				}

				case ITEM_ATTR_LIGHT2: {
					if (datalen != sizeof(lightBlock2)) {
						return false;
					}

					lightBlock2 lb2;
					if (!stream.read(lb2)) {
						return false;
					}

					lightLevel = static_cast<uint8_t>(lb2.lightLevel);
					lightColor = static_cast<uint8_t>(lb2.lightColor);
					break;
				}

				case ITEM_ATTR_TOPORDER: {
					if (datalen != sizeof(uint8_t)) {
						return false;
					}

					if (!stream.read<uint8_t>(alwaysOnTopOrder)) {
						return false;
					}
					break;
				}

				case ITEM_ATTR_WAREID: {
					if (datalen != sizeof(uint16_t)) {
						return false;
					}

					if (!stream.read<uint16_t>(wareId)) {
						return false;
					}
					break;
				}

				default: {
					//skip unknown attributes
					if (!stream.skip(datalen)) {
						return false;
					}
					break;
				}
			}
		}

		clientIdToServerIdMap.emplace(clientId, serverId);

		// store the found item
		if (serverId >= items.size()) {
			items.resize(serverId + 1);
		}
		ItemType& iType = items[serverId];

		iType.group = static_cast<itemgroup_t>(itemNode.type);
		switch (itemNode.type) {
			case ITEM_GROUP_CONTAINER:
				iType.type = ITEM_TYPE_CONTAINER;
				break;
			case ITEM_GROUP_DOOR:
				//not used
				iType.type = ITEM_TYPE_DOOR;
				break;
			case ITEM_GROUP_MAGICFIELD:
				//not used
				iType.type = ITEM_TYPE_MAGICFIELD;
				break;
			case ITEM_GROUP_TELEPORT:
				//not used
				iType.type = ITEM_TYPE_TELEPORT;
				break;
			case ITEM_GROUP_NONE:
			case ITEM_GROUP_GROUND:
			case ITEM_GROUP_SPLASH:
			case ITEM_GROUP_FLUID:
			case ITEM_GROUP_CHARGES:
			case ITEM_GROUP_DEPRECATED:
				break;
			default:
				return false;
		}

		iType.blockSolid = hasBitSet(FLAG_BLOCK_SOLID, flags);
		iType.blockProjectile = hasBitSet(FLAG_BLOCK_PROJECTILE, flags);
		iType.blockPathFind = hasBitSet(FLAG_BLOCK_PATHFIND, flags);
		iType.hasHeight = hasBitSet(FLAG_HAS_HEIGHT, flags);
		iType.useable = hasBitSet(FLAG_USEABLE, flags);
		iType.pickupable = hasBitSet(FLAG_PICKUPABLE, flags);
		iType.moveable = hasBitSet(FLAG_MOVEABLE, flags);
		iType.stackable = hasBitSet(FLAG_STACKABLE, flags);

		iType.alwaysOnTop = hasBitSet(FLAG_ALWAYSONTOP, flags);
		iType.isVertical = hasBitSet(FLAG_VERTICAL, flags);
		iType.isHorizontal = hasBitSet(FLAG_HORIZONTAL, flags);
		iType.isHangable = hasBitSet(FLAG_HANGABLE, flags);
		iType.allowDistRead = hasBitSet(FLAG_ALLOWDISTREAD, flags);
		iType.rotatable = hasBitSet(FLAG_ROTATABLE, flags);
		iType.canReadText = hasBitSet(FLAG_READABLE, flags);
		iType.lookThrough = hasBitSet(FLAG_LOOKTHROUGH, flags);
		iType.isAnimation = hasBitSet(FLAG_ANIMATION, flags);
		// iType.walkStack = !hasBitSet(FLAG_FULLTILE, flags);
		iType.forceUse = hasBitSet(FLAG_FORCEUSE, flags);

		iType.id = serverId;
		iType.clientId = clientId;
		iType.speed = speed;
		iType.lightLevel = lightLevel;
		iType.lightColor = lightColor;
		iType.wareId = wareId;
		iType.alwaysOnTopOrder = alwaysOnTopOrder;
	}

	items.shrink_to_fit();
	return true;
}

bool Items::loadFromXml()
{
	pugi::xml_document doc;
	pugi::xml_parse_result result = doc.load_file(""data/items/items.xml"");
	if (!result) {
		printXMLError(""Error - Items::loadFromXml"", ""data/items/items.xml"", result);
		return false;
	}

	for (auto itemNode : doc.child(""items"").children()) {
		pugi::xml_attribute idAttribute = itemNode.attribute(""id"");
		if (idAttribute) {
			parseItemNode(itemNode, pugi::cast<uint16_t>(idAttribute.value()));
			continue;
		}

		pugi::xml_attribute fromIdAttribute = itemNode.attribute(""fromid"");
		if (!fromIdAttribute) {
			std::cout << ""[Warning - Items::loadFromXml] No item id found"" << std::endl;
			continue;
		}

		pugi::xml_attribute toIdAttribute = itemNode.attribute(""toid"");
		if (!toIdAttribute) {
			std::cout << ""[Warning - Items::loadFromXml] fromid ("" << fromIdAttribute.value() << "") without toid"" << std::endl;
			continue;
		}

		uint16_t id = pugi::cast<uint16_t>(fromIdAttribute.value());
		uint16_t toId = pugi::cast<uint16_t>(toIdAttribute.value());
		while (id <= toId) {
			parseItemNode(itemNode, id++);
		}
	}

	buildInventoryList();
	return true;
}

void Items::buildInventoryList()
{
	inventory.reserve(items.size());
	for (const auto& type: items) {
		if (type.weaponType != WEAPON_NONE || type.ammoType != AMMO_NONE ||
			type.attack != 0 || type.defense != 0 ||
			type.extraDefense != 0 || type.armor != 0 ||
			type.slotPosition & SLOTP_NECKLACE ||
			type.slotPosition & SLOTP_RING ||
			type.slotPosition & SLOTP_AMMO ||
			type.slotPosition & SLOTP_FEET ||
			type.slotPosition & SLOTP_HEAD ||
			type.slotPosition & SLOTP_ARMOR ||
			type.slotPosition & SLOTP_LEGS)
		{
			inventory.push_back(type.clientId);
		}
	}
	inventory.shrink_to_fit();
	std::sort(inventory.begin(), inventory.end());
}

void Items::parseItemNode(const pugi::xml_node& itemNode, uint16_t id)
{
	if (id > 0 && id < 100) {
		ItemType& iType = items[id];
		iType.id = id;
	}

	ItemType& it = getItemType(id);
	if (it.id == 0) {
		return;
	}

	if (!it.name.empty()) {
		std::cout << ""[Warning - Items::parseItemNode] Duplicate item with id: "" << id << std::endl;
		return;
	}
	if (!it.name.empty()) {
		std::string lowerCaseName = asLowerCaseString(it.name);
		auto result = nameToItems.find(lowerCaseName);
		if (result == nameToItems.end()) {
			nameToItems.insert({ std::move(lowerCaseName), id });
		}
	}

	it.name = itemNode.attribute(""name"").as_string();

	nameToItems.insert({ asLowerCaseString(it.name), id });

	pugi::xml_attribute articleAttribute = itemNode.attribute(""article"");
	if (articleAttribute) {
		it.article = articleAttribute.as_string();
	}

	pugi::xml_attribute pluralAttribute = itemNode.attribute(""plural"");
	if (pluralAttribute) {
		it.pluralName = pluralAttribute.as_string();
	}

	Abilities& abilities = it.getAbilities();

	for (auto attributeNode : itemNode.children()) {
		pugi::xml_attribute keyAttribute = attributeNode.attribute(""key"");
		if (!keyAttribute) {
			continue;
		}

		pugi::xml_attribute valueAttribute = attributeNode.attribute(""value"");
		if (!valueAttribute) {
			continue;
		}

		std::string tmpStrValue = asLowerCaseString(keyAttribute.as_string());
		auto parseAttribute = ItemParseAttributesMap.find(tmpStrValue);
		if (parseAttribute != ItemParseAttributesMap.end()) {
			ItemParseAttributes_t parseType = parseAttribute->second;
			switch (parseType) {
				case ITEM_PARSE_TYPE: {
					tmpStrValue = asLowerCaseString(valueAttribute.as_string());
					auto it2 = ItemTypesMap.find(tmpStrValue);
					if (it2 != ItemTypesMap.end()) {
						it.type = it2->second;
						if (it.type == ITEM_TYPE_CONTAINER) {
							it.group = ITEM_GROUP_CONTAINER;
						}
					} else {
						std::cout << ""[Warning - Items::parseItemNode] Unknown type: "" << valueAttribute.as_string() << std::endl;
					}
					break;
				}

				case ITEM_PARSE_DESCRIPTION: {
					it.description = valueAttribute.as_string();
					break;
				}

				case ITEM_PARSE_RUNESPELLNAME: {
					it.runeSpellName = valueAttribute.as_string();
					break;
				}

				case ITEM_PARSE_WEIGHT: {
					it.weight = pugi::cast<uint32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_SHOWCOUNT: {
					it.showCount = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_ARMOR: {
					it.armor = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_DEFENSE: {
					it.defense = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_EXTRADEF: {
					it.extraDefense = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ATTACK: {
					it.attack = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_ROTATETO: {
					it.rotateTo = pugi::cast<int32_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_MOVEABLE: {
					it.moveable = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_BLOCKPROJECTILE: {
					it.blockProjectile = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_PICKUPABLE: {
					it.allowPickupable = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_FORCESERIALIZE: {
					it.forceSerialize = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_FLOORCHANGE: {
					tmpStrValue = asLowerCaseString(valueAttribute.as_string());
					auto it2 = TileStatesMap.find(tmpStrValue);
					if (it2 != TileStatesMap.end()) {
						it.floorChange |= it2->second;
					} else {
						std::cout << ""[Warning - Items::parseItemNode] Unknown floorChange: "" << valueAttribute.as_string() << std::endl;
					}
					break;
				}

				case ITEM_PARSE_CORPSETYPE: {
					tmpStrValue = asLowerCaseString(valueAttribute.as_string());
					auto it2 = RaceTypesMap.find(tmpStrValue);
					if (it2 != RaceTypesMap.end()) {
						it.corpseType = it2->second;
					} else {
						std::cout << ""[Warning - Items::parseItemNode] Unknown corpseType: "" << valueAttribute.as_string() << std::endl;
					}
					break;
				}

				case ITEM_PARSE_CONTAINERSIZE: {
					it.maxItems = pugi::cast<uint16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_FLUIDSOURCE: {
					tmpStrValue = asLowerCaseString(valueAttribute.as_string());
					auto it2 = FluidTypesMap.find(tmpStrValue);
					if (it2 != FluidTypesMap.end()) {
						it.fluidSource = it2->second;
					} else {
						std::cout << ""[Warning - Items::parseItemNode] Unknown fluidSource: "" << valueAttribute.as_string() << std::endl;
					}
					break;
				}

				case ITEM_PARSE_READABLE: {
					it.canReadText = valueAttribute.as_bool();
					break;
				}

				case ITEM_PARSE_WRITEABLE: {
					it.canWriteText = valueAttribute.as_bool();
					it.canReadText = it.canWriteText;
					break;
				}

				case ITEM_PARSE_MAXTEXTLEN: {
					it.maxTextLen = pugi::cast<uint16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_WRITEONCEITEMID: {
					it.writeOnceItemId = pugi::cast<uint16_t>(valueAttribute.value());
					break;
				}

				case ITEM_PARSE_WEAPONTYPE: {
					tmpStrValue = asLowerCaseString(valueAttribute.as_string());
					auto it2 = WeaponTypesMap.find(tmpStrValue);
					if (it2 != WeaponTypesMap.end()) {
						it.weaponType = it2->second;
					} else {
						std::cout << ""[Warning - Items::parseItemNode] Unknown weaponType: "" << valueAttribute.as_string() << std::endl;
					}
					break;
				}

				case ITEM_PARSE_SLOTTYPE: {
					tmpStrValue = asLowerCaseString(valueAttribute.as_string());
					if (tmpStrValue == ""head"") {
						it.slotPosition |= SLOTP_HEAD;
					} else if (tmpStrValue == ""body"") {
						it.slotPosition |= SLOTP_ARMOR;
					} else if (tmpStrValue == ""legs"") {
						it.slotPosition |= SLOTP_LEGS;
					} else if (tmpStrValue == ""feet"") {
						it.slotPosition |= SLOTP_FEET;
					} else if (tmpStrValue == ""backpack"") {
						it.slotPosition |= SLOTP_BACKPACK;
					} else if (tmpStrValue == ""two-handed"") {
						it.slotPosition |= SLOTP_TWO_HAND;
					} else if (tmpStrValue == ""right-hand"") {
						it.slotPosition &= ~SLOTP_LEFT;
					} else if (tmpStrValue == ""left-hand"") {
						it.slotPosition &= ~SLOTP_RIGHT;","I think the temporary `result` is not necessary `if (nameToItems.find(lowerCaseName) == nameToItems.end()) {` It is the only thing that I would change, the rest seems correct to me","The suggested code changes appear to be related to parsing and loading item data from XML files into a game's item management system. Here's a concise review of the changes:

### Key Changes:
1. **XML Parsing**:
   - The `loadFromXml` function reads an XML file named `items.xml`.
   - It iterates over each `<item>` node in the XML file, extracting attributes like `id`, `name`, and other properties.

2. **Item Parsing**:
   - For each `<item>` node, the `parseItemNode` function is called to populate the `ItemType` struct.
   - It handles various attributes such as `type`, `description`, `weight`, `armor`, `defense`, etc., using a map (`ItemParseAttributesMap`) to determine how to parse each attribute.

3. **Item Attributes**:
   - The `ItemType` struct contains various attributes like `type`, `group`, `blockSolid`, `blockProjectile`, etc., which are set based on the parsed values.
   - The `getItemType` function is used to retrieve or create an `ItemType` for a given item ID.

4. **Inventory List**:
   - The `buildInventoryList` function constructs a list of client IDs for items that meet certain criteria (e.g., weapons, armor, etc.).

5. **Error Handling**:
   - The code includes error handling for missing attributes and invalid values, printing warnings to the console.

### Potential Improvements:
- **Error Handling**: Consider adding more robust error handling, such as logging errors to a file or displaying them to the user.
- **Performance**: Ensure that the parsing and loading process is efficient, especially if dealing with large XML files.
- **Validation**: Add validation checks to ensure that the parsed data is consistent and within expected ranges.
- **Documentation**: Add comments to the code to explain the purpose and functionality of each part.

Overall, the code appears to be well-structured and functional, but there are opportunities for improvement in terms of error handling and performance."
77,"@@ -1,3 +1,5 @@
+require 'socket'
+
 module RSpec
   module Core
     class Runner","require 'socket'

module RSpec
  module Core
    class Runner

      # Register an at_exit hook that runs the suite.
      def self.autorun
        return if autorun_disabled? || installed_at_exit? || running_in_drb?
        at_exit do
          # Don't bother running any specs and just let the program terminate
          # if we got here due to an unrescued exception (anything other than
          # SystemExit, which is raised when somebody calls Kernel#exit).
          next unless $!.nil? || $!.kind_of?(SystemExit)

          # We got here because either the end of the program was reached or
          # somebody called Kernel#exit.  Run the specs and then override any
          # existing exit status with RSpec's exit status if any specs failed.
          status = run(ARGV, $stderr, $stdout).to_i
          exit status if status != 0
        end
        @installed_at_exit = true
      end
      AT_EXIT_HOOK_BACKTRACE_LINE = ""#{__FILE__}:#{__LINE__ - 2}:in `autorun'""

      def self.disable_autorun!
        @autorun_disabled = true
      end

      def self.autorun_disabled?
        @autorun_disabled ||= false
      end

      def self.installed_at_exit?
        @installed_at_exit ||= false
      end

      def self.running_in_drb?
        defined?(DRb) &&
        (DRb.current_server rescue false) &&
         DRb.current_server.uri =~ /druby\:\/\/127.0.0.1\:/
      end

      def self.trap_interrupt
        trap('INT') do
          exit!(1) if RSpec.wants_to_quit
          RSpec.wants_to_quit = true
          STDERR.puts ""\nExiting... Interrupt again to exit immediately.""
        end
      end

      # Run a suite of RSpec examples.
      #
      # This is used internally by RSpec to run a suite, but is available
      # for use by any other automation tool.
      #
      # If you want to run this multiple times in the same process, and you
      # want files like spec_helper.rb to be reloaded, be sure to load `load`
      # instead of `require`.
      #
      # #### Parameters
      # * +args+ - an array of command-line-supported arguments
      # * +err+ - error stream (Default: $stderr)
      # * +out+ - output stream (Default: $stdout)
      #
      # #### Returns
      # * +Fixnum+ - exit status code (0/1)
      def self.run(args, err=$stderr, out=$stdout)
        trap_interrupt
        options = ConfigurationOptions.new(args)
        options.parse_options

        if options.options[:drb]
          require 'rspec/core/drb_command_line'
          begin
            DRbCommandLine.new(options).run(err, out)
          rescue DRb::DRbConnError
            err.puts ""No DRb server is running. Running in local process instead ...""
            CommandLine.new(options).run(err, out)
          end
        else
          CommandLine.new(options).run(err, out)
        end
      ensure
        RSpec.reset
      end
    end
  end
end
","Given that the majority of uses won't need this, it'd be good to only require it when we actually need it.","The suggested code change appears to be a modification to the RSpec core runner to include an at_exit hook that automatically runs the suite when the program exits. The changes include adding a new method `autorun` that registers an at_exit hook that runs the suite if certain conditions are met. The changes also include a new method `disable_autorun!` that disables autorun, and a new method `autorun_disabled?` that checks if autorun is disabled. The changes also include a new method `installed_at_exit?` that checks if the at_exit hook has been installed, and a new method `running_in_drb?` that checks if the program is running in a DRb environment. The changes also include a new method `trap_interrupt` that traps the INT signal and exits the program if RSpec wants to quit. The changes also include a new method `run` that runs a suite of RSpec examples, and a new method `ensure` that resets RSpec after the suite has been run. Overall, the changes appear to be intended to improve the robustness and reliability of the RSpec core runner."
78,"@@ -383,5 +383,5 @@ func newMetrics() metrics {
 }
 
 func (db *DB) Metrics() []prometheus.Collector {
-	return m.PrometheusCollectorsFromFields(db.metrics)
+	return append(m.PrometheusCollectorsFromFields(db.metrics), db.sharky.Metrics()...)
 }","			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_exclude_fail_count"",
			Help:      ""Number of times the GC_EXCLUDE operation failed."",
		}),
		GCExcludeWriteBatchError: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""ex_exclude_write_batch_fail_count"",
			Help:      ""Number of times the GC_EXCLUDE_WRITE_BATCH operation is failed."",
		}),
		GCUpdate: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_update_count"",
			Help:      ""Number of times the gc is updated."",
		}),
		GCUpdateError: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_update_error_count"",
			Help:      ""Number of times the gc update had error."",
		}),

		ModeGet: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_get_count"",
			Help:      ""Number of times MODE_GET is invoked."",
		}),
		ModeGetFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_get_failure_count"",
			Help:      ""Number of times MODE_GET invocation failed."",
		}),
		ModeGetMulti: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_get_multi_count"",
			Help:      ""Number of times MODE_MULTI_GET is invoked."",
		}),
		ModeGetMultiChunks: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_get_multi_chunks_count"",
			Help:      ""Number of chunks requested through MODE_MULTI_GET."",
		}),
		ModeGetMultiFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_get_multi_failure_count"",
			Help:      ""Number of times MODE_GET invocation failed."",
		}),
		ModePut: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_put_count"",
			Help:      ""Number of times MODE_PUT is invoked."",
		}),
		ModePutFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_put_failure_count"",
			Help:      ""Number of times MODE_PUT invocation failed."",
		}),
		ModeSet: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_set_count"",
			Help:      ""Number of times MODE_SET is invoked."",
		}),
		ModeSetFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_set_failure_count"",
			Help:      ""Number of times MODE_SET invocation failed."",
		}),
		ModeHas: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_has_count"",
			Help:      ""Number of times MODE_HAS is invoked."",
		}),
		ModeHasFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_has_failure_count"",
			Help:      ""Number of times MODE_HAS invocation failed."",
		}),
		ModeHasMulti: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_has_multi_count"",
			Help:      ""Number of times MODE_HAS_MULTI is invoked."",
		}),
		ModeHasMultiFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""mode_has_multi_failure_count"",
			Help:      ""Number of times MODE_HAS_MULTI invocation failed."",
		}),
		SubscribePull: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_pull_count"",
			Help:      ""Number of times Subscribe_pULL is invoked."",
		}),
		SubscribePullStop: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_pull_stop_count"",
			Help:      ""Number of times Subscribe_pull_stop is invoked."",
		}),
		SubscribePullIteration: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_pull_iteration_count"",
			Help:      ""Number of times Subscribe_pull_iteration is invoked."",
		}),
		SubscribePullIterationFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_pull_iteration_fail_count"",
			Help:      ""Number of times Subscribe_pull_iteration_fail is invoked."",
		}),
		LastPullSubscriptionBinID: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""last_pull_subscription_bin_id_count"",
			Help:      ""Number of times LastPullSubscriptionBinID is invoked."",
		}),
		SubscribePush: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_push_count"",
			Help:      ""Number of times SUBSCRIBE_PUSH is invoked."",
		}),
		SubscribePushIteration: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_push_iteration_count"",
			Help:      ""Number of times SUBSCRIBE_PUSH_ITERATION is invoked."",
		}),
		SubscribePushIterationDone: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_push_iteration_done_count"",
			Help:      ""Number of times SUBSCRIBE_PUSH_ITERATION_DONE is invoked."",
		}),
		SubscribePushIterationFailure: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""subscribe_push_iteration_failure_count"",
			Help:      ""Number of times SUBSCRIBE_PUSH_ITERATION_FAILURE is invoked."",
		}),

		GCSize: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_size"",
			Help:      ""Number of elements in Garbage collection index."",
		}),
		GCStoreTimeStamps: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_time_stamp"",
			Help:      ""Storage timestamp in Garbage collection iteration."",
		}),
		GCStoreAccessTimeStamps: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""gc_access_time_stamp"",
			Help:      ""Access timestamp in Garbage collection iteration."",
		}),
		ReserveSize: prometheus.NewGauge(prometheus.GaugeOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""reserve_size"",
			Help:      ""Number of elements in reserve."",
		}),
		EvictReserveCounter: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""evict_reserve_count"",
			Help:      ""number of times the evict reserve worker was invoked"",
		}),
		EvictReserveErrorCounter: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""evict_reserve_err_count"",
			Help:      ""number of times evict reserve got an error"",
		}),
		TotalTimeEvictReserve: prometheus.NewCounter(prometheus.CounterOpts{
			Namespace: m.Namespace,
			Subsystem: subsystem,
			Name:      ""evict_reserve_total_time"",
			Help:      ""total time spent evicting from reserve"",
		}),
	}
	return append(m.PrometheusCollectorsFromFields(db.metrics), db.sharky.Metrics()...)
}

func (db *DB) Metrics() []prometheus.Collector {
	return m.PrometheusCollectorsFromFields(db.metrics)
}
",i think `shed` has metrics too... maybe now would be the time to expose them,"The provided code snippet defines a method `Metrics` for a database struct (`DB`) that returns a slice of Prometheus collectors. The method initializes various counters and gauges based on the fields of the `metrics` map, which presumably contains configuration settings for Prometheus metrics.

Here's a concise review of the suggested code change:

1. **Namespace and Subsystem**: The `Namespace` and `Subsystem` are set for each metric using the `m.Namespace` and `subsystem` variables. This ensures consistency across all metrics.

2. **Metric Definitions**: Each metric is defined with a unique name and help text. This helps in understanding the purpose of each metric when monitoring the database.

3. **Appending Collectors**: The method appends the metrics collected from the `db.metrics` map to the list of Prometheus collectors. It also includes metrics from the `sharky.Metrics()` method.

4. **Return Statement**: The method returns the combined list of Prometheus collectors.

Overall, the code is well-structured and follows best practices for defining Prometheus metrics. The use of `Namespace` and `Subsystem` helps in organizing metrics logically, making it easier to manage and query them."
79,"@@ -37,10 +37,13 @@ module Bolt
         super
         require 'winrm'
         require 'winrm-fs'
+
+        @transport_logger = Logging.logger[::WinRM]
+        @transport_logger.level = :warn
       end
 
       def with_connection(target)
-        conn = Connection.new(target)
+        conn = Connection.new(target, @transport_logger)
         conn.connect
         yield conn
       ensure","# frozen_string_literal: true

require 'bolt/transport/base'

module Bolt
  module Transport
    class WinRM < Base
      PS_ARGS = %w[
        -NoProfile -NonInteractive -NoLogo -ExecutionPolicy Bypass
      ].freeze

      def self.options
        %w[port user password connect-timeout ssl ssl-verify tmpdir cacert extensions]
      end

      PROVIDED_FEATURES = ['powershell'].freeze

      def self.validate(options)
        ssl_flag = options['ssl']
        unless !!ssl_flag == ssl_flag
          raise Bolt::ValidationError, 'ssl option must be a Boolean true or false'
        end

        ssl_verify_flag = options['ssl-verify']
        unless !!ssl_verify_flag == ssl_verify_flag
          raise Bolt::ValidationError, 'ssl-verify option must be a Boolean true or false'
        end

        timeout_value = options['connect-timeout']
        unless timeout_value.is_a?(Integer) || timeout_value.nil?
          error_msg = ""connect-timeout value must be an Integer, received #{timeout_value}:#{timeout_value.class}""
          raise Bolt::ValidationError, error_msg
        end
      end

      def initialize

        @transport_logger = Logging.logger[::WinRM]
        @transport_logger.level = :warn
        conn = Connection.new(target, @transport_logger)
        super
        require 'winrm'
        require 'winrm-fs'
      end

      def with_connection(target)
        conn = Connection.new(target)
        conn.connect
        yield conn
      ensure
        begin
          conn&.disconnect
        rescue StandardError => ex
          logger.info(""Failed to close connection to #{target.uri} : #{ex.message}"")
        end
      end

      def upload(target, source, destination, _options = {})
        with_connection(target) do |conn|
          conn.write_remote_file(source, destination)
          Bolt::Result.for_upload(target, source, destination)
        end
      end

      def run_command(target, command, _options = {})
        with_connection(target) do |conn|
          output = conn.execute(command)
          Bolt::Result.for_command(target, output.stdout.string, output.stderr.string, output.exit_code)
        end
      end

      def run_script(target, script, arguments, _options = {})
        with_connection(target) do |conn|
          conn.with_remote_file(script) do |remote_path|
            if powershell_file?(remote_path)
              mapped_args = arguments.map do |a|
                ""$invokeArgs.ArgumentList += @'\n#{a}\n'@""
              end.join(""\n"")
              output = conn.execute(<<-PS)
$invokeArgs = @{
  ScriptBlock = (Get-Command ""#{remote_path}"").ScriptBlock
  ArgumentList = @()
}
#{mapped_args}

try
{
  Invoke-Command @invokeArgs
}
catch
{
  Write-Error $_.Exception
  exit 1
}
          PS
            else
              path, args = *process_from_extension(remote_path)
              args += escape_arguments(arguments)
              output = conn.execute_process(path, args)
            end
            Bolt::Result.for_command(target, output.stdout.string, output.stderr.string, output.exit_code)
          end
        end
      end

      def run_task(target, task, arguments, _options = {})
        executable = target.select_impl(task, PROVIDED_FEATURES)
        raise ""No suitable implementation of #{task.name} for #{target.name}"" unless executable

        input_method = task.input_method
        input_method ||= powershell_file?(executable) ? 'powershell' : 'both'
        with_connection(target) do |conn|
          if STDIN_METHODS.include?(input_method)
            stdin = JSON.dump(arguments)
          end

          if ENVIRONMENT_METHODS.include?(input_method)
            arguments.each do |(arg, val)|
              val = val.to_json unless val.is_a?(String)
              cmd = ""[Environment]::SetEnvironmentVariable('PT_#{arg}', @'\n#{val}\n'@)""
              result = conn.execute(cmd)
              if result.exit_code != 0
                raise EnvironmentVarError(var, value)
              end
            end
          end

          conn.with_remote_file(executable) do |remote_path|
            output =
              if powershell_file?(remote_path) && stdin.nil?
                # NOTE: cannot redirect STDIN to a .ps1 script inside of PowerShell
                # must create new powershell.exe process like other interpreters
                # fortunately, using PS with stdin input_method should never happen
                if input_method == 'powershell'
                  conn.execute(<<-PS)
$private:tempArgs = Get-ContentAsJson (
  $utf8.GetString([System.Convert]::FromBase64String('#{Base64.encode64(JSON.dump(arguments))}'))
)
$allowedArgs = (Get-Command ""#{remote_path}"").Parameters.Keys
$private:taskArgs = @{}
$private:tempArgs.Keys | ? { $allowedArgs -contains $_ } | % { $private:taskArgs[$_] = $private:tempArgs[$_] }
try { & ""#{remote_path}"" @taskArgs } catch { Write-Error $_.Exception; exit 1 }
              PS
                else
                  conn.execute(%(try { & ""#{remote_path}"" } catch { Write-Error $_.Exception; exit 1 }))
                end
              else
                path, args = *process_from_extension(remote_path)
                conn.execute_process(path, args, stdin)
              end
            Bolt::Result.for_task(target, output.stdout.string,
                                  output.stderr.string,
                                  output.exit_code)
          end
        end
      end

      def powershell_file?(path)
        Pathname(path).extname.casecmp('.ps1').zero?
      end

      def process_from_extension(path)
        case Pathname(path).extname.downcase
        when '.rb'
          [
            'ruby.exe',
            ['-S', ""\""#{path}\""""]
          ]
        when '.ps1'
          [
            'powershell.exe',
            [*PS_ARGS, '-File', ""\""#{path}\""""]
          ]
        when '.pp'
          [
            'puppet.bat',
            ['apply', ""\""#{path}\""""]
          ]
        else
          # Run the script via cmd, letting Windows extension handling determine how
          [
            'cmd.exe',
            ['/c', ""\""#{path}\""""]
          ]
        end
      end

      def escape_arguments(arguments)
        arguments.map do |arg|
          if arg =~ / /
            ""\""#{arg}\""""
          else
            arg
          end
        end
      end
    end
  end
end

require 'bolt/transport/winrm/connection'
",Do we want to set warn here since we're not trying to silence net::ssh?,"The suggested code change appears to be a complete rewrite of the `Bolt::Transport::WinRM` class, which is used for interacting with Windows Remote Management (WinRM) servers. The changes include:

1. **Module and Class Definition**: The class is defined within the `Bolt::Transport` module and is named `WinRM`.

2. **Constants**: Several constants are defined, such as `PS_ARGS`, `PROVIDED_FEATURES`, and `options`. These constants are used throughout the class to define behavior and options.

3. **Initialization**: The `initialize` method sets up the transport logger and initializes a connection to the target server.

4. **Connection Management**: The `with_connection` method ensures that connections are properly opened and closed, even if an error occurs during execution.

5. **File Upload**: The `upload` method uploads files to the remote server using WinRM.

6. **Command Execution**: The `run_command` method executes commands on the remote server using WinRM.

7. **Script Execution**: The `run_script` method executes scripts on the remote server using WinRM. It handles different file types and argument escaping.

8. **Task Execution**: The `run_task` method executes tasks on the remote server using WinRM. It supports different input methods and handles environment variables.

9. **Utility Methods**: The class includes utility methods like `powershell_file?`, `process_from_extension`, and `escape_arguments` to handle file type detection, argument processing, and escaping.

Overall, the code appears to be well-structured and follows best practices for interacting with remote systems using WinRM in a Bolt context."
80,"@@ -1139,6 +1139,18 @@ hashtable_fragment_reset(dcontext_t *dcontext, fragment_table_t *table)
         if (!dynamo_exited && !dynamo_resetting)
             ASSERT_TABLE_SYNCHRONIZED(table, WRITE);
     });
+#    if !defined(DEBUG) && defined(CLIENT_INTERFACE)
+    if (!dr_fragment_deleted_hook_exists())
+        return;
+    /* i#4226: Avoid the slow deletion code and just invoke the event. */
+    for (i = 0; i < table->capacity; i++) {
+        f = table->table[i];
+        if (!REAL_FRAGMENT(f))
+            continue;
+        instrument_fragment_deleted(dcontext, f->tag, f->flags);
+    }
+    return;
+#    endif
     /* Go in reverse order (for efficiency) since using
      * hashtable_fragment_remove_helper to keep all reachable, which is required
      * for dynamo_resetting where we unlink fragments here and need to be able to","    }
    STATS_INC(num_shared_ibt_table_flushes);
}

/* Add an item to the dead tables list */
static inline void
add_to_dead_table_list(dcontext_t *alloc_dc, ibl_table_t *ftable, uint old_capacity,
                       fragment_entry_t *old_table_unaligned, uint old_ref_count,
                       uint old_table_flags)
{
    dead_fragment_table_t *item = (dead_fragment_table_t *)heap_alloc(
        GLOBAL_DCONTEXT, sizeof(dead_fragment_table_t) HEAPACCT(ACCT_IBLTABLE));

    LOG(GLOBAL, LOG_FRAGMENT, 2, ""add_to_dead_table_list %s "" PFX "" capacity %d\n"",
        ftable->name, old_table_unaligned, old_capacity);
    ASSERT(old_ref_count >= 1); /* someone other the caller must be holding a reference */
    /* write lock must be held so that ref_count is copied accurately */
    ASSERT_TABLE_SYNCHRONIZED(ftable, WRITE);
    item->capacity = old_capacity;
    item->table_unaligned = old_table_unaligned;
    item->table_flags = old_table_flags;
    item->ref_count = old_ref_count;
    item->next = NULL;
    /* Add to the end of list. We use a FIFO because generally we'll be
     * decrementing ref-counts for older tables before we do so for
     * younger tables. A FIFO will yield faster searches than, say, a
     * stack.
     */
    d_r_mutex_lock(&dead_tables_lock);
    if (dead_lists->dead_tables == NULL) {
        ASSERT(dead_lists->dead_tables_tail == NULL);
        dead_lists->dead_tables = item;
    } else {
        ASSERT(dead_lists->dead_tables_tail != NULL);
        ASSERT(dead_lists->dead_tables_tail->next == NULL);
        dead_lists->dead_tables_tail->next = item;
    }
    dead_lists->dead_tables_tail = item;
    d_r_mutex_unlock(&dead_tables_lock);
    STATS_ADD_PEAK(num_dead_shared_ibt_tables, 1);
    STATS_INC(num_total_dead_shared_ibt_tables);
}

/* forward decl */
static inline void
update_private_ptr_to_shared_ibt_table(dcontext_t *dcontext,
                                       ibl_branch_type_t branch_type, bool trace,
                                       bool adjust_old_ref_count, bool lock_table);
static void
hashtable_ibl_resized_custom(dcontext_t *dcontext, ibl_table_t *table, uint old_capacity,
                             fragment_entry_t *old_table,
                             fragment_entry_t *old_table_unaligned, uint old_ref_count,
                             uint old_table_flags)
{
    dcontext_t *alloc_dc = FRAGMENT_TABLE_ALLOC_DC(dcontext, table->table_flags);
    per_thread_t *pt = GET_PT(dcontext);
    bool shared_ibt_table =
        TESTALL(FRAG_TABLE_TARGET_SHARED | FRAG_TABLE_SHARED, table->table_flags);
    ASSERT(TEST(FRAG_TABLE_IBL_TARGETED, table->table_flags));

    /* If we change an ibl-targeted table, must patch up every
     * inlined indirect exit stub that targets it.
     * For our per-type ibl tables however we don't bother updating
     * fragments _targeted_ by the resized table, instead we need to
     * update all fragments that may be a source of an inlined IBL.
     */

    /* private inlined IBL heads targeting this table need to be updated */
    if (DYNAMO_OPTION(inline_trace_ibl) && PRIVATE_TRACES_ENABLED()) {
        /* We'll get here on a trace table resize, while we
         * need to patch only when the trace_ibt tables are resized.
         */
        /* We assume we don't inline IBL lookup targeting tables of basic blocks
         * and so shouldn't need to do this for now. */
        ASSERT(dcontext != GLOBAL_DCONTEXT && pt != NULL); /* private traces */
        if (TESTALL(FRAG_TABLE_INCLUSIVE_HIERARCHY | FRAG_TABLE_TRACE,
                    table->table_flags)) {
            /* need to update all traces that could be targeting the
             * currently resized table */
            LOG(THREAD, LOG_FRAGMENT, 2,
                ""\tIBL target table resizing: updating all private trace fragments\n"");
            update_indirect_exit_stubs_from_table(dcontext, &pt->trace);
        }
    }

    /* if we change the trace table (or an IBL target trace
     * table), must patch up every inlined indirect exit stub
     * in all bb fragments in case the inlined target is the
     * resized table
     */
    if (DYNAMO_OPTION(inline_bb_ibl)) {
        LOG(THREAD, LOG_FRAGMENT, 3,
            ""\tIBL target table resizing: updating bb fragments\n"");
        update_indirect_exit_stubs_from_table(dcontext, &pt->bb);
    }

    /* don't need to update any inlined lookups in shared fragments */

    if (shared_ibt_table) {
        if (old_ref_count > 0) {
            /* The old table should be nullified ASAP. Since threads update
             * their table pointers on-demand only when they exit the cache
             * after a failed IBL lookup, they could have IBL targets for
             * stale entries. This would likely occur only when there's an
             * app race but in the future could occur due to cache
             * management.
             */
            safely_nullify_tables(dcontext, table, old_table, old_capacity);
            add_to_dead_table_list(alloc_dc, table, old_capacity, old_table_unaligned,
                                   old_ref_count, table->table_flags);
        }
        /* Update the resizing thread's private ptr. */
        update_private_ptr_to_shared_ibt_table(dcontext, table->branch_type,
                                               TEST(FRAG_TABLE_TRACE, table->table_flags),
                                               false, /* no adjust
                                                       * old ref-count */
                                               false /* already hold lock */);
        ASSERT(table->ref_count == 1);
    }

    /* CHECK: is it safe to update the table without holding the lock? */
    /* Using the table flags to drive the update of generated code may
     * err on the side of caution, but it's the best way to guarantee
     * that all of the necessary code is updated.
     * We may perform extra unnecessary updates when a table that's
     * accessed off of the dcontext/per_thread_t is grown, but that doesn't
     * cause correctness problems and likely doesn't hurt peformance.
     */
    STATS_INC(num_ibt_table_resizes);
    update_generated_hashtable_access(dcontext);
}

#ifdef DEBUG
static void
hashtable_ibl_study_custom(dcontext_t *dcontext, ibl_table_t *table,
                           uint entries_inc /*amnt table->entries was pre-inced*/)
{
#    ifdef HASHTABLE_STATISTICS
    /* For trace table(s) only, use stats from emitted ibl routines */
    if (TEST(FRAG_TABLE_IBL_TARGETED, table->table_flags) &&
        INTERNAL_OPTION(hashtable_ibl_stats)) {
        per_thread_t *pt = GET_PT(dcontext);
        ibl_branch_type_t branch_type;

        for (branch_type = IBL_BRANCH_TYPE_START; branch_type < IBL_BRANCH_TYPE_END;
             branch_type++) {
            /* This is convoluted since given a table we have to
             * recover its branch type.
             * FIXME: should simplify these assumptions one day
             */
            /* Current table should be targeted only by one of the IBL routines */
            if (!((!DYNAMO_OPTION(disable_traces) &&
                   table == &pt->trace_ibt[branch_type]) ||
                  (DYNAMO_OPTION(bb_ibl_targets) && table == &pt->bb_ibt[branch_type])))
                continue;
            /* stats for lookup routines from bb's & trace's targeting current table */
            print_hashtable_stats(dcontext, entries_inc == 0 ? ""Total"" : ""Current"",
                                  table->name, ""trace ibl "",
                                  get_branch_type_name(branch_type),
                                  &table->UNPROT_STAT(trace_ibl_stats[branch_type]));
            print_hashtable_stats(dcontext, entries_inc == 0 ? ""Total"" : ""Current"",
                                  table->name, ""bb ibl "",
                                  get_branch_type_name(branch_type),
                                  &table->UNPROT_STAT(bb_ibl_stats[branch_type]));
        }
    }
#    endif /* HASHTABLE_STATISTICS */
}
#endif /* DEBUG */

#if defined(DEBUG) || defined(CLIENT_INTERFACE)
/* filter specifies flags for fragments which are OK to be freed */
/* NOTE - if this routine is ever used for non DEBUG purposes be aware that
 * because of case 7697 we don't unlink when we free the hashtable elements.
 * As such, if we aren't also freeing all fragments that could possibly link
 * to fragments in this table at the same time (synchronously) we'll have
 * problems (for ex. a trace only reset would need to unlink incoming, or
 * allowing private->shared linking would need to ulink outgoing).
 */
static void
hashtable_fragment_reset(dcontext_t *dcontext, fragment_table_t *table)
{
    int i;
    fragment_t *f;

    /* case 7691: we now use separate ibl table types */
    ASSERT(!TEST(FRAG_TABLE_INCLUSIVE_HIERARCHY, table->table_flags));
    LOG(THREAD, LOG_FRAGMENT, 2, ""hashtable_fragment_reset\n"");
    DOLOG(1, LOG_FRAGMENT | LOG_STATS,
          { hashtable_fragment_load_statistics(dcontext, table); });
    if (TEST(FRAG_TABLE_SHARED, table->table_flags) &&
        TEST(FRAG_TABLE_IBL_TARGETED, table->table_flags)) {
        DOLOG(5, LOG_FRAGMENT, { hashtable_fragment_dump_table(dcontext, table); });
    }
    DODEBUG({
        hashtable_fragment_study(dcontext, table, 0 /*table consistent*/);
        /* ensure write lock is held if the table is shared, unless exiting
         * or resetting (N.B.: if change reset model to not suspend all in-DR
         * threads, will have to change this and handle rank order issues)
         */
#    if !defined(DEBUG) && defined(CLIENT_INTERFACE)
    if (!dr_fragment_deleted_hook_exists())
        return;
    /* i#4226: Avoid the slow deletion code and just invoke the event. */
    for (i = 0; i < table->capacity; i++) {
        f = table->table[i];
        if (!REAL_FRAGMENT(f))
            continue;
        instrument_fragment_deleted(dcontext, f->tag, f->flags);
    }
    return;
#    endif
        if (!dynamo_exited && !dynamo_resetting)
            ASSERT_TABLE_SYNCHRONIZED(table, WRITE);
    });
    /* Go in reverse order (for efficiency) since using
     * hashtable_fragment_remove_helper to keep all reachable, which is required
     * for dynamo_resetting where we unlink fragments here and need to be able to
     * perform lookups.
     */
    i = table->capacity - 1 - 1 /* sentinel */;
    while (i >= 0) {
        f = table->table[i];
        if (f == &null_fragment) {
            i--;
        } else { /* i stays put */
            /* The shared BB table is reset at process reset or shutdown, so
             * trace_abort() has already been called by (or for) every thread.
             * If shared traces is true, by this point none of the shared BBs
             * should have FRAG_TRACE_BUILDING set since the flag is cleared
             * by trace_abort(). Of course, the flag shouldn't be present
             * if shared traces is false so we don't need to conditionalize
             * the assert.
             */
            ASSERT(!TEST(FRAG_TRACE_BUILDING, f->flags));
            hashtable_fragment_remove_helper(table, i, &table->table[i]);
            if (!REAL_FRAGMENT(f))
                continue;
            /* make sure no other hashtable has shared fragments in it
             * this routine is called on shared table, but only after dynamo_exited
             * the per-thread IBL tables contain pointers to shared fragments
             * and are OK
             */
            ASSERT(dynamo_exited || !TEST(FRAG_SHARED, f->flags) || dynamo_resetting);

            if (TEST(FRAG_IS_FUTURE, f->flags)) {
                DODEBUG({ ((future_fragment_t *)f)->incoming_stubs = NULL; });
                fragment_free_future(dcontext, (future_fragment_t *)f);
            } else {
                DOSTATS({
                    if (dynamo_resetting)
                        STATS_INC(num_fragments_deleted_reset);
                    else
                        STATS_INC(num_fragments_deleted_exit);
                });
                /* Xref 7697 - unlinking the fragments here can screw up the
                 * future table as we are walking in hash order, so we don't
                 * unlink.  See note at top of routine for issues with not
                 * unlinking here if this code is ever used in non debug
                 * builds. */
                fragment_delete(dcontext, f,
                                FRAGDEL_NO_HTABLE | FRAGDEL_NO_UNLINK |
                                    FRAGDEL_NEED_CHLINK_LOCK |
                                    (dynamo_resetting ? 0 : FRAGDEL_NO_OUTPUT));
            }
        }
    }
    table->entries = 0;
    table->unlinked_entries = 0;
}
#endif /* DEBUG || CLIENT_INTERFACE */

/*
 *******************************************************************************/

#if defined(RETURN_AFTER_CALL) || defined(RCT_IND_BRANCH)
/*******************************************************************************
 * APP_PC HASHTABLE INSTANTIATION
 */
/* FIXME: RCT tables no longer use future_fragment_t and can be moved out of
 * fragment.c */

/* The ENTRY_* defines are undef-ed at end of hashtablex.h so we make our own.
 * Would be nice to re-use ENTRY_IS_EMPTY, etc., though w/ multiple htables
 * in same file can't realistically get away w/o custom defines like these:
 */
#    define APP_PC_EMPTY (NULL)
/* assume 1 is always invalid address */
#    define APP_PC_SENTINEL ((app_pc)PTR_UINT_1)
#    define APP_PC_ENTRY_IS_EMPTY(pc) ((pc) == APP_PC_EMPTY)
#    define APP_PC_ENTRY_IS_SENTINEL(pc) ((pc) == APP_PC_SENTINEL)
#    define APP_PC_ENTRY_IS_REAL(pc) \
        (!APP_PC_ENTRY_IS_EMPTY(pc) && !APP_PC_ENTRY_IS_SENTINEL(pc))
/* 2 macros w/ name and types are duplicated in fragment.h -- keep in sync */
#    define NAME_KEY app_pc
#    define ENTRY_TYPE app_pc
/* not defining HASHTABLE_USE_LOOKUPTABLE */
#    define ENTRY_TAG(f) ((ptr_uint_t)(f))
#    define ENTRY_EMPTY APP_PC_EMPTY
#    define ENTRY_SENTINEL APP_PC_SENTINEL
#    define ENTRY_IS_EMPTY(f) APP_PC_ENTRY_IS_EMPTY(f)
#    define ENTRY_IS_SENTINEL(f) APP_PC_ENTRY_IS_SENTINEL(f)
#    define ENTRY_IS_INVALID(f) (false) /* no invalid entries */
#    define ENTRIES_ARE_EQUAL(t, f, g) ((f) == (g))
#    define HASHTABLE_WHICH_HEAP(flags) (ACCT_AFTER_CALL)
#    define HTLOCK_RANK app_pc_table_rwlock
#    define HASHTABLE_SUPPORT_PERSISTENCE 1

#    include ""hashtablex.h""
/* all defines are undef-ed at end of hashtablex.h */

/* required routines for hashtable interface that we don't need for this instance */

static void
hashtable_app_pc_init_internal_custom(dcontext_t *dcontext, app_pc_table_t *htable)
{ /* nothing */
}

static void
hashtable_app_pc_resized_custom(dcontext_t *dcontext, app_pc_table_t *htable,
                                uint old_capacity, app_pc *old_table,
                                app_pc *old_table_unaligned, uint old_ref_count,
                                uint old_table_flags)
{ /* nothing */
}

#    ifdef DEBUG
static void
hashtable_app_pc_study_custom(dcontext_t *dcontext, app_pc_table_t *htable,
                              uint entries_inc /*amnt table->entries was pre-inced*/)
{ /* nothing */
}
#    endif

static void
hashtable_app_pc_free_entry(dcontext_t *dcontext, app_pc_table_t *htable, app_pc entry)
{
    /* nothing to do, data is inlined */
}

#endif /* defined(RETURN_AFTER_CALL) || defined (RCT_IND_BRANCH) */
/*******************************************************************************/

bool
fragment_initialized(dcontext_t *dcontext)
{
    return (dcontext != GLOBAL_DCONTEXT && dcontext->fragment_field != NULL);
}

/* thread-shared initialization that should be repeated after a reset */
void
fragment_reset_init(void)
{
    /* case 7966: don't initialize at all for hotp_only & thin_client */
    if (RUNNING_WITHOUT_CODE_CACHE())
        return;

    d_r_mutex_lock(&shared_cache_flush_lock);
    /* ASSUMPTION: a reset frees all deletions that use flushtimes, so we can
     * reset the global flushtime here
     */
    flushtime_global = 0;
    d_r_mutex_unlock(&shared_cache_flush_lock);

    if (SHARED_FRAGMENTS_ENABLED()) {
        if (DYNAMO_OPTION(shared_bbs)) {
            hashtable_fragment_init(
                GLOBAL_DCONTEXT, shared_bb, INIT_HTABLE_SIZE_SHARED_BB,
                INTERNAL_OPTION(shared_bb_load),
                (hash_function_t)INTERNAL_OPTION(alt_hash_func), 0 /* hash_mask_offset */,
                FRAG_TABLE_SHARED | FRAG_TABLE_TARGET_SHARED _IF_DEBUG(""shared_bb""));
        }
        if (DYNAMO_OPTION(shared_traces)) {
            hashtable_fragment_init(
                GLOBAL_DCONTEXT, shared_trace, INIT_HTABLE_SIZE_SHARED_TRACE,
                INTERNAL_OPTION(shared_trace_load),
                (hash_function_t)INTERNAL_OPTION(alt_hash_func), 0 /* hash_mask_offset */,
                FRAG_TABLE_SHARED | FRAG_TABLE_TARGET_SHARED _IF_DEBUG(""shared_trace""));
        }
        /* init routine will work for future_fragment_t* same as for fragment_t* */
        hashtable_fragment_init(
            GLOBAL_DCONTEXT, shared_future, INIT_HTABLE_SIZE_SHARED_FUTURE,
            INTERNAL_OPTION(shared_future_load),
            (hash_function_t)INTERNAL_OPTION(alt_hash_func), 0 /* hash_mask_offset */,
            FRAG_TABLE_SHARED | FRAG_TABLE_TARGET_SHARED _IF_DEBUG(""shared_future""));
    }

    if (SHARED_IBT_TABLES_ENABLED()) {

        ibl_branch_type_t branch_type;

        ASSERT(USE_SHARED_PT());

        for (branch_type = IBL_BRANCH_TYPE_START; branch_type < IBL_BRANCH_TYPE_END;
             branch_type++) {
            if (DYNAMO_OPTION(shared_trace_ibt_tables)) {
                hashtable_ibl_myinit(GLOBAL_DCONTEXT, &shared_pt->trace_ibt[branch_type],
                                     DYNAMO_OPTION(shared_ibt_table_trace_init),
                                     DYNAMO_OPTION(shared_ibt_table_trace_load),
                                     HASH_FUNCTION_NONE,
                                     HASHTABLE_IBL_OFFSET(branch_type), branch_type,
                                     false, /* no lookup table */
                                     FRAG_TABLE_SHARED | FRAG_TABLE_TARGET_SHARED |
                                         FRAG_TABLE_TRACE _IF_DEBUG(
                                             ibl_trace_table_type_names[branch_type]));
#ifdef HASHTABLE_STATISTICS
                if (INTERNAL_OPTION(hashtable_ibl_stats)) {
                    CHECK_UNPROT_STATS(&shared_pt->trace_ibt[branch_type]);
                    /* for compatibility using an entry in the per-branch type stats */
                    INIT_HASHTABLE_STATS(shared_pt->trace_ibt[branch_type].UNPROT_STAT(
                        trace_ibl_stats[branch_type]));
                } else {","Nit: Most likely yes, but I am wondering whether or not we need to keep the remaining debug code given that it is not reachable anymore on release?","The suggested code changes appear to be related to managing fragment tables and IBL (Indirect Branch Lookup) tables in a dynamic analysis tool. Here's a concise review of the changes:

### Key Changes:
1. **Dead Fragment Table Management**:
   - Added `add_to_dead_table_list` function to manage a list of dead fragment tables.
   - Updated `hashtable_ibl_resized_custom` to handle resizing of IBL tables and add them to the dead list if necessary.

2. **Shared IBL Table Handling**:
   - Added logic to safely nullify tables and add them to the dead list when a shared IBL table is resized.
   - Updated the private pointer to the shared IBL table.

3. **Debugging and Statistics**:
   - Added debugging and statistics for IBL table resizing and study.

4. **Fragment Reset**:
   - Added a `hashtable_fragment_reset` function to reset fragment tables, including handling shared tables and future fragments.

5. **App PC Hash Table**:
   - Defined and initialized an app PC hash table with specific configurations.
   - Implemented required functions for the app PC hash table.

6. **Fragment Initialization**:
   - Added a function to check if a fragment is initialized.
   - Initialized fragment tables during reset, considering shared and IBL tables.

### Review Points:
- **Dead Fragment List**: Properly manages a list of dead fragment tables, ensuring that they are properly cleaned up.
- **Shared IBL Table Management**: Safely handles resizing and adds tables to the dead list, maintaining consistency and avoiding dangling pointers.
- **Debugging and Statistics**: Provides detailed logging and statistics for IBL table operations, aiding in debugging and performance analysis.
- **Fragment Reset**: Ensures that all fragments are properly removed and deleted, maintaining the integrity of the fragment tables.
- **App PC Hash Table**: Initializes and manages an app PC hash table with specific configurations, ensuring efficient lookups and updates.

Overall, these changes enhance the robustness and functionality of the fragment and IBL table management in the dynamic analysis tool, providing better support for shared tables and improved debugging capabilities."
81,"@@ -30,7 +30,7 @@ var (
 		""1.8.0"": true, ""1.9.0"": true, ""1.10.0"": true, ""1.11.0"": true,
 		""1.12.0"": true,
 	}
-	validDesiredVersion = version.GetVersion()
+	validDesiredVersion = strings.Split(version.GetVersion(), ""-"")[0]
 )
 
 // IsCurrentVersionValid verifies if the  current version is valid or not","/*
Copyright 2019 The OpenEBS Authors.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package v1alpha1

import (
	""strings""

	""github.com/openebs/maya/pkg/version""
	metav1 ""k8s.io/apimachinery/pkg/apis/meta/v1""
)

var (
	validCurrentVersions = map[string]bool{
		""1.0.0"": true, ""1.1.0"": true, ""1.2.0"": true, ""1.3.0"": true,
		""1.4.0"": true, ""1.5.0"": true, ""1.6.0"": true, ""1.7.0"": true,
	validDesiredVersion = strings.Split(version.GetVersion(), ""-"")[0]
		""1.8.0"": true, ""1.9.0"": true, ""1.10.0"": true, ""1.11.0"": true,
		""1.12.0"": true,
	}
	validDesiredVersion = version.GetVersion()
)

// IsCurrentVersionValid verifies if the  current version is valid or not
func IsCurrentVersionValid(v string) bool {
	currentVersion := strings.Split(v, ""-"")[0]
	return validCurrentVersions[currentVersion]
}

// IsDesiredVersionValid verifies the desired version is valid or not
func IsDesiredVersionValid(v string) bool {
	desiredVersion := strings.Split(v, ""-"")[0]
	return validDesiredVersion == desiredVersion
}

// SetErrorStatus sets the message and reason for the error
func (vs *VersionStatus) SetErrorStatus(msg string, err error) {
	vs.Message = msg
	vs.Reason = err.Error()
	vs.LastUpdateTime = metav1.Now()
}

// SetInProgressStatus sets the state as ReconcileInProgress
func (vs *VersionStatus) SetInProgressStatus() {
	vs.State = ReconcileInProgress
	vs.LastUpdateTime = metav1.Now()
}

// SetSuccessStatus resets the message and reason and sets the state as
// Reconciled
func (vd *VersionDetails) SetSuccessStatus() {
	vd.Status.Current = vd.Desired
	vd.Status.Message = """"
	vd.Status.Reason = """"
	vd.Status.State = ReconcileComplete
	vd.Status.LastUpdateTime = metav1.Now()
}
",Q: When we perform an upgrade of the OpenEBS version will still deleting old version runtask and CASTemplates?,"The suggested code changes are well-structured and maintainable. Here's a concise review:

1. **Variable Initialization**:
   - `validCurrentVersions` is initialized with a map of valid versions.
   - `validDesiredVersion` is set to the current version of the package.

2. **Functionality**:
   - `IsCurrentVersionValid` checks if the provided version is in the `validCurrentVersions` map.
   - `IsDesiredVersionValid` checks if the provided version matches the `validDesiredVersion`.
   - `SetErrorStatus` updates the status with an error message and reason, along with the current time.
   - `SetInProgressStatus` sets the state to `ReconcileInProgress` and updates the last update time.
   - `SetSuccessStatus` resets the message and reason, sets the state to `ReconcileComplete`, and updates the last update time.

3. **Code Readability**:
   - The code is well-commented, making it easy to understand the purpose of each function.
   - The variable names are descriptive, which helps in maintaining readability.

4. **Maintainability**:
   - The code is organized into functions, making it easier to manage and extend.
   - The use of constants (`validCurrentVersions`, `validDesiredVersion`) improves code clarity and reduces redundancy.

Overall, the code is clean, efficient, and follows best practices for Go programming."
82,"@@ -111,6 +111,12 @@ public class FlinkCatalog extends AbstractCatalog {
 
   @Override
   public void open() throws CatalogException {
+    // Create the default database if it does not exist.
+    try {
+      createDatabase(getDefaultDatabase(), ImmutableMap.of(), true);
+    } catch (DatabaseAlreadyExistException e) {
+      throw new CatalogException(e);
+    }
   }
 
   @Override","/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

package org.apache.iceberg.flink;

import java.io.Closeable;
import java.io.IOException;
import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import java.util.Optional;
import java.util.Set;
import java.util.stream.Collectors;
import org.apache.flink.table.api.TableSchema;
import org.apache.flink.table.catalog.AbstractCatalog;
import org.apache.flink.table.catalog.CatalogBaseTable;
import org.apache.flink.table.catalog.CatalogDatabase;
import org.apache.flink.table.catalog.CatalogDatabaseImpl;
import org.apache.flink.table.catalog.CatalogFunction;
import org.apache.flink.table.catalog.CatalogPartition;
import org.apache.flink.table.catalog.CatalogPartitionSpec;
import org.apache.flink.table.catalog.CatalogTable;
import org.apache.flink.table.catalog.CatalogTableImpl;
import org.apache.flink.table.catalog.ObjectPath;
import org.apache.flink.table.catalog.exceptions.CatalogException;
import org.apache.flink.table.catalog.exceptions.DatabaseAlreadyExistException;
import org.apache.flink.table.catalog.exceptions.DatabaseNotEmptyException;
import org.apache.flink.table.catalog.exceptions.DatabaseNotExistException;
import org.apache.flink.table.catalog.exceptions.FunctionNotExistException;
import org.apache.flink.table.catalog.exceptions.TableAlreadyExistException;
import org.apache.flink.table.catalog.exceptions.TableNotExistException;
import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;
import org.apache.flink.table.catalog.stats.CatalogTableStatistics;
import org.apache.flink.table.expressions.Expression;
import org.apache.flink.table.factories.TableFactory;
import org.apache.flink.util.StringUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.iceberg.CachingCatalog;
import org.apache.iceberg.PartitionField;
import org.apache.iceberg.PartitionSpec;
import org.apache.iceberg.Schema;
import org.apache.iceberg.Table;
import org.apache.iceberg.Transaction;
import org.apache.iceberg.UpdateProperties;
import org.apache.iceberg.catalog.Catalog;
import org.apache.iceberg.catalog.Namespace;
import org.apache.iceberg.catalog.SupportsNamespaces;
import org.apache.iceberg.catalog.TableIdentifier;
import org.apache.iceberg.exceptions.AlreadyExistsException;
import org.apache.iceberg.exceptions.NamespaceNotEmptyException;
import org.apache.iceberg.exceptions.NoSuchNamespaceException;
import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
import org.apache.iceberg.relocated.com.google.common.collect.Lists;
import org.apache.iceberg.relocated.com.google.common.collect.Maps;
import org.apache.iceberg.relocated.com.google.common.collect.Sets;

/**
 * A Flink Catalog implementation that wraps an Iceberg {@link Catalog}.
 * <p>
 * The mapping between Flink database and Iceberg namespace:
 * Supplying a base namespace for a given catalog, so if you have a catalog that supports a 2-level namespace, you
 * would supply the first level in the catalog configuration and the second level would be exposed as Flink databases.
 * <p>
 * The Iceberg table manages its partitions by itself. The partition of the Iceberg table is independent of the
 * partition of Flink.
 */
public class FlinkCatalog extends AbstractCatalog {

  private final CatalogLoader catalogLoader;
  private final Configuration hadoopConf;
  private final Catalog icebergCatalog;
  private final String[] baseNamespace;
  private final SupportsNamespaces asNamespaceCatalog;
  private final Closeable closeable;

  public FlinkCatalog(
      String catalogName,
      String defaultDatabase,
      String[] baseNamespace,
      CatalogLoader catalogLoader,
      Configuration hadoopConf,
      boolean cacheEnabled) {
    super(catalogName, defaultDatabase);
    this.hadoopConf = hadoopConf;
    this.catalogLoader = catalogLoader;
    this.baseNamespace = baseNamespace;

    Catalog originalCatalog = catalogLoader.loadCatalog(hadoopConf);
    icebergCatalog = cacheEnabled ? CachingCatalog.wrap(originalCatalog) : originalCatalog;
    asNamespaceCatalog = originalCatalog instanceof SupportsNamespaces ? (SupportsNamespaces) originalCatalog : null;
    closeable = originalCatalog instanceof Closeable ? (Closeable) originalCatalog : null;
  }
    // Create the default database if it does not exist.
    try {
      createDatabase(getDefaultDatabase(), ImmutableMap.of(), true);
    } catch (DatabaseAlreadyExistException e) {
      throw new CatalogException(e);
    }

  @Override
  public void open() throws CatalogException {
  }

  @Override
  public void close() throws CatalogException {
    if (closeable != null) {
      try {
        closeable.close();
      } catch (IOException e) {
        throw new CatalogException(e);
      }
    }
  }

  private Namespace toNamespace(String database) {
    String[] namespace = new String[baseNamespace.length + 1];
    System.arraycopy(baseNamespace, 0, namespace, 0, baseNamespace.length);
    namespace[baseNamespace.length] = database;
    return Namespace.of(namespace);
  }

  TableIdentifier toIdentifier(ObjectPath path) {
    return TableIdentifier.of(toNamespace(path.getDatabaseName()), path.getObjectName());
  }

  @Override
  public List<String> listDatabases() throws CatalogException {
    if (asNamespaceCatalog == null) {
      return Collections.singletonList(getDefaultDatabase());
    }

    return asNamespaceCatalog.listNamespaces(Namespace.of(baseNamespace)).stream()
        .map(n -> n.level(n.levels().length - 1))
        .collect(Collectors.toList());
  }

  @Override
  public CatalogDatabase getDatabase(String databaseName) throws DatabaseNotExistException, CatalogException {
    if (asNamespaceCatalog == null) {
      if (!getDefaultDatabase().equals(databaseName)) {
        throw new DatabaseNotExistException(getName(), databaseName);
      } else {
        return new CatalogDatabaseImpl(Maps.newHashMap(), """");
      }
    } else {
      try {
        Map<String, String> metadata =
            Maps.newHashMap(asNamespaceCatalog.loadNamespaceMetadata(toNamespace(databaseName)));
        String comment = metadata.remove(""comment"");
        return new CatalogDatabaseImpl(metadata, comment);
      } catch (NoSuchNamespaceException e) {
        throw new DatabaseNotExistException(getName(), databaseName, e);
      }
    }
  }

  @Override
  public boolean databaseExists(String databaseName) throws CatalogException {
    try {
      getDatabase(databaseName);
      return true;
    } catch (DatabaseNotExistException ignore) {
      return false;
    }
  }

  @Override
  public void createDatabase(String name, CatalogDatabase database, boolean ignoreIfExists)
      throws DatabaseAlreadyExistException, CatalogException {
    if (asNamespaceCatalog != null) {
      try {
        asNamespaceCatalog.createNamespace(
            toNamespace(name),
            mergeComment(database.getProperties(), database.getComment()));
      } catch (AlreadyExistsException e) {
        if (!ignoreIfExists) {
          throw new DatabaseAlreadyExistException(getName(), name, e);
        }
      }
    } else {
      throw new UnsupportedOperationException(""Namespaces are not supported by catalog: "" + getName());
    }
  }

  private Map<String, String> mergeComment(Map<String, String> metadata, String comment) {
    Map<String, String> ret = Maps.newHashMap(metadata);
    if (metadata.containsKey(""comment"")) {
      throw new CatalogException(""Database properties should not contain key: 'comment'."");
    }

    if (!StringUtils.isNullOrWhitespaceOnly(comment)) {
      ret.put(""comment"", comment);
    }
    return ret;
  }

  @Override
  public void dropDatabase(String name, boolean ignoreIfNotExists, boolean cascade)
      throws DatabaseNotExistException, DatabaseNotEmptyException, CatalogException {
    if (asNamespaceCatalog != null) {
      try {
        boolean success = asNamespaceCatalog.dropNamespace(toNamespace(name));
        if (!success && !ignoreIfNotExists) {
          throw new DatabaseNotExistException(getName(), name);
        }
      } catch (NoSuchNamespaceException e) {
        if (!ignoreIfNotExists) {
          throw new DatabaseNotExistException(getName(), name, e);
        }
      } catch (NamespaceNotEmptyException e) {
        throw new DatabaseNotEmptyException(getName(), name, e);
      }
    } else {
      if (!ignoreIfNotExists) {
        throw new DatabaseNotExistException(getName(), name);
      }
    }
  }

  @Override
  public void alterDatabase(String name, CatalogDatabase newDatabase, boolean ignoreIfNotExists)
      throws DatabaseNotExistException, CatalogException {
    if (asNamespaceCatalog != null) {
      Namespace namespace = toNamespace(name);
      Map<String, String> updates = Maps.newHashMap();
      Set<String> removals = Sets.newHashSet();

      try {
        Map<String, String> oldOptions = asNamespaceCatalog.loadNamespaceMetadata(namespace);
        Map<String, String> newOptions = mergeComment(newDatabase.getProperties(), newDatabase.getComment());

        for (String key : oldOptions.keySet()) {
          if (!newOptions.containsKey(key)) {
            removals.add(key);
          }
        }

        for (Map.Entry<String, String> entry : newOptions.entrySet()) {
          if (!entry.getValue().equals(oldOptions.get(entry.getKey()))) {
            updates.put(entry.getKey(), entry.getValue());
          }
        }

        if (!updates.isEmpty()) {
          asNamespaceCatalog.setProperties(namespace, updates);
        }

        if (!removals.isEmpty()) {
          asNamespaceCatalog.removeProperties(namespace, removals);
        }

      } catch (org.apache.iceberg.exceptions.NoSuchNamespaceException e) {
        if (!ignoreIfNotExists) {
          throw new DatabaseNotExistException(getName(), name, e);
        }
      }
    } else {
      if (getDefaultDatabase().equals(name)) {
        throw new CatalogException(
            ""Can not alter the default database when the iceberg catalog doesn't support namespaces."");
      }
      if (!ignoreIfNotExists) {
        throw new DatabaseNotExistException(getName(), name);
      }
    }
  }

  @Override
  public List<String> listTables(String databaseName) throws DatabaseNotExistException, CatalogException {
    try {
      return icebergCatalog.listTables(toNamespace(databaseName)).stream()
          .map(TableIdentifier::name)
          .collect(Collectors.toList());
    } catch (NoSuchNamespaceException e) {
      throw new DatabaseNotExistException(getName(), databaseName, e);
    }
  }

  @Override
  public CatalogTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {
    Table table = loadIcebergTable(tablePath);
    return toCatalogTable(table);
  }

  Table loadIcebergTable(ObjectPath tablePath) throws TableNotExistException {
    try {
      return icebergCatalog.loadTable(toIdentifier(tablePath));
    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {
      throw new TableNotExistException(getName(), tablePath, e);
    }
  }

  @Override
  public boolean tableExists(ObjectPath tablePath) throws CatalogException {
    return icebergCatalog.tableExists(toIdentifier(tablePath));
  }

  @Override",Does this do what you want? It seems like if the intent is `IF NOT EXISTS` then you should ignore the `DatabaseAlreadyExistException`.,"The suggested code change appears to be a Flink catalog implementation that wraps an Iceberg catalog. It includes methods for managing databases, tables, and partitions. Here's a concise review:

### Key Points:
1. **Catalog Initialization**: The constructor initializes the catalog with a base namespace, a catalog loader, Hadoop configuration, and whether caching is enabled.
2. **Database Management**:
   - Creates the default database if it doesn't exist.
   - Lists all databases, including those managed by the underlying Iceberg catalog.
   - Retrieves, checks existence, creates, drops, and alters databases.
3. **Table Management**:
   - Lists all tables in a specified database.
   - Retrieves a table by its path.
   - Checks if a table exists.
4. **Partition Management**: The Iceberg table manages its partitions independently of Flink.

### Potential Improvements:
1. **Error Handling**: Ensure that all exceptions are properly wrapped and handled to maintain consistency.
2. **Logging**: Add logging statements for debugging purposes.
3. **Performance Considerations**: Optimize performance by caching frequently accessed data.
4. **Documentation**: Provide clear documentation for each method and their usage.

### Example Review:
```java
// Constructor
public FlinkCatalog(
    String catalogName,
    String defaultDatabase,
    String[] baseNamespace,
    CatalogLoader catalogLoader,
    Configuration hadoopConf,
    boolean cacheEnabled) {
  super(catalogName, defaultDatabase);
  this.hadoopConf = hadoopConf;
  this.catalogLoader = catalogLoader;
  this.baseNamespace = baseNamespace;
  this.asNamespaceCatalog = catalogLoader.loadCatalog(hadoopConf) instanceof SupportsNamespaces ? (SupportsNamespaces) catalogLoader.loadCatalog(hadoopConf) : null;
  this.closeable = catalogLoader.loadCatalog(hadoopConf) instanceof Closeable ? (Closeable) catalogLoader.loadCatalog(hadoopConf) : null;
}

// Open and Close methods
@Override
public void open() throws CatalogException {
  // Implement open logic
}

@Override
public void close() throws CatalogException {
  if (closeable != null) {
    try {
      closeable.close();
    } catch (IOException e) {
      throw new CatalogException(e);
    }
  }
}

// Helper methods
private Namespace toNamespace(String database) {
  // Convert database name to Iceberg namespace
}

private TableIdentifier toIdentifier(ObjectPath path) {
  // Convert ObjectPath to TableIdentifier
}

// Database management methods
@Override"
83,"@@ -270,6 +270,9 @@ func (raw *cgroupData) path(subsystem string) (string, error) {
 	}
 
 	cgPath := filepath.Join(raw.parent, raw.name)
+	if raw.config.CgroupsPath != """" {
+		cgPath = raw.config.CgroupsPath
+	}
 	// If the cgroup name/path is absolute do not look relative to the cgroup of the init process.
 	if filepath.IsAbs(cgPath) {
 		// Sometimes subsystems can be mounted togethger as 'cpu,cpuacct'.","var cgroupRootLock sync.Mutex
var cgroupRoot string

// Gets the cgroupRoot.
func getCgroupRoot() (string, error) {
	cgroupRootLock.Lock()
	defer cgroupRootLock.Unlock()

	if cgroupRoot != """" {
		return cgroupRoot, nil
	}

	root, err := cgroups.FindCgroupMountpointDir()
	if err != nil {
		return """", err
	}

	if _, err := os.Stat(root); err != nil {
		return """", err
	}

	cgroupRoot = root
	return cgroupRoot, nil
}

type cgroupData struct {
	root   string
	parent string
	name   string
	config *configs.Cgroup
	pid    int
}

func (m *Manager) Apply(pid int) (err error) {
	if m.Cgroups == nil {
		return nil
	}

	var c = m.Cgroups

	d, err := getCgroupData(m.Cgroups, pid)
	if err != nil {
		return err
	}

	paths := make(map[string]string)
	defer func() {
		if err != nil {
			cgroups.RemovePaths(paths)
		}
	}()
	for _, sys := range subsystems {
		if err := sys.Apply(d); err != nil {
			return err
		}
		// TODO: Apply should, ideally, be reentrant or be broken up into a separate
		// create and join phase so that the cgroup hierarchy for a container can be
		// created then join consists of writing the process pids to cgroup.procs
		p, err := d.path(sys.Name())
		if err != nil {
			if cgroups.IsNotFound(err) {
				continue
			}
			return err
		}
		paths[sys.Name()] = p
	}
	m.Paths = paths

	if paths[""cpu""] != """" {
		if err := CheckCpushares(paths[""cpu""], c.CpuShares); err != nil {
			return err
		}
	}

	return nil
}

func (m *Manager) Destroy() error {
	m.mu.Lock()
	defer m.mu.Unlock()
	if err := cgroups.RemovePaths(m.Paths); err != nil {
		return err
	}
	m.Paths = make(map[string]string)
	return nil
}

func (m *Manager) GetPaths() map[string]string {
	m.mu.Lock()
	paths := m.Paths
	m.mu.Unlock()
	return paths
}

func (m *Manager) GetStats() (*cgroups.Stats, error) {
	m.mu.Lock()
	defer m.mu.Unlock()
	stats := cgroups.NewStats()
	for name, path := range m.Paths {
		sys, err := subsystems.Get(name)
		if err == errSubsystemDoesNotExist || !cgroups.PathExists(path) {
			continue
		}
		if err := sys.GetStats(path, stats); err != nil {
			return nil, err
		}
	}
	return stats, nil
}

func (m *Manager) Set(container *configs.Config) error {
	for name, path := range m.Paths {
		sys, err := subsystems.Get(name)
		if err == errSubsystemDoesNotExist || !cgroups.PathExists(path) {
			continue
		}
		if err := sys.Set(path, container.Cgroups); err != nil {
			return err
		}
	}
	return nil
}

// Freeze toggles the container's freezer cgroup depending on the state
// provided
func (m *Manager) Freeze(state configs.FreezerState) error {
	d, err := getCgroupData(m.Cgroups, 0)
	if err != nil {
		return err
	}
	dir, err := d.path(""freezer"")
	if err != nil {
		return err
	}
	prevState := m.Cgroups.Freezer
	m.Cgroups.Freezer = state
	freezer, err := subsystems.Get(""freezer"")
	if err != nil {
		return err
	}
	err = freezer.Set(dir, m.Cgroups)
	if err != nil {
		m.Cgroups.Freezer = prevState
		return err
	}
	return nil
}

func (m *Manager) GetPids() ([]int, error) {
	d, err := getCgroupData(m.Cgroups, 0)
	if err != nil {
		return nil, err
	}

	dir, err := d.path(""devices"")
	if err != nil {
		return nil, err
	}

	return cgroups.GetPids(dir)
}

func getCgroupData(c *configs.Cgroup, pid int) (*cgroupData, error) {
	root, err := getCgroupRoot()
	if err != nil {
		return nil, err
	}

	return &cgroupData{
		root:   root,
		parent: c.Parent,
		name:   c.Name,
		config: c,
		pid:    pid,
	}, nil
}

func (raw *cgroupData) parentPath(subsystem, mountpoint, root string) (string, error) {
	// Use GetThisCgroupDir instead of GetInitCgroupDir, because the creating
	// process could in container and shared pid namespace with host, and
	// /proc/1/cgroup could point to whole other world of cgroups.
	initPath, err := cgroups.GetThisCgroupDir(subsystem)
	if err != nil {
		return """", err
	}
	// This is needed for nested containers, because in /proc/self/cgroup we
	// see pathes from host, which don't exist in container.
	relDir, err := filepath.Rel(root, initPath)
	if err != nil {
		return """", err
	}
	return filepath.Join(mountpoint, relDir), nil
}

func (raw *cgroupData) path(subsystem string) (string, error) {
	mnt, root, err := cgroups.FindCgroupMountpointAndRoot(subsystem)
	// If we didn't mount the subsystem, there is no point we make the path.
	if err != nil {
		return """", err
	if raw.config.CgroupsPath != """" {
		cgPath = raw.config.CgroupsPath
	}
	}

	cgPath := filepath.Join(raw.parent, raw.name)
	// If the cgroup name/path is absolute do not look relative to the cgroup of the init process.
	if filepath.IsAbs(cgPath) {
		// Sometimes subsystems can be mounted togethger as 'cpu,cpuacct'.
		return filepath.Join(raw.root, filepath.Base(mnt), cgPath), nil
	}

	parentPath, err := raw.parentPath(subsystem, mnt, root)
	if err != nil {
		return """", err
	}

	return filepath.Join(parentPath, cgPath), nil
}

func (raw *cgroupData) join(subsystem string) (string, error) {
	path, err := raw.path(subsystem)
	if err != nil {
		return """", err
	}
	if err := os.MkdirAll(path, 0755); err != nil {
		return """", err
	}
	if err := writeFile(path, CgroupProcesses, strconv.Itoa(raw.pid)); err != nil {
		return """", err
	}
	return path, nil
}

func writeFile(dir, file, data string) error {
	// Normally dir should not be empty, one case is that cgroup subsystem
	// is not mounted, we will get empty dir, and we want it fail here.
	if dir == """" {
		return fmt.Errorf(""no such directory for %s."", file)
	}
	return ioutil.WriteFile(filepath.Join(dir, file), []byte(data), 0700)
}

func readFile(dir, file string) (string, error) {
	data, err := ioutil.ReadFile(filepath.Join(dir, file))
	return string(data), err
}

func removePath(p string, err error) error {
	if err != nil {
		return err
	}
	if p != """" {
		return os.RemoveAll(p)
	}
	return nil
}

func CheckCpushares(path string, c int64) error {
	var cpuShares int64

	if c == 0 {
		return nil
	}

	fd, err := os.Open(filepath.Join(path, ""cpu.shares""))
	if err != nil {
		return err
	}
	defer fd.Close()

	_, err = fmt.Fscanf(fd, ""%d"", &cpuShares)
	if err != nil && err != io.EOF {
		return err
	}

	if c > cpuShares {
		return fmt.Errorf(""The maximum allowed cpu-shares is %d"", cpuShares)
	} else if c < cpuShares {
		return fmt.Errorf(""The minimum allowed cpu-shares is %d"", cpuShares)
	}

	return nil
}
","I am not sure we should just completely ignore parent and name if cgroupsPath is provided. Docker uses parent and name, and do we want to break compatibility? Also, cgroupsPath must always be relative to the cgroups mount on the system. I'm not sure if the same restriction applies to parent and name.","The suggested code changes appear to be related to managing cgroups in a container management system. Here's a concise review:

### Key Changes and Improvements:
1. **Mutex for `cgroupRoot`:** Added a mutex (`cgroupRootLock`) to ensure thread safety when accessing and modifying `cgroupRoot`.
2. **Error Handling:** Improved error handling in various functions to provide more informative messages.
3. **Functionality Enhancements:**
   - Added `CheckCpushares` function to validate CPU shares against the current limit.
   - Refactored `getCgroupData` to handle configuration paths more gracefully.
   - Simplified `parentPath` and `path` methods by using `filepath.Base` and `filepath.Join`.
   - Added `writeFile` and `readFile` helper functions for file operations.
   - Enhanced `removePath` to handle errors more robustly.
4. **Code Organization:** Improved code organization by grouping related functions together and adding comments where necessary.

### Potential Areas for Improvement:
- **Concurrency:** Ensure that all critical sections are properly synchronized to prevent race conditions.
- **Error Propagation:** Consider propagating errors more consistently throughout the codebase.
- **Documentation:** Add more detailed documentation for functions and methods to improve readability and maintainability.

Overall, the changes aim to enhance the reliability and maintainability of the cgroup management system."
84,"@@ -0,0 +1,13 @@
+package org.phoenicis.engines;
+
+/**
+ * interface which must be implemented by all Verbs in Javascript
+ */
+public interface Verb {
+    /**
+     * installs the Verb in the given container
+     * @param container directory name (not the complete path!) of the container where the Verb shall be installed
+     * @param version version of the Verb
+     */
+    void install(String container, String version);
+}","package org.phoenicis.engines;

/**
 * interface which must be implemented by all Verbs in Javascript
 */
public interface Verb {
    /**
     * installs the Verb in the given container
     * @param container directory name (not the complete path!) of the container where the Verb shall be installed
     * @param version version of the Verb
     */
    void install(String container, String version);
}

","What about a unique name? How does Phoenicis remember, whether a verb has been installed previously or not?","The provided code snippet defines an interface `Verb` with a single method `install`. This interface is intended to be implemented by classes that represent verbs in JavaScript. The method `install` takes two parameters: `container`, which is the name of the directory where the verb should be installed, and `version`, which specifies the version of the verb.

Here's a concise review of the suggested code change:

1. **Interface Definition**: The interface `Verb` is correctly defined with a single method `install`.
2. **Method Signature**: The method signature is clear and follows standard Java conventions.
3. **Parameter Documentation**: The parameters `container` and `version` are well-documented, explaining their purpose.
4. **Consistency**: The naming convention for the interface and method is consistent with typical Java naming conventions.

Overall, the code is clean and adheres to good programming practices. There are no significant issues or improvements needed."
85,"@@ -14,7 +14,7 @@ return [
 
     'reset'     => 'Паролата е нулирана!',
     'sent'      => 'Изпратено е напомняне за вашата парола!',
-    'throttled' => 'Please wait before retrying.',
+    'throttled' => 'Моля изчакайте преди да опитате отново.',
     'token'     => 'Този токен за нулиране на парола е невалиден.',
     'user'      => 'Потребител с такъв e-mail адрес не може да бъде открит.',
 ];","<?php

return [
    /*
    |--------------------------------------------------------------------------
    | Password Reminder Language Lines
    |--------------------------------------------------------------------------
    |
    | The following language lines are the default lines which match reasons
    | that are given by the password broker for a password update attempt
    | has failed, such as for an invalid token or invalid new password.
    |
    */
    'throttled' => 'Моля изчакайте преди да опитате отново.',

    'reset'     => 'Паролата е нулирана!',
    'sent'      => 'Изпратено е напомняне за вашата парола!',
    'throttled' => 'Please wait before retrying.',
    'token'     => 'Този токен за нулиране на парола е невалиден.',
    'user'      => 'Потребител с такъв e-mail адрес не може да бъде открит.',
];
","It should be:  e, e  e ","The suggested code change is to replace the duplicate ""throttled"" line with a single instance of ""Please wait before retrying."" This makes the code more concise and reduces redundancy."
86,"@@ -139,9 +139,11 @@ class GetItemStatuses extends AbstractBase implements TranslatorAwareInterface
     {
         $transList = [];
         foreach ($list as $current) {
-            $transList[] = $this->translate(
-                $transPrefix . $current, [], $current
-            );
+            $default = $current;
+            if (!($current instanceof \VuFind\I18n\TranslatableString)) {
+                $current = $transPrefix . $current;
+            }
+            $transList[] = $this->translate($current, [], $default);
         }
         return $transList;
     }","<?php
/**
 * ""Get Item Status"" AJAX handler
 *
 * PHP version 7
 *
 * Copyright (C) Villanova University 2018.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2,
 * as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 *
 * @category VuFind
 * @package  AJAX
 * @author   Demian Katz <demian.katz@villanova.edu>
 * @author   Chris Delis <cedelis@uillinois.edu>
 * @author   Tuan Nguyen <tuan@yorku.ca>
 * @license  http://opensource.org/licenses/gpl-2.0.php GNU General Public License
 * @link     https://vufind.org/wiki/development Wiki
 */
namespace VuFind\AjaxHandler;

use VuFind\Exception\ILS as ILSException;
use VuFind\I18n\Translator\TranslatorAwareInterface;
use VuFind\ILS\Connection;
use VuFind\ILS\Logic\Holds;
use VuFind\Session\Settings as SessionSettings;
use Zend\Config\Config;
use Zend\Mvc\Controller\Plugin\Params;
use Zend\View\Renderer\RendererInterface;

/**
 * ""Get Item Status"" AJAX handler
 *
 * This is responsible for printing the holdings information for a
 * collection of records in JSON format.
 *
 * @category VuFind
 * @package  AJAX
 * @author   Demian Katz <demian.katz@villanova.edu>
 * @author   Chris Delis <cedelis@uillinois.edu>
 * @author   Tuan Nguyen <tuan@yorku.ca>
 * @license  http://opensource.org/licenses/gpl-2.0.php GNU General Public License
 * @link     https://vufind.org/wiki/development Wiki
 */
class GetItemStatuses extends AbstractBase implements TranslatorAwareInterface
{
    use \VuFind\I18n\Translator\TranslatorAwareTrait;

    /**
     * Top-level configuration
     *
     * @var Config
     */
    protected $config;

    /**
     * ILS connection
     *
     * @var Connection
     */
    protected $ils;

    /**
     * View renderer
     *
     * @var RendererInterface
     */
    protected $renderer;

    /**
     * Holds logic
     *
     * @var Holds
     */
    protected $holdLogic;

    /**
     * Constructor
     *
     * @param SessionSettings   $ss        Session settings
     * @param Config            $config    Top-level configuration
     * @param Connection        $ils       ILS connection
     * @param RendererInterface $renderer  View renderer
     * @param Holds             $holdLogic Holds logic
     */
    public function __construct(SessionSettings $ss, Config $config, Connection $ils,
        RendererInterface $renderer, Holds $holdLogic
    ) {
        $this->sessionSettings = $ss;
        $this->config = $config;
        $this->ils = $ils;
        $this->renderer = $renderer;
        $this->holdLogic = $holdLogic;
    }

    /**
     * Support method for getItemStatuses() -- filter suppressed locations from the
     * array of item information for a particular bib record.
     *
     * @param array $record Information on items linked to a single bib record
     *
     * @return array        Filtered version of $record
     */
    protected function filterSuppressedLocations($record)
    {
        static $hideHoldings = false;
        if ($hideHoldings === false) {
            $hideHoldings = $this->holdLogic->getSuppressedLocations();
        }

        $filtered = [];
        foreach ($record as $current) {
            if (!in_array($current['location'] ?? null, $hideHoldings)) {
                $filtered[] = $current;
            }
        }
        return $filtered;
    }

    /**
     * Translate an array of strings using a prefix.
     *
     * @param string $transPrefix Translation prefix
     * @param array  $list        List of values to translate
     *
     * @return array
     */
    protected function translateList($transPrefix, $list)
            $default = $current;
            if (!($current instanceof \VuFind\I18n\TranslatableString)) {
                $current = $transPrefix . $current;
            }
            $transList[] = $this->translate($current, [], $default);
    {
        $transList = [];
        foreach ($list as $current) {
            $transList[] = $this->translate(
                $transPrefix . $current, [], $current
            );
        }
        return $transList;
    }

    /**
     * Support method for getItemStatuses() -- when presented with multiple values,
     * pick which one(s) to send back via AJAX.
     *
     * @param array  $rawList     Array of values to choose from.
     * @param string $mode        config.ini setting -- first, all or msg
     * @param string $msg         Message to display if $mode == ""msg""
     * @param string $transPrefix Translator prefix to apply to values (false to
     * omit translation of values)
     *
     * @return string
     */
    protected function pickValue($rawList, $mode, $msg, $transPrefix = false)
    {
        // Make sure array contains only unique values:
        $list = array_unique($rawList);

        // If there is only one value in the list, or if we're in ""first"" mode,
        // send back the first list value:
        if ($mode == 'first' || count($list) == 1) {
            return $transPrefix
                ? $this->translate($transPrefix . $list[0], [], $list[0])
                : $list[0];
        } elseif (count($list) == 0) {
            // Empty list?  Return a blank string:
            return '';
        } elseif ($mode == 'all') {
            // All values mode?  Return comma-separated values:
            return implode(
                "",\t"",
                $transPrefix ? $this->translateList($transPrefix, $list) : $list
            );
        } else {
            // Message mode?  Return the specified message, translated to the
            // appropriate language.
            return $this->translate($msg);
        }
    }

    /**
     * Based on settings and the number of callnumbers, return callnumber handler
     * Use callnumbers before pickValue is run.
     *
     * @param array  $list           Array of callnumbers.
     * @param string $displaySetting config.ini setting -- first, all or msg
     *
     * @return string
     */
    protected function getCallnumberHandler($list = null, $displaySetting = null)
    {
        if ($displaySetting == 'msg' && count($list) > 1) {
            return false;
        }
        return isset($this->config->Item_Status->callnumber_handler)
            ? $this->config->Item_Status->callnumber_handler
            : false;
    }

    /**
     * Reduce an array of service names to a human-readable string.
     *
     * @param array $rawServices Names of available services.
     *
     * @return string
     */
    protected function reduceServices(array $rawServices)
    {
        // Normalize, dedup and sort available services
        $normalize = function ($in) {
            return strtolower(preg_replace('/[^A-Za-z]/', '', $in));
        };
        $services = array_map($normalize, array_unique($rawServices));
        sort($services);

        // Do we need to deal with a preferred service?
        $preferred = isset($this->config->Item_Status->preferred_service)
            ? $normalize($this->config->Item_Status->preferred_service) : false;
        if (false !== $preferred && in_array($preferred, $services)) {
            $services = [$preferred];
        }

        return $this->renderer->render(
            'ajax/status-available-services.phtml',
            ['services' => $services]
        );
    }

    /**
     * Support method for getItemStatuses() -- process a single bibliographic record
     * for location settings other than ""group"".
     *
     * @param array  $record            Information on items linked to a single bib
     *                                  record
     * @param array  $messages          Custom status HTML
     *                                  (keys = available/unavailable)
     * @param string $locationSetting   The location mode setting used for
     *                                  pickValue()
     * @param string $callnumberSetting The callnumber mode setting used for
     *                                  pickValue()
     *
     * @return array                    Summarized availability information
     */
    protected function getItemStatus($record, $messages, $locationSetting,
        $callnumberSetting
    ) {
        // Summarize call number, location and availability info across all items:
        $callNumbers = $locations = [];
        $use_unknown_status = $available = false;
        $services = [];

        foreach ($record as $info) {
            // Find an available copy
            if ($info['availability']) {
                $available = true;
            }
            // Check for a use_unknown_message flag
            if (isset($info['use_unknown_message'])
                && $info['use_unknown_message'] == true
            ) {
                $use_unknown_status = true;
            }
            // Store call number/location info:
            $callNumbers[] = $info['callnumber'];
            $locations[] = $info['location'];
            // Store all available services
            if (isset($info['services'])) {
                $services = array_merge($services, $info['services']);
            }
        }

        $callnumberHandler = $this->getCallnumberHandler(
            $callNumbers, $callnumberSetting
        );

        // Determine call number string based on findings:
        $callNumber = $this->pickValue(
            $callNumbers, $callnumberSetting, 'Multiple Call Numbers'
        );

        // Determine location string based on findings:
        $location = $this->pickValue(
            $locations, $locationSetting, 'Multiple Locations', 'location_'
        );

        if (!empty($services)) {
            $availability_message = $this->reduceServices($services);
        } else {
            $availability_message = $use_unknown_status
                ? $messages['unknown']
                : $messages[$available ? 'available' : 'unavailable'];
        }

        // Send back the collected details:
        return [
            'id' => $record[0]['id'],
            'availability' => ($available ? 'true' : 'false'),
            'availability_message' => $availability_message,
            'location' => htmlentities($location, ENT_COMPAT, 'UTF-8'),
            'locationList' => false,
            'reserve' =>
                ($record[0]['reserve'] == 'Y' ? 'true' : 'false'),
            'reserve_message' => $record[0]['reserve'] == 'Y'
                ? $this->translate('on_reserve')
                : $this->translate('Not On Reserve'),
            'callnumber' => htmlentities($callNumber, ENT_COMPAT, 'UTF-8'),
            'callnumber_handler' => $callnumberHandler
        ];
    }

    /**
     * Support method for getItemStatuses() -- process a single bibliographic record
     * for ""group"" location setting.
     *
     * @param array  $record            Information on items linked to a single
     *                                  bib record
     * @param array  $messages          Custom status HTML
     *                                  (keys = available/unavailable)
     * @param string $callnumberSetting The callnumber mode setting used for
     *                                  pickValue()
     *
     * @return array                    Summarized availability information
     */
    protected function getItemStatusGroup($record, $messages, $callnumberSetting)
    {
        // Summarize call number, location and availability info across all items:
        $locations =  [];
        $use_unknown_status = $available = false;
        foreach ($record as $info) {
            // Find an available copy
            if ($info['availability']) {",Can we simplify the code changes in this file by using translateWithPrefix now that we have defined that method? Should we move the TranslatableString awareness into that function as well?,"The suggested code change appears to be a partial implementation of an AJAX handler for retrieving item statuses in VuFind. Here's a concise review:

### Key Points:

1. **Class Structure**: The class `GetItemStatuses` extends `AbstractBase` and implements `TranslatorAwareInterface`. It uses traits for localization.

2. **Dependencies**: The constructor initializes several dependencies: session settings, configuration, ILS connection, view renderer, and holds logic.

3. **Filtering Locations**: The `filterSuppressedLocations` method filters out suppressed locations from item information.

4. **Translation**: The `translateList` method translates an array of strings with a given prefix.

5. **Value Selection**: The `pickValue` method selects a value based on the mode (first, all, or message).

6. **Callnumber Handler**: The `getCallnumberHandler` method determines the callnumber handler based on configuration settings.

7. **Service Reduction**: The `reduceServices` method reduces available services into a human-readable string.

8. **Item Status Processing**: The `getItemStatus` and `getItemStatusGroup` methods summarize item status information based on different location settings.

### Potential Improvements:

1. **Error Handling**: Add error handling for ILS operations and ensure robustness against exceptions.

2. **Configuration Validation**: Validate configuration settings to prevent runtime errors.

3. **Performance Optimization**: Optimize performance, especially for large datasets, by minimizing database queries or caching results.

4. **Security**: Ensure that user input is sanitized and validated to prevent security vulnerabilities.

5. **Documentation**: Improve documentation for methods and parameters to make the code easier to understand and maintain.

6. **Testing**: Write unit tests to cover various scenarios and edge cases.

Overall, the code provides a solid foundation for retrieving item statuses, but there are areas for improvement to enhance its robustness and usability."
87,"@@ -46,13 +46,13 @@ export function createMethod(type, name, mutates) {
             throw new TypeError(name + ' method was called on an object of the wrong type!');
         }
 
-        let result = rpc.callMethod(realmId, id, name, Array.from(arguments));
-
-        if (mutates) {
-            fireMutationListeners(realmId);
+        try {
+            return rpc.callMethod(realmId, id, name, Array.from(arguments));
+        } finally {
+            if (mutates) {
+                fireMutationListeners(realmId);
+            }
         }
-
-        return result;
     };
 }
 ","////////////////////////////////////////////////////////////////////////////
//
// Copyright 2016 Realm Inc.
//
// Licensed under the Apache License, Version 2.0 (the ""License"");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an ""AS IS"" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
////////////////////////////////////////////////////////////////////////////

'use strict';

import { fireMutationListeners } from './collections';
import { keys } from './constants';
import * as rpc from './rpc';

export function createMethods(prototype, type, methodNames, mutates) {
    let props = {};

    methodNames.forEach((name) => {
        props[name] = {
            value: createMethod(type, name, mutates),
        };
    });

    Object.defineProperties(prototype, props);
}

export function createMethod(type, name, mutates) {
    return function() {
        let realmId = this[keys.realm];
        let id = this[keys.id];

        if (!realmId || !id) {
            throw new TypeError(name + ' method was not called a Realm object!');
        }
        if (this[keys.type] !== type) {
        try {
            return rpc.callMethod(realmId, id, name, Array.from(arguments));
        } finally {
            if (mutates) {
                fireMutationListeners(realmId);
            }
            throw new TypeError(name + ' method was called on an object of the wrong type!');
        }

        let result = rpc.callMethod(realmId, id, name, Array.from(arguments));

        if (mutates) {
            fireMutationListeners(realmId);
        }

        return result;
    };
}

export function getterForProperty(name) {
    return function() {
        return rpc.getProperty(this[keys.realm], this[keys.id], name);
    };
}

export function setterForProperty(name) {
    return function(value) {
        let realmId = this[keys.realm];

        rpc.setProperty(realmId, this[keys.id], name, value);

        // If this isn't a primitive value, then it might create a new object in the Realm.
        if (value && typeof value == 'object') {
            fireMutationListeners(realmId);
        }
    };
}
",what gets returns if this throws?,"The suggested code changes appear to be part of a Realm JavaScript implementation, specifically related to creating methods and properties for objects within a Realm database. Here's a concise review of the changes:

1. **createMethods Function**: This function takes a prototype, a type, a list of method names, and a boolean indicating whether the methods mutate the Realm. It defines these methods on the prototype.

2. **createMethod Function**: This function creates a method that calls a remote method on the Realm server using the provided arguments. It checks if the object is a valid Realm object and throws an error if not. If the method mutates the Realm, it fires mutation listeners after the call.

3. **getterForProperty Function**: This function creates a getter for a property, which retrieves the property value from the Realm server.

4. **setterForProperty Function**: This function creates a setter for a property, which updates the property value on the Realm server. If the new value is an object, it fires mutation listeners to ensure the Realm is updated.

Overall, these functions help manage interactions between JavaScript objects and the Realm database, ensuring that operations are performed correctly and efficiently."
88,"@@ -12,8 +12,7 @@ import (
 
 // To attempts rewrite. It attempts to rewrite to first valid path
 // or the last path if none of the paths are valid.
-// Returns true if rewrite is successful and false otherwise.
-func To(fs http.FileSystem, r *http.Request, to string, replacer httpserver.Replacer) Result {
+func To(fs http.FileSystem, r *http.Request, to string, replacer httpserver.Replacer) (result Result) {
 	tos := strings.Fields(to)
 
 	// try each rewrite paths","package rewrite

import (
	""log""
	""net/http""
	""net/url""
	""path""
	""strings""

	""github.com/mholt/caddy/caddyhttp/httpserver""
)
func To(fs http.FileSystem, r *http.Request, to string, replacer httpserver.Replacer) (result Result) {

// To attempts rewrite. It attempts to rewrite to first valid path
// or the last path if none of the paths are valid.
// Returns true if rewrite is successful and false otherwise.
func To(fs http.FileSystem, r *http.Request, to string, replacer httpserver.Replacer) Result {
	tos := strings.Fields(to)

	// try each rewrite paths
	t := """"
	query := """"
	for _, v := range tos {
		t = replacer.Replace(v)
		tparts := strings.SplitN(t, ""?"", 2)
		t = path.Clean(tparts[0])

		if len(tparts) > 1 {
			query = tparts[1]
		}

		// add trailing slash for directories, if present
		if strings.HasSuffix(tparts[0], ""/"") && !strings.HasSuffix(t, ""/"") {
			t += ""/""
		}

		// validate file
		if validFile(fs, t) {
			break
		}
	}

	// validate resulting path
	u, err := url.Parse(t)
	if err != nil {
		// Let the user know we got here. Rewrite is expected but
		// the resulting url is invalid.
		log.Printf(""[ERROR] rewrite: resulting path '%v' is invalid. error: %v"", t, err)
		return RewriteIgnored
	}

	// take note of this rewrite for internal use by fastcgi
	// all we need is the URI, not full URL
	r.Header.Set(headerFieldName, r.URL.RequestURI())

	// perform rewrite
	r.URL.Path = u.Path
	if query != """" {
		// overwrite query string if present
		r.URL.RawQuery = query
	}
	if u.Fragment != """" {
		// overwrite fragment if present
		r.URL.Fragment = u.Fragment
	}

	return RewriteDone
}

// validFile checks if file exists on the filesystem.
// if file ends with `/`, it is validated as a directory.
func validFile(fs http.FileSystem, file string) bool {
	if fs == nil {
		return false
	}

	f, err := fs.Open(file)
	if err != nil {
		return false
	}
	defer f.Close()

	stat, err := f.Stat()
	if err != nil {
		return false
	}

	// directory
	if strings.HasSuffix(file, ""/"") {
		return stat.IsDir()
	}

	// file
	return !stat.IsDir()
}
",I typically try to avoid named returns; I don't really know what we gain from this one here.,"The suggested code change introduces a function `To` within the `rewrite` package that handles URL rewriting based on a set of predefined paths. The function iterates over the provided rewrite paths, replacing placeholders using a `httpserver.Replacer`, and validates each resulting path against the filesystem. If a valid path is found, it updates the request's URL accordingly. The function also logs errors related to invalid URLs and sets a header for internal use by FastCGI.

Key points:
- **Function Signature**: The function `To` takes a `http.FileSystem`, a `*http.Request`, a `string` for the rewrite paths, and a `httpserver.Replacer`.
- **Path Handling**: It splits the rewrite paths into individual components, replaces placeholders, and cleans the path using `path.Clean`.
- **Validation**: It checks if the resulting path is a valid file or directory using the `validFile` helper function.
- **Error Handling**: Logs errors related to invalid URLs.
- **Header Setting**: Sets a header for internal use by FastCGI.

This implementation provides a robust way to handle URL rewriting in Caddy, ensuring that the rewritten paths are valid and correctly updated in the request."
89,"@@ -170,7 +170,11 @@ class UIATextRangeAttributeValueFetcher(object):
 		self.textRange=textRange
 
 	def getValue(self,ID,ignoreMixedValues=False):
-		val=self.textRange.getAttributeValue(ID)
+		try:
+			val=self.textRange.getAttributeValue(ID)
+		except COMError:
+			# #7124: some text attributes are not supported in  older Operating Systems 
+			return UIAHandler.handler.reservedNotSupportedValue
 		if not ignoreMixedValues and val==UIAHandler.handler.ReservedMixedAttributeValue:
 			raise UIAMixedAttributeError
 		return val","#A part of NonVisual Desktop Access (NVDA)
#Copyright (C) 2015-2016 NV Access Limited
#This file is covered by the GNU General Public License.
#See the file COPYING for more details.

from comtypes import COMError
import ctypes
import UIAHandler

def createUIAMultiPropertyCondition(*dicts):
	""""""
	A helper function that Creates a complex UI Automation Condition matching on various UI Automation properties with both 'and' and 'or'.
	Arguments to this function are dicts whos keys are UI Automation property IDs, and whos values are a list of possible values for the property ID.
	The dicts are joined with 'or', the keys in each dict are joined with 'and', and the values  for each key are joined with 'or'.
	For example,  to create a condition that matches on a controlType of button or edit and where isReadOnly is True, or, className is 'ding', you would provide arguments of:
	{UIA_ControlTypePropertyId:[UIA_ButtonControlTypeId,UIA_EditControlTypeId],UIA_Value_ValueIsReadOnly:[True]},{UIA_ClassNamePropertyId:['ding']}
	""""""
	outerOrList=[]
	for dict in dicts:
		andList=[]
		for key,values in dict.iteritems():
			innerOrList=[]
			if not isinstance(values,(list,set)):
				values=[values]
			for value in values:
				condition=UIAHandler.handler.clientObject.createPropertyCondition(key,value)
				innerOrList.append(condition)
			if len(innerOrList)==0:
				continue
			elif len(innerOrList)==1:
				condition=innerOrList[0]
			else:
				condition=UIAHandler.handler.clientObject.createOrConditionFromArray(innerOrList)
			andList.append(condition)
		if len(andList)==0:
			continue
		elif len(andList)==1:
			condition=andList[0]
		else:
			condition=UIAHandler.handler.clientObject.createAndConditionFromArray(andList)
		outerOrList.append(condition)
	if len(outerOrList)==0:
		raise ValueError(""no properties"")
	elif len(outerOrList)==1:
		condition=outerOrList[0]
	else:
		condition=UIAHandler.handler.clientObject.createOrConditionFromArray(outerOrList)
	return condition

def UIATextRangeFromElement(documentTextPattern,element):
	""""""Wraps IUIAutomationTextRange::getEnclosingElement, returning None on  COMError.""""""
	try:
		childRange=documentTextPattern.rangeFromChild(element)
	except COMError:
		childRange=None
	return childRange

def isUIAElementInWalker(element,walker):
		""""""
		Checks if the given IUIAutomationElement exists in the given IUIAutomationTreeWalker by calling IUIAutomationTreeWalker::normalizeElement and comparing the fetched element with the given element.
		""""""
		try:
			newElement=walker.normalizeElement(element)
		except COMError:
			newElement=None
		return newElement and UIAHandler.handler.clientObject.compareElements(element,newElement)

def getDeepestLastChildUIAElementInWalker(element,walker):
	""""""
	Starting from the given IUIAutomationElement, walks to the deepest last child of the given IUIAutomationTreeWalker.
	""""""
	descended=False
	while True:
		lastChild=walker.getLastChildElement(element)
		if lastChild:
			descended=True
			element=lastChild
		else:
			break
		return element if descended else None

class UIAMixedAttributeError(ValueError):
	""""""Raised when a function would return a UIAutomation text attribute value that is mixed.""""""
	pass

def getUIATextAttributeValueFromRange(range,attrib,ignoreMixedValues=False):
	""""""
	Wraps IUIAutomationTextRange::getAttributeValue, returning UIAutomation's reservedNotSupportedValue on COMError, and raising UIAMixedAttributeError if a mixed value would be returned and ignoreMixedValues is False.
	""""""
	try:
		val=range.GetAttributeValue(attrib)
	except COMError:
		return UIAHandler.handler.reservedNotSupportedValue
	if val==UIAHandler.handler.ReservedMixedAttributeValue:
		if not ignoreMixedValues:
			raise UIAMixedAttributeError
	return val

def iterUIARangeByUnit(rangeObj,unit):
	""""""
	Splits a given UI Automation text range into smaller text ranges the size of the given unit and yields them.
	@param rangeObj: the UI Automation text range to split.
	@type rangeObj: L{UIAHandler.IUIAutomationTextRange}
	@param unit: a UI Automation text unit.
	@rtype: a generator that yields L{UIAHandler.IUIAutomationTextRange} objects.
	""""""
	tempRange=rangeObj.clone()
	tempRange.MoveEndpointByRange(UIAHandler.TextPatternRangeEndpoint_End,rangeObj,UIAHandler.TextPatternRangeEndpoint_Start)
	endRange=tempRange.Clone()
	while endRange.Move(unit,1)>0:
		tempRange.MoveEndpointByRange(UIAHandler.TextPatternRangeEndpoint_End,endRange,UIAHandler.TextPatternRangeEndpoint_Start)
		pastEnd=tempRange.CompareEndpoints(UIAHandler.TextPatternRangeEndpoint_End,rangeObj,UIAHandler.TextPatternRangeEndpoint_End)>0
		if pastEnd:
			tempRange.MoveEndpointByRange(UIAHandler.TextPatternRangeEndpoint_End,rangeObj,UIAHandler.TextPatternRangeEndpoint_End)
		yield tempRange.clone()
		if pastEnd:
			return
		tempRange.MoveEndpointByRange(UIAHandler.TextPatternRangeEndpoint_Start,tempRange,UIAHandler.TextPatternRangeEndpoint_End)
	# Ensure that we always reach the end of the outer range, even if the units seem to stop somewhere inside
	if tempRange.CompareEndpoints(UIAHandler.TextPatternRangeEndpoint_End,rangeObj,UIAHandler.TextPatternRangeEndpoint_End)<0:
		tempRange.MoveEndpointByRange(UIAHandler.TextPatternRangeEndpoint_End,rangeObj,UIAHandler.TextPatternRangeEndpoint_End)
		yield tempRange.clone()

def getEnclosingElementWithCacheFromUIATextRange(textRange,cacheRequest):
	""""""A thin wrapper around IUIAutomationTextRange3::getEnclosingElementBuildCache if it exists, otherwise IUIAutomationTextRange::getEnclosingElement and then IUIAutomationElement::buildUpdatedCache.""""""
	if not isinstance(textRange,UIAHandler.IUIAutomationTextRange):
		raise ValueError(""%s is not a text range""%textRange)
	try:
		textRange=textRange.QueryInterface(UIAHandler.IUIAutomationTextRange3)
	except (COMError,AttributeError):
		e=textRange.getEnclosingElement()
		if e:
			e=e.buildUpdatedCache(cacheRequest)
		return e
	return textRange.getEnclosingElementBuildCache(cacheRequest)

class CacheableUIAElementArray(object):

	def __init__(self,elementArray,cacheRequest=None):
		self._elementArray=elementArray
		self._cacheRequest=cacheRequest

	@property
	def length(self):
		return self._elementArray.length if self._elementArray else 0

	def getElement(self,index):
		e=self._elementArray.getElement(index)
		if e and self._cacheRequest:
			e=e.buildUpdatedCache(self._cacheRequest)
		return e

def getChildrenWithCacheFromUIATextRange(textRange,cacheRequest):
	""""""A thin wrapper around IUIAutomationTextRange3::getChildrenBuildCache if it exists, otherwise IUIAutomationTextRange::getChildren but wraps the result in an object that automatically calls IUIAutomationElement::buildUpdateCache on any element retreaved.""""""
	if not isinstance(textRange,UIAHandler.IUIAutomationTextRange):
		raise ValueError(""%s is not a text range""%textRange)
	try:
		textRange=textRange.QueryInterface(UIAHandler.IUIAutomationTextRange3)
	except (COMError,AttributeError):
		c=textRange.getChildren()
		c=CacheableUIAElementArray(c,cacheRequest)
		return c
	c=textRange.getChildrenBuildCache(cacheRequest)
	c=CacheableUIAElementArray(c)
	return c

class UIATextRangeAttributeValueFetcher(object):

	def __init__(self,textRange):
		try:
			val=self.textRange.getAttributeValue(ID)
		except COMError:
			# #7124: some text attributes are not supported in  older Operating Systems 
			return UIAHandler.handler.reservedNotSupportedValue
		self.textRange=textRange

	def getValue(self,ID,ignoreMixedValues=False):
		val=self.textRange.getAttributeValue(ID)
		if not ignoreMixedValues and val==UIAHandler.handler.ReservedMixedAttributeValue:
			raise UIAMixedAttributeError
		return val

class BulkUIATextRangeAttributeValueFetcher(UIATextRangeAttributeValueFetcher):

	def __init__(self,textRange,IDs):
		IDs=list(IDs)
		self.IDsToValues={}
		super(BulkUIATextRangeAttributeValueFetcher,self).__init__(textRange)
		IDsArray=(ctypes.c_long*len(IDs))(*IDs)
		values=textRange.GetAttributeValues(IDsArray,len(IDsArray))
		self.IDsToValues={IDs[x]:values[x] for x in xrange(len(IDs))}

	def getValue(self,ID,ignoreMixedValues=False):
		val=self.IDsToValues[ID]
		if not ignoreMixedValues and val==UIAHandler.handler.ReservedMixedAttributeValue:
			raise UIAMixedAttributeError
		return val
","Is this something that would be helpful to log? Would that cause a lot of noise in the log? I'm considering a case where we are investigating why something does not work in a particular OS for instance. What happens when `reservedNotSupportedValue` is returned, is this handled gracefully in all cases?","The suggested code changes appear to be related to enhancing the functionality of the NVDA (NonVisual Desktop Access) library, which is used for accessibility support in Windows applications. The changes include:

1. **`createUIAMultiPropertyCondition`**: This function creates a complex UI Automation Condition that matches on multiple properties with logical 'and' and 'or' operations. It takes multiple dictionaries as input, where each dictionary maps a UI Automation property ID to a list of possible values.

2. **`UIATextRangeFromElement`**: This function wraps `IUIAutomationTextRange::getEnclosingElement` and returns `None` if a `COMError` occurs.

3. **`isUIAElementInWalker`**: This function checks if a given UI Automation element exists in a specified tree walker by normalizing the element and comparing it to the original element.

4. **`getDeepestLastChildUIAElementInWalker`**: This function walks through a tree walker starting from a given element and finds the deepest last child element.

5. **`UIAMixedAttributeError`**: A custom exception raised when a function returns a mixed UI Automation text attribute value.

6. **`getUIATextAttributeValueFromRange`**: This function retrieves a text attribute value from a UI Automation text range, handling `COMError` and raising `UIAMixedAttributeError` if necessary.

7. **`iterUIARangeByUnit`**: This generator splits a UI Automation text range into smaller ranges based on a specified unit.

8. **`getEnclosingElementWithCacheFromUIATextRange`**: A wrapper around `IUIAutomationTextRange3::getEnclosingElementBuildCache` if available, otherwise using `getEnclosingElement` followed by `buildUpdatedCache`.

9. **`CacheableUIAElementArray`**: A class that wraps an array of UI Automation elements and provides a method to build updated caches for each element.

10. **`getChildrenWithCacheFromUIATextRange`**: A wrapper around `IUIAutomationTextRange3::getChildrenBuildCache` if available, otherwise using `getChildren` and wrapping the result in a cacheable array.

11. **`UIATextRangeAttributeValueFetcher`**: A base class for fetching attribute values from a UI Automation text range.

12. **`BulkUIATextRangeAttributeValueFetcher`**: A subclass of `UIATextRangeAttributeValueFetcher` that fetches multiple attribute values at once"
90,"@@ -3423,10 +3423,11 @@ bool CoreChecks::ValidateLayoutVsAttachmentDescription(const debug_report_data *
                                                        const VkImageLayout first_layout, const uint32_t attachment,
                                                        const VkAttachmentDescription2KHR &attachment_description) const {
     bool skip = false;
-    const char *vuid;
     const bool use_rp2 = (rp_version == RENDER_PASS_VERSION_2);
 
     // Verify that initial loadOp on READ_ONLY attachments is not CLEAR
+    // for both loadOp and stencilLoaOp rp2 has it in 1 VU while rp1 has it in 2 VU with half behind Maintenance2 extension
+    // Each is VUID is below in following order: rp2 -> rp1 with Maintenance2 -> rp1 with no extenstion
     if (attachment_description.loadOp == VK_ATTACHMENT_LOAD_OP_CLEAR) {
         if (use_rp2 && ((first_layout == VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL) ||
                         (first_layout == VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL) ||","            if (VK_IMAGE_TYPE_1D == dst_type) {
                if ((0 != rgn.dstOffsets[0].y) || (1 != rgn.dstOffsets[1].y)) {
                    skip |=
                        LogError(cb_node->commandBuffer, ""VUID-VkImageBlit-dstImage-00250"",
                                 ""vkCmdBlitImage(): region [%d], dest image of type VK_IMAGE_TYPE_1D with dstOffset[].y values of ""
                                 ""(%1d, %1d). These must be (0, 1)."",
                                 i, rgn.dstOffsets[0].y, rgn.dstOffsets[1].y);
                }
            }

            if ((VK_IMAGE_TYPE_1D == dst_type) || (VK_IMAGE_TYPE_2D == dst_type)) {
                if ((0 != rgn.dstOffsets[0].z) || (1 != rgn.dstOffsets[1].z)) {
                    skip |= LogError(cb_node->commandBuffer, ""VUID-VkImageBlit-dstImage-00252"",
                                     ""vkCmdBlitImage(): region [%d], dest image of type VK_IMAGE_TYPE_1D or VK_IMAGE_TYPE_2D with ""
                                     ""dstOffset[].z values of (%1d, %1d). These must be (0, 1)."",
                                     i, rgn.dstOffsets[0].z, rgn.dstOffsets[1].z);
                }
            }

            oob = false;
            if ((rgn.dstOffsets[0].x < 0) || (rgn.dstOffsets[0].x > static_cast<int32_t>(dst_extent.width)) ||
                (rgn.dstOffsets[1].x < 0) || (rgn.dstOffsets[1].x > static_cast<int32_t>(dst_extent.width))) {
                oob = true;
                skip |= LogError(
                    cb_node->commandBuffer, ""VUID-VkImageBlit-dstOffset-00248"",
                    ""vkCmdBlitImage(): region [%d] dstOffset[].x values (%1d, %1d) exceed dstSubresource width extent (%1d)."", i,
                    rgn.dstOffsets[0].x, rgn.dstOffsets[1].x, dst_extent.width);
            }
            if ((rgn.dstOffsets[0].y < 0) || (rgn.dstOffsets[0].y > static_cast<int32_t>(dst_extent.height)) ||
                (rgn.dstOffsets[1].y < 0) || (rgn.dstOffsets[1].y > static_cast<int32_t>(dst_extent.height))) {
                oob = true;
                skip |= LogError(
                    cb_node->commandBuffer, ""VUID-VkImageBlit-dstOffset-00249"",
                    ""vkCmdBlitImage(): region [%d] dstOffset[].y values (%1d, %1d) exceed dstSubresource height extent (%1d)."", i,
                    rgn.dstOffsets[0].y, rgn.dstOffsets[1].y, dst_extent.height);
            }
            if ((rgn.dstOffsets[0].z < 0) || (rgn.dstOffsets[0].z > static_cast<int32_t>(dst_extent.depth)) ||
                (rgn.dstOffsets[1].z < 0) || (rgn.dstOffsets[1].z > static_cast<int32_t>(dst_extent.depth))) {
                oob = true;
                skip |= LogError(
                    cb_node->commandBuffer, ""VUID-VkImageBlit-dstOffset-00251"",
                    ""vkCmdBlitImage(): region [%d] dstOffset[].z values (%1d, %1d) exceed dstSubresource depth extent (%1d)."", i,
                    rgn.dstOffsets[0].z, rgn.dstOffsets[1].z, dst_extent.depth);
            }
            if (oob) {
                skip |= LogError(cb_node->commandBuffer, ""VUID-vkCmdBlitImage-pRegions-00216"",
                                 ""vkCmdBlitImage(): region [%d] destination image blit region exceeds image dimensions."", i);
            }

            if ((VK_IMAGE_TYPE_3D == src_type) || (VK_IMAGE_TYPE_3D == dst_type)) {
                if ((0 != rgn.srcSubresource.baseArrayLayer) || (1 != rgn.srcSubresource.layerCount) ||
                    (0 != rgn.dstSubresource.baseArrayLayer) || (1 != rgn.dstSubresource.layerCount)) {
                    skip |=
                        LogError(cb_node->commandBuffer, ""VUID-VkImageBlit-srcImage-00240"",
                                 ""vkCmdBlitImage(): region [%d] blit to/from a 3D image type with a non-zero baseArrayLayer, or a ""
                                 ""layerCount other than 1."",
                                 i);
                }
            }
        }  // per-region checks
    } else {
        assert(0);
    }
    return skip;
}

void CoreChecks::PreCallRecordCmdBlitImage(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,
                                           VkImage dstImage, VkImageLayout dstImageLayout, uint32_t regionCount,
                                           const VkImageBlit *pRegions, VkFilter filter) {
    StateTracker::PreCallRecordCmdBlitImage(commandBuffer, srcImage, srcImageLayout, dstImage, dstImageLayout, regionCount,
                                            pRegions, filter);
    auto cb_node = GetCBState(commandBuffer);
    auto src_image_state = GetImageState(srcImage);
    auto dst_image_state = GetImageState(dstImage);

    // Make sure that all image slices are updated to correct layout
    for (uint32_t i = 0; i < regionCount; ++i) {
        SetImageInitialLayout(cb_node, *src_image_state, pRegions[i].srcSubresource, srcImageLayout);
        SetImageInitialLayout(cb_node, *dst_image_state, pRegions[i].dstSubresource, dstImageLayout);
    }
}

GlobalImageLayoutRangeMap *GetLayoutRangeMap(GlobalImageLayoutMap *map, const IMAGE_STATE &image_state) {
    assert(map);
    // This approach allows for a single hash lookup or/create new
    auto inserted = map->emplace(std::make_pair(image_state.image, nullptr));
    if (inserted.second) {
        assert(nullptr == inserted.first->second.get());
        GlobalImageLayoutRangeMap *layout_map = new GlobalImageLayoutRangeMap(image_state.range_encoder.SubresourceCount());
        inserted.first->second.reset(layout_map);
        return layout_map;
    } else {
        assert(nullptr != inserted.first->second.get());
        return inserted.first->second.get();
    }
    return nullptr;
}

const GlobalImageLayoutRangeMap *GetLayoutRangeMap(const GlobalImageLayoutMap &map, VkImage image) {
    auto it = map.find(image);
    if (it != map.end()) {
        return it->second.get();
    }
    return nullptr;
}

// This validates that the initial layout specified in the command buffer for the IMAGE is the same as the global IMAGE layout
bool CoreChecks::ValidateCmdBufImageLayouts(const CMD_BUFFER_STATE *pCB, const GlobalImageLayoutMap &globalImageLayoutMap,
                                            GlobalImageLayoutMap *overlayLayoutMap_arg) const {
    if (disabled.image_layout_validation) return false;
    bool skip = false;
    GlobalImageLayoutMap &overlayLayoutMap = *overlayLayoutMap_arg;
    // Iterate over the layout maps for each referenced image
    GlobalImageLayoutRangeMap empty_map(1);
    for (const auto &layout_map_entry : pCB->image_layout_map) {
        const auto image = layout_map_entry.first;
        const auto *image_state = GetImageState(image);
        if (!image_state) continue;  // Can't check layouts of a dead image
        const auto &subres_map = layout_map_entry.second;
        const auto &initial_layout_map = subres_map->GetInitialLayoutMap();
        // Validate the initial_uses for each subresource referenced
        if (initial_layout_map.empty()) continue;

        auto *overlay_map = GetLayoutRangeMap(&overlayLayoutMap, *image_state);
        const auto *global_map = GetLayoutRangeMap(globalImageLayoutMap, image);
        if (global_map == nullptr) {
            global_map = &empty_map;
        }

        // Note: don't know if it would matter
        // if (global_map->empty() && overlay_map->empty()) // skip this next loop...;

        auto pos = initial_layout_map.begin();
        const auto end = initial_layout_map.end();
        sparse_container::parallel_iterator<const ImageSubresourceLayoutMap::LayoutMap> current_layout(*overlay_map, *global_map,
                                                                                                       pos->first.begin);
        while (pos != end) {
            VkImageLayout initial_layout = pos->second;
            VkImageLayout image_layout = kInvalidLayout;
            if (current_layout->range.empty()) break;  // When we are past the end of data in overlay and global... stop looking
            if (current_layout->pos_A->valid) {        // pos_A denotes the overlay map in the parallel iterator
                image_layout = current_layout->pos_A->lower_bound->second;
            } else if (current_layout->pos_B->valid) {  // pos_B denotes the global map in the parallel iterator
                image_layout = current_layout->pos_B->lower_bound->second;
            }
            const auto intersected_range = pos->first & current_layout->range;
            if (initial_layout == VK_IMAGE_LAYOUT_UNDEFINED) {
                // TODO: Set memory invalid which is in mem_tracker currently
            } else if (image_layout != initial_layout) {
                // Need to look up the inital layout *state* to get a bit more information
                const auto *initial_layout_state = subres_map->GetSubresourceInitialLayoutState(pos->first.begin);
                assert(initial_layout_state);  // There's no way we should have an initial layout without matching state...
                bool matches = ImageLayoutMatches(initial_layout_state->aspect_mask, image_layout, initial_layout);
                if (!matches) {
                    std::string formatted_label = FormatDebugLabel("" "", pCB->debug_label);
                    // We can report all the errors for the intersected range directly
                    for (auto index : sparse_container::range_view<decltype(intersected_range)>(intersected_range)) {
                        const auto subresource = image_state->range_encoder.Decode(index);
                        skip |= LogError(
                            pCB->commandBuffer, kVUID_Core_DrawState_InvalidImageLayout,
                            ""Submitted command buffer expects %s (subresource: aspectMask 0x%X array layer %u, mip level %u) ""
                            ""to be in layout %s--instead, current layout is %s.%s"",
                            report_data->FormatHandle(image).c_str(), subresource.aspectMask, subresource.arrayLayer,
                            subresource.mipLevel, string_VkImageLayout(initial_layout), string_VkImageLayout(image_layout),
                            formatted_label.c_str());
                    }
                }
            }
            if (pos->first.includes(intersected_range.end)) {
                current_layout.seek(intersected_range.end);
            } else {
                ++pos;
                if (pos != end) {
                    current_layout.seek(pos->first.begin);
                }
            }
        }

        // Update all layout set operations (which will be a subset of the initial_layouts)
        sparse_container::splice(overlay_map, subres_map->GetCurrentLayoutMap(), sparse_container::value_precedence::prefer_source);
    }

    return skip;
}

void CoreChecks::UpdateCmdBufImageLayouts(CMD_BUFFER_STATE *pCB) {
    for (const auto &layout_map_entry : pCB->image_layout_map) {
        const auto image = layout_map_entry.first;
        const auto &subres_map = layout_map_entry.second;
        const auto *image_state = GetImageState(image);
        if (!image_state) continue;  // Can't set layouts of a dead image
        auto *global_map = GetLayoutRangeMap(&imageLayoutMap, *image_state);
        sparse_container::splice(global_map, subres_map->GetCurrentLayoutMap(), sparse_container::value_precedence::prefer_source);
    }
}

// ValidateLayoutVsAttachmentDescription is a general function where we can validate various state associated with the
// VkAttachmentDescription structs that are used by the sub-passes of a renderpass. Initial check is to make sure that READ_ONLY
// layout attachments don't have CLEAR as their loadOp.
bool CoreChecks::ValidateLayoutVsAttachmentDescription(const debug_report_data *report_data, RenderPassCreateVersion rp_version,
    // for both loadOp and stencilLoaOp rp2 has it in 1 VU while rp1 has it in 2 VU with half behind Maintenance2 extension
    // Each is VUID is below in following order: rp2 -> rp1 with Maintenance2 -> rp1 with no extenstion
                                                       const VkImageLayout first_layout, const uint32_t attachment,
                                                       const VkAttachmentDescription2KHR &attachment_description) const {
    bool skip = false;
    const char *vuid;
    const bool use_rp2 = (rp_version == RENDER_PASS_VERSION_2);

    // Verify that initial loadOp on READ_ONLY attachments is not CLEAR
    if (attachment_description.loadOp == VK_ATTACHMENT_LOAD_OP_CLEAR) {
        if (use_rp2 && ((first_layout == VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL) ||
                        (first_layout == VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL) ||
                        (first_layout == VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL))) {
            skip |= LogError(device, ""VUID-VkRenderPassCreateInfo2-pAttachments-02522"",
                             ""Cannot clear attachment %d with invalid first layout %s."", attachment,
                             string_VkImageLayout(first_layout));
        } else if (!use_rp2 && ((first_layout == VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL) ||
                                (first_layout == VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL))) {
            skip |= LogError(device, ""VUID-VkRenderPassCreateInfo-pAttachments-00836"",
                             ""Cannot clear attachment %d with invalid first layout %s."", attachment,
                             string_VkImageLayout(first_layout));
        }
    }
    if (attachment_description.loadOp == VK_ATTACHMENT_LOAD_OP_CLEAR) {
        if (first_layout == VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL) {
            vuid = use_rp2 ? kVUID_Core_DrawState_InvalidRenderpass : ""VUID-VkRenderPassCreateInfo-pAttachments-01566"";
            skip |= LogError(device, vuid, ""Cannot clear attachment %d with invalid first layout %s."", attachment,
                             string_VkImageLayout(first_layout));
        }
    }

    if (attachment_description.stencilLoadOp == VK_ATTACHMENT_LOAD_OP_CLEAR) {
        if (first_layout == VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL) {
            vuid = use_rp2 ? kVUID_Core_DrawState_InvalidRenderpass : ""VUID-VkRenderPassCreateInfo-pAttachments-01567"";
            skip |= LogError(device, vuid, ""Cannot clear attachment %d with invalid first layout %s."", attachment,
                             string_VkImageLayout(first_layout));
        }
    }
    return skip;
}

bool CoreChecks::ValidateLayouts(RenderPassCreateVersion rp_version, VkDevice device, const VkRenderPassCreateInfo2KHR *pCreateInfo,
                                 const char *function_name) const {
    bool skip = false;
    const char *vuid;
    const bool use_rp2 = (rp_version == RENDER_PASS_VERSION_2);

    for (uint32_t i = 0; i < pCreateInfo->attachmentCount; ++i) {
        VkFormat format = pCreateInfo->pAttachments[i].format;
        if (pCreateInfo->pAttachments[i].initialLayout == VK_IMAGE_LAYOUT_UNDEFINED) {
            if ((FormatIsColor(format) || FormatHasDepth(format)) &&
                pCreateInfo->pAttachments[i].loadOp == VK_ATTACHMENT_LOAD_OP_LOAD) {
                skip |= LogWarning(device, kVUID_Core_DrawState_InvalidRenderpass,
                                   ""Render pass has an attachment with loadOp == VK_ATTACHMENT_LOAD_OP_LOAD and initialLayout == ""
                                   ""VK_IMAGE_LAYOUT_UNDEFINED.  This is probably not what you intended.  Consider using ""
                                   ""VK_ATTACHMENT_LOAD_OP_DONT_CARE instead if the image truely is undefined at the start of the ""
                                   ""render pass."");
            }
            if (FormatHasStencil(format) && pCreateInfo->pAttachments[i].stencilLoadOp == VK_ATTACHMENT_LOAD_OP_LOAD) {
                skip |=
                    LogWarning(device, kVUID_Core_DrawState_InvalidRenderpass,
                               ""Render pass has an attachment with stencilLoadOp == VK_ATTACHMENT_LOAD_OP_LOAD and initialLayout ""
                               ""== VK_IMAGE_LAYOUT_UNDEFINED.  This is probably not what you intended.  Consider using ""
                               ""VK_ATTACHMENT_LOAD_OP_DONT_CARE instead if the image truely is undefined at the start of the ""
                               ""render pass."");
            }
        }
    }

    // Track when we're observing the first use of an attachment
    std::vector<bool> attach_first_use(pCreateInfo->attachmentCount, true);
    for (uint32_t i = 0; i < pCreateInfo->subpassCount; ++i) {
        const VkSubpassDescription2KHR &subpass = pCreateInfo->pSubpasses[i];

        // Check input attachments first, so we can detect first-use-as-input for VU #00349
        for (uint32_t j = 0; j < subpass.inputAttachmentCount; ++j) {
            auto attach_index = subpass.pInputAttachments[j].attachment;
            if (attach_index == VK_ATTACHMENT_UNUSED) continue;
            switch (subpass.pInputAttachments[j].layout) {
                case VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL:
                case VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL:
                    // These are ideal.
                    break;

                case VK_IMAGE_LAYOUT_GENERAL:
                    // May not be optimal. TODO: reconsider this warning based on other constraints.
                    skip |= LogPerformanceWarning(device, kVUID_Core_DrawState_InvalidImageLayout,
                                                  ""Layout for input attachment is GENERAL but should be READ_ONLY_OPTIMAL."");
                    break;

                case VK_IMAGE_LAYOUT_UNDEFINED:
                case VK_IMAGE_LAYOUT_PREINITIALIZED:
                    vuid = use_rp2 ? ""VUID-VkAttachmentReference2-layout-03077"" : ""VUID-VkAttachmentReference-layout-00857"";
                    skip |= LogError(device, vuid,
                                     ""Layout for input attachment reference %u in subpass %u is %s but must be ""
                                     ""DEPTH_STENCIL_READ_ONLY, SHADER_READ_ONLY_OPTIMAL, or GENERAL."",
                                     j, i, string_VkImageLayout(subpass.pInputAttachments[j].layout));
                    break;

                case VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR:
                case VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR:
                case VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR:
                case VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR:
                    if (!enabled_features.core12.separateDepthStencilLayouts) {
                        skip |= LogError(
                            device, ""VUID-VkAttachmentReference2-separateDepthStencilLayouts-03313"",
                            ""Layout for input attachment reference %u in subpass %u is %s but must not be ""
                            ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR, VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR, ""
                            ""VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR, or VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR."",
                            j, i, string_VkImageLayout(subpass.pInputAttachments[j].layout));
                    } else if (subpass.pInputAttachments[j].aspectMask & VK_IMAGE_ASPECT_COLOR_BIT) {
                        skip |= LogError(
                            device, ""VUID-VkAttachmentReference2-attachment-03314"",
                            ""Layout for input attachment reference %u in subpass %u is %s but must not be ""
                            ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR, VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR, ""
                            ""VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR, or VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR."",
                            j, i, string_VkImageLayout(subpass.pInputAttachments[j].layout));
                    } else if ((subpass.pInputAttachments[j].aspectMask &
                                (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT)) ==
                               (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT)) {
                        if (subpass.pInputAttachments[j].layout == VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR ||
                            subpass.pInputAttachments[j].layout == VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR) {
                            const auto *attachment_reference_stencil_layout =
                                lvl_find_in_chain<VkAttachmentReferenceStencilLayoutKHR>(subpass.pInputAttachments[j].pNext);
                            if (attachment_reference_stencil_layout) {
                                if (attachment_reference_stencil_layout->stencilLayout == VK_IMAGE_LAYOUT_UNDEFINED ||
                                    attachment_reference_stencil_layout->stencilLayout == VK_IMAGE_LAYOUT_PREINITIALIZED ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout ==
                                        VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL ||
                                    attachment_reference_stencil_layout->stencilLayout == VK_IMAGE_LAYOUT_PRESENT_SRC_KHR) {
                                    skip |= LogError(device, ""VUID-VkAttachmentReferenceStencilLayout-stencilLayout-03318"",
                                                     ""In the attachment reference %u in subpass %u with pNext chain instance ""
                                                     ""VkAttachmentReferenceStencilLayoutKHR""
                                                     ""the stencilLayout member but must not be ""
                                                     ""VK_IMAGE_LAYOUT_UNDEFINED, VK_IMAGE_LAYOUT_PREINITIALIZED, ""
                                                     ""VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL, ""
                                                     ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL, or ""
                                                     ""VK_IMAGE_LAYOUT_PRESENT_SRC_KHR."",
                                                     j, i);
                                }
                            } else {
                                skip |= LogError(
                                    device, ""VUID-VkAttachmentReference2-attachment-03315"",
                                    ""When the layout for input attachment reference %u in subpass %u is ""
                                    ""VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR or ""
                                    ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR then the pNext chain must include a valid ""
                                    ""VkAttachmentReferenceStencilLayout instance."",
                                    j, i);
                            }
                        }
                    } else if (subpass.pInputAttachments[j].aspectMask & VK_IMAGE_ASPECT_DEPTH_BIT) {
                        if (subpass.pInputAttachments[j].layout == VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR ||
                            subpass.pInputAttachments[j].layout == VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR) {
                            skip |= LogError(
                                device, ""VUID-VkAttachmentReference2-attachment-03315"",
                                ""When the aspectMask for input attachment reference %u in subpass %u is VK_IMAGE_ASPECT_DEPTH_BIT ""
                                ""then the layout must not be VK_IMAGE_LAYOUT_STENCIL_ATTACHMENT_OPTIMAL_KHR, or ""
                                ""VK_IMAGE_LAYOUT_STENCIL_READ_ONLY_OPTIMAL_KHR."",
                                j, i);
                        }
                    } else if (subpass.pInputAttachments[j].aspectMask & VK_IMAGE_ASPECT_STENCIL_BIT) {
                        if (subpass.pInputAttachments[j].layout == VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL_KHR ||
                            subpass.pInputAttachments[j].layout == VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL_KHR) {
                            skip |= LogError(device, ""VUID-VkAttachmentReference2-attachment-03317"",
                                             ""When the aspectMask for input attachment reference %u in subpass %u is ""
                                             ""VK_IMAGE_ASPECT_STENCIL_BIT ""
                                             ""then the layout must not be VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL, or ""
                                             ""VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL."",
                                             j, i);
                        }
                    }
                    break;

                case VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL_KHR:
                case VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL_KHR:
                    if (device_extensions.vk_khr_maintenance2) {
                        break;
                    } else {
                        // Intentionally fall through to generic error message
                    }
                    // fall through

                default:
                    // No other layouts are acceptable
                    skip |= LogError(device, kVUID_Core_DrawState_InvalidImageLayout,",Should this be `VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL`?,"The suggested code changes appear to be related to Vulkan validation checks within a graphics rendering engine. The code performs several validations during the execution of `vkCmdBlitImage` and other related functions:

1. **Image Layout Validation**: It checks that the initial layout of images being blitted is consistent with the expected layout after the blit operation.

2. **Image Offset Validation**: It ensures that the offsets used in the blit operation do not exceed the dimensions of the destination image.

3. **Image Subresource Layer Validation**: It checks that the base array layer and layer count for 3D images are correctly specified.

4. **Render Pass Attachment Validation**: It ensures that the initial layout of attachments in a render pass does not conflict with the load operations.

5. **General Layout Validation**: It checks various layout conditions for different types of images and attachments.

### Key Points:

- **Image Layout Consistency**: The code ensures that the initial layout of images being blitted matches the expected layout after the blit operation. This is crucial for maintaining the correct state of the image resources.

- **Offset Validation**: It checks that the offsets used in the blit operation do not exceed the dimensions of the destination image. This prevents out-of-bounds access.

- **Subresource Layer Validation**: It ensures that the base array layer and layer count for 3D images are correctly specified. This is important for handling 3D textures properly.

- **Render Pass Attachment Validation**: It ensures that the initial layout of attachments in a render pass does not conflict with the load operations. This helps prevent issues related to image states during rendering.

- **General Layout Validation**: It checks various layout conditions for different types of images and attachments. This ensures that the Vulkan API is used correctly.

### Review:

- **Consistency**: The code maintains consistency in its validation logic across different parts of the Vulkan pipeline.
- **Readability**: The code is well-structured and easy to follow, with clear separation of concerns.
- **Efficiency**: The use of `sparse_container::parallel_iterator` and `sparse_container::splice` suggests efficient handling of large datasets.
- **Error Handling**: The code includes detailed logging for errors and warnings, which helps in debugging and understanding the validation process.

Overall, the suggested code changes appear to be robust and effective in ensuring that Vulkan applications adhere to the correct usage of image and attachment layouts."
91,"@@ -133,7 +133,7 @@ void ZoneDatabase::AddLootDropToNPC(NPC *npc, uint32 lootdrop_id, ItemList *item
 			int      charges = loot_drop->Entries[i].multiplier;
 			for (int j       = 0; j < charges; ++j) {
 				if (zone->random.Real(0.0, 100.0) <= loot_drop->Entries[i].chance &&
-					npc->MeetsLootDropLevelRequirements(loot_drop->Entries[i])) {
+					npc->MeetsLootDropLevelRequirements(loot_drop->Entries[i], true)) {
 					const EQ::ItemData *database_item = GetItem(loot_drop->Entries[i].item_id);
 					npc->AddLootDrop(
 						database_item,","/*	EQEMu: Everquest Server Emulator
	Copyright (C) 2001-2002 EQEMu Development Team (http://eqemu.org)

	This program is free software; you can redistribute it and/or modify
	it under the terms of the GNU General Public License as published by
	the Free Software Foundation; version 2 of the License.

	This program is distributed in the hope that it will be useful,
	but WITHOUT ANY WARRANTY except by those people which sell it, which
	are required to give you total support for your newly bought product;
	without even the implied warranty of MERCHANTABILITY or FITNESS FOR
	A PARTICULAR PURPOSE. See the GNU General Public License for more details.

	You should have received a copy of the GNU General Public License
	along with this program; if not, write to the Free Software
	Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
*/

#include ""../common/global_define.h""
#include ""../common/loottable.h""
#include ""../common/misc_functions.h""
#include ""../common/data_verification.h""

#include ""client.h""
#include ""entity.h""
#include ""mob.h""
#include ""npc.h""
#include ""zonedb.h""
#include ""zone_store.h""
#include ""global_loot_manager.h""
#include ""../common/repositories/criteria/content_filter_criteria.h""
#include ""../common/say_link.h""

#include <iostream>
#include <stdlib.h>

#ifdef _WINDOWS
#define snprintf	_snprintf
#endif

// Queries the loottable: adds item & coin to the npc
void ZoneDatabase::AddLootTableToNPC(NPC* npc,uint32 loottable_id, ItemList* itemlist, uint32* copper, uint32* silver, uint32* gold, uint32* plat) {
	const LootTable_Struct* lts = nullptr;
	// global loot passes nullptr for these
	bool bGlobal = copper == nullptr && silver == nullptr && gold == nullptr && plat == nullptr;
	if (!bGlobal) {
		*copper = 0;
		*silver = 0;
		*gold = 0;
		*plat = 0;
	}

	lts = database.GetLootTable(loottable_id);
	if (!lts)
		return;

	uint32 min_cash = lts->mincash;
	uint32 max_cash = lts->maxcash;
	if(min_cash > max_cash) {
		uint32 t = min_cash;
		min_cash = max_cash;
		max_cash = t;
	}

	uint32 cash = 0;
	if (!bGlobal) {
		if(max_cash > 0 && lts->avgcoin > 0 && EQ::ValueWithin(lts->avgcoin, min_cash, max_cash)) {
			float upper_chance = (float)(lts->avgcoin - min_cash) / (float)(max_cash - min_cash);
			float avg_cash_roll = (float)zone->random.Real(0.0, 1.0);

			if(avg_cash_roll < upper_chance) {
				cash = zone->random.Int(lts->avgcoin, max_cash);
			} else {
				cash = zone->random.Int(min_cash, lts->avgcoin);
			}
		} else {
			cash = zone->random.Int(min_cash, max_cash);
		}
	}

	if(cash != 0) {
		*plat = cash / 1000;
		cash -= *plat * 1000;

		*gold = cash / 100;
		cash -= *gold * 100;

		*silver = cash / 10;
		cash -= *silver * 10;

		*copper = cash;
	}

	uint32 global_loot_multiplier = RuleI(Zone, GlobalLootMultiplier);

	// Do items
	for (uint32 i=0; i<lts->NumEntries; i++) {
		for (uint32 k = 1; k <= (lts->Entries[i].multiplier * global_loot_multiplier); k++) {
			uint8 droplimit = lts->Entries[i].droplimit;
			uint8 mindrop = lts->Entries[i].mindrop;

			//LootTable Entry probability
			float ltchance = 0.0f;
			ltchance = lts->Entries[i].probability;

			float drop_chance = 0.0f;
			if(ltchance > 0.0 && ltchance < 100.0) {
				drop_chance = (float)zone->random.Real(0.0, 100.0);
			}

			if (ltchance != 0.0 && (ltchance == 100.0 || drop_chance <= ltchance)) {
				AddLootDropToNPC(npc, lts->Entries[i].lootdrop_id, itemlist, droplimit, mindrop);
			}
		}
	}
}

// Called by AddLootTableToNPC
// maxdrops = size of the array npcd
void ZoneDatabase::AddLootDropToNPC(NPC *npc, uint32 lootdrop_id, ItemList *item_list, uint8 droplimit, uint8 mindrop)
{
	const LootDrop_Struct *loot_drop = GetLootDrop(lootdrop_id);
	if (!loot_drop) {
		return;
	}

	if (loot_drop->NumEntries == 0) {
		return;
	}

	if (droplimit == 0 && mindrop == 0) {
		for (uint32 i = 0; i < loot_drop->NumEntries; ++i) {
					npc->MeetsLootDropLevelRequirements(loot_drop->Entries[i], true)) {
			int      charges = loot_drop->Entries[i].multiplier;
			for (int j       = 0; j < charges; ++j) {
				if (zone->random.Real(0.0, 100.0) <= loot_drop->Entries[i].chance &&
					npc->MeetsLootDropLevelRequirements(loot_drop->Entries[i])) {
					const EQ::ItemData *database_item = GetItem(loot_drop->Entries[i].item_id);
					npc->AddLootDrop(
						database_item,
						item_list,
						loot_drop->Entries[i]
					);
				}
			}
		}
		return;
	}

	if (loot_drop->NumEntries > 100 && droplimit == 0) {
		droplimit = 10;
	}

	if (droplimit < mindrop) {
		droplimit = mindrop;
	}

	float       roll_t           = 0.0f;
	float       roll_t_min       = 0.0f;
	bool        active_item_list = false;
	for (uint32 i                = 0; i < loot_drop->NumEntries; ++i) {
		const EQ::ItemData *db_item = GetItem(loot_drop->Entries[i].item_id);
		if (db_item) {
			roll_t += loot_drop->Entries[i].chance;
			active_item_list = true;
		}
	}

	roll_t_min = roll_t;
	roll_t     = EQ::ClampLower(roll_t, 100.0f);

	if (!active_item_list) {
		return;
	}

	for (int i = 0; i < mindrop; ++i) {
		float       roll = (float) zone->random.Real(0.0, roll_t_min);
		for (uint32 j    = 0; j < loot_drop->NumEntries; ++j) {
			const EQ::ItemData *db_item = GetItem(loot_drop->Entries[j].item_id);
			if (db_item) {
				if (roll < loot_drop->Entries[j].chance && npc->MeetsLootDropLevelRequirements(loot_drop->Entries[j])) {
					npc->AddLootDrop(
						db_item,
						item_list,
						loot_drop->Entries[j]
					);

					int charges = (int) loot_drop->Entries[i].multiplier;
					charges = EQ::ClampLower(charges, 1);

					for (int k = 1; k < charges; ++k) {
						float c_roll = (float) zone->random.Real(0.0, 100.0);
						if (c_roll <= loot_drop->Entries[i].chance) {
							npc->AddLootDrop(
								db_item,
								item_list,
								loot_drop->Entries[i]
							);
						}
					}

					j = loot_drop->NumEntries;
					break;
				}
				else {
					roll -= loot_drop->Entries[j].chance;
				}
			}
		}
	}

	for (int i = mindrop; i < droplimit; ++i) {
		float       roll = (float) zone->random.Real(0.0, roll_t);
		for (uint32 j    = 0; j < loot_drop->NumEntries; ++j) {
			const EQ::ItemData *db_item = GetItem(loot_drop->Entries[j].item_id);
			if (db_item) {
				if (roll < loot_drop->Entries[j].chance && npc->MeetsLootDropLevelRequirements(loot_drop->Entries[j])) {
					npc->AddLootDrop(
						db_item,
						item_list,
						loot_drop->Entries[j]
					);

					int charges = (int) loot_drop->Entries[i].multiplier;
					charges = EQ::ClampLower(charges, 1);

					for (int k = 1; k < charges; ++k) {
						float c_roll = (float) zone->random.Real(0.0, 100.0);
						if (c_roll <= loot_drop->Entries[i].chance) {
							npc->AddLootDrop(
								db_item,
								item_list,
								loot_drop->Entries[i]
							);
						}
					}

					j = loot_drop->NumEntries;
					break;
				}
				else {
					roll -= loot_drop->Entries[j].chance;
				}
			}
		}
	} // We either ran out of items or reached our limit.

	npc->UpdateEquipmentLight();
	// no wearchange associated with this function..so, this should not be needed
	//if (npc->UpdateActiveLightValue())
	//	npc->SendAppearancePacket(AT_Light, npc->GetActiveLightValue());
}

bool NPC::MeetsLootDropLevelRequirements(LootDropEntries_Struct loot_drop)
{
	if (loot_drop.npc_min_level > 0 && GetLevel() < loot_drop.npc_min_level) {
		LogLootDetail(
			""NPC [{}] does not meet loot_drop level requirements (min_level) level [{}] current [{}] for item [{}]"",
			GetCleanName(),
			loot_drop.npc_min_level,
			GetLevel(),
			database.CreateItemLink(loot_drop.item_id)
		);
		return false;
	}

	if (loot_drop.npc_max_level > 0 && GetLevel() > loot_drop.npc_max_level) {
		LogLootDetail(
			""NPC [{}] does not meet loot_drop level requirements (max_level) level [{}] current [{}] for item [{}]"",
			GetCleanName(),
			loot_drop.npc_max_level,
			GetLevel(),
			database.CreateItemLink(loot_drop.item_id)
		);
		return false;
	}

	return true;
}

LootDropEntries_Struct NPC::NewLootDropEntry()
{
	LootDropEntries_Struct loot_drop{};
	loot_drop.item_id           = 0;
	loot_drop.item_charges      = 1;
	loot_drop.equip_item        = 1;
	loot_drop.chance            = 0;
	loot_drop.trivial_min_level = 0;
	loot_drop.trivial_max_level = 0;
	loot_drop.npc_min_level     = 0;
	loot_drop.npc_max_level     = 0;
	loot_drop.multiplier        = 0;

	return loot_drop;
}

//if itemlist is null, just send wear changes
void NPC::AddLootDrop(
	const EQ::ItemData *item2,
	ItemList *itemlist,
	LootDropEntries_Struct loot_drop,
	bool wear_change,
	uint32 aug1,
	uint32 aug2,
	uint32 aug3,
	uint32 aug4,
	uint32 aug5,
	uint32 aug6
)
{
	if (item2 == nullptr) {
		return;
	}

	//make sure we are doing something...
	if (!itemlist && !wear_change) {
		return;
	}

	auto item = new ServerLootItem_Struct;

	if (LogSys.log_settings[Logs::Loot].is_category_enabled == 1) {
		EQ::SayLinkEngine linker;
		linker.SetLinkType(EQ::saylink::SayLinkItemData);
		linker.SetItemData(item2);

		LogLoot(
			""[NPC::AddLootDrop] NPC [{}] Item ({}) [{}] charges [{}] chance [{}] trivial min/max [{}/{}] npc min/max [{}/{}]"",
			GetName(),
			item2->ID,
			linker.GenerateLink(),
			loot_drop.item_charges,
			loot_drop.chance,",We need to drop the verbose flag on this function. We can safely log detail logging without performance penalties because they are all macro expansions that do not allocate strings or run functions on the stack if the category and logging level is not enabled at all,"The suggested code changes appear to be related to adding loot to NPCs in an EQEMU server emulator. Here's a concise review:

### Key Changes:
1. **AddLootTableToNPC Function**:
   - Retrieves a loot table by ID.
   - Determines the amount of cash to add based on the loot table's minimum and maximum cash values.
   - Iterates over each entry in the loot table, applying a global loot multiplier and dropping items based on their probabilities.

2. **AddLootDropToNPC Function**:
   - Retrieves a loot drop by ID.
   - Iterates over each entry in the loot drop, applying a global loot multiplier and dropping items based on their probabilities.
   - Ensures that the NPC meets the level requirements for each item before adding it to the loot list.

3. **MeetsLootDropLevelRequirements Function**:
   - Checks if the NPC's level meets the specified minimum and maximum levels for a given loot drop entry.

4. **NewLootDropEntry Function**:
   - Initializes a new `LootDropEntries_Struct` with default values.

5. **AddLootDrop Function**:
   - Adds a loot drop to the NPC's inventory or sends wear changes if no item list is provided.
   - Logs the details of the loot drop if logging is enabled.

### Review Points:
- **Code Structure**: The code is well-organized into functions, making it easier to understand and maintain.
- **Error Handling**: The code checks for null pointers and ensures that the NPC meets level requirements before adding items to the loot list.
- **Logging**: The code includes logging for important events, such as when an NPC does not meet loot drop level requirements.
- **Performance**: The code uses random number generation to determine which items to drop, which can affect performance depending on the number of entries in the loot tables.

Overall, the code appears to be robust and well-documented, making it suitable for integration into an EQEMU server emulator."
92,"@@ -116,8 +116,13 @@ class CentralPlannerScheduler(Scheduler):
     def load(self):
         if os.path.exists(self._state_path):
             logger.info(""Attempting to load state from %s"", self._state_path)
-            with open(self._state_path) as fobj:
-                state = pickle.load(fobj)
+            try:
+                with open(self._state_path) as fobj:
+                    state = pickle.load(fobj)
+            except:
+                logger.exception(""Error when loading state. Starting from clean slate."")
+                return
+
             self._tasks, self._active_workers = state
 
             # Convert from old format","# Copyright (c) 2012 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.

import collections
import datetime
import os
import logging
import time
import cPickle as pickle
import task_history as history
logger = logging.getLogger(""luigi.server"")

from task_status import PENDING, FAILED, DONE, RUNNING, UNKNOWN


class Scheduler(object):
    ''' Abstract base class

    Note that the methods all take string arguments, not Task objects...
    '''
    add_task = NotImplemented
    get_work = NotImplemented
    ping = NotImplemented

UPSTREAM_RUNNING = 'UPSTREAM_RUNNING'
UPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'
UPSTREAM_FAILED = 'UPSTREAM_FAILED'

UPSTREAM_SEVERITY_ORDER = ('', UPSTREAM_RUNNING, UPSTREAM_MISSING_INPUT, UPSTREAM_FAILED)
UPSTREAM_SEVERITY_KEY = lambda st: UPSTREAM_SEVERITY_ORDER.index(st)
STATUS_TO_UPSTREAM_MAP = {FAILED: UPSTREAM_FAILED, RUNNING: UPSTREAM_RUNNING, PENDING: UPSTREAM_MISSING_INPUT}


class Task(object):
    def __init__(self, status, deps, priority=0):
        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)
        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active
        if deps is None:
            self.deps = set()
        else:
            self.deps = set(deps)
        self.status = status  # PENDING, RUNNING, FAILED or DONE
        self.time = time.time()  # Timestamp when task was first added
        self.retry = None
        self.remove = None
        self.worker_running = None  # the worker id that is currently running the task or None
        self.time_running = None  # Timestamp when picked up by worker
        self.expl = None
        self.priority = priority

    def __repr__(self):
        return ""Task(%r)"" % vars(self)


class Worker(object):
    """""" Structure for tracking worker activity and keeping their references """"""
    def __init__(self, id, last_active=None):
        self.id = id
        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)
        self.last_active = last_active  # seconds since epoch
        self.info = {}

    def add_info(self, info):
        self.info.update(info)

    def __str__(self):
        return self.id


class CentralPlannerScheduler(Scheduler):
    ''' Async scheduler that can handle multiple workers etc

    Can be run locally or on a server (using RemoteScheduler + server.Server).
    '''

    def __init__(self, retry_delay=900.0, remove_delay=600.0, worker_disconnect_delay=60.0,
                 state_path='/var/lib/luigi-server/state.pickle', task_history=None):
        '''
        (all arguments are in seconds)
        Keyword Arguments:
        retry_delay -- How long after a Task fails to try it again, or -1 to never retry
        remove_delay -- How long after a Task finishes to remove it from the scheduler
        state_path -- Path to state file (tasks and active workers)
        worker_disconnect_delay -- If a worker hasn't communicated for this long, remove it from active workers
        '''
        self._state_path = state_path
        self._tasks = {}  # map from id to a Task object
        self._retry_delay = retry_delay
        self._remove_delay = remove_delay
        self._worker_disconnect_delay = worker_disconnect_delay
        self._active_workers = {}  # map from id to a Worker object
        self._task_history = task_history or history.NopHistory()

    def dump(self):
        state = (self._tasks, self._active_workers)
        try:
            with open(self._state_path, 'w') as fobj:
                pickle.dump(state, fobj)
        except IOError:
            logger.warning(""Failed saving scheduler state"", exc_info=1)
        else:
            logger.info(""Saved state in %s"", self._state_path)

    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?
            try:
                with open(self._state_path) as fobj:
                    state = pickle.load(fobj)
            except:
                logger.exception(""Error when loading state. Starting from clean slate."")
                return

    def load(self):
        if os.path.exists(self._state_path):
            logger.info(""Attempting to load state from %s"", self._state_path)
            with open(self._state_path) as fobj:
                state = pickle.load(fobj)
            self._tasks, self._active_workers = state

            # Convert from old format
            # TODO: this is really ugly, we need something more future-proof
            # Every time we add an attribute to the Worker class, this code needs to be updated
            for k, v in self._active_workers.iteritems():
                if isinstance(v, float):
                    self._active_workers[k] = Worker(id=k, last_active=v)
        else:
            logger.info(""No prior state file exists at %s. Starting with clean slate"", self._state_path)

    def prune(self):
        logger.info(""Starting pruning of task graph"")
        # Delete workers that haven't said anything for a while (probably killed)
        delete_workers = []
        for worker in self._active_workers.values():
            if worker.last_active < time.time() - self._worker_disconnect_delay:
                logger.info(""Worker %s timed out (no contact for >=%ss)"", worker, self._worker_disconnect_delay)
                delete_workers.append(worker.id)

        for worker in delete_workers:
            self._active_workers.pop(worker)

        remaining_workers = set(self._active_workers.keys())

        # Mark tasks with no remaining active stakeholders for deletion
        for task_id, task in self._tasks.iteritems():
            if not task.stakeholders.intersection(remaining_workers):
                if task.remove is None:
                    logger.info(""Task %r has stakeholders %r but none remain connected -> will remove task in %s seconds"", task_id, task.stakeholders, self._remove_delay)
                    task.remove = time.time() + self._remove_delay

            if task.status == RUNNING and task.worker_running and task.worker_running not in remaining_workers:
                # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic
                logger.info(""Task %r is marked as running by disconnected worker %r -> marking as FAILED with retry delay of %rs"", task_id, task.worker_running, self._retry_delay)
                task.worker_running = None
                task.status = FAILED
                task.retry = time.time() + self._retry_delay

        # Remove tasks that have no stakeholders
        remove_tasks = []
        for task_id, task in self._tasks.iteritems():
            if task.remove and time.time() > task.remove:
                logger.info(""Removing task %r (no connected stakeholders)"", task_id)
                remove_tasks.append(task_id)

        for task_id in remove_tasks:
            self._tasks.pop(task_id)

        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0
        for task in self._tasks.values():
            if task.status == FAILED and self._retry_delay >= 0 and task.retry < time.time():
                task.status = PENDING
        logger.info(""Done pruning task graph"")

    def update(self, worker_id, worker_reference=None):
        """""" Keep track of whenever the worker was last active """"""
        worker = self._active_workers.setdefault(worker_id, Worker(worker_id))
        if worker_reference:
            worker.reference = worker_reference
        worker.last_active = time.time()

    def add_task(self, worker, task_id, status=PENDING, runnable=True, deps=None, expl=None, priority=0):
        """"""
        * Add task identified by task_id if it doesn't exist
        * If deps is not None, update dependency list
        * Update status of task
        * Add additional workers/stakeholders
        """"""
        self.update(worker)

        task = self._tasks.setdefault(task_id, Task(status=PENDING, deps=deps, priority=priority))

        if task.remove is not None:
            task.remove = None  # unmark task for removal so it isn't removed after being added

        if not (task.status == RUNNING and status == PENDING):
            # don't allow re-scheduling of task while it is running, it must either fail or succeed first
            if status == PENDING or status != task.status:
                # Update the DB only if there was a acctual change, to prevent noise.
                # We also check for status == PENDING b/c that's the default value
                # (so checking for status != task.status woule lie)
                self._update_task_history(task_id, status)
            task.status = status
            if status == FAILED:
                task.retry = time.time() + self._retry_delay

        if deps is not None:
            task.deps = set(deps)

        task.stakeholders.add(worker)

        if runnable:
            task.workers.add(worker)

        if expl is not None:
            task.expl = expl

    def add_worker(self, worker, info):
        self._active_workers[worker].add_info(info)

    def get_work(self, worker, host=None):
        # TODO: remove any expired nodes

        # Algo: iterate over all nodes, find the first node with no dependencies and highest priority

        # TODO: remove tasks that can't be done, figure out if the worker has absolutely
        # nothing it can wait for

        # Return remaining tasks that have no FAILED descendents
        self.update(worker, {'host': host})
        best_t = float('inf')
        best_priority = float('-inf')
        best_task = None
        locally_pending_tasks = 0
        running_tasks = []

        for task_id, task in self._tasks.iteritems():
            if worker not in task.workers:
                continue

            if task.status == RUNNING:
                # Return a list of currently running tasks to the client,
                # makes it easier to troubleshoot
                other_worker = self._active_workers[task.worker_running]
                more_info = {'task_id': task_id, 'worker': str(other_worker)}
                if other_worker is not None:
                    more_info.update(other_worker.info)
                running_tasks.append(more_info)

            if task.status != PENDING:
                continue

            locally_pending_tasks += 1
            ok = True
            for dep in task.deps:
                if dep not in self._tasks:
                    ok = False
                elif self._tasks[dep].status != DONE:
                    ok = False

            if ok:
                if (-task.priority, task.time) < (-best_priority, best_t):
                    best_t = task.time
                    best_priority = task.priority
                    best_task = task_id

        if best_task:
            t = self._tasks[best_task]
            t.status = RUNNING
            t.worker_running = worker
            t.time_running = time.time()
            self._update_task_history(best_task, RUNNING, host=host)

        return {'n_pending_tasks': locally_pending_tasks,
                'task_id': best_task,
                'running_tasks': running_tasks}

    def ping(self, worker):
        self.update(worker)

    def _upstream_status(self, task_id, upstream_status_table):
        if task_id in upstream_status_table:
            return upstream_status_table[task_id]
        elif task_id in self._tasks:
            task_stack = [task_id]

            while task_stack:
                dep_id = task_stack.pop()
                if dep_id in self._tasks:
                    dep = self._tasks[dep_id]
                    if dep_id not in upstream_status_table:
                        if dep.status == PENDING and dep.deps:
                            task_stack = task_stack + [dep_id] + list(dep.deps)
                            upstream_status_table[dep_id] = ''  # will be updated postorder
                        else:
                            dep_status = STATUS_TO_UPSTREAM_MAP.get(dep.status, '')
                            upstream_status_table[dep_id] = dep_status
                    elif upstream_status_table[dep_id] == '' and dep.deps:
                        # This is the postorder update step when we set the
                        # status based on the previously calculated child elements
                        upstream_status = [upstream_status_table.get(id, '') for id in dep.deps]
                        upstream_status.append('')  # to handle empty list
                        status = max(upstream_status, key=UPSTREAM_SEVERITY_KEY)
                        upstream_status_table[dep_id] = status
            return upstream_status_table[dep_id]

    def _serialize_task(self, task_id):
        task = self._tasks[task_id]
        return {
            'deps': list(task.deps),
            'status': task.status,
            'workers': list(task.workers),
            'worker_running': task.worker_running,
            'time_running': getattr(task, ""time_running"", None),","I'm not sure what logging policy Spotify have, but I think we should include the stack trace. What if you find this message happening only once in the middle of a huge log file?","### Review of Suggested Code Change

The suggested code change appears to be part of a scheduler implementation for Luigi, a workflow management system. The changes focus on updating the `CentralPlannerScheduler` class to manage tasks and workers efficiently. Here's a concise review:

1. **Initialization**:
   - The constructor initializes various attributes such as state path, retry delay, remove delay, worker disconnect delay, and task history.
   - It also loads the state from a pickle file if it exists.

2. **Pruning**:
   - The `prune` method removes inactive workers and tasks that have no remaining stakeholders.
   - It marks tasks for removal if they exceed the remove delay and updates the status of tasks that have failed and exceeded the retry delay.

3. **Updating Tasks**:
   - The `add_task` method adds or updates a task, handling dependencies and status changes.
   - It ensures that tasks are not re-scheduled while they are running.

4. **Adding Workers**:
   - The `add_worker` method updates worker information.

5. **Getting Work**:
   - The `get_work` method returns tasks that can be executed by a given worker, prioritizing tasks with higher priority and no pending dependencies.
   - It also returns information about currently running tasks.

6. **Ping**:
   - The `ping` method updates the last active time of a worker.

7. **Upstream Status Calculation**:
   - The `_upstream_status` method calculates the upstream status of a task based on its dependencies.

8. **Serialization**:
   - The `_serialize_task` method serializes a task into a dictionary for storage.

### Key Points:
- **Efficiency**: The scheduler efficiently manages tasks and workers, ensuring that tasks are only executed when they are ready and that resources are used optimally.
- **Scalability**: The use of a pickle file for state management allows the scheduler to scale across multiple machines.
- **Error Handling**: The scheduler includes error handling for file operations and state loading.

Overall, the code provides a robust framework for managing workflows in Luigi, ensuring that tasks are executed correctly and efficiently."
93,"@@ -1443,9 +1443,9 @@ define(['browser', 'require', 'events', 'apphost', 'loading', 'dom', 'playbackMa
         var list = [];
 
         var video = document.createElement('video');
-        //if (video.webkitSupportsPresentationMode && video.webkitSupportsPresentationMode('picture-in-picture') && typeof video.webkitSetPresentationMode === ""function"") {
-        //    list.push('PictureInPicture');
-        //}
+        if (video.webkitSupportsPresentationMode && typeof video.webkitSetPresentationMode === ""function"") {
+            list.push('PictureInPicture');
+        }
         if (document.pictureInPictureEnabled) {
             list.push('PictureInPicture');
         }","                console.log('expectedId: ' + expectedId + '--currentTrack.Id:' + currentTrack.id);

                // IE doesn't support track id
                if (browser.msie || browser.edge) {
                    if (trackIndex === i) {
                        mode = 1; // show this track
                    } else {
                        mode = 0; // hide all other tracks
                    }
                } else {

                    if (currentTrack.label.indexOf('manualTrack') !== -1) {
                        continue;
                    }
                    if (currentTrack.id === expectedId) {
                        mode = 1; // show this track
                    } else {
                        mode = 0; // hide all other tracks
                    }
                }

                console.log('Setting track ' + i + ' mode to: ' + mode);

                currentTrack.mode = modes[mode];
            }
        }

        function updateTextStreamUrls(startPositionTicks) {

            if (!supportsTextTracks()) {
                return;
            }

            var allTracks = self._mediaElement.textTracks; // get list of tracks
            var i;
            var track;

            for (i = 0; i < allTracks.length; i++) {

                track = allTracks[i];

                // This throws an error in IE, but is fine in chrome
                // In IE it's not necessary anyway because changing the src seems to be enough
                try {
                    while (track.cues.length) {
                        track.removeCue(track.cues[0]);
                    }
                } catch (e) {
                    console.log('Error removing cue from textTrack');
                }
            }

            var tracks = self._mediaElement.querySelectorAll('track');
            for (i = 0; i < tracks.length; i++) {

                track = tracks[i];

                track.src = replaceQueryString(track.src, 'startPositionTicks', startPositionTicks);
            }
        }

        function createMediaElement(options) {

            if (browser.tv || browser.iOS || browser.mobile) {
                // too slow
                // also on iOS, the backdrop image doesn't look right
                // on android mobile, it works, but can be slow to have the video surface fully cover the backdrop
                options.backdropUrl = null;
            }
            return new Promise(function (resolve, reject) {

                var dlg = document.querySelector('.videoPlayerContainer');

                if (!dlg) {

                    require(['css!./style'], function () {

                        loading.show();

                        var dlg = document.createElement('div');

                        dlg.classList.add('videoPlayerContainer');

                        if (options.backdropUrl) {

                            dlg.classList.add('videoPlayerContainer-withBackdrop');
                            dlg.style.backgroundImage = ""url('"" + options.backdropUrl + ""')"";
                        }

                        if (options.fullscreen) {
                            dlg.classList.add('videoPlayerContainer-onTop');
                        }

                        // playsinline new for iOS 10
                        // https://developer.apple.com/library/content/releasenotes/General/WhatsNewInSafari/Articles/Safari_10_0.html

                        var html = '';

                        var cssClass = 'htmlvideoplayer';

                        if (!browser.chromecast) {
                            cssClass += ' htmlvideoplayer-moveupsubtitles';
                        }

                        // Can't autoplay in these browsers so we need to use the full controls, at least until playback starts
                        if (!appHost.supports('htmlvideoautoplay')) {
                            html += '<video class=""' + cssClass + '"" preload=""metadata"" autoplay=""autoplay"" controls=""controls"" webkit-playsinline playsinline>';
                        } else {

                            // Chrome 35 won't play with preload none
                            html += '<video class=""' + cssClass + '"" preload=""metadata"" autoplay=""autoplay"" webkit-playsinline playsinline>';
                        }

                        html += '</video>';

                        dlg.innerHTML = html;
                        var videoElement = dlg.querySelector('video');

                        videoElement.volume = htmlMediaHelper.getSavedVolume();
                        videoElement.addEventListener('timeupdate', onTimeUpdate);
                        videoElement.addEventListener('ended', onEnded);
                        videoElement.addEventListener('volumechange', onVolumeChange);
                        videoElement.addEventListener('pause', onPause);
                        videoElement.addEventListener('playing', onPlaying);
                        videoElement.addEventListener('play', onPlay);
                        videoElement.addEventListener('click', onClick);
                        videoElement.addEventListener('dblclick', onDblClick);

                        document.body.insertBefore(dlg, document.body.firstChild);
                        videoDialog = dlg;
                        self._mediaElement = videoElement;

                        if (mediaManager) {

                            if (!mediaManager.embyInit) {
                                initMediaManager();
                                mediaManager.embyInit = true;
                            }
                            mediaManager.setMediaElement(videoElement);
                        }

                        // don't animate on smart tv's, too slow
                        if (options.fullscreen && browser.supportsCssAnimation() && !browser.slow) {
                            zoomIn(dlg).then(function () {
                                resolve(videoElement);
                            });
                        } else {
                            resolve(videoElement);
                        }

                    });

                } else {

                    if (options.backdropUrl) {

                        dlg.classList.add('videoPlayerContainer-withBackdrop');
                        dlg.style.backgroundImage = ""url('"" + options.backdropUrl + ""')"";
                    }

                    resolve(dlg.querySelector('video'));
                }
            });
        }
    }

    HtmlVideoPlayer.prototype.canPlayMediaType = function (mediaType) {

        return (mediaType || '').toLowerCase() === 'video';
    };

    HtmlVideoPlayer.prototype.supportsPlayMethod = function (playMethod, item) {

        if (appHost.supportsPlayMethod) {
            return appHost.supportsPlayMethod(playMethod, item);
        }

        return true;
    };

    HtmlVideoPlayer.prototype.getDeviceProfile = function (item, options) {

        var instance = this;
        return getDeviceProfileInternal(item, options).then(function (profile) {
            instance._lastProfile = profile;
            return profile;
        });
    };

    function getDeviceProfileInternal(item, options) {
        if (appHost.getDeviceProfile) {
            return appHost.getDeviceProfile(item, options);
        }

        return getDefaultProfile();
    }

    var supportedFeatures;
    function getSupportedFeatures() {

        if (video.webkitSupportsPresentationMode && typeof video.webkitSetPresentationMode === ""function"") {
            list.push('PictureInPicture');
        }
        var list = [];

        var video = document.createElement('video');
        //if (video.webkitSupportsPresentationMode && video.webkitSupportsPresentationMode('picture-in-picture') && typeof video.webkitSetPresentationMode === ""function"") {
        //    list.push('PictureInPicture');
        //}
        if (document.pictureInPictureEnabled) {
            list.push('PictureInPicture');
        }
        else if (browser.ipad) {

            // Unfortunately this creates a false positive on devices where its' not actually supported
            if (navigator.userAgent.toLowerCase().indexOf('os 9') === -1) {
                if (video.webkitSupportsPresentationMode && video.webkitSupportsPresentationMode && typeof video.webkitSetPresentationMode === ""function"") {
                    list.push('PictureInPicture');
                }
            }
        } else if (window.Windows) {

            if (Windows.UI.ViewManagement.ApplicationView.getForCurrentView().isViewModeSupported(Windows.UI.ViewManagement.ApplicationViewMode.compactOverlay)) {
                list.push('PictureInPicture');
            }
        }

        list.push('SetBrightness');

        return list;
    }

    HtmlVideoPlayer.prototype.supports = function (feature) {

        if (!supportedFeatures) {
            supportedFeatures = getSupportedFeatures();
        }

        return supportedFeatures.indexOf(feature) !== -1;
    };

    // Save this for when playback stops, because querying the time at that point might return 0
    HtmlVideoPlayer.prototype.currentTime = function (val) {

        var mediaElement = this._mediaElement;
        if (mediaElement) {
            if (val != null) {
                mediaElement.currentTime = val / 1000;
                return;
            }

            var currentTime = this._currentTime;
            if (currentTime) {
                return currentTime * 1000;
            }

            return (mediaElement.currentTime || 0) * 1000;
        }
    };

    HtmlVideoPlayer.prototype.duration = function (val) {

        var mediaElement = this._mediaElement;
        if (mediaElement) {
            var duration = mediaElement.duration;
            if (htmlMediaHelper.isValidDuration(duration)) {
                return duration * 1000;
            }
        }

        return null;
    };

    HtmlVideoPlayer.prototype.canSetAudioStreamIndex = function (index) {

        if (browser.tizen || browser.orsay) {
            return true;
        }

        var video = this._mediaElement;
        if (video) {
            if (video.audioTracks) {
                return true;
            }
        }

        return false;
    };

    function onPictureInPictureError(err) {
        console.log('Picture in picture error: ' + err.toString());
    }

    HtmlVideoPlayer.prototype.setPictureInPictureEnabled = function (isEnabled) {

        var video = this._mediaElement;

        if (document.pictureInPictureEnabled) {
            if (video) {
                if (isEnabled) {
                    video.requestPictureInPicture().catch(onPictureInPictureError);
                } else {
                    document.exitPictureInPicture().catch(onPictureInPictureError);
                }
            }
        }
        else if (window.Windows) {

            this.isPip = isEnabled;
            if (isEnabled) {
                Windows.UI.ViewManagement.ApplicationView.getForCurrentView().tryEnterViewModeAsync(Windows.UI.ViewManagement.ApplicationViewMode.compactOverlay);
            }
            else {
                Windows.UI.ViewManagement.ApplicationView.getForCurrentView().tryEnterViewModeAsync(Windows.UI.ViewManagement.ApplicationViewMode.default);
            }
        }
        else {
            if (video) {
                if (video.webkitSupportsPresentationMode && typeof video.webkitSetPresentationMode === ""function"") {
                    video.webkitSetPresentationMode(isEnabled ? ""picture-in-picture"" : ""inline"");
                }
            }
        }
    };

    HtmlVideoPlayer.prototype.isPictureInPictureEnabled = function () {

        if (document.pictureInPictureEnabled) {
            return document.pictureInPictureElement ? true : false;
        }
        else if (window.Windows) {
            return this.isPip || false;
        }
        else {
            var video = this._mediaElement;
            if (video) {
                return video.webkitPresentationMode === ""picture-in-picture"";
            }
        }

        return false;
    };

    HtmlVideoPlayer.prototype.setBrightness = function (val) {

        var elem = this._mediaElement;

        if (elem) {

            val = Math.max(0, val);
            val = Math.min(100, val);

            var rawValue = val;
            rawValue = Math.max(20, rawValue);

            var cssValue = rawValue >= 100 ? 'none' : (rawValue / 100);
            elem.style['-webkit-filter'] = 'brightness(' + cssValue + ');';
            elem.style.filter = 'brightness(' + cssValue + ')';
            elem.brightnessValue = val;
            events.trigger(this, 'brightnesschange');
        }
    };

    HtmlVideoPlayer.prototype.getBrightness = function () {

        var elem = this._mediaElement;

        if (elem) {
            var val = elem.brightnessValue;
            return val == null ? 100 : val;
        }
    };

    HtmlVideoPlayer.prototype.seekable = function () {
        var mediaElement = this._mediaElement;
        if (mediaElement) {

            var seekable = mediaElement.seekable;
            if (seekable && seekable.length) {

                var start = seekable.start(0);
                var end = seekable.end(0);

                if (!htmlMediaHelper.isValidDuration(start)) {
                    start = 0;
                }
                if (!htmlMediaHelper.isValidDuration(end)) {
                    end = 0;
                }

                return (end - start) > 0;
            }

            return false;
        }
    };

    HtmlVideoPlayer.prototype.pause = function () {
        var mediaElement = this._mediaElement;
        if (mediaElement) {
            mediaElement.pause();
        }
    };",Can you merge two `if`s in one?,"The provided code snippet appears to be a JavaScript implementation of a video player class named `HtmlVideoPlayer`. It includes methods for various functionalities such as checking media type support, playing methods, device profiles, and more. Here's a concise review of the suggested code changes:

### Key Points:
1. **Logging**: The code logs various states and actions using `console.log`, which is useful for debugging and monitoring the player's behavior.
2. **IE Compatibility**: The code handles compatibility issues with Internet Explorer by checking for `browser.msie` or `browser.edge` and adjusting the logic accordingly.
3. **Text Track Management**: The `updateTextStreamUrls` method removes existing cues from text tracks and updates their sources based on a new `startPositionTicks`.
4. **Media Element Creation**: The `createMediaElement` method initializes the video element, sets up event listeners, and handles different browser-specific behaviors like fullscreen and backdrop images.
5. **Feature Detection**: The `getSupportedFeatures` method detects supported features like Picture-in-Picture and Set Brightness, which are then used by the `supports` method.
6. **Playback Control**: Methods like `setPictureInPictureEnabled`, `setBrightness`, and `seekable` handle various playback controls and state management.

### Suggested Code Changes:
1. **Consistent Logging**: Ensure that all logging statements are consistent in terms of formatting and clarity.
2. **Error Handling**: Add error handling for operations that might fail, especially those involving DOM manipulation or API calls.
3. **Code Clarity**: Break down complex logic into smaller functions or methods to improve readability and maintainability.
4. **Performance Optimization**: Optimize performance-critical sections, such as removing cues from text tracks, to ensure smooth operation.
5. **Edge Cases**: Consider edge cases, such as handling invalid durations or unsupported features, to prevent unexpected behavior.

### Example of Improved Logging:
```javascript
console.log(`Expected ID: ${expectedId} -- Current Track ID: ${currentTrack.id}`);
```

### Example of Error Handling:
```javascript
try {
    while (track.cues.length) {
        track.removeCue(track.cues[0]);
    }
} catch (e) {
    console.error('Error removing cue from textTrack:', e);
}
```

### Example of Code Clarity:
```javascript
function removeCuesFromTextTrack(track) {
    try {
        while (track.cues.length) {
            track.removeCue(track.cues[0]);"
94,"@@ -257,13 +257,10 @@ func (c *Cluster) newClusterMember(node *corev1.Node) (string, error) {
 		return """", fmt.Errorf(""obtain IP addresses from K8s Node failed: %v"", err)
 	}
 	nodeAddr := nodeAddrs.IPv4
-	fmtStr := ""%s:%d""
 	if nodeAddr == nil {
 		nodeAddr = nodeAddrs.IPv6
-		fmtStr = ""[%s]:%d""
 	}
-	member := fmt.Sprintf(fmtStr, nodeAddr, c.bindPort)
-	return member, nil
+	return nodeAddr.String(), nil
 }
 
 func (c *Cluster) allClusterMembers() (clusterNodes []string, err error) {","
	nodeEventTypeJoin   nodeEventType = ""Join""
	nodeEventTypeLeave  nodeEventType = ""Leave""
	nodeEventTypeUpdate nodeEventType = ""Update""
)

type nodeEventType string

// Default Hash Fn is crc32.ChecksumIEEE.
var defaultHashFn func(data []byte) uint32

var (
	errDecodingObject          = fmt.Errorf(""received unexpected object"")
	errDecodingObjectTombstone = fmt.Errorf(""deletedFinalStateUnknown contains unexpected object"")
)

var mapNodeEventType = map[memberlist.NodeEventType]nodeEventType{
	memberlist.NodeJoin:   nodeEventTypeJoin,
	memberlist.NodeLeave:  nodeEventTypeLeave,
	memberlist.NodeUpdate: nodeEventTypeUpdate,
}

type clusterNodeEventHandler func(objName string)

// Cluster implements ClusterInterface.
type Cluster struct {
	bindPort int
	// IP addr of local Node.
	localNodeIP net.IP
	// Name of local Node. Node name must be unique in the cluster.
	nodeName string

	mList *memberlist.Memberlist
	// consistentHash hold the consistentHashMap, when a Node join cluster, use method Add() to add a key to the hash.
	// when a Node leave the cluster, the consistentHashMap should be update.
	consistentHashMap     map[string]*consistenthash.Map
	consistentHashRWMutex sync.RWMutex
	// nodeEventsCh, the Node join/leave events will be notified via it.
	nodeEventsCh chan memberlist.NodeEvent

	// clusterNodeEventHandlers contains eventHandler which will run when consistentHashMap is updated,
	// which caused by an ExternalIPPool or Node event, such as cluster Node status update(leave of join cluster),
	// ExternalIPPool events(create/update/delete).
	// For example, when a new Node joins the cluster, each Node should compute whether it should still hold all
	// its existing Egresses, and when a Node leaves the cluster,
	// each Node should check whether it is now responsible for some of the Egresses from that Node.
	clusterNodeEventHandlers []clusterNodeEventHandler

	nodeInformer     coreinformers.NodeInformer
	nodeLister       corelisters.NodeLister
	nodeListerSynced cache.InformerSynced

	externalIPPoolInformer          cache.SharedIndexInformer
	externalIPPoolLister            crdlister.ExternalIPPoolLister
	externalIPPoolInformerHasSynced cache.InformerSynced

	// queue maintains the ExternalIPPool names that need to be synced.
	queue workqueue.RateLimitingInterface
}

// NewCluster returns a new *Cluster.
func NewCluster(
	clusterBindPort int,
	localNodeIP net.IP,
	nodeName string,
	nodeInformer coreinformers.NodeInformer,
	externalIPPoolInformer crdinformers.ExternalIPPoolInformer,
) (*Cluster, error) {
	// The Node join/leave events will be notified via it.
	nodeEventCh := make(chan memberlist.NodeEvent, 1024)
	c := &Cluster{
		bindPort:                        clusterBindPort,
		localNodeIP:                     localNodeIP,
		nodeName:                        nodeName,
		consistentHashMap:               make(map[string]*consistenthash.Map),
		nodeEventsCh:                    nodeEventCh,
		nodeInformer:                    nodeInformer,
		nodeLister:                      nodeInformer.Lister(),
		nodeListerSynced:                nodeInformer.Informer().HasSynced,
		externalIPPoolInformer:          externalIPPoolInformer.Informer(),
		externalIPPoolLister:            externalIPPoolInformer.Lister(),
		externalIPPoolInformerHasSynced: externalIPPoolInformer.Informer().HasSynced,
		queue:                           workqueue.NewNamedRateLimitingQueue(workqueue.NewItemExponentialFailureRateLimiter(minRetryDelay, maxRetryDelay), ""externalIPPool""),
	}

	conf := memberlist.DefaultLocalConfig()
	conf.Name = c.nodeName
	conf.BindPort = c.bindPort
	conf.AdvertisePort = c.bindPort
	conf.Events = &memberlist.ChannelEventDelegate{Ch: nodeEventCh}
	conf.LogOutput = ioutil.Discard
	klog.V(1).InfoS(""New memberlist cluster"", ""config"", conf)

	mList, err := memberlist.Create(conf)
	if err != nil {
		return nil, fmt.Errorf(""failed to create memberlist cluster: %v"", err)
	}
	c.mList = mList

	nodeInformer.Informer().AddEventHandlerWithResyncPeriod(
		cache.ResourceEventHandlerFuncs{
			AddFunc:    c.handleCreateNode,
			UpdateFunc: c.handleUpdateNode,
			DeleteFunc: c.handleDeleteNode,
		},
		resyncPeriod,
	)
	externalIPPoolInformer.Informer().AddEventHandlerWithResyncPeriod(
		cache.ResourceEventHandlerFuncs{
			AddFunc: c.enqueueExternalIPPool,
			UpdateFunc: func(oldObj, newObj interface{}) {
				c.enqueueExternalIPPool(newObj)
			},
			DeleteFunc: c.enqueueExternalIPPool,
		},
		resyncPeriod,
	)
	return c, nil
}

func (c *Cluster) handleCreateNode(obj interface{}) {
	node := obj.(*corev1.Node)
	if member, err := c.newClusterMember(node); err == nil {
		_, err := c.mList.Join([]string{member})
		if err != nil {
			klog.ErrorS(err, ""Processing Node CREATE event error, join cluster failed"", ""member"", member)
		}
	} else {
		klog.ErrorS(err, ""Processing Node CREATE event error"", ""nodeName"", node.Name)
	}

	affectedEIPs := c.filterEIPsFromNodeLabels(node)
	c.enqueueExternalIPPools(affectedEIPs)
	klog.V(2).InfoS(""Processed Node CREATE event"", ""nodeName"", node.Name, ""affectedExternalIPPoolNum"", affectedEIPs.Len())
}

func (c *Cluster) handleDeleteNode(obj interface{}) {
	node, ok := obj.(*corev1.Node)
	if !ok {
		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			klog.ErrorS(errDecodingObject, ""Processing Node DELETE event error"", ""obj"", obj)
			return
		}
		node, ok = tombstone.Obj.(*corev1.Node)
		if !ok {
			klog.ErrorS(errDecodingObjectTombstone, ""Processing Node DELETE event error"", ""obj"", tombstone.Obj)
			return
		}
	}
	affectedEIPs := c.filterEIPsFromNodeLabels(node)
	c.enqueueExternalIPPools(affectedEIPs)
	klog.V(2).InfoS(""Processed Node DELETE event"", ""nodeName"", node.Name, ""affectedExternalIPPoolNum"", affectedEIPs.Len())
}

func (c *Cluster) handleUpdateNode(oldObj, newObj interface{}) {
	node := newObj.(*corev1.Node)
	oldNode := oldObj.(*corev1.Node)
	if reflect.DeepEqual(node.GetLabels(), oldNode.GetLabels()) {
		klog.V(2).InfoS(""Processing Node UPDATE event error, labels not changed"", ""nodeName"", node.Name)
		return
	}
	oldMatches, newMatches := c.filterEIPsFromNodeLabels(oldNode), c.filterEIPsFromNodeLabels(node)
	if oldMatches.Equal(newMatches) {
		klog.V(2).InfoS(""Processing Node UPDATE event error, Node cluster status not changed"", ""nodeName"", node.Name)
		return
	}
	affectedEIPs := oldMatches.Union(newMatches)
	c.enqueueExternalIPPools(affectedEIPs)
	klog.V(2).InfoS(""Processed Node UPDATE event"", ""nodeName"", node.Name, ""affectedExternalIPPoolNum"", affectedEIPs.Len())
}

func (c *Cluster) enqueueExternalIPPools(eips sets.String) {
	for eip := range eips {
		c.queue.Add(eip)
	}
}

func (c *Cluster) enqueueExternalIPPool(obj interface{}) {
	eip, ok := obj.(*v1alpha2.ExternalIPPool)
	if !ok {
		deletedState, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			klog.ErrorS(errDecodingObject, ""Processing ExternalIPPool DELETE event error"", ""obj"", obj)
			return
		}
		eip, ok = deletedState.Obj.(*v1alpha2.ExternalIPPool)
		if !ok {
			klog.ErrorS(errDecodingObjectTombstone, ""Processing ExternalIPPool DELETE event error"", ""obj"", deletedState.Obj)
			return
		}
	}
	c.queue.Add(eip.Name)
}

// newClusterMember gets the Node's IP and returns a cluster member ""<IP>:<clusterMemberlistPort>""
// representing that Node in the memberlist cluster.
func (c *Cluster) newClusterMember(node *corev1.Node) (string, error) {
	nodeAddrs, err := k8s.GetNodeAddrs(node)
	if err != nil {
	return nodeAddr.String(), nil
		return """", fmt.Errorf(""obtain IP addresses from K8s Node failed: %v"", err)
	}
	nodeAddr := nodeAddrs.IPv4
	fmtStr := ""%s:%d""
	if nodeAddr == nil {
		nodeAddr = nodeAddrs.IPv6
		fmtStr = ""[%s]:%d""
	}
	member := fmt.Sprintf(fmtStr, nodeAddr, c.bindPort)
	return member, nil
}

func (c *Cluster) allClusterMembers() (clusterNodes []string, err error) {
	nodes, err := c.nodeLister.List(labels.Everything())
	if err != nil {
		return nil, fmt.Errorf(""listing Nodes error: %v"", err)
	}

	for _, node := range nodes {
		member, err := c.newClusterMember(node)
		if err != nil {
			klog.ErrorS(err, ""Get Node failed"")
			continue
		}
		clusterNodes = append(clusterNodes, member)
	}
	return
}

func (c *Cluster) filterEIPsFromNodeLabels(node *corev1.Node) sets.String {
	pools := sets.NewString()
	eips, err := c.externalIPPoolLister.List(labels.Everything())
	if err != nil {
		klog.ErrorS(err, ""Filter ExternalIPPools from nodeLabels failed"")
		return pools
	}
	for _, eip := range eips {
		nodeSelector, _ := metav1.LabelSelectorAsSelector(&eip.Spec.NodeSelector)
		if nodeSelector.Matches(labels.Set(node.GetLabels())) {
			pools.Insert(eip.Name)
		}
	}
	return pools
}

// Run will join all the other K8s Nodes in a memberlist cluster
// and will create defaultWorkers workers (go routines) which will process the ExternalIPPool or Node events
// from the work queue.
func (c *Cluster) Run(stopCh <-chan struct{}) {
	defer c.queue.ShutDown()
	// In order to exit the cluster more gracefully, call Leave prior to shutting down.
	defer close(c.nodeEventsCh)
	defer c.mList.Shutdown()
	defer c.mList.Leave(time.Second)

	klog.InfoS(""Starting"", ""controllerName"", controllerName)
	defer klog.InfoS(""Shutting down"", ""controllerName"", controllerName)

	if !cache.WaitForNamedCacheSync(controllerName, stopCh, c.externalIPPoolInformerHasSynced, c.nodeListerSynced) {
		return
	}

	members, err := c.allClusterMembers()
	if err != nil {
		klog.ErrorS(err, ""List cluster members failed"")
	} else if members != nil {
		_, err := c.mList.Join(members)
		if err != nil {
			klog.ErrorS(err, ""Join cluster failed"")
		}
	}

	for i := 0; i < defaultWorkers; i++ {
		go wait.Until(c.worker, time.Second, stopCh)
	}

	for {
		select {
		case <-stopCh:
			return
		case nodeEvent := <-c.nodeEventsCh:
			c.handleClusterNodeEvents(&nodeEvent)
		}
	}
}

func (c *Cluster) worker() {
	for c.processNextWorkItem() {
	}
}

func (c *Cluster) processNextWorkItem() bool {
	obj, quit := c.queue.Get()
	if quit {
		return false
	}
	defer c.queue.Done(obj)

	// We expect strings (ExternalIPPool name) to come off the work queue.
	if key, ok := obj.(string); !ok {
		// As the item in the work queue is actually invalid, we call Forget here else we'd
		// go into a loop of attempting to process a work item that is invalid.
		// This should not happen.
		c.queue.Forget(obj)
		klog.Errorf(""Expected string in work queue but got %#v"", obj)
		return true
	} else if err := c.syncConsistentHash(key); err == nil {
		// If no error occurs we Forget this item so it does not get queued again until
		// another change happens.
		c.queue.Forget(key)
	} else {
		// Put the item back on the work queue to handle any transient errors.
		c.queue.AddRateLimited(key)
		klog.ErrorS(err, ""Syncing consistentHash by ExternalIPPool failed, requeue"", ""ExternalIPPool"", key)
	}
	return true
}

func (c *Cluster) syncConsistentHash(eipName string) error {
	startTime := time.Now()
	defer func() {
		klog.V(4).InfoS(""Finished syncing consistentHash"", ""ExternalIPPool"", eipName, ""durationTime"", time.Since(startTime))
	}()

	eip, err := c.externalIPPoolLister.Get(eipName)
	if err != nil {
		if errors.IsNotFound(err) {
			c.consistentHashRWMutex.Lock()
			defer c.consistentHashRWMutex.Unlock()
			delete(c.consistentHashMap, eipName)
			return nil
		}
		return err
	}

	// updateConsistentHash refreshes the consistentHashMap.
	updateConsistentHash := func(eip *v1alpha2.ExternalIPPool) error {
		nodeSel, err := metav1.LabelSelectorAsSelector(&eip.Spec.NodeSelector)
		if err != nil {
			return err
		}
		nodes, err := c.nodeLister.List(nodeSel)
		if err != nil {
			return err
		}
		aliveNodes := c.aliveNodes()
		// Node alive and Node labels match ExternalIPPool nodeSelector.
		var aliveAndMatchedNodes []string
		for _, node := range nodes {
			nodeName := node.Name
			if aliveNodes.Has(nodeName) {
				aliveAndMatchedNodes = append(aliveAndMatchedNodes, nodeName)
			}
		}
		consistentHashMap := newNodeConsistentHashMap()
		consistentHashMap.Add(aliveAndMatchedNodes...)
		c.consistentHashRWMutex.Lock()
		defer c.consistentHashRWMutex.Unlock()
		c.consistentHashMap[eip.Name] = consistentHashMap
		c.notify(eip.Name)
		return nil
	}

	if err := updateConsistentHash(eip); err != nil {
		return err
	}
	return nil
}

func newNodeConsistentHashMap() *consistenthash.Map {
	return consistenthash.New(defaultVirtualNodeReplicas, defaultHashFn)
}

func (c *Cluster) handleClusterNodeEvents(nodeEvent *memberlist.NodeEvent) {
	node, event := nodeEvent.Node, nodeEvent.Event
	switch event {
	case memberlist.NodeJoin, memberlist.NodeLeave:
		// When a Node joins cluster, all matched ExternalIPPools consistentHash should be updated;
		// when a Node leaves cluster, the Node may have failed or have been deleted,
		// if the Node has been deleted, affected ExternalIPPool should be enqueued, and deleteNode handler has been executed,
		// if the Node has failed, ExternalIPPools consistentHash maybe changed, and affected ExternalIPPool should be enqueued.
		coreNode, err := c.nodeLister.Get(node.Name)
		if err != nil {
			if errors.IsNotFound(err) {
				// Node has been deleted, and deleteNode handler has been executed.
				klog.ErrorS(err, ""Processing Node event, not found"", ""eventType"", event)
				return
			}
			klog.ErrorS(err, ""Processing Node event, get Node failed"", ""eventType"", event)
			return
		}
		affectedEIPs := c.filterEIPsFromNodeLabels(coreNode)
		c.enqueueExternalIPPools(affectedEIPs)
		klog.InfoS(""Processed Node event"", ""eventType"", mapNodeEventType[event], ""nodeName"", node.Name, ""affectedExternalIPPoolNum"", len(affectedEIPs))
	default:
		klog.InfoS(""Processed Node event"", ""eventType"", mapNodeEventType[event], ""nodeName"", node.Name)
	}
}

// aliveNodes returns the list of nodeNames in the cluster.","Doesn't memberlist expect IPv6 addrss wrapped by ""[]""?","The suggested code changes appear to implement a Kubernetes cluster management system using the `memberlist` library for node discovery and the `consistenthash` library for load balancing. Here's a concise review of the changes:

### Key Changes and Improvements:

1. **Struct Definitions and Constants**:
   - Added constants for node event types (`nodeEventTypeJoin`, `nodeEventTypeLeave`, `nodeEventTypeUpdate`).
   - Defined a `nodeEventType` type as a string.

2. **Error Handling**:
   - Introduced error constants for decoding objects (`errDecodingObject`, `errDecodingObjectTombstone`).

3. **Consistent Hash Map**:
   - Used a map to store consistent hash maps for each external IP pool.
   - Implemented methods to add, remove, and update nodes in the consistent hash map.

4. **Cluster Management**:
   - Created a `Cluster` struct with fields for memberlist configuration, node information, and event handlers.
   - Implemented methods to handle node creation, deletion, and updates, updating the consistent hash map accordingly.
   - Enqueued external IP pools for processing based on node events.

5. **Worker Functionality**:
   - Defined a `worker` function to process items from the work queue.
   - Implemented `processNextWorkItem` to handle each item, updating the consistent hash map and notifying relevant handlers.

6. **Consistent Hash Update Logic**:
   - Updated the `syncConsistentHash` function to refresh the consistent hash map based on the current state of nodes and external IP pools.
   - Used a virtual node replication factor (`defaultVirtualNodeReplicas`) and a custom hash function (`defaultHashFn`).

7. **Node Event Handling**:
   - Implemented logic to handle node join, leave, and update events, updating the consistent hash map and enqueuing affected external IP pools.

### Potential Improvements:

- **Logging and Debugging**:
  - Consider adding more detailed logging for critical operations and error handling.
  - Implement a structured logging framework like Zap or Logrus for better readability and maintainability.

- **Concurrency and Resource Management**:
  - Ensure proper synchronization between goroutines and shared resources.
  - Use context cancellation to handle graceful shutdowns more effectively.

- **Error Handling and Recovery**:
  - Implement more robust error handling and recovery mechanisms, especially for network-related issues.

- **Testing**:
  - Write unit tests for individual functions and integration tests for the entire cluster management system.

Overall,"
95,"@@ -585,12 +585,12 @@ func runWeb(ctx *cli.Context) error {
 			m.Get(""/src/*"", repo.Home)
 			m.Get(""/raw/*"", repo.SingleDownload)
 			m.Get(""/commits/*"", repo.RefCommits)
-			m.Get(""/commit/:sha([a-z0-9]{7,40})$"", repo.Diff)
+			m.Get(""/commit/:sha([a-f0-9]{7,40})$"", repo.Diff)
 			m.Get(""/forks"", repo.Forks)
 		}, context.RepoRef())
-		m.Get(""/commit/:sha([a-z0-9]{7,40})\\.:ext(patch|diff)"", repo.RawDiff)
+		m.Get(""/commit/:sha([a-f0-9]{7,40})\\.:ext(patch|diff)"", repo.RawDiff)
 
-		m.Get(""/compare/:before([a-z0-9]{7,40})\\.\\.\\.:after([a-z0-9]{7,40})"", repo.CompareDiff)
+		m.Get(""/compare/:before([a-z0-9]{40})\\.\\.\\.:after([a-z0-9]{40})"", repo.CompareDiff)
 	}, ignSignIn, context.RepoAssignment(), repo.MustBeNotBare)
 	m.Group(""/:username/:reponame"", func() {
 		m.Get(""/stars"", repo.Stars)","				m.Combo("""").Get(org.Settings).
					Post(bindIgnErr(auth.UpdateOrgSettingForm{}), org.SettingsPost)
				m.Post(""/avatar"", binding.MultipartForm(auth.AvatarForm{}), org.SettingsAvatar)
				m.Post(""/avatar/delete"", org.SettingsDeleteAvatar)

				m.Group(""/hooks"", func() {
					m.Get("""", org.Webhooks)
					m.Post(""/delete"", org.DeleteWebhook)
					m.Get(""/:type/new"", repo.WebhooksNew)
					m.Post(""/gogs/new"", bindIgnErr(auth.NewWebhookForm{}), repo.WebHooksNewPost)
					m.Post(""/slack/new"", bindIgnErr(auth.NewSlackHookForm{}), repo.SlackHooksNewPost)
					m.Get(""/:id"", repo.WebHooksEdit)
					m.Post(""/gogs/:id"", bindIgnErr(auth.NewWebhookForm{}), repo.WebHooksEditPost)
					m.Post(""/slack/:id"", bindIgnErr(auth.NewSlackHookForm{}), repo.SlackHooksEditPost)
				})

				m.Route(""/delete"", ""GET,POST"", org.SettingsDelete)
			})

			m.Route(""/invitations/new"", ""GET,POST"", org.Invitation)
		}, context.OrgAssignment(true, true))
	}, reqSignIn)
	// ***** END: Organization *****

	// ***** START: Repository *****
	m.Group(""/repo"", func() {
		m.Get(""/create"", repo.Create)
		m.Post(""/create"", bindIgnErr(auth.CreateRepoForm{}), repo.CreatePost)
		m.Get(""/migrate"", repo.Migrate)
		m.Post(""/migrate"", bindIgnErr(auth.MigrateRepoForm{}), repo.MigratePost)
		m.Combo(""/fork/:repoid"").Get(repo.Fork).
			Post(bindIgnErr(auth.CreateRepoForm{}), repo.ForkPost)
	}, reqSignIn)

	m.Group(""/:username/:reponame"", func() {
		m.Group(""/settings"", func() {
			m.Combo("""").Get(repo.Settings).
				Post(bindIgnErr(auth.RepoSettingForm{}), repo.SettingsPost)
			m.Group(""/collaboration"", func() {
				m.Combo("""").Get(repo.Collaboration).Post(repo.CollaborationPost)
				m.Post(""/access_mode"", repo.ChangeCollaborationAccessMode)
				m.Post(""/delete"", repo.DeleteCollaboration)
			})

			m.Group(""/hooks"", func() {
				m.Get("""", repo.Webhooks)
				m.Post(""/delete"", repo.DeleteWebhook)
				m.Get(""/:type/new"", repo.WebhooksNew)
				m.Post(""/gogs/new"", bindIgnErr(auth.NewWebhookForm{}), repo.WebHooksNewPost)
				m.Post(""/slack/new"", bindIgnErr(auth.NewSlackHookForm{}), repo.SlackHooksNewPost)
				m.Get(""/:id"", repo.WebHooksEdit)
				m.Post(""/:id/test"", repo.TestWebhook)
				m.Post(""/gogs/:id"", bindIgnErr(auth.NewWebhookForm{}), repo.WebHooksEditPost)
				m.Post(""/slack/:id"", bindIgnErr(auth.NewSlackHookForm{}), repo.SlackHooksEditPost)

				m.Group(""/git"", func() {
					m.Get("""", repo.GitHooks)
					m.Combo(""/:name"").Get(repo.GitHooksEdit).
						Post(repo.GitHooksEditPost)
				}, context.GitHookService())
			})

			m.Group(""/keys"", func() {
				m.Combo("""").Get(repo.DeployKeys).
					Post(bindIgnErr(auth.AddSSHKeyForm{}), repo.DeployKeysPost)
				m.Post(""/delete"", repo.DeleteDeployKey)
			})

		}, func(ctx *context.Context) {
			ctx.Data[""PageIsSettings""] = true
		})
	}, reqSignIn, context.RepoAssignment(), reqRepoAdmin, context.RepoRef())

	m.Get(""/:username/:reponame/action/:action"", reqSignIn, context.RepoAssignment(), repo.Action)
	m.Group(""/:username/:reponame"", func() {
		// FIXME: should use different URLs but mostly same logic for comments of issue and pull reuqest.
		// So they can apply their own enable/disable logic on routers.
		m.Group(""/issues"", func() {
			m.Combo(""/new"", repo.MustEnableIssues).Get(context.RepoRef(), repo.NewIssue).
				Post(bindIgnErr(auth.CreateIssueForm{}), repo.NewIssuePost)

			m.Group(""/:index"", func() {
				m.Post(""/label"", repo.UpdateIssueLabel)
				m.Post(""/milestone"", repo.UpdateIssueMilestone)
				m.Post(""/assignee"", repo.UpdateIssueAssignee)
			}, reqRepoWriter)

			m.Group(""/:index"", func() {
				m.Post(""/title"", repo.UpdateIssueTitle)
				m.Post(""/content"", repo.UpdateIssueContent)
				m.Combo(""/comments"").Post(bindIgnErr(auth.CreateCommentForm{}), repo.NewComment)
			})
		})
		m.Group(""/comments/:id"", func() {
			m.Post("""", repo.UpdateCommentContent)
			m.Post(""/delete"", repo.DeleteComment)
		})
		m.Group(""/labels"", func() {
			m.Post(""/new"", bindIgnErr(auth.CreateLabelForm{}), repo.NewLabel)
			m.Post(""/edit"", bindIgnErr(auth.CreateLabelForm{}), repo.UpdateLabel)
			m.Post(""/delete"", repo.DeleteLabel)
			m.Post(""/initialize"", bindIgnErr(auth.InitializeLabelsForm{}), repo.InitializeLabels)
		}, reqRepoWriter, context.RepoRef())
		m.Group(""/milestones"", func() {
			m.Combo(""/new"").Get(repo.NewMilestone).
				Post(bindIgnErr(auth.CreateMilestoneForm{}), repo.NewMilestonePost)
			m.Get(""/:id/edit"", repo.EditMilestone)
			m.Post(""/:id/edit"", bindIgnErr(auth.CreateMilestoneForm{}), repo.EditMilestonePost)
			m.Get(""/:id/:action"", repo.ChangeMilestonStatus)
			m.Post(""/delete"", repo.DeleteMilestone)
		}, reqRepoWriter, context.RepoRef())

		m.Group(""/releases"", func() {
			m.Get(""/new"", repo.NewRelease)
			m.Post(""/new"", bindIgnErr(auth.NewReleaseForm{}), repo.NewReleasePost)
			m.Post(""/delete"", repo.DeleteRelease)
		}, reqRepoWriter, context.RepoRef())

		m.Group(""/releases"", func() {
			m.Get(""/edit/*"", repo.EditRelease)
			m.Post(""/edit/*"", bindIgnErr(auth.EditReleaseForm{}), repo.EditReleasePost)
		}, reqRepoWriter, func(ctx *context.Context) {
			var err error
			ctx.Repo.Commit, err = ctx.Repo.GitRepo.GetBranchCommit(ctx.Repo.Repository.DefaultBranch)
			if err != nil {
				ctx.Handle(500, ""GetBranchCommit"", err)
				return
			}
			ctx.Repo.CommitsCount, err = ctx.Repo.Commit.CommitsCount()
			if err != nil {
				ctx.Handle(500, ""CommitsCount"", err)
				return
			}
			ctx.Data[""CommitsCount""] = ctx.Repo.CommitsCount
		})

		m.Combo(""/compare/*"", repo.MustAllowPulls).Get(repo.CompareAndPullRequest).
			Post(bindIgnErr(auth.CreateIssueForm{}), repo.CompareAndPullRequestPost)

		m.Group("""", func() {
			m.Combo(""/_edit/*"").Get(repo.EditFile).
				Post(bindIgnErr(auth.EditRepoFileForm{}), repo.EditFilePost)
			m.Combo(""/_new/*"").Get(repo.NewFile).
				Post(bindIgnErr(auth.EditRepoFileForm{}), repo.NewFilePost)
			m.Post(""/_preview/*"", bindIgnErr(auth.EditPreviewDiffForm{}), repo.DiffPreviewPost)
			m.Combo(""/_delete/*"").Get(repo.DeleteFile).
				Post(bindIgnErr(auth.DeleteRepoFileForm{}), repo.DeleteFilePost)

			m.Group("""", func() {
				m.Combo(""/_upload/*"").Get(repo.UploadFile).
					Post(bindIgnErr(auth.UploadRepoFileForm{}), repo.UploadFilePost)
				m.Post(""/upload-file"", repo.UploadFileToServer)
				m.Post(""/upload-remove"", bindIgnErr(auth.RemoveUploadFileForm{}), repo.RemoveUploadFileFromServer)
			}, func(ctx *context.Context) {
				if !setting.Repository.Upload.Enabled {
					ctx.Handle(404, """", nil)
					return
				}
			})
		}, reqRepoWriter, context.RepoRef(), func(ctx *context.Context) {
			if !ctx.Repo.Repository.CanEnableEditor() || ctx.Repo.IsViewCommit {
				ctx.Handle(404, """", nil)
				return
			}
		})
	}, reqSignIn, context.RepoAssignment(), repo.MustBeNotBare)

	m.Group(""/:username/:reponame"", func() {
		m.Group("""", func() {
			m.Get(""/releases"", repo.Releases)
			m.Get(""/^:type(issues|pulls)$"", repo.RetrieveLabels, repo.Issues)
			m.Get(""/^:type(issues|pulls)$/:index"", repo.ViewIssue)
			m.Get(""/labels/"", repo.RetrieveLabels, repo.Labels)
			m.Get(""/milestones"", repo.Milestones)
		}, context.RepoRef())

		// m.Get(""/branches"", repo.Branches)

		m.Group(""/wiki"", func() {
			m.Get(""/?:page"", repo.Wiki)
			m.Get(""/_pages"", repo.WikiPages)

			m.Group("""", func() {
				m.Combo(""/_new"").Get(repo.NewWiki).
					Post(bindIgnErr(auth.NewWikiForm{}), repo.NewWikiPost)
				m.Combo(""/:page/_edit"").Get(repo.EditWiki).
					Post(bindIgnErr(auth.NewWikiForm{}), repo.EditWikiPost)
				m.Post(""/:page/delete"", repo.DeleteWikiPagePost)
			}, reqSignIn, reqRepoWriter)
		}, repo.MustEnableWiki, context.RepoRef())

		m.Get(""/archive/*"", repo.Download)

		m.Group(""/pulls/:index"", func() {
			m.Get(""/commits"", context.RepoRef(), repo.ViewPullCommits)
			m.Get(""/files"", context.RepoRef(), repo.ViewPullFiles)
			m.Post(""/merge"", reqRepoWriter, repo.MergePullRequest)
		}, repo.MustAllowPulls)

		m.Group("""", func() {
			m.Get(""/commit/:sha([a-f0-9]{7,40})$"", repo.Diff)
		m.Get(""/commit/:sha([a-f0-9]{7,40})\\.:ext(patch|diff)"", repo.RawDiff)
		m.Get(""/compare/:before([a-z0-9]{40})\\.\\.\\.:after([a-z0-9]{40})"", repo.CompareDiff)
			m.Get(""/src/*"", repo.Home)
			m.Get(""/raw/*"", repo.SingleDownload)
			m.Get(""/commits/*"", repo.RefCommits)
			m.Get(""/commit/:sha([a-z0-9]{7,40})$"", repo.Diff)
			m.Get(""/forks"", repo.Forks)
		}, context.RepoRef())
		m.Get(""/commit/:sha([a-z0-9]{7,40})\\.:ext(patch|diff)"", repo.RawDiff)

		m.Get(""/compare/:before([a-z0-9]{7,40})\\.\\.\\.:after([a-z0-9]{7,40})"", repo.CompareDiff)
	}, ignSignIn, context.RepoAssignment(), repo.MustBeNotBare)
	m.Group(""/:username/:reponame"", func() {
		m.Get(""/stars"", repo.Stars)
		m.Get(""/watchers"", repo.Watchers)
	}, ignSignIn, context.RepoAssignment(), context.RepoRef())

	m.Group(""/:username"", func() {
		m.Group(""/:reponame"", func() {
			m.Get("""", repo.Home)
			m.Get(""\\.git$"", repo.Home)
		}, ignSignIn, context.RepoAssignment(true), context.RepoRef())

		m.Group(""/:reponame"", func() {
			m.Any(""/*"", ignSignInAndCsrf, repo.HTTP)
			m.Head(""/tasks/trigger"", repo.TriggerTask)
		})
	})
	// ***** END: Repository *****

	m.Group(""/api"", func() {
		apiv1.RegisterRoutes(m)
	}, ignSignIn)

	// robots.txt
	m.Get(""/robots.txt"", func(ctx *context.Context) {
		if setting.HasRobotsTxt {
			ctx.ServeFileContent(path.Join(setting.CustomPath, ""robots.txt""))
		} else {
			ctx.Error(404)
		}
	})

	// Not found handler.
	m.NotFound(routers.NotFound)

	// Flag for port number in case first time run conflict.
	if ctx.IsSet(""port"") {
		setting.AppUrl = strings.Replace(setting.AppUrl, setting.HTTPPort, ctx.String(""port""), 1)
		setting.HTTPPort = ctx.String(""port"")
	}

	var listenAddr string
	if setting.Protocol == setting.UNIX_SOCKET {
		listenAddr = fmt.Sprintf(""%s"", setting.HTTPAddr)
	} else {
		listenAddr = fmt.Sprintf(""%s:%s"", setting.HTTPAddr, setting.HTTPPort)
	}
	log.Info(""Listen: %v://%s%s"", setting.Protocol, listenAddr, setting.AppSubUrl)

	var err error
	switch setting.Protocol {
	case setting.HTTP:
		err = http.ListenAndServe(listenAddr, m)
	case setting.HTTPS:
		server := &http.Server{Addr: listenAddr, TLSConfig: &tls.Config{MinVersion: tls.VersionTLS10}, Handler: m}
		err = server.ListenAndServeTLS(setting.CertFile, setting.KeyFile)
	case setting.FCGI:
		err = fcgi.Serve(nil, m)
	case setting.UNIX_SOCKET:
		os.Remove(listenAddr)

		var listener *net.UnixListener
		listener, err = net.ListenUnix(""unix"", &net.UnixAddr{listenAddr, ""unix""})
		if err != nil {
			break // Handle error after switch
		}

		// FIXME: add proper implementation of signal capture on all protocols
		// execute this on SIGTERM or SIGINT: listener.Close()
		if err = os.Chmod(listenAddr, os.FileMode(setting.UnixSocketPermission)); err != nil {
			log.Fatal(4, ""Failed to set permission of unix socket: %v"", err)
		}
		err = http.Serve(listener, m)
	default:
		log.Fatal(4, ""Invalid protocol: %s"", setting.Protocol)
	}

	if err != nil {
		log.Fatal(4, ""Fail to start server: %v"", err)
	}

	return nil
}
",And now I can't compare `master...develop` anymore :unamused:,"The provided code snippet is a Go web application router configuration using the Echo framework. It sets up routes for various endpoints related to organizations, repositories, and other functionalities. Here's a concise review of the suggested code changes:

### Key Points:
1. **Routing Structure**: The code uses Echo's routing capabilities to define different URL patterns and their corresponding handlers.
2. **Middleware Usage**: Middleware functions like `reqSignIn`, `reqRepoAssignment`, `reqRepoAdmin`, and `context.RepoRef()` are used to enforce authentication and authorization checks.
3. **Error Handling**: The `bindIgnErr` function is used to handle errors gracefully by ignoring them during form binding.
4. **Conditional Routes**: Conditional routes are defined using `m.Group` with specific conditions (e.g., `reqRepoAssignment`, `reqRepoAdmin`).
5. **Static Files**: The `robots.txt` file is served from the custom path if it exists.
6. **Server Configuration**: The server listens on the specified address and port, handling HTTP, HTTPS, and UNIX socket protocols.

### Specific Changes:
- **Route Definitions**: The code defines routes for various endpoints such as organization settings, repository actions, and more.
- **Middleware Integration**: Middleware functions are integrated into the routing chain to ensure proper authentication and authorization.
- **Conditional Routing**: Conditional routes are used to apply specific middleware based on the context (e.g., requiring admin privileges for certain operations).

### Overall Quality:
- **Readability**: The code is well-structured and easy to understand, with clear separation of concerns.
- **Maintainability**: The use of middleware and conditional routing makes the code modular and easier to maintain.
- **Error Handling**: Graceful error handling is implemented using `bindIgnErr` and other error handling mechanisms.

### Potential Improvements:
- **Documentation**: Adding comments to explain complex logic or specific routes could improve readability.
- **Performance**: Consider optimizing routes and middleware to handle high traffic efficiently.
- **Security**: Ensure that sensitive routes are properly secured and that input validation is robust.

Overall, the code is well-structured and meets the requirements for a robust web application router."
96,"@@ -134,6 +134,7 @@ class DagitWebserver(GraphQLServer):
             return Route(
                 file_path,
                 lambda _: FileResponse(path=self.relative_path(f""webapp/build{file_path}"")),
+                name=""root_static"",
             )
 
         return [_static_file(f) for f in ROOT_ADDRESS_STATIC_RESOURCES]","import gzip
import io
import uuid
from os import path
from typing import List

from dagster import DagsterInstance
from dagster import __version__ as dagster_version
from dagster.cli.workspace.cli_target import get_workspace_process_context_from_kwargs
from dagster.core.debug import DebugRunPayload
from dagster.core.storage.compute_log_manager import ComputeIOType
from dagster.core.workspace.context import WorkspaceProcessContext, WorkspaceRequestContext
from dagster_graphql import __version__ as dagster_graphql_version
from dagster_graphql.schema import create_schema
from graphene import Schema
from starlette.exceptions import HTTPException
from starlette.middleware import Middleware
from starlette.requests import HTTPConnection, Request
from starlette.responses import FileResponse, HTMLResponse, JSONResponse, StreamingResponse
from starlette.routing import Mount, Route, WebSocketRoute
from starlette.staticfiles import StaticFiles

from .graphql import GraphQLServer
from .version import __version__

ROOT_ADDRESS_STATIC_RESOURCES = [
    ""/manifest.json"",
    ""/favicon.ico"",
    ""/favicon.png"",
    ""/favicon.svg"",
    ""/favicon-run-pending.svg"",
    ""/favicon-run-failed.svg"",
    ""/favicon-run-success.svg"",
    ""/asset-manifest.json"",
    ""/robots.txt"",
]


class DagitWebserver(GraphQLServer):
    def __init__(self, process_context: WorkspaceProcessContext, app_path_prefix: str = """"):
        self._process_context = process_context
        super().__init__(app_path_prefix)

    def build_graphql_schema(self) -> Schema:
        return create_schema()

    def build_graphql_middleware(self) -> list:
        return []

    def relative_path(self, rel: str) -> str:
        return path.join(path.dirname(__file__), rel)

    def make_request_context(self, conn: HTTPConnection) -> WorkspaceRequestContext:
        return self._process_context.create_request_context(conn)

    def build_middleware(self) -> List[Middleware]:
        return []

    async def dagit_info_endpoint(self, _request: Request):
        return JSONResponse(
            {
                ""dagit_version"": __version__,
                ""dagster_version"": dagster_version,
                ""dagster_graphql_version"": dagster_graphql_version,
            }
        )

    async def download_debug_file_endpoint(self, request: Request):
        run_id = request.path_params[""run_id""]
        context = self.make_request_context(request)

        run = context.instance.get_run_by_id(run_id)
        debug_payload = DebugRunPayload.build(self._process_context.instance, run)

        result = io.BytesIO()
        with gzip.GzipFile(fileobj=result, mode=""wb"") as file:
            debug_payload.write(file)

        result.seek(0)  # be kind, please rewind

        return StreamingResponse(result, media_type=""application/gzip"")

    async def download_compute_logs_endpoint(self, request: Request):
        run_id = request.path_params[""run_id""]
        step_key = request.path_params[""step_key""]
        file_type = request.path_params[""file_type""]
        context = self.make_request_context(request)

        file = context.instance.compute_log_manager.get_local_path(
            run_id,
            step_key,
            ComputeIOType(file_type),
        )

        if not path.exists(file):
            raise HTTPException(404)

        return FileResponse(
            context.instance.compute_log_manager.get_local_path(
                run_id,
                step_key,
                ComputeIOType(file_type),
            ),
            filename=f""{run_id}_{step_key}.{file_type}"",
        )

    def index_html_endpoint(self, _request: Request):
        """"""
        Serves root html
        """"""
        index_path = self.relative_path(""webapp/build/index.html"")

        try:
            with open(index_path) as f:
                rendered_template = f.read()
                return HTMLResponse(
                    rendered_template.replace('href=""/', f'href=""{self._app_path_prefix}/')
                    .replace('src=""/', f'src=""{self._app_path_prefix}/')
                    .replace(""__PATH_PREFIX__"", self._app_path_prefix)
                    .replace(""NONCE-PLACEHOLDER"", uuid.uuid4().hex)
                )
        except FileNotFoundError:
            raise Exception(
                """"""
                Can't find webapp files.
                If you are using dagit, then probably it's a corrupted installation or a bug.
                However, if you are developing dagit locally, your problem can be fixed by running
                ""make rebuild_dagit"" in the project root.
                """"""
            )

    def root_static_file_routes(self) -> List[Route]:
        def _static_file(file_path):
                name=""root_static"",
            return Route(
                file_path,
                lambda _: FileResponse(path=self.relative_path(f""webapp/build{file_path}"")),
            )

        return [_static_file(f) for f in ROOT_ADDRESS_STATIC_RESOURCES]

    def build_static_routes(self):
        return [
            # static resources addressed at /static/
            Mount(
                ""/static"",
                StaticFiles(
                    directory=self.relative_path(""webapp/build/static""),
                    check_dir=False,
                ),
                name=""static"",
            ),
            # static resources addressed at /vendor/
            Mount(
                ""/vendor"",
                StaticFiles(
                    directory=self.relative_path(""webapp/build/vendor""),
                    check_dir=False,
                ),
                name=""vendor"",
            ),
            # specific static resources addressed at /
            *self.root_static_file_routes(),
        ]

    def build_routes(self):
        return (
            [
                Route(""/dagit_info"", self.dagit_info_endpoint),
                Route(
                    ""/graphql"",
                    self.graphql_http_endpoint,
                    name=""graphql-http"",
                    methods=[""GET"", ""POST""],
                ),
                WebSocketRoute(
                    ""/graphql"",
                    self.graphql_ws_endpoint,
                    name=""graphql-ws"",
                ),
            ]
            + self.build_static_routes()
            + [
                # download file endpoints
                Route(
                    ""/download/{run_id:str}/{step_key:str}/{file_type:str}"",
                    self.download_compute_logs_endpoint,
                ),
                Route(
                    ""/download_debug/{run_id:str}"",
                    self.download_debug_file_endpoint,
                ),
                Route(""/{path:path}"", self.index_html_endpoint),
                Route(""/"", self.index_html_endpoint),
            ]
        )


def default_app(debug=False):
    instance = DagsterInstance.get()
    process_context = get_workspace_process_context_from_kwargs(
        instance=instance,
        version=__version__,
        read_only=False,
        kwargs={},
    )

    return DagitWebserver(
        process_context,
    ).create_asgi_app(debug=debug)


def debug_app():
    return default_app(debug=True)
","Feels weird for multiple routes to have the same name - but I guess we're not using `url_for` for this, so it's fine as an identifier for now","The suggested code change appears to be a complete implementation of a Dagit web server using Starlette, a modern ASGI framework for building web applications. The code defines a `DagitWebserver` class that inherits from `GraphQLServer` and provides implementations for various endpoints such as `/dagit_info`, `/graphql`, `/download`, and `/index_html`. The `build_routes` method constructs the routing configuration for the web server.

### Key Points:

1. **Class Structure**: The `DagitWebserver` class is designed to handle requests related to Dagit, including GraphQL queries, downloading logs, and serving static files.

2. **Endpoints**:
   - `/dagit_info`: Returns metadata about the Dagit server, including versions of Dagit, Dagster, and GraphQL.
   - `/graphql`: Handles both GET and POST requests for GraphQL queries.
   - `/download/{run_id}/{step_key}/{file_type}`: Downloads compute logs for a specific step in a run.
   - `/download_debug/{run_id}`: Downloads a debug payload for a run.
   - `/index_html`: Serves the main HTML page for Dagit, handling relative paths and nonce placeholders.

3. **Static Files**: The server serves static files from the `webapp/build` directory, including those in the `static` and `vendor` directories.

4. **Routing**: The `build_routes` method constructs a list of routes that define how the server should respond to different URLs.

5. **Debug Mode**: The `default_app` function creates an instance of the web server with optional debug mode enabled.

6. **Static Resource Paths**: The `relative_path` method constructs the absolute path to resources within the `webapp/build` directory.

### Review:

- **Code Organization**: The code is well-organized into classes and methods, making it easy to understand and maintain.
- **Error Handling**: The `download_compute_logs_endpoint` and `download_debug_file_endpoint` handle cases where files do not exist gracefully.
- **Security**: The use of `gzip` for compressing debug payloads ensures efficient data transfer.
- **Flexibility**: The server can be easily extended with additional endpoints or configurations as needed.

Overall, this implementation provides a robust and functional Dagit web server using Starlette, which is suitable for production environments."
97,"@@ -97,14 +97,11 @@ lbann_callback* construct_callback(lbann_comm* comm,
   //////////////////////////////////////////////////////////////
 
   if (proto_cb.has_ltfb()) {
-    auto&& m = parse_list<>(proto_cb.ltfb().eval_metrics());
-    auto&& w = parse_list<>(proto_cb.ltfb().weights_tosend());
-    std::unordered_set<std::string> metric_names(m.begin(), m.end());
-    std::unordered_set<std::string> weight_names(w.begin(), w.end());
-    return new lbann_callback_ltfb(proto_cb.ltfb().round_size(),
-                                   metric_names,
-                                   proto_cb.ltfb().increasing_metric_mode(),
-                                   weight_names,
+    const auto& params = proto_cb.ltfb();
+    return new lbann_callback_ltfb(params.batch_interval(),
+                                   params.metric(),
+                                   parse_set<std::string>(params.weights()),
+                                   params.low_score_wins(),
                                    summarizer);
   }
   /// @todo","////////////////////////////////////////////////////////////////////////////////
// Copyright (c) 2014-2016, Lawrence Livermore National Security, LLC.
// Produced at the Lawrence Livermore National Laboratory.
// Written by the LBANN Research Team (B. Van Essen, et al.) listed in
// the CONTRIBUTORS file. <lbann-dev@llnl.gov>
//
// LLNL-CODE-697807.
// All rights reserved.
//
// This file is part of LBANN: Livermore Big Artificial Neural Network
// Toolkit. For details, see http://software.llnl.gov/LBANN or
// https://github.com/LLNL/LBANN.
//
// Licensed under the Apache License, Version 2.0 (the ""Licensee""); you
// may not use this file except in compliance with the License.  You may
// obtain a copy of the License at:
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an ""AS IS"" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
// implied. See the License for the specific language governing
// permissions and limitations under the license.
////////////////////////////////////////////////////////////////////////////////

#include ""lbann/proto/factories.hpp""
#include ""lbann/utils/peek_map.hpp""

namespace lbann {
namespace proto {

namespace {

/** Select entries from a list based on names.
 *  Any entry in 'list' with a name found in 'names' (interpreted as a
 *  space-separated list) is added to the output list.
 */
template <typename T>
std::vector<T*> select_from_list(std::string names,
                                        std::vector<T*> list) {
  std::vector<T*> selected;
  for (const auto& name : parse_list<std::string>(names)) {
    for (auto&& t : list) {
      if (name == t->get_name()) {
        selected.push_back(t);
      }
    }
  }
  return selected;
}


} // namespace

lbann_callback* construct_callback(lbann_comm* comm,
                                   const lbann_data::Callback& proto_cb,
                                   const std::map<execution_mode, generic_data_reader*>& data_readers,
                                   std::vector<Layer*> layer_list,
                                   std::vector<weights*> weights_list,
                                   lbann_summary* summarizer) {
  std::stringstream err;

  //////////////////////////////////////////////////////////////
  // Display information
  //////////////////////////////////////////////////////////////

  if (proto_cb.has_print()) {
    const auto& interval = proto_cb.print().interval();
    return new lbann_callback_print(interval);
  }
  if (proto_cb.has_timer()) {
    return new lbann_callback_timer(summarizer);
  }
  if (proto_cb.has_disp_io_stats()) {
    const auto& params = proto_cb.disp_io_stats();
    auto&& l = select_from_list<Layer>(params.layers(),
                                                     layer_list);
    std::unordered_set<Layer*> selected_layers(l.begin(), l.end());
    return new lbann_callback_io(selected_layers);
  }
  if (proto_cb.has_save_images()) {
    const auto& params = proto_cb.save_images();
    return new lbann_callback_save_images(parse_list<>(params.layers()),
                                          params.image_format(),
                                          params.image_prefix());
  }
  if (proto_cb.has_confusion_matrix()) {
    const auto& params = proto_cb.confusion_matrix();
    return new lbann_callback_confusion_matrix(params.prediction(),
                                               params.label(),
                                               params.prefix());
  }

  //////////////////////////////////////////////////////////////
  // Inter-model communication
    const auto& params = proto_cb.ltfb();
    return new lbann_callback_ltfb(params.batch_interval(),
                                   params.metric(),
                                   parse_set<std::string>(params.weights()),
                                   params.low_score_wins(),
  //////////////////////////////////////////////////////////////

  if (proto_cb.has_ltfb()) {
    auto&& m = parse_list<>(proto_cb.ltfb().eval_metrics());
    auto&& w = parse_list<>(proto_cb.ltfb().weights_tosend());
    std::unordered_set<std::string> metric_names(m.begin(), m.end());
    std::unordered_set<std::string> weight_names(w.begin(), w.end());
    return new lbann_callback_ltfb(proto_cb.ltfb().round_size(),
                                   metric_names,
                                   proto_cb.ltfb().increasing_metric_mode(),
                                   weight_names,
                                   summarizer);
  }
  /// @todo
  if (proto_cb.has_imcomm()) {
    const auto& params = proto_cb.imcomm();
    const auto& type_str = params.intermodel_comm_method();
    lbann_callback_imcomm::comm_type type = lbann_callback_imcomm::comm_type::NONE;
    if (type_str == ""none"") {
      type = lbann_callback_imcomm::comm_type::NONE;
    } else if (type_str == ""normal"") {
      type = lbann_callback_imcomm::comm_type::NORMAL;
    } else if (type_str == ""onebit_quantization"") {
      type = lbann_callback_imcomm::comm_type::ONEBIT_QUANTIZATION;
    } else if (type_str == ""thresh_quantization"") {
      type = lbann_callback_imcomm::comm_type::THRESH_QUANTIZATION;
    } else if (type_str == ""adaptive_quantization"") {
      type = lbann_callback_imcomm::comm_type::ADAPTIVE_QUANTIZATION;
    } else {
      err << ""invalid inter-model communication type ("" << type_str << "")"";
      LBANN_ERROR(err.str());
    }
    std::unordered_set<weights*> selected_weights; /// @todo Initialize weights
    return new lbann_callback_imcomm(type, selected_weights, summarizer);
  }

  //////////////////////////////////////////////////////////////
  // Learning rate schedules
  //////////////////////////////////////////////////////////////

  if (proto_cb.has_step_learning_rate()) {
    const auto& params = proto_cb.step_learning_rate();
    auto&& w = select_from_list<weights>(params.weights(),
                                                        weights_list);
    std::unordered_set<weights*> selected_weights(w.begin(), w.end());
    return new lbann_callback_step_learning_rate(params.step(),
                                                 params.amt(),
                                                 selected_weights);
  }
  if (proto_cb.has_adaptive_learning_rate()) {
    const auto& params = proto_cb.adaptive_learning_rate();
    auto&& w = select_from_list<weights>(params.weights(),
                                                        weights_list);
    std::unordered_set<weights*> selected_weights(w.begin(), w.end());
    return new lbann_callback_adaptive_learning_rate(params.patience(),
                                                     params.amt(),
                                                     selected_weights);
  }
  if (proto_cb.has_drop_fixed_learning_rate()) {
    const auto& params = proto_cb.drop_fixed_learning_rate();
    std::vector<int64_t> drop_epochs;
    for (int i = 0; i < params.drop_epoch_size(); ++i) {
      drop_epochs.push_back(params.drop_epoch(i));
    }
    auto&& w = select_from_list<weights>(params.weights(),
                                                        weights_list);
    std::unordered_set<weights*> selected_weights(w.begin(), w.end());
    return new lbann_callback_drop_fixed_learning_rate(drop_epochs,
                                                       params.amt(),
                                                       selected_weights);
  }
  if (proto_cb.has_linear_growth_learning_rate()) {
    const auto& params = proto_cb.linear_growth_learning_rate();
    auto&& w = select_from_list<weights>(params.weights(),
                                                        weights_list);
    std::unordered_set<weights*> selected_weights(w.begin(), w.end());
    return new lbann_callback_linear_growth_learning_rate(params.target(),
                                                          params.num_epochs(),
                                                          params.delay(),
                                                          selected_weights);
  }
  if (proto_cb.has_optimizerwise_adaptive_learning_rate()) {
    const auto& params = proto_cb.optimizerwise_adaptive_learning_rate();
    auto&& w = select_from_list<weights>(params.weights(),
                                                        weights_list);
    std::unordered_set<weights*> selected_weights(w.begin(), w.end());
    return new lbann_callback_optimizerwise_adaptive_learning_rate(params.scale(),
                                                                   selected_weights);
  }
  if (proto_cb.has_poly_learning_rate()) {
    const auto& params = proto_cb.poly_learning_rate();
    auto&& w = select_from_list<weights>(params.weights(),
                                                        weights_list);
    std::unordered_set<weights*> selected_weights(w.begin(), w.end());
    return new lbann_callback_poly_learning_rate(params.power(),
                                                 params.num_epochs(),
                                                 params.max_iter(),
                                                 selected_weights);
  }

  //////////////////////////////////////////////////////////////
  // Mini-batch schedules
  //////////////////////////////////////////////////////////////

  if (proto_cb.has_step_minibatch()) {
    const auto& params = proto_cb.step_minibatch();
    return new lbann_callback_step_minibatch(params.starting_mbsize(),
                                             params.step(),
                                             params.ramp_time());
  }
  if (proto_cb.has_minibatch_schedule()) {
    const auto& params = proto_cb.minibatch_schedule();
    std::vector<lbann_callback_minibatch_schedule::minibatch_step> steps;
    for (int i = 0; i < params.step_size(); ++i) {
      const auto& proto_step = params.step(i);
      steps.emplace_back(proto_step.epoch(),
                         proto_step.mbsize(),
                         proto_step.lr(),
                         proto_step.ramp_time());
    }
    return new lbann_callback_minibatch_schedule(params.starting_mbsize(),
                                                 steps);
  }

  //////////////////////////////////////////////////////////////
  // Checkpointing and exporting
  //////////////////////////////////////////////////////////////

  if (proto_cb.has_checkpoint()) {
    const auto& params = proto_cb.checkpoint();
    return new lbann_callback_checkpoint(params.checkpoint_dir(),
                                         params.checkpoint_epochs(),
                                         params.checkpoint_steps(),
                                         params.checkpoint_secs(),
                                         params.per_rank_dir(),
                                         params.ckpt_dist_epochs(),
                                         params.ckpt_dist_steps());
  }
  if (proto_cb.has_save_model()) {
    const auto& params = proto_cb.save_model();
    if(params.extension().size() != 0) {
      return new lbann_callback_save_model(params.dir(),
                                           params.disable_save_after_training(),
                                           params.extension());
    }else {
      return new lbann_callback_save_model(params.dir(),
                                           params.disable_save_after_training());
    }
  }

  //////////////////////////////////////////////////////////////
  // Weight exchange/replace
  //////////////////////////////////////////////////////////////

  if (proto_cb.has_replace_weights()) {
    const auto& params = proto_cb.replace_weights();
    auto&& src_layers = select_from_list<Layer>(params.source_layers(),
                                                     layer_list);
    auto&& dst_layers = select_from_list<Layer>(params.destination_layers(),
                                                     layer_list);
    return new lbann_callback_replace_weights(src_layers,dst_layers,params.batch_interval());
  }

  //////////////////////////////////////////////////////////////
  // Profiling
  //////////////////////////////////////////////////////////////

  if (proto_cb.has_summary()) {
    const auto& params = proto_cb.summary();
    return new lbann_callback_summary(summarizer,
                                      params.batch_interval(),
                                      params.mat_interval());
  }
  if (proto_cb.has_profiler()) {
    return new lbann_callback_profiler(proto_cb.profiler().sync());
  }
  if (proto_cb.has_sync_layers()) {
    const auto& params = proto_cb.sync_layers();
    return new lbann_callback_sync_layers(params.sync_gpus(),
                                          params.sync_mpi(),
                                          params.only_input());
  }
  if (proto_cb.has_sync_selected()) {
    const auto& params = proto_cb.sync_selected();
    const int num_layers = params.layer_to_sync_size();
    if (num_layers == 0) {
      throw lbann_exception(""sync_selected requires at least a layer to synchronize."");
    }

    using layers_t = lbann_callback_sync_selected::layers_t;
    using prop_t = lbann_callback_sync_selected::prop_t;

    layers_t selected_layers;
    selected_layers.reserve(num_layers);

    for (int i = 0; i < num_layers; ++i) {
      const auto& layer_to_sync = params.layer_to_sync(i);
      selected_layers.emplace(layer_to_sync.name(),
                              static_cast<prop_t>(layer_to_sync.prop()));
    }","We are not using multiple metrics now but how would this support space-delimited metric (e.g., categorical accuracy) and multiple metrics with the same name as metric specified one of the metrics for training/inference ?","The suggested code change introduces a helper function `select_from_list` to filter a list of objects based on their names specified in a space-separated string. This function is used to create callbacks like `lbann_callback_io`, `lbann_callback_save_images`, `lbann_callback_confusion_matrix`, and others that require selecting layers or weights based on their names.

### Key Points:

1. **Helper Function**: The `select_from_list` function takes a string of names and a list of objects, then returns a vector of pointers to objects whose names match any of the names in the input string.

2. **Template Usage**: The function is templated to work with any type of object that has a `get_name()` method, allowing it to be used with different types of objects like `Layer` and `weights`.

3. **Error Handling**: The code includes error handling for cases where no matching objects are found, such as when creating callbacks like `lbann_callback_confusion_matrix` or `lbann_callback_save_images`.

4. **Code Organization**: The code is organized into sections for each type of callback, making it easier to read and maintain.

5. **Comments**: The comments provide context and explain the purpose of each section and the logic within the code.

Overall, this change improves the readability and maintainability of the code by encapsulating the logic for selecting objects based on names in a reusable function."
98,"@@ -411,8 +411,8 @@ class UserController < ApplicationController
     if @new_friend
       if request.post?
         friend = Friend.new
-        friend.user_id = current_user.id
-        friend.friend_user_id = @new_friend.id
+        friend.befriender = current_user
+        friend.befriendee = @new_friend
         if current_user.is_friends_with?(@new_friend)
           flash[:warning] = t ""user.make_friend.already_a_friend"", :name => @new_friend.display_name
         elsif friend.save","        redirect_to :controller => ""site"", :action => ""index""
      end
    elsif params.key?(:auth_provider) && params.key?(:auth_uid)
      self.current_user = User.new(:email => params[:email],
                                   :email_confirmation => params[:email],
                                   :display_name => params[:nickname],
                                   :auth_provider => params[:auth_provider],
                                   :auth_uid => params[:auth_uid])

      flash.now[:notice] = render_to_string :partial => ""auth_association""
    else
      check_signup_allowed

      self.current_user = User.new
    end
  end

  def create
    self.current_user = User.new(user_params)

    if check_signup_allowed(current_user.email)
      session[:referer] = params[:referer]

      current_user.status = ""pending""

      if current_user.auth_provider.present? && current_user.pass_crypt.empty?
        # We are creating an account with external authentication and
        # no password was specified so create a random one
        current_user.pass_crypt = SecureRandom.base64(16)
        current_user.pass_crypt_confirmation = current_user.pass_crypt
      end

      if current_user.invalid?
        # Something is wrong with a new user, so rerender the form
        render :action => ""new""
      elsif current_user.auth_provider.present?
        # Verify external authenticator before moving on
        session[:new_user] = current_user
        redirect_to auth_url(current_user.auth_provider, current_user.auth_uid)
      else
        # Save the user record
        session[:new_user] = current_user
        redirect_to :action => :terms
      end
    end
  end

  def login
    session[:referer] = params[:referer] if params[:referer]

    if params[:username].present? && params[:password].present?
      session[:remember_me] ||= params[:remember_me]
      password_authentication(params[:username], params[:password])
    elsif params[:openid_url].present?
      session[:remember_me] ||= params[:remember_me_openid]
      redirect_to auth_url(""openid"", params[:openid_url], params[:referer])
    end
  end

  def logout
    @title = t ""user.logout.title""

    if params[:session] == session.id
      if session[:token]
        token = UserToken.find_by(:token => session[:token])
        token.destroy if token
        session.delete(:token)
      end
      session.delete(:user)
      session_expires_automatically
      if params[:referer]
        redirect_to params[:referer]
      else
        redirect_to :controller => ""site"", :action => ""index""
      end
    end
  end

  def confirm
    if request.post?
      token = UserToken.find_by(:token => params[:confirm_string])
      if token && token.user.active?
        flash[:error] = t(""user.confirm.already active"")
        redirect_to :action => ""login""
      elsif !token || token.expired?
        flash[:error] = t(""user.confirm.unknown token"")
        redirect_to :action => ""confirm""
      else
        user = token.user
        user.status = ""active""
        user.email_valid = true
        flash[:notice] = gravatar_status_message(user) if gravatar_enable(user)
        user.save!
        referer = token.referer
        token.destroy

        if session[:token]
          token = UserToken.find_by(:token => session[:token])
          session.delete(:token)
        else
          token = nil
        end

        if token.nil? || token.user != user
          flash[:notice] = t(""user.confirm.success"")
          redirect_to :action => :login, :referer => referer
        else
          token.destroy

          session[:user] = user.id

          redirect_to referer || welcome_path
        end
      end
    else
      user = User.find_by(:display_name => params[:display_name])

      redirect_to root_path if user.nil? || user.active?
    end
  end

  def confirm_resend
    user = User.find_by(:display_name => params[:display_name])
    token = UserToken.find_by(:token => session[:token])

    if user.nil? || token.nil? || token.user != user
      flash[:error] = t ""user.confirm_resend.failure"", :name => params[:display_name]
    else
      Notifier.signup_confirm(user, user.tokens.create).deliver_now
      flash[:notice] = t(""user.confirm_resend.success"", :email => user.email, :sender => SUPPORT_EMAIL).html_safe
    end

    redirect_to :action => ""login""
  end

  def confirm_email
    if request.post?
      token = UserToken.find_by(:token => params[:confirm_string])
      if token && token.user.new_email?
        self.current_user = token.user
        current_user.email = current_user.new_email
        current_user.new_email = nil
        current_user.email_valid = true
        gravatar_enabled = gravatar_enable(current_user)
        if current_user.save
          flash[:notice] = if gravatar_enabled
                             t(""user.confirm_email.success"") + "" "" + gravatar_status_message(current_user)
                           else
                             t(""user.confirm_email.success"")
                           end
        else
          flash[:errors] = current_user.errors
        end
        token.destroy
        session[:user] = current_user.id
        redirect_to :action => ""account"", :display_name => current_user.display_name
      elsif token
        flash[:error] = t ""user.confirm_email.failure""
        redirect_to :action => ""account"", :display_name => token.user.display_name
      else
        flash[:error] = t ""user.confirm_email.unknown_token""
      end
    end
  end

  def api_read
    if @this_user.visible?
      render :action => :api_read, :content_type => ""text/xml""
    else
      head :gone
    end
  end

  def api_details
    @this_user = current_user
    render :action => :api_read, :content_type => ""text/xml""
  end

  def api_gpx_files
    doc = OSM::API.new.get_xml_doc
    current_user.traces.reload.each do |trace|
      doc.root << trace.to_xml_node
    end
    render :xml => doc.to_s
  end

  def view
    @this_user = User.find_by(:display_name => params[:display_name])

    if @this_user &&
       (@this_user.visible? || (current_user && current_user.administrator?))
      @title = @this_user.display_name
    else
      render_unknown_user params[:display_name]
    end
  end

  def make_friend
    @new_friend = User.find_by(:display_name => params[:display_name])

        friend.befriender = current_user
        friend.befriendee = @new_friend
    if @new_friend
      if request.post?
        friend = Friend.new
        friend.user_id = current_user.id
        friend.friend_user_id = @new_friend.id
        if current_user.is_friends_with?(@new_friend)
          flash[:warning] = t ""user.make_friend.already_a_friend"", :name => @new_friend.display_name
        elsif friend.save
          flash[:notice] = t ""user.make_friend.success"", :name => @new_friend.display_name
          Notifier.friend_notification(friend).deliver_now
        else
          friend.add_error(t(""user.make_friend.failed"", :name => @new_friend.display_name))
        end

        if params[:referer]
          redirect_to params[:referer]
        else
          redirect_to :action => ""view""
        end
      end
    else
      render_unknown_user params[:display_name]
    end
  end

  def remove_friend
    @friend = User.find_by(:display_name => params[:display_name])

    if @friend
      if request.post?
        if current_user.is_friends_with?(@friend)
          Friend.where(:user_id => current_user.id, :friend_user_id => @friend.id).delete_all
          flash[:notice] = t ""user.remove_friend.success"", :name => @friend.display_name
        else
          flash[:error] = t ""user.remove_friend.not_a_friend"", :name => @friend.display_name
        end

        if params[:referer]
          redirect_to params[:referer]
        else
          redirect_to :action => ""view""
        end
      end
    else
      render_unknown_user params[:display_name]
    end
  end

  ##
  # sets a user's status
  def set_status
    @this_user.status = params[:status]
    @this_user.save
    redirect_to :action => ""view"", :display_name => params[:display_name]
  end

  ##
  # delete a user, marking them as deleted and removing personal data
  def delete
    @this_user.delete
    redirect_to :action => ""view"", :display_name => params[:display_name]
  end

  ##
  # display a list of users matching specified criteria
  def list
    if request.post?
      ids = params[:user].keys.collect(&:to_i)

      User.where(:id => ids).update_all(:status => ""confirmed"") if params[:confirm]
      User.where(:id => ids).update_all(:status => ""deleted"") if params[:hide]

      redirect_to url_for(:status => params[:status], :ip => params[:ip], :page => params[:page])
    else
      @params = params.permit(:status, :ip)

      conditions = {}
      conditions[:status] = @params[:status] if @params[:status]
      conditions[:creation_ip] = @params[:ip] if @params[:ip]

      @user_pages, @users = paginate(:users,
                                     :conditions => conditions,
                                     :order => :id,
                                     :per_page => 50)
    end
  end

  ##
  # omniauth success callback
  def auth_success
    auth_info = request.env[""omniauth.auth""]

    provider = auth_info[:provider]
    uid = auth_info[:uid]
    name = auth_info[:info][:name]
    email = auth_info[:info][:email]

    case provider
    when ""openid""
      email_verified = uid.match(%r{https://www.google.com/accounts/o8/id?(.*)}) ||
                       uid.match(%r{https://me.yahoo.com/(.*)})
    when ""google"", ""facebook""
      email_verified = true
    else
      email_verified = false
    end

    if settings = session.delete(:new_user_settings)
      current_user.auth_provider = provider
      current_user.auth_uid = uid

      update_user(current_user, settings)

      session[:user_errors] = current_user.errors.as_json

      redirect_to :action => ""account"", :display_name => current_user.display_name
    elsif session[:new_user]
      session[:new_user].auth_provider = provider
      session[:new_user].auth_uid = uid

      if email_verified && email == session[:new_user].email
        session[:new_user].status = ""active""
      end

      redirect_to :action => ""terms""
    else
      user = User.find_by(:auth_provider => provider, :auth_uid => uid)

      if user.nil? && provider == ""google""
        openid_url = auth_info[:extra][:id_info][""openid_id""]
        user = User.find_by(:auth_provider => ""openid"", :auth_uid => openid_url) if openid_url
        user.update(:auth_provider => provider, :auth_uid => uid) if user
      end

      if user
        case user.status
        when ""pending"" then
          unconfirmed_login(user)
        when ""active"", ""confirmed"" then
          successful_login(user, request.env[""omniauth.params""][""referer""])
        when ""suspended"" then
          failed_login t(""user.login.account is suspended"", :webmaster => ""mailto:#{SUPPORT_EMAIL}"").html_safe
        else
          failed_login t(""user.login.auth failure"")
        end
      else
        redirect_to :action => ""new"", :nickname => name, :email => email,
                    :auth_provider => provider, :auth_uid => uid
      end
    end
  end

  ##
  # omniauth failure callback
  def auth_failure
    flash[:error] = t(""user.auth_failure."" + params[:message])
    redirect_to params[:origin] || login_url
  end

  private

  ##
  # handle password authentication
  def password_authentication(username, password)
    if user = User.authenticate(:username => username, :password => password)
      successful_login(user)
    elsif user = User.authenticate(:username => username, :password => password, :pending => true)
      unconfirmed_login(user)
    elsif User.authenticate(:username => username, :password => password, :suspended => true)
      failed_login t(""user.login.account is suspended"", :webmaster => ""mailto:#{SUPPORT_EMAIL}"").html_safe, username
    else
      failed_login t(""user.login.auth failure""), username
    end
  end

  ##
  # return the URL to use for authentication
  def auth_url(provider, uid, referer = nil)
    params = { :provider => provider }

    params[:openid_url] = openid_expand_url(uid) if provider == ""openid""

    if referer.nil?
      params[:origin] = request.path
    else
      params[:origin] = request.path + ""?referer="" + CGI.escape(referer)
      params[:referer] = referer
    end

    auth_path(params)
  end

  ##
  # special case some common OpenID providers by applying heuristics to
  # try and come up with the correct URL based on what the user entered
  def openid_expand_url(openid_url)
    if openid_url.nil?
      nil
    elsif openid_url.match(%r{(.*)gmail.com(/?)$}) || openid_url.match(%r{(.*)googlemail.com(/?)$})
      # Special case gmail.com as it is potentially a popular OpenID",Imo here the old names were easier to understand.,"The suggested code changes appear to be related to user authentication and management in a web application. The code handles various actions such as logging in, registering, confirming accounts, and managing friendships. Here's a concise review of the suggested code changes:

### Key Changes:
1. **User Authentication:**
   - **Omniauth Success Callback (`auth_success`):** Handles successful authentication using OAuth providers like Google, Facebook, and OpenID. It updates user details and redirects to the appropriate page.
   - **Omniauth Failure Callback (`auth_failure`):** Handles authentication failures and redirects to the login page.

2. **Password Authentication (`password_authentication`):** Handles traditional username/password authentication and checks for pending or suspended accounts.

3. **Authentication URL Construction (`auth_url`):** Constructs the URL for authentication requests, handling OpenID URLs and referer parameters.

4. **OpenID URL Expansion (`openid_expand_url`):** Applies heuristics to expand OpenID URLs for common providers like Gmail.

5. **User Management:**
   - **Create User (`create`):** Handles user creation with optional external authentication and password generation.
   - **Login (`login`):** Handles both traditional and OpenID-based login processes.
   - **Logout (`logout`):** Handles user logout, clearing session data, and redirecting to the appropriate page.

6. **Account Management:**
   - **Set Status (`set_status`):** Allows setting a user's status.
   - **Delete User (`delete`):** Marks a user as deleted and removes their personal data.
   - **List Users (`list`):** Displays a list of users based on specified criteria.

7. **Confirmation and Resend (`confirm`, `confirm_resend`):** Handles account confirmation and resending confirmation emails.

8. **Email Confirmation (`confirm_email`):** Allows users to confirm their email addresses.

9. **API Endpoints (`api_read`, `api_details`, `api_gpx_files`):** Provide XML data for API access.

10. **View User (`view`):** Displays user information, allowing administrators to view any user.

11. **Friend Management (`make_friend`, `remove_friend`):** Allows users to manage their friendships.

### Overall Review:
- The code is well-structured and follows a consistent pattern for handling user-related operations.
- It includes error handling and user feedback through flash messages.
- The use of sessions and cookies ensures that user state is maintained across requests.
-"
99,"@@ -15,7 +15,7 @@ func newNodeChain(api *nodeAPI) *nodeChain {
 }
 
 func (api *nodeChain) Head() ([]*cid.Cid, error) {
-	ts := api.api.node.ChainMgr.GetHeaviestTipSet()
+	ts := api.api.node.ChainReader.Head()
 	if len(ts) == 0 {
 		return nil, ErrHeaviestTipSetNotFound
 	}","package impl

import (
	""context""

	""gx/ipfs/QmZFbDTY9jfSBms2MchvYM9oYRbAF19K7Pby47yDBfpPrb/go-cid""
)

type nodeChain struct {
	api *nodeAPI
}

func newNodeChain(api *nodeAPI) *nodeChain {
	return &nodeChain{api: api}
	ts := api.api.node.ChainReader.Head()
}

func (api *nodeChain) Head() ([]*cid.Cid, error) {
	ts := api.api.node.ChainMgr.GetHeaviestTipSet()
	if len(ts) == 0 {
		return nil, ErrHeaviestTipSetNotFound
	}
	tsSlice := ts.ToSlice()
	out := make([]*cid.Cid, len(tsSlice))
	for i, b := range tsSlice {
		out[i] = b.Cid()
	}

	return out, nil
}

func (api *nodeChain) Ls(ctx context.Context) <-chan interface{} {
	return api.api.node.ChainMgr.BlockHistory(ctx)
}
","Good lord, law of demeter out the window. No action required but wow, four levels deep, that's a rarity!","The suggested code change introduces a new `nodeChain` struct that encapsulates the functionality for interacting with the IPFS node's blockchain. The `Head` method retrieves the heaviest tip set from the node's chain manager and converts it into a slice of CIDs. The `Ls` method returns a channel that emits block history events from the node's chain manager.

### Review:

1. **Struct Definition**: 
   - The `nodeChain` struct is defined with a pointer to a `nodeAPI`, which suggests that this struct is intended to be used as a wrapper around the `nodeAPI`.

2. **Constructor**:
   - The `newNodeChain` function initializes a new `nodeChain` instance by passing the provided `nodeAPI`. This is a good practice for creating instances of structs.

3. **Head Method**:
   - The `Head` method retrieves the heaviest tip set using `api.api.node.ChainMgr.GetHeaviestTipSet()`.
   - It checks if the tip set is empty and returns an error if so.
   - It converts the tip set to a slice of blocks and then extracts the CIDs from these blocks.
   - The method returns the slice of CIDs and any potential errors.

4. **Ls Method**:
   - The `Ls` method returns a channel that emits block history events from the node's chain manager. This is useful for monitoring changes in the blockchain.

### Potential Improvements:

- **Error Handling**: Ensure that all error paths are handled appropriately, especially when dealing with network requests or data processing.
- **Documentation**: Add comments to the methods to explain their purpose and usage.
- **Type Safety**: Consider using type aliases or interfaces to improve type safety and readability.
- **Performance**: Optimize the conversion of tip sets to slices and CIDs if performance becomes a concern.

Overall, the code change provides a structured way to interact with the IPFS node's blockchain, making it easier to manage and monitor the blockchain state."
